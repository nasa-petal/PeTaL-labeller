# -*- coding: utf-8 -*-
"""auto-labeler-prototype.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WXv498bhXWMhllGpWI9LEPvKUyGlaMfu
"""

!pip install transformers
!pip install tensorboardX
!pip install wikipedia
!pip install swifter

import torch
import tensorflow as tf
import pandas as pd
import wikipedia 
import swifter
import numpy as np

"""## GPU Detection"""

# GPU detection 

# Get GPU device name
device_name = tf.test.gpu_device_name()

if device_name == '/device:GPU:0':
    print('Found GPU at: {}'.format(device_name))
else:
    raise SystemError('GPU device not found')

# If there is a GPU available
if torch.cuda.is_available():    

    # Tell PyTorch to use GPU
    device = torch.device("cuda")

    print('There are %d GPU(s) available.' % torch.cuda.device_count())

    print('We will use the GPU:', torch.cuda.get_device_name(0))

# If not
else:
    print('No GPU available, using the CPU instead.')
    device = torch.device("cpu")

"""## Import, Parse, and Store Data"""

#Creating PyDrive instance to load in data from PeTaL shared drive, follow the steps to authenticate
!pip install -U -q PyDrive 
  
from pydrive.auth import GoogleAuth 
from pydrive.drive import GoogleDrive 
from google.colab import auth 
from oauth2client.client import GoogleCredentials 
  
  
# Authenticate and create the PyDrive client. 
auth.authenticate_user() 
gauth = GoogleAuth() 
gauth.credentials = GoogleCredentials.get_application_default() 
drive = GoogleDrive(gauth)

#this is the un-parsed articles
# link = 'https://drive.google.com/file/d/1iIZgKs1swHHJuumCU5xyW8tXSAnKAg18/view?usp=sharing'
# id = link.split("/")[-2] 
  
# downloaded = drive.CreateFile({'id':id})  
# downloaded.GetContentFile('articles.csv')   
#df = pd.read_csv('articles.csv')

#'https://petscan.wmflabs.org/' link to pull wikipedia articles and their page ID's

#Scraping article content by ID
#def wiki_content(row):
#  id = row['pageid']
#  try:
#    content = wikipedia.page(pageid=id).content
#  except:
#    content = 'error'
#  return content
#
#df['Content'] = df.swifter.apply(wiki_content, axis=1)

#Scraping article summary by ID

#def wiki_summary(row):
#  id = row['pageid']
#  try:
#    summary = wikipedia.page(pageid=id).summary
#  except:
#    summary = 'error'
#  return summary
#
#df['Summary'] = df.swifter.apply(wiki_summary, axis=1)

#Saving parsed articles as csv, can be accessed in the "Files" folder on the left, then download if you want
#df.to_csv('parsed_articles.csv')

#Google drive link to the parsed articles
#link = 'https://drive.google.com/file/d/1XRWsEsNUHjWOjPavwrfuUpaq3DwGGE4D/view?usp=sharing'
#id = link.split("/")[-2] 
# 
#downloaded = drive.CreateFile({'id':id})  
#downloaded.GetContentFile('parsed_articles.csv') 
#df = pd.read_csv('parsed_articles.csv')
#
#df = df[(df['Content'] != 'error') & df['Content'].notnull()]
#
#Df 'Content' column into list
#docs = list(df['Content'].values)

#Labels
#
#labels = ['Maintain homeostasis', 'Protect from temperature']

#df['Content'].value_counts().to_frame()

labeled_df_link = 'https://drive.google.com/file/d/1MJDIPe1C0dFHIPWu0w18IEJhVk4Xbk2x/view?usp=sharing'
#'https://drive.google.com/file/d/1OZnAk64SPXfnaEzQFfhDJd6AX3dntIy9/view?usp=sharing'
labeled_id = labeled_df_link.split("/")[-2]
labeled_downloaded = drive.CreateFile({'id':labeled_id})  
labeled_downloaded.GetContentFile('single_label.csv') 
#'Biological-Strategies-Export-2020-October-01-1849 (1).csv'
labeled_df = pd.read_csv('single_label.csv')

new_labeled_df_link = 'https://drive.google.com/file/d/1tKxbJeMlJU_Dh62Xqgqdi7ua-AQRFEM9/view'
new_labeled_id = new_labeled_df_link.split("/")[-2]
new_labeled_downloaded = drive.CreateFile({'id':new_labeled_id})  
new_labeled_downloaded.GetContentFile('labeled_abstracts_for_ML.csv') 
new_labeled_df = pd.read_csv('labeled_abstracts_for_ML.csv')

labeled_df = labeled_df[['id', 'Title', 'Living Systems', 'Sources_source_link', 'Functions', 'Wikipedia', 'pdf_links', 'single_label']]
labeled_df = labeled_df[labeled_df['Functions'].notnull( )]
labeled_df = labeled_df[labeled_df['Sources_source_link'].notnull()]

new_labeled_df = new_labeled_df[['title', 'abstract', 'labels', 'doi']]
#new_labeled_df = new_labeled_df[labeled_df['labels'].notnull( )]
#new_labeled_df = new_labeled_df[labeled_df['abstracts'].notnull()]

#import urllib.request
#!pip install pdfminer
#from pdfminer.converter import TextConverter
#from pdfminer.layout import LAParams
#from pdfminer.pdfdocument import PDFDocument
#from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter
#from pdfminer.pdfpage import PDFPage
#from pdfminer.pdfparser import PDFParser
#from io import StringIO

#convert pdf into text corpus
#def convert_pdf_to_string(file_path):
#  output_string = StringIO()
#  with open(file_path, 'rb') as in_file:
#    parser = PDFParser(in_file)
#    doc = PDFDocument(parser)
#    rsrcmgr = PDFResourceManager()
#    device = TextConverter(rsrcmgr, output_string, laparams=LAParams())
#    interpreter = PDFPageInterpreter(rsrcmgr, device)
#    for page in PDFPage.create_pages(doc):
#      interpreter.process_page(page)
#
#  return (output_string.getvalue())
#
#Parsing into text
#def parse_text(row):
#  link = row['Sources_source_link']
#  try:
#      response = urllib.request.urlopen(link)
#      file = open('doc.pdf', 'wb')
#      file.write(response.read())
#      file.close()
#      corpus = convert_pdf_to_string('doc.pdf')
#  except:
#      corpus = 'Web error occurred'
#  
#  return corpus

#pdf_links = labeled_df[labeled_df['Sources_source_link'].str.endswith('.pdf')]
#pdf_links['Text'] = pdf_links.apply(parse_text, axis=1)

# pdf_links
labeled_df

new_labeled_df

# All labels in dataset sorted by frequency
labeled_df['single_label'] = labeled_df.apply(lambda x: x['single_label'].split('|')[0], axis=1)
labeled_df['single_label'].value_counts().index

# All labels in NEW dataset sorted by frequency
new_labeled_df['labels'] = new_labeled_df.apply(lambda x: x['labels'].split('|')[0], axis=1)
new_labeled_df['labels'].value_counts().index

import re
import string

labels = []
docs = []
labels_test = []
docs_test = []
labels_dict = ['Capture, absorb, or filter organisms', 'Capture, absorb, or filter chemical entities', 'Protect from microbe', 'Move in/on liquids', 'Attach temporarily']

single_label = labeled_df["single_label"].tolist()
wikipedia = labeled_df["Wikipedia"].tolist()
title = labeled_df["Title"].tolist()
living_systems = labeled_df["Living Systems"].tolist()
for i in range(len(title)):
  if i < len(title) - 73: #310, 36
    if single_label[i] == 'Capture, absorb, or filter organisms':
      docs.append(wikipedia[i])
      labels.append(labels_dict.index(single_label[i]))
    if single_label[i] == 'Capture, absorb, or filter chemical entities':
      docs.append(wikipedia[i])
      labels.append(labels_dict.index(single_label[i]))
    if single_label[i] == 'Protect from microbe':
      docs.append(wikipedia[i])
      labels.append(labels_dict.index(single_label[i]))
    if single_label[i] == 'Move in/on liquids':
      docs.append(wikipedia[i])
      labels.append(labels_dict.index(single_label[i]))
    if single_label[i] == 'Attach temporarily':
      docs.append(wikipedia[i])
      labels.append(labels_dict.index(single_label[i]))
  else:
    if single_label[i] == 'Capture, absorb, or filter organisms':
      docs_test.append(wikipedia[i])
      labels_test.append(labels_dict.index(single_label[i]))
    if single_label[i] == 'Capture, absorb, or filter chemical entities':
      docs_test.append(wikipedia[i])
      labels_test.append(labels_dict.index(single_label[i]))
    if single_label[i] == 'Protect from microbe':
      docs_test.append(wikipedia[i])
      labels_test.append(labels_dict.index(single_label[i]))
    if single_label[i] == 'Move in/on liquids':
      docs_test.append(wikipedia[i])
      labels_test.append(labels_dict.index(single_label[i]))
    if single_label[i] == 'Attach temporarily':
      docs_test.append(wikipedia[i])
      labels_test.append(labels_dict.index(single_label[i]))

print ("Number of training labels: {:}".format(len(labels)))
print ("Number of training docs: {:}".format(len(docs)))
print ("Number of test labels: {:}".format(len(labels_test)))
print ("Number of test docs: {:}".format(len(docs_test)))
# print(labels)
# print(docs)
# print(labels_test)
# print(docs_test)

#import re
#import string
#
#labels = []
#docs = []
#labels_test = []
#docs_test = []
#labels_dict = ['Protect from harm', 'Move', 'Process information', 'Attach', 'Maintain structural integrity']
#
#title = new_labeled_df["title"].tolist()
#abstract = new_labeled_df["abstract"].tolist()
#labels = new_labeled_df["labels"].tolist()
#doi = new_labeled_df["doi"].tolist()
#for i in range(len(title)):
#  if i < len(title) - 73: #310, 36
#    if labels[i] == labels_dict[0]:
#      docs.append(abstract[i])
#      labels.append(labels_dict.index(labels[i]))
#    if labels[i] == labels_dict[1]:
#      docs.append(abstract[i])
#      labels.append(labels_dict.index(labels[i]))
#    if labels[i] == labels_dict[2]:
#      docs.append(abstract[i])
#      labels.append(labels_dict.index(labels[i]))
#    if labels[i] == labels_dict[3]:
#      docs.append(abstract[i])
#      labels.append(labels_dict.index(labels[i]))
#    if labels[i] == labels_dict[4]:
#      docs.append(abstract[i])
#      labels.append(labels_dict.index(labels[i]))
#  else:
#    if labels[i] == labels_dict[0]:
#      docs_test.append(abstract[i])
#      labels_test.append(labels_dict.index(labels[i]))
#    if labels[i] == labels_dict[1]:
#      docs_test.append(abstract[i])
#      labels_test.append(labels_dict.index(labels[i]))
#    if labels[i] == labels_dict[2]:
#      docs_test.append(abstract[i])
#      labels_test.append(labels_dict.index(labels[i]))
#    if labels[i] == labels_dict[3]:
#      docs_test.append(abstract[i])
#      labels_test.append(labels_dict.index(labels[i]))
#    if labels[i] == labels_dict[4]:
#      docs_test.append(abstract[i])
#      labels_test.append(labels_dict.index(labels[i]))
#
#print ("Number of training labels: {:}".format(len(labels)))
#print ("Number of training docs: {:}".format(len(docs)))
#print ("Number of test labels: {:}".format(len(labels_test)))
#print ("Number of test docs: {:}".format(len(docs_test)))
#print(labels)
#print(docs)
#print(labels_test)
#print(docs_test)

"""## Helper Functions"""

# Calculate accuracy of predictions vs labels
def flat_accuracy(preds, labels):
    pred_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()
    return np.sum(pred_flat == labels_flat) / len(labels_flat)

# Format elapsed times as hh:mm:ss
import time
import datetime

def format_time(elapsed):
    elapsed_rounded = int(round((elapsed)))
    return str(datetime.timedelta(seconds=elapsed_rounded))

"""## BERT Tokenizer"""

from transformers import BertTokenizer, BertModel, BertConfig

# Load BERT tokenizer
print('Loading BERT tokenizer')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)

# Make sure it is tokenizing correctly:

# Print original articles
print(' Original: ', docs[0])

# Print a doc split into tokens
print('Tokenized: ', tokenizer.tokenize(docs[0]))

# Print docs as mapped to ids
print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(docs[0])))

max_len = 0

for d in docs:
    # tokenize text and add `[CLS]` and `[SEP]` tokens
    input_ids = tokenizer.encode(d, add_special_tokens=True)
    max_len = max(max_len, len(input_ids))

print('Max length: ', max_len)

# Finishing tokenizing all docs and map tokens to thier word IDs
input_ids = []
attention_masks = []

for d in docs:

    encoded_dict = tokenizer.encode_plus(
                        d,                      # Docs to encode.
                        truncation=True,
                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'
                        max_length = 256,           # Pad & truncate all docs
                        pad_to_max_length = True,
                        return_attention_mask = True,   # Attention masks
                        return_tensors = 'pt',     # Return pytorch tensors.
                   )
    
    input_ids.append(encoded_dict['input_ids'])
    
    attention_masks.append(encoded_dict['attention_mask'])

# Convert the lists into tensors.
input_ids = torch.cat(input_ids, dim=0)
attention_masks = torch.cat(attention_masks, dim=0)


labels = torch.tensor(labels)

# Print sentence 0, now as a list of IDs.
# print('Original: ', docs[0])
# print('Token IDs:', input_ids[0])
# print('Reverse:', tokenizer.convert_ids_to_tokens(input_ids[0]))

# Split up training & testing/validation

from torch.utils.data import TensorDataset, random_split

dataset = TensorDataset(input_ids, attention_masks, labels)

# 80:20 split

# Number of docs to include per set
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size

train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

print('{:>5,} training docs'.format(train_size))
print('{:>5,} validation docs'.format(val_size))

# Iterator using torch DataLoader class so that entire dataset doesn't need to be stored in memory

from torch.utils.data import DataLoader, RandomSampler, SequentialSampler

# batch size can be 16 or 32
batch_size = 32

# Sample in random order when training
train_dataloader = DataLoader(
            train_dataset,  
            sampler = RandomSampler(train_dataset), 
            batch_size = batch_size 
        )

# Sample sequentially for validation
validation_dataloader = DataLoader(
            val_dataset, 
            sampler = SequentialSampler(val_dataset), 
            batch_size = batch_size # Evaluate with this batch size.
        )

"""## Training the Classification Model w/ Sequence Classification
  (fine-tune BERT)

  [HuggingFace documentation](https://huggingface.co/transformers/v2.2.0/model_doc/bert.html)
"""

from transformers import BertForSequenceClassification, AdamW, BertConfig

# BertForSequenceClassification -> BERT model w/ added classification layer 
model = BertForSequenceClassification.from_pretrained(
    "bert-base-uncased", # 12-layer model, uncased vocab
    num_labels = 5, # Number of labels 
    output_attentions = False, 
    output_hidden_states = False, 
)

# this needs to be run on GPU
model.cuda()

"""## Optimizer for our hypermarameters / Learning Rate Scheduler
AdamW

Possible hyperparamters: 
* batch size: 16, 32
* learning rate: 5e-5, 3e-5, 2e-5
* number of epochs: 2, 3, 4
"""

# Exeprimenting w/ different parameters
optimizer = AdamW(model.parameters(),
                  lr = 2e-5, 
                  eps = 1e-8 # epsilon prevents division by 0??
                )

from transformers import get_linear_schedule_with_warmup

# Training epochs should be betw 2- 4 (reduce if overfitting)
epochs = 4

total_steps = len(train_dataloader) * epochs

# LR scheduler
scheduler = get_linear_schedule_with_warmup(optimizer, 
                                            num_warmup_steps = 0, 
                                            num_training_steps = total_steps)

"""## Training Loop"""

import random
# based on the `run_glue.py` script here:
# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128
seed_val = 42
random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)
loss_values = []

for epoch_i in range(0, epochs):

    print("")
    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))
    print('Training...')
    # Measure how long the training epoch takes.
    t0 = time.time()
    # Reset the total loss for this epoch.
    total_loss = 0

    model.train()
    # For each batch of training data...
    for step, batch in enumerate(train_dataloader):
        # Progress update every 40 batches.
        if step % 40 == 0 and not step == 0:
            # Calculate elapsed time in minutes.
            elapsed = format_time(time.time() - t0)
            
            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))
        # Unpack this training batch from our dataloader. 

        # `batch` contains three pytorch tensors:
        #   [0]: input ids 
        #   [1]: attention masks
        #   [2]: labels 
        b_input_ids = batch[0].to(device)
        b_input_mask = batch[1].to(device)
        b_labels = batch[2].to(device)

        model.zero_grad()        

        outputs = model(b_input_ids, 
                    token_type_ids=None, 
                    attention_mask=b_input_mask, 
                    labels=b_labels)
        

        loss = outputs[0]

        total_loss += loss.item()
        loss.backward()

        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

        optimizer.step()

        scheduler.step()
    avg_train_loss = total_loss / len(train_dataloader)            
    
    loss_values.append(avg_train_loss)
    print("")
    print("  Average training loss: {0:.2f}".format(avg_train_loss))
    print("  Training epcoh took: {:}".format(format_time(time.time() - t0)))
        

    print("")
    print("Running Validation...")
    t0 = time.time()

    model.eval()
    eval_loss, eval_accuracy = 0, 0
    nb_eval_steps, nb_eval_examples = 0, 0
    for batch in validation_dataloader:
        
        batch = tuple(t.to(device) for t in batch)
        
        b_input_ids, b_input_mask, b_labels = batch

        with torch.no_grad():        

            outputs = model(b_input_ids, 
                            token_type_ids=None, 
                            attention_mask=b_input_mask)
        
        logits = outputs[0]
        logits = logits.detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()
        
        tmp_eval_accuracy = flat_accuracy(logits, label_ids)
        
        eval_accuracy += tmp_eval_accuracy
        nb_eval_steps += 1
    # Report the final accuracy for this validation run.
    print("  Accuracy: {0:.2f}".format(eval_accuracy/nb_eval_steps))
    print("  Validation took: {:}".format(format_time(time.time() - t0)))
print("")
print("Training complete!")

# Display metrics of training process in a dataframe

import pandas as pd

pd.set_option('precision', 2)

df_vals = pd.DataFrame(data=training_vals)
df_vals = df_vals.set_index('epoch')

df_vals

input_ids_test = []
attention_masks_test = []
actual_labels_test=[]

for i in range(9):

    encoded_dict = tokenizer.encode_plus(
                        docs_test[i],                      
                        add_special_tokens = True, 
                        max_length = 256,           
                        pad_to_max_length = True,
                        return_attention_mask = True,   
                        return_tensors = 'pt',     
                   )
    
   
    input_ids_test.append(encoded_dict['input_ids'])
    

    attention_masks_test.append(encoded_dict['attention_mask'])
    actual_labels_test.append(labels_test[i])

# lists -> tensors
input_ids_test = torch.cat(input_ids_test, dim=0)
attention_masks_test = torch.cat(attention_masks_test, dim=0)
actual_labels_test = torch.tensor(actual_labels_test)

batch_size = 32  

# build DataLoader
prediction_data = TensorDataset(input_ids_test, attention_masks_test, actual_labels_test)
prediction_sampler = SequentialSampler(prediction_data)
prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)

"""## Testing Classification"""

print('Label predictions for {:,} test publications...'.format(len(input_ids_test)))
model.eval()

predictions, actual_labels = [], []

for batch in prediction_dataloader:
  batch = tuple(t.to(device) for t in batch)
  
  b_input_ids, b_input_mask, b_labels = batch
  # save memory and accelerate predictions w/o storing gradients
  with torch.no_grad():
      # forward pass and logit predictions
      outputs = model(b_input_ids, token_type_ids=None, 
                      attention_mask=b_input_mask)
  logits = outputs[0]

  # move logits and labels -> CPU
  logits = logits.detach().cpu().numpy()
  labels_ids_test = b_labels.to('cpu').numpy()
  
  predictions.append(logits)
  actual_labels.append(labels_ids_test)

classification_correct = 0

for i in range(len(predictions)):
  for j in range(len(predictions[i])):
    prediction = np.argmax(predictions[i][j])
    print ('Prediction: ' , prediction , ', actual: ', actual_labels[i][j])
    if prediction == actual_labels[i][j]:
      classification_correct = classification_correct + 1

print ('Classification correctly: ',  classification_correct)

print ('Model accuracy from testing: {0:.2f}'.format(classification_correct / len(input_ids_test)))

#from sagemaker.pytorch import model.PyTorchModel

#pytorch_mode = PyTorchModel ( model_data=model_data,
    
#)

