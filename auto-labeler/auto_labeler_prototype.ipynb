{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "auto-labeler-prototype.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f28036d6270a471fa762b48c609e21ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_839809e4f4ba41cd890169781cb780c0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3063dcb432594ba6bda10d352af55dc4",
              "IPY_MODEL_e51ff2fa70c240419c048710d69dcd27"
            ]
          }
        },
        "839809e4f4ba41cd890169781cb780c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3063dcb432594ba6bda10d352af55dc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_063ca14df7de4f3ebef2144ab8a6e848",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_12be118ec9c340eea8a9cf630c3d0091"
          }
        },
        "e51ff2fa70c240419c048710d69dcd27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_84051212aff5440cbc2ce79d4905e917",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 806kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3dc87de5241348d1a2352b2a97c42e6a"
          }
        },
        "063ca14df7de4f3ebef2144ab8a6e848": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "12be118ec9c340eea8a9cf630c3d0091": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "84051212aff5440cbc2ce79d4905e917": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3dc87de5241348d1a2352b2a97c42e6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ft-7aQbboP7f"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install tensorboardX\n",
        "!pip install wikipedia\n",
        "!pip install swifter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpdAH3eLn9KA"
      },
      "source": [
        "import torch\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import wikipedia \n",
        "import swifter\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pe0Z6A0rNUgf"
      },
      "source": [
        "## GPU Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9vxghfPocM5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a65cebc6-b6e8-4012-911e-3b118b9bd52e"
      },
      "source": [
        "# GPU detection \n",
        "\n",
        "# Get GPU device name\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGAOqbInomgX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f960fdc2-d9e5-404c-9e6e-58abaf6420ef"
      },
      "source": [
        "# If there is a GPU available\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use GPU\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-btVsbT0WETf"
      },
      "source": [
        "## Import, Parse, and Store Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz8CT3qvovyy"
      },
      "source": [
        "#Creating PyDrive instance to load in data from PeTaL shared drive, follow the steps to authenticate\n",
        "!pip install -U -q PyDrive \n",
        "  \n",
        "from pydrive.auth import GoogleAuth \n",
        "from pydrive.drive import GoogleDrive \n",
        "from google.colab import auth \n",
        "from oauth2client.client import GoogleCredentials \n",
        "  \n",
        "  \n",
        "# Authenticate and create the PyDrive client. \n",
        "auth.authenticate_user() \n",
        "gauth = GoogleAuth() \n",
        "gauth.credentials = GoogleCredentials.get_application_default() \n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5MO_gJnov1A"
      },
      "source": [
        "\n",
        "\n",
        "#this is the un-parsed articles\n",
        "# link = 'https://drive.google.com/file/d/1iIZgKs1swHHJuumCU5xyW8tXSAnKAg18/view?usp=sharing'\n",
        "# id = link.split(\"/\")[-2] \n",
        "  \n",
        "# downloaded = drive.CreateFile({'id':id})  \n",
        "# downloaded.GetContentFile('articles.csv')   \n",
        "#df = pd.read_csv('articles.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yveU_TvB7ZkD"
      },
      "source": [
        "#'https://petscan.wmflabs.org/' link to pull wikipedia articles and their page ID's"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCTYZEDsov41"
      },
      "source": [
        "#Scraping article content by ID\n",
        "#def wiki_content(row):\n",
        "#  id = row['pageid']\n",
        "#  try:\n",
        "#    content = wikipedia.page(pageid=id).content\n",
        "#  except:\n",
        "#    content = 'error'\n",
        "#  return content\n",
        "#\n",
        "#df['Content'] = df.swifter.apply(wiki_content, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryadSHM_GdAO"
      },
      "source": [
        "#Scraping article summary by ID\n",
        "\n",
        "#def wiki_summary(row):\n",
        "#  id = row['pageid']\n",
        "#  try:\n",
        "#    summary = wikipedia.page(pageid=id).summary\n",
        "#  except:\n",
        "#    summary = 'error'\n",
        "#  return summary\n",
        "#\n",
        "#df['Summary'] = df.swifter.apply(wiki_summary, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSf2JM6Ub7lI"
      },
      "source": [
        "#Saving parsed articles as csv, can be accessed in the \"Files\" folder on the left, then download if you want\n",
        "#df.to_csv('parsed_articles.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-mVCjGb-nsw"
      },
      "source": [
        "#Google drive link to the parsed articles\n",
        "#link = 'https://drive.google.com/file/d/1XRWsEsNUHjWOjPavwrfuUpaq3DwGGE4D/view?usp=sharing'\n",
        "#id = link.split(\"/\")[-2] \n",
        "# \n",
        "#downloaded = drive.CreateFile({'id':id})  \n",
        "#downloaded.GetContentFile('parsed_articles.csv') \n",
        "#df = pd.read_csv('parsed_articles.csv')\n",
        "#\n",
        "#df = df[(df['Content'] != 'error') & df['Content'].notnull()]\n",
        "#\n",
        "#Df 'Content' column into list\n",
        "#docs = list(df['Content'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXiUqqPwBZfB"
      },
      "source": [
        "#Labels\n",
        "#\n",
        "#labels = ['Maintain homeostasis', 'Protect from temperature']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnZWpxgW07DY"
      },
      "source": [
        "#df['Content'].value_counts().to_frame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ktg_q4es2Oz3"
      },
      "source": [
        "labeled_df_link = 'https://drive.google.com/file/d/1MJDIPe1C0dFHIPWu0w18IEJhVk4Xbk2x/view?usp=sharing'\n",
        "#'https://drive.google.com/file/d/1OZnAk64SPXfnaEzQFfhDJd6AX3dntIy9/view?usp=sharing'\n",
        "labeled_id = labeled_df_link.split(\"/\")[-2]\n",
        "labeled_downloaded = drive.CreateFile({'id':labeled_id})  \n",
        "labeled_downloaded.GetContentFile('single_label.csv') \n",
        "#'Biological-Strategies-Export-2020-October-01-1849 (1).csv'\n",
        "labeled_df = pd.read_csv('single_label.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3ti6Y24z1V1"
      },
      "source": [
        "new_labeled_df_link = 'https://drive.google.com/file/d/1tKxbJeMlJU_Dh62Xqgqdi7ua-AQRFEM9/view'\n",
        "new_labeled_id = new_labeled_df_link.split(\"/\")[-2]\n",
        "new_labeled_downloaded = drive.CreateFile({'id':new_labeled_id})  \n",
        "new_labeled_downloaded.GetContentFile('labeled_abstracts_for_ML.csv') \n",
        "new_labeled_df = pd.read_csv('labeled_abstracts_for_ML.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll5Y5I3C2Ske"
      },
      "source": [
        "labeled_df = labeled_df[['id', 'Title', 'Living Systems', 'Sources_source_link', 'Functions', 'Wikipedia', 'pdf_links', 'single_label']]\n",
        "labeled_df = labeled_df[labeled_df['Functions'].notnull( )]\n",
        "labeled_df = labeled_df[labeled_df['Sources_source_link'].notnull()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHPssYKUz5lP"
      },
      "source": [
        "new_labeled_df = new_labeled_df[['title', 'abstract', 'labels', 'doi']]\n",
        "#new_labeled_df = new_labeled_df[labeled_df['labels'].notnull( )]\n",
        "#new_labeled_df = new_labeled_df[labeled_df['abstracts'].notnull()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3yE-cvH2Upv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e4d4c84d-5ba1-4e5d-e99f-f9a9d27d03d5"
      },
      "source": [
        "#import urllib.request\n",
        "#!pip install pdfminer\n",
        "#from pdfminer.converter import TextConverter\n",
        "#from pdfminer.layout import LAParams\n",
        "#from pdfminer.pdfdocument import PDFDocument\n",
        "#from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
        "#from pdfminer.pdfpage import PDFPage\n",
        "#from pdfminer.pdfparser import PDFParser\n",
        "#from io import StringIO"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pdfminer in /usr/local/lib/python3.6/dist-packages (20191125)\n",
            "Requirement already satisfied: pycryptodome in /usr/local/lib/python3.6/dist-packages (from pdfminer) (3.9.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdR50dTb2Xa1"
      },
      "source": [
        "#convert pdf into text corpus\n",
        "#def convert_pdf_to_string(file_path):\n",
        "#  output_string = StringIO()\n",
        "#  with open(file_path, 'rb') as in_file:\n",
        "#    parser = PDFParser(in_file)\n",
        "#    doc = PDFDocument(parser)\n",
        "#    rsrcmgr = PDFResourceManager()\n",
        "#    device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
        "#    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
        "#    for page in PDFPage.create_pages(doc):\n",
        "#      interpreter.process_page(page)\n",
        "#\n",
        "#  return (output_string.getvalue())\n",
        "#\n",
        "#Parsing into text\n",
        "#def parse_text(row):\n",
        "#  link = row['Sources_source_link']\n",
        "#  try:\n",
        "#      response = urllib.request.urlopen(link)\n",
        "#      file = open('doc.pdf', 'wb')\n",
        "#      file.write(response.read())\n",
        "#      file.close()\n",
        "#      corpus = convert_pdf_to_string('doc.pdf')\n",
        "#  except:\n",
        "#      corpus = 'Web error occurred'\n",
        "#  \n",
        "#  return corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O01roYkt2Zqb"
      },
      "source": [
        "#pdf_links = labeled_df[labeled_df['Sources_source_link'].str.endswith('.pdf')]\n",
        "#pdf_links['Text'] = pdf_links.apply(parse_text, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYydK3S02bsy"
      },
      "source": [
        "# pdf_links\n",
        "labeled_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wI9jUxWo0Te3"
      },
      "source": [
        "new_labeled_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbCGX-XPRZVi"
      },
      "source": [
        "# All labels in dataset sorted by frequency\n",
        "labeled_df['single_label'] = labeled_df.apply(lambda x: x['single_label'].split('|')[0], axis=1)\n",
        "labeled_df['single_label'].value_counts().index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6kRLP-I0bR2"
      },
      "source": [
        "# All labels in NEW dataset sorted by frequency\n",
        "new_labeled_df['labels'] = new_labeled_df.apply(lambda x: x['labels'].split('|')[0], axis=1)\n",
        "new_labeled_df['labels'].value_counts().index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BFsWLy00Xdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3deb9c3-4a26-49e7-9fdd-58a4aa8e46a4"
      },
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "labels = []\n",
        "docs = []\n",
        "labels_test = []\n",
        "docs_test = []\n",
        "labels_dict = ['Capture, absorb, or filter organisms', 'Capture, absorb, or filter chemical entities', 'Protect from microbe', 'Move in/on liquids', 'Attach temporarily']\n",
        "\n",
        "single_label = labeled_df[\"single_label\"].tolist()\n",
        "wikipedia = labeled_df[\"Wikipedia\"].tolist()\n",
        "title = labeled_df[\"Title\"].tolist()\n",
        "living_systems = labeled_df[\"Living Systems\"].tolist()\n",
        "for i in range(len(title)):\n",
        "  if i < len(title) - 73: #310, 36\n",
        "    if single_label[i] == 'Capture, absorb, or filter organisms':\n",
        "      docs.append(wikipedia[i])\n",
        "      labels.append(labels_dict.index(single_label[i]))\n",
        "    if single_label[i] == 'Capture, absorb, or filter chemical entities':\n",
        "      docs.append(wikipedia[i])\n",
        "      labels.append(labels_dict.index(single_label[i]))\n",
        "    if single_label[i] == 'Protect from microbe':\n",
        "      docs.append(wikipedia[i])\n",
        "      labels.append(labels_dict.index(single_label[i]))\n",
        "    if single_label[i] == 'Move in/on liquids':\n",
        "      docs.append(wikipedia[i])\n",
        "      labels.append(labels_dict.index(single_label[i]))\n",
        "    if single_label[i] == 'Attach temporarily':\n",
        "      docs.append(wikipedia[i])\n",
        "      labels.append(labels_dict.index(single_label[i]))\n",
        "  else:\n",
        "    if single_label[i] == 'Capture, absorb, or filter organisms':\n",
        "      docs_test.append(wikipedia[i])\n",
        "      labels_test.append(labels_dict.index(single_label[i]))\n",
        "    if single_label[i] == 'Capture, absorb, or filter chemical entities':\n",
        "      docs_test.append(wikipedia[i])\n",
        "      labels_test.append(labels_dict.index(single_label[i]))\n",
        "    if single_label[i] == 'Protect from microbe':\n",
        "      docs_test.append(wikipedia[i])\n",
        "      labels_test.append(labels_dict.index(single_label[i]))\n",
        "    if single_label[i] == 'Move in/on liquids':\n",
        "      docs_test.append(wikipedia[i])\n",
        "      labels_test.append(labels_dict.index(single_label[i]))\n",
        "    if single_label[i] == 'Attach temporarily':\n",
        "      docs_test.append(wikipedia[i])\n",
        "      labels_test.append(labels_dict.index(single_label[i]))\n",
        "\n",
        "print (\"Number of training labels: {:}\".format(len(labels)))\n",
        "print (\"Number of training docs: {:}\".format(len(docs)))\n",
        "print (\"Number of test labels: {:}\".format(len(labels_test)))\n",
        "print (\"Number of test docs: {:}\".format(len(docs_test)))\n",
        "# print(labels)\n",
        "# print(docs)\n",
        "# print(labels_test)\n",
        "# print(docs_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training labels: 51\n",
            "Number of training docs: 51\n",
            "Number of test labels: 9\n",
            "Number of test docs: 9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zj-k8cZC2ay7"
      },
      "source": [
        "#import re\n",
        "#import string\n",
        "#\n",
        "#labels = []\n",
        "#docs = []\n",
        "#labels_test = []\n",
        "#docs_test = []\n",
        "#labels_dict = ['Protect from harm', 'Move', 'Process information', 'Attach', 'Maintain structural integrity']\n",
        "#\n",
        "#title = new_labeled_df[\"title\"].tolist()\n",
        "#abstract = new_labeled_df[\"abstract\"].tolist()\n",
        "#labels = new_labeled_df[\"labels\"].tolist()\n",
        "#doi = new_labeled_df[\"doi\"].tolist()\n",
        "#for i in range(len(title)):\n",
        "#  if i < len(title) - 73: #310, 36\n",
        "#    if labels[i] == labels_dict[0]:\n",
        "#      docs.append(abstract[i])\n",
        "#      labels.append(labels_dict.index(labels[i]))\n",
        "#    if labels[i] == labels_dict[1]:\n",
        "#      docs.append(abstract[i])\n",
        "#      labels.append(labels_dict.index(labels[i]))\n",
        "#    if labels[i] == labels_dict[2]:\n",
        "#      docs.append(abstract[i])\n",
        "#      labels.append(labels_dict.index(labels[i]))\n",
        "#    if labels[i] == labels_dict[3]:\n",
        "#      docs.append(abstract[i])\n",
        "#      labels.append(labels_dict.index(labels[i]))\n",
        "#    if labels[i] == labels_dict[4]:\n",
        "#      docs.append(abstract[i])\n",
        "#      labels.append(labels_dict.index(labels[i]))\n",
        "#  else:\n",
        "#    if labels[i] == labels_dict[0]:\n",
        "#      docs_test.append(abstract[i])\n",
        "#      labels_test.append(labels_dict.index(labels[i]))\n",
        "#    if labels[i] == labels_dict[1]:\n",
        "#      docs_test.append(abstract[i])\n",
        "#      labels_test.append(labels_dict.index(labels[i]))\n",
        "#    if labels[i] == labels_dict[2]:\n",
        "#      docs_test.append(abstract[i])\n",
        "#      labels_test.append(labels_dict.index(labels[i]))\n",
        "#    if labels[i] == labels_dict[3]:\n",
        "#      docs_test.append(abstract[i])\n",
        "#      labels_test.append(labels_dict.index(labels[i]))\n",
        "#    if labels[i] == labels_dict[4]:\n",
        "#      docs_test.append(abstract[i])\n",
        "#      labels_test.append(labels_dict.index(labels[i]))\n",
        "#\n",
        "#print (\"Number of training labels: {:}\".format(len(labels)))\n",
        "#print (\"Number of training docs: {:}\".format(len(docs)))\n",
        "#print (\"Number of test labels: {:}\".format(len(labels_test)))\n",
        "#print (\"Number of test docs: {:}\".format(len(docs_test)))\n",
        "#print(labels)\n",
        "#print(docs)\n",
        "#print(labels_test)\n",
        "#print(docs_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pX5JoachgKHG"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9X9M8btjgL4h"
      },
      "source": [
        "# Calculate accuracy of predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJ0gJZfGgfbt"
      },
      "source": [
        "# Format elapsed times as hh:mm:ss\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foqKWTvsNdaz"
      },
      "source": [
        "## BERT Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZ21pfQvgtNA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "f28036d6270a471fa762b48c609e21ce",
            "839809e4f4ba41cd890169781cb780c0",
            "3063dcb432594ba6bda10d352af55dc4",
            "e51ff2fa70c240419c048710d69dcd27",
            "063ca14df7de4f3ebef2144ab8a6e848",
            "12be118ec9c340eea8a9cf630c3d0091",
            "84051212aff5440cbc2ce79d4905e917",
            "3dc87de5241348d1a2352b2a97c42e6a"
          ]
        },
        "outputId": "88608bf9-e653-4c79-e20a-5a6e99577c5a"
      },
      "source": [
        "from transformers import BertTokenizer, BertModel, BertConfig\n",
        "\n",
        "# Load BERT tokenizer\n",
        "print('Loading BERT tokenizer')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f28036d6270a471fa762b48c609e21ce",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTUSIIZAhcVT"
      },
      "source": [
        "# Make sure it is tokenizing correctly:\n",
        "\n",
        "# Print original articles\n",
        "print(' Original: ', docs[0])\n",
        "\n",
        "# Print a doc split into tokens\n",
        "print('Tokenized: ', tokenizer.tokenize(docs[0]))\n",
        "\n",
        "# Print docs as mapped to ids\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(docs[0])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9mSOV7GhvdA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07f23c5d-68b5-4043-8098-d1e0310acd4d"
      },
      "source": [
        "max_len = 0\n",
        "\n",
        "for d in docs:\n",
        "    # tokenize text and add `[CLS]` and `[SEP]` tokens\n",
        "    input_ids = tokenizer.encode(d, add_special_tokens=True)\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max length: ', max_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (3334 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Max length:  11537\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgtsq-3s0jyF"
      },
      "source": [
        "# Finishing tokenizing all docs and map tokens to thier word IDs\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for d in docs:\n",
        "\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        d,                      \n",
        "                        truncation=True,\n",
        "                        add_special_tokens = True, \n",
        "                        max_length = 256,           \n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   \n",
        "                        return_tensors = 'pt',     \n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# print('Original: ', docs[0])\n",
        "# print('Token IDs:', input_ids[0])\n",
        "# print('Reverse:', tokenizer.convert_ids_to_tokens(input_ids[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVofTi9U01pb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56ad33ff-bc54-49eb-a46c-714b8e29a747"
      },
      "source": [
        "# Split up training & testing/validation\n",
        "\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# 80:20 split\n",
        "\n",
        "# Number of docs to include per set\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training docs'.format(train_size))\n",
        "print('{:>5,} validation docs'.format(val_size))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   40 training docs\n",
            "   11 validation docs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eo-Gh4q78BJM"
      },
      "source": [
        "# Iterator using torch DataLoader class so that entire dataset doesn't need to be stored in memory\n",
        "\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# batch size can be 16 or 32\n",
        "batch_size = 32\n",
        "\n",
        "# Sample in random order when training\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  \n",
        "            sampler = RandomSampler(train_dataset), \n",
        "            batch_size = batch_size \n",
        "        )\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, \n",
        "            sampler = SequentialSampler(val_dataset), \n",
        "            batch_size = batch_size \n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "za2M5jHMV3LU"
      },
      "source": [
        "## Training the Classification Model w/ Sequence Classification\n",
        "  (fine-tune BERT)\n",
        "\n",
        "  [HuggingFace documentation](https://huggingface.co/transformers/v2.2.0/model_doc/bert.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BMF2gFEV0Y8"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# BertForSequenceClassification -> BERT model w/ added classification layer \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # 12-layer model, uncased vocab\n",
        "    num_labels = 5, # Number of labels \n",
        "    output_attentions = False, \n",
        "    output_hidden_states = False, \n",
        ")\n",
        "model.save_pretrained('results/tokenizer/')\n",
        "\n",
        "# this needs to be run on GPU\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "775rSApKXaHu"
      },
      "source": [
        "## Optimizer for our hypermarameters / Learning Rate Scheduler\n",
        "AdamW\n",
        "\n",
        "Possible hyperparamters: \n",
        "* batch size: 16, 32\n",
        "* learning rate: 5e-5, 3e-5, 2e-5\n",
        "* number of epochs: 2, 3, 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa5RhInLXfIv"
      },
      "source": [
        "# Exeprimenting w/ different parameters\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, \n",
        "                  eps = 1e-8 \n",
        "                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV6rf_-BYLWS"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Training epochs should be betw 2- 4 (reduce if overfitting)\n",
        "epochs = 4\n",
        "\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# LR scheduler\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, \n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAdyiqxGYpg5"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vk9UkHBSYsDS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3745a04-e7d8-4ed8-e004-120a83651e6f"
      },
      "source": [
        "import random\n",
        "# based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "loss_values = []\n",
        "\n",
        "for epoch_i in range(0, epochs):\n",
        "\n",
        "    print(\"\")\n",
        "    print('Epoch {:} / {:} '.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "    t0 = time.time()\n",
        "    total_loss = 0\n",
        "\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # `batch` pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        model.zero_grad()        \n",
        "\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "\n",
        "        loss = outputs[0]\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    loss_values.append(avg_train_loss)\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch time: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "\n",
        "    print(\"\")\n",
        "    print(\" Validation ->\")\n",
        "    t0 = time.time()\n",
        "\n",
        "    model.eval()\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        logits = outputs[0]\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "        nb_eval_steps += 1\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation time: {:}\".format(format_time(time.time() - t0)))\n",
        "print(\"\")\n",
        "print(\"training complete\")\n",
        "print(\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1 / 4 \n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.73\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.09\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Epoch 2 / 4 \n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.54\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.09\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Epoch 3 / 4 \n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.48\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.36\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Epoch 4 / 4 \n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.36\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.45\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "training complete\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-ijNX7XD9-n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "2e19f73c-6d78-4d6a-f3a9-d089f368672b"
      },
      "source": [
        "# display training metrics in pd df\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "pd.set_option('precision', 2)\n",
        "\n",
        "df_vals = pd.DataFrame(data=training_vals)\n",
        "df_vals = df_vals.set_index('epoch')\n",
        "\n",
        "df_vals"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-e103e660451e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'precision'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mdf_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_vals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'training_vals' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MCG2WV5y15i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8df538ca-6f5c-48b0-ceff-f4f8beedb373"
      },
      "source": [
        "input_ids_test = []\n",
        "attention_masks_test = []\n",
        "actual_labels_test=[]\n",
        "\n",
        "for i in range(9):\n",
        "\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        docs_test[i],                      \n",
        "                        add_special_tokens = True, \n",
        "                        max_length = 256,           \n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   \n",
        "                        return_tensors = 'pt',     \n",
        "                   )\n",
        "    \n",
        "   \n",
        "    input_ids_test.append(encoded_dict['input_ids'])\n",
        "    \n",
        "\n",
        "    attention_masks_test.append(encoded_dict['attention_mask'])\n",
        "    actual_labels_test.append(labels_test[i])\n",
        "\n",
        "# lists -> tensors\n",
        "input_ids_test = torch.cat(input_ids_test, dim=0)\n",
        "attention_masks_test = torch.cat(attention_masks_test, dim=0)\n",
        "actual_labels_test = torch.tensor(actual_labels_test)\n",
        "\n",
        "batch_size = 32  \n",
        "\n",
        "# build DataLoader\n",
        "prediction_data = TensorDataset(input_ids_test, attention_masks_test, actual_labels_test)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aspjuWhzC8h"
      },
      "source": [
        "## Testing Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giqwQoNlzHmu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a12ee7b0-9cb3-417a-801d-be5868ca6d32"
      },
      "source": [
        "print('Label predictions for {:,} test publications...'.format(len(input_ids_test)))\n",
        "model.eval()\n",
        "\n",
        "predictions, actual_labels = [], []\n",
        "\n",
        "for batch in prediction_dataloader:\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  # save memory and accelerate predictions w/o storing gradients\n",
        "  with torch.no_grad():\n",
        "      # forward pass and logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # move logits and labels -> CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  labels_ids_test = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  predictions.append(logits)\n",
        "  actual_labels.append(labels_ids_test)\n",
        "\n",
        "classification_correct = 0\n",
        "\n",
        "for i in range(len(predictions)):\n",
        "  for j in range(len(predictions[i])):\n",
        "    prediction = np.argmax(predictions[i][j])\n",
        "    print ('Prediction: ' , prediction , ', actual: ', actual_labels[i][j])\n",
        "    if prediction == actual_labels[i][j]:\n",
        "      classification_correct = classification_correct + 1\n",
        "\n",
        "print ('Classification correctly: ',  classification_correct)\n",
        "\n",
        "print ('Model accuracy from testing: {0:.2f}'.format(classification_correct / len(input_ids_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label predictions for 9 test publications...\n",
            "Prediction:  4 , actual:  0\n",
            "Prediction:  3 , actual:  4\n",
            "Prediction:  1 , actual:  0\n",
            "Prediction:  3 , actual:  0\n",
            "Prediction:  4 , actual:  4\n",
            "Prediction:  1 , actual:  3\n",
            "Prediction:  3 , actual:  0\n",
            "Prediction:  0 , actual:  4\n",
            "Prediction:  0 , actual:  0\n",
            "Classification correctly:  2\n",
            "Model accuracy from testing: 0.22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "rQKHYyASuZGQ",
        "outputId": "ee36d28c-ac5c-411b-9a93-b64d5fd17758"
      },
      "source": [
        "PATH = \"state_dict_model.pt\"\n",
        "\n",
        "# Save\n",
        "torch.save(model.state_dict(), PATH)\n",
        "\n",
        "#input_ids = []\n",
        "#for d in docs:\n",
        "#\n",
        "#    encoded_dict = tokenizer.encode_plus(\n",
        "#                        d,                      \n",
        "#                        truncation=True,\n",
        "#                        add_special_tokens = True, \n",
        "#                        max_length = 256,           \n",
        "#                        pad_to_max_length = True,\n",
        "#                        return_attention_mask = True,   \n",
        "#                        return_tensors = 'pt',     \n",
        "#                   )\n",
        "#    \n",
        "#    input_ids.append(encoded_dict['input_ids'])\n",
        "#\n",
        "#input_ids = torch.cat(input_ids, dim=0)\n",
        "\n",
        "# Load\n",
        "model = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "#(input_ids, attention_masks)\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-88fec10ed2b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#input_ids = []\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'SequenceClassifierOutput' object has no attribute 'state_dict'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nrt0lg9VudjZ"
      },
      "source": [
        "PATH = \"model.pt\"\n",
        "\n",
        "# Save\n",
        "torch.save(model, PATH)\n",
        "\n",
        "# Load\n",
        "model = torch.load(PATH)\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHkD6f727KGI"
      },
      "source": [
        "torch.save(model.state_dict(), 'model.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUaT37HY_MxE"
      },
      "source": [
        "import sagemaker\n",
        "from sagemaker.pytorch import model.PyTorchModel\n",
        "\n",
        "pytorch_model = PyTorchModel ( model_data=model_data,\n",
        "                    role=role,\n",
        "                     framework_version='1.0.0',\n",
        "                     py_version=\"py3\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9Ve_HWk_UE8"
      },
      "source": [
        "estimator = PyTorch(entry_point=' ',\n",
        "                     role=role,\n",
        "                     framework_version='1.4.0',\n",
        "                     train_instance_count=1,\n",
        "                     train_instance_type=' ',\n",
        "                     source_dir=' s',\n",
        "                     git_config=git_config,\n",
        "                     # available hyperparameters: emsize, nhid, nlayers, lr, clip, epochs, batch_size,\n",
        "                     #                            bptt, dropout, tied, seed, log_interval\n",
        "                     hyperparameters={\n",
        "                         'epochs': 1,\n",
        "                         'tied': True\n",
        "                     })\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QV149uFd8YYt"
      },
      "source": [
        "import tarfile\n",
        "import os.path\n",
        "\n",
        "def make_tarfile(output_filename, source_dir):\n",
        "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
        "        tar.add(source_dir, arcname=os.path.basename(source_dir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kd8qxJ5NBRLR"
      },
      "source": [
        "def model_fn(model_dir)\n",
        "def input_fn(request_body, request_content_type)\n",
        "def predict_fn(input_data, model)\n",
        "def output_fn(prediction, content_type)\n",
        "\n",
        "def model_fn(model_dir):\n",
        "    model = \n",
        "    if torch.__version__ == '1.5.1':\n",
        "        model = model.eval()\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceVT2Jmoph4x"
      },
      "source": [
        "## Ignore this cell for now\n",
        "# Trying out example BERT\n",
        "\n",
        "# Single training/test example for simple sequence classification\n",
        "class InputExample(object):\n",
        "\n",
        "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "        \"\"\"Constructs a InputExample.\n",
        "\n",
        "        Args:\n",
        "            guid: Unique id for the example.\n",
        "            text_a: string. The untokenized text of the first sequence. For single\n",
        "            sequence tasks, only this sequence must be specified.\n",
        "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "            Only must be specified for sequence pair tasks.\n",
        "            label: (Optional) string. The label of the example. This should be\n",
        "            specified for train and dev examples, but not for test examples.\n",
        "        \"\"\"\n",
        "        self.guid = guid\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        self.label = label\n",
        "\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"Single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.label_id = label_id"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}