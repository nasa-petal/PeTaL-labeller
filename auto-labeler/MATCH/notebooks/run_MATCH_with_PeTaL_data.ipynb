{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "run_MATCH_with_PeTaL_data.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "DYbr2McO78q7",
        "pVkLBZhCdUGh",
        "tsYoo4ymh-Dy",
        "GBhMpPdH3DuI"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Run MATCH with PeTaL data\n",
        "\n",
        "Created by Eric Kong on 21 June 2021.\n",
        "Last modified by Eric Kong on 9 July 2021.\n",
        "\n",
        "In this notebook we run the MATCH algorithm (GitHub: https://github.com/yuzhimanhua/MATCH, arXiv: https://arxiv.org/abs/2102.07349) on Lens data labelled with PeTaL's taxonomy.\n",
        "\n",
        "This notebook was originally run in Google Colaboratory with GPU acceleration."
      ],
      "metadata": {
        "id": "6mUF_9SjH8-6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "In this section we download and install the `MATCH` directory and its requirements."
      ],
      "metadata": {
        "id": "9fDodpwOeXeJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip3 install gdown\n",
        "!pip install wandb -qqq"
      ],
      "outputs": [],
      "metadata": {
        "id": "0HG5EtwyflIJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "import os\n",
        "import gdown\n",
        "import wandb"
      ],
      "outputs": [],
      "metadata": {
        "id": "UU4GmAtqzAWd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "wandb.login()"
      ],
      "outputs": [],
      "metadata": {
        "id": "Hvc28a1_H1Tl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the computing devices available to this notebook using `nvidia-smi`."
      ],
      "metadata": {
        "id": "KxoRC0jttaC_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!nvidia-smi"
      ],
      "outputs": [],
      "metadata": {
        "id": "1o1d5YtSag3y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e164457-640a-468f-cbd2-70e3c731d7f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the MATCH repository using gdown (thanks Paht!)"
      ],
      "metadata": {
        "id": "9t4vEF3kZqg9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "if not os.path.exists('MATCH/'):\n",
        "    url = \"https://drive.google.com/uc?id=1Ly--Y2w9ZQWZ_v9Kb6o742DTWokR7Rbi\" # MATCH_20210716\n",
        "    # url = \"https://drive.google.com/uc?id=1iUwxS7HsP-T9kBkPR3ZMn_bGnn80ydTv\" # MATCH_20210714\n",
        "\n",
        "    output = \"MATCH.tar.gz\"\n",
        "    gdown.download(url, output, quiet=False)\n",
        "\n",
        "    !tar -xvf MATCH.tar.gz\n",
        "else:\n",
        "    print(\"You have already downloaded our modified MATCH repository.\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "04WvUlUxfpNr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the rest of the notebook, we will want to run scripts using `MATCH/` as our working directory."
      ],
      "metadata": {
        "id": "FVzW49Pke2a-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%cd ./MATCH\n",
        "# !ls"
      ],
      "outputs": [],
      "metadata": {
        "id": "TH4Sftq2eT5E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the MATCH requirements. NOTE: ~~You may have to restart the runtime after installing the requirements.  This is annoying but not prohibitively so.~~ You don't have to restart the runtime."
      ],
      "metadata": {
        "id": "D18K9oHOZ63y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Install requirements in requirements.txt\n",
        "!chmod 755 -R .\n",
        "!pip3 install -r requirements.txt"
      ],
      "outputs": [],
      "metadata": {
        "id": "BXEInyjhbPDy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Default preprocessing, training, and testing of MATCH with PeTaL data\n",
        "\n",
        "In this section we preprocess the PeTaL data, train on MATCH on it, and evaluate it on test data.\n",
        "\n",
        "The input that MATCH expects is in newline-delimited JSON format, where each line is a JSON object with the following fields.\n",
        "\n",
        "```\n",
        "{\n",
        "  \"paper\": \"020-134-448-948-932\",\n",
        "  \"mag\": [\n",
        "    \"microtubule_polymerization\", \"microtubule\", \"tubulin\", \"guanosine_triphosphate\", \"growth_rate\", \"gtp'\", \"optical_tweezers\", \"biophysics\", \"dimer\", \"biology\"\n",
        "  ],\n",
        "  \"mesh\": [\n",
        "    \"D048429\", \"D000431\"\n",
        "  ],\n",
        "  \"venue\": \"Current biology\",\n",
        "  \"author\": [\n",
        "    \"2305659199\", \"2275630009\", \"2294310593\", \"1706693917\", \"2152058803\"\n",
        "  ],\n",
        "  \"reference\": [\n",
        "    \"020-720-960-216-820\", \"052-873-952-181-099\", \"000-849-951-902-070\"\n",
        "  ],\n",
        "  \"scholarly_citations\": [\n",
        "    \"000-393-690-357-939\", \"000-539-388-379-773\", \"002-134-932-426-244\"\n",
        "  ],\n",
        "  \"text\": \"microtubule assembly dynamics at the nanoscale background the labile nature of microtubules is critical for establishing cellular morphology and motility yet the molecular basis of assembly remains unclear here we use optical tweezers to track microtubule polymerization against microfabricated barriers permitting unprecedented spatial resolution\",\n",
        "  \"label\": [\n",
        "    \"change_size_or_color\", \"move\", \"physically_assemble/disassemble\", \"maintain_ecological_community\"\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "This file is provided as `cleaned_lens_output.json` (`https://github.com/nasa-petal/PeTaL-labeller/blob/main/scripts/lens-cleaner/cleaned_lens_output.json`)."
      ],
      "metadata": {
        "id": "9vxsGKwffJ1d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "DATASET = \"PeTaL\"\n",
        "MODEL = \"MATCH\""
      ],
      "outputs": [],
      "metadata": {
        "id": "__quj3W6O1Np"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing\n",
        "\n",
        "`PeTaL/Split.py` is a custom script which takes `cleaned_lens_output.json` and performs a training-validation-testing split (currently 80%-10%-10%), outputting `train.json`, `dev.json`, and `test.json`.\n",
        "\n",
        "`transform_data_PeTaL.py` transforms the above `json` files into plain text files, where each line is a sequence of tokens delimited by spaces. In `*_texts.txt` files, the `text` tokens are prepended by metadata tokens such as `author`, `venue`, and `references`. In `*_labels.txt` files, each line contains the PeTaL taxonomy labels for each paper.t\n",
        "\n",
        "`preprocess.py`, among other things, transforms the `*.txt` data into `numpy`-compliant `*.npy` files, using the embedding files `emb_init.npy` and `PeTaL.joint.emb`. These embeddings come from *metadata-aware embedding pre-training* (performed with PeTaL data on `hpc.grc.nasa.gov`), which embeds the text and its metadata in the same latent space in order to capture the relationships between them."
      ],
      "metadata": {
        "id": "GTOvPiEfhWJM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def get_transform_arg_string(config):\n",
        "    \"\"\"Transforms config arguments into a CLI-option string \n",
        "    for transform_data_PeTaL.py.\n",
        "\n",
        "    Args:\n",
        "        config (dict[str]): JSON dictionary of config arguments.\n",
        "\n",
        "    Returns:\n",
        "        str: CLI-option string\n",
        "    \"\"\"\n",
        "    transform_args = []\n",
        "    if not config['use_mag']:\n",
        "        transform_args.append(\"--no-mag\")\n",
        "    if not config['use_mesh']:\n",
        "        transform_args.append(\"--no-mesh\")\n",
        "    if not config['use_author']:\n",
        "        transform_args.append(\"--no-author\")\n",
        "    if not config['use_venue']:\n",
        "        transform_args.append(\"--no-venue\")\n",
        "    if not config['use_references']:\n",
        "        transform_args.append(\"--no-reference\")\n",
        "    if not config['use_text']:\n",
        "        transform_args.append(\"--no-text\")\n",
        "    return ' '.join(transform_args)\n",
        "\n",
        "def run_preprocessing(config):\n",
        "    \"\"\"Runs train-test split and preprocessing scripts\n",
        "\n",
        "    Args:\n",
        "        config (dict[str]): JSON dictionary of config arguments.\n",
        "    \"\"\"\n",
        "    # Train-test split\n",
        "    %cd PeTaL/\n",
        "    !python3 Split.py \\\n",
        "        --train {config['train_proportion']} \\\n",
        "        --dev {config['dev_proportion']} \\\n",
        "        --skip {config['skip']}\n",
        "    %cd ..\n",
        "    !wc PeTaL/train.json\n",
        "\n",
        "    # Slightly modified preprocess.sh\n",
        "    !python3 transform_data_PeTaL.py --dataset {DATASET} \\\n",
        "    {get_transform_arg_string(config)}\n",
        "\n",
        "    !python preprocess.py \\\n",
        "    --text-path {DATASET}/train_texts.txt \\\n",
        "    --label-path {DATASET}/train_labels.txt \\\n",
        "    --vocab-path {DATASET}/vocab.npy \\\n",
        "    --emb-path {DATASET}/emb_init.npy \\\n",
        "    --w2v-model {DATASET}/{DATASET}.joint.emb \\\n",
        "\n",
        "    !python preprocess.py \\\n",
        "    --text-path {DATASET}/test_texts.txt \\\n",
        "    --label-path {DATASET}/test_labels.txt \\\n",
        "    --vocab-path {DATASET}/vocab.npy \\"
      ],
      "outputs": [],
      "metadata": {
        "id": "vXPqI8bqLnEd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and testing\n",
        "\n",
        "`main.py` with `--mode train` performs training. During training, the model will occasionally (every `step` batches, where currently `step = 10` in the configuration file `configure/models/MATCH-PeTaL.yaml`) print out a logger line including epoch number, steps, training loss, validation loss, precisions and Normalized Discounted Cumulative Gains (nDCGs) at top `{1, 3, 5}`, and an early stopping count (currently set to interrupt training at `50`). The model is available in `PeTaL/models`.\n",
        "\n",
        "`main.py` with `--mode eval` performs testing. Precision and nDCG statistics are printed, and the results are available in `PeTaL/results`.\n",
        "\n",
        "`evaluation.py` performs inference. The top `k` (currently `k = 5`) label predictions for each paper are printed line by line in `predictions.txt`."
      ],
      "metadata": {
        "id": "w23xlLw2lF_p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from main import main # main.py\n",
        "\n",
        "def run_train_test(config, group):\n",
        "    \"\"\"Runs training, testing, and evaluation.\n",
        "\n",
        "    Args:\n",
        "        config (dict[str]): JSON dictionary of config arguments.\n",
        "        group (str): experiment group name for wandb logging.\n",
        "    \"\"\"\n",
        "    # Slightly modified run_models.sh\n",
        "\n",
        "    wandb.init(\n",
        "        project=\"MATCH\",\n",
        "        group=group,\n",
        "        config=config\n",
        "    )\n",
        "\n",
        "    %cp configure/datasets/{DATASET}.yaml {wandb.run.dir}\n",
        "    %cp configure/models/{MODEL}-{DATASET}.yaml {wandb.run.dir}\n",
        "\n",
        "    train_args = [\"--data-cnf\", f\"configure/datasets/{DATASET}.yaml\",\n",
        "        \"--model-cnf\", f\"configure/models/{MODEL}-{DATASET}.yaml\",\n",
        "        \"--mode\", \"train\",\n",
        "        \"--reg\", \"1\" if config['hypernymy_regularization'] else \"0\"]\n",
        "    main(args=train_args, standalone_mode=False)\n",
        "\n",
        "    test_args = [\"--data-cnf\", f\"configure/datasets/{DATASET}.yaml\",\n",
        "        \"--model-cnf\", f\"configure/models/{MODEL}-{DATASET}.yaml\",\n",
        "        \"--mode\", \"eval\"]\n",
        "    main(args=test_args, standalone_mode=False)\n",
        "    \n",
        "    wandb.finish()\n",
        "\n",
        "    !python evaluation.py \\\n",
        "    --results {DATASET}/results/{MODEL}-{DATASET}-labels.npy \\\n",
        "    --targets {DATASET}/test_labels.npy \\\n",
        "    --train-labels {DATASET}/train_labels.npy\n",
        "\n",
        "def run_trial(config, group):\n",
        "    \"\"\"Runs both preprocessing and training-testing. The whole enchilada.\n",
        "\n",
        "    Args:\n",
        "        config (dict[str]): JSON dictionary of config arguments.\n",
        "        group (str): experiment group name for wandb logging.\n",
        "    \"\"\"\n",
        "    run_preprocessing(config)\n",
        "    run_train_test(config, group)"
      ],
      "outputs": [],
      "metadata": {
        "id": "b5nUH8rtLsyP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "config={\n",
        "    'train_proportion': 0.8,\n",
        "    'dev_proportion': 0.1,\n",
        "    'skip': 0,\n",
        "    'use_mag': True,\n",
        "    'use_mesh': True,\n",
        "    'use_author': True,\n",
        "    'use_venue': True,\n",
        "    'use_references': True,\n",
        "    'use_text': True,\n",
        "    'hypernymy_regularization': True,\n",
        "    'leaf_labels_only': False,\n",
        "    'other_notes': \"\",\n",
        "}\n",
        "group = 'integration-test-2021-07-09a'\n",
        "\n",
        "run_trial(config, group)"
      ],
      "outputs": [],
      "metadata": {
        "id": "AnojGa52Juc-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ablation study: Effect of adding MAG and MeSH labels to text\n",
        "\n",
        "Relevant to PeTaL Labeller Issues #53 (https://github.com/nasa-petal/PeTaL-labeller/issues/53) and #58 (https://github.com/nasa-petal/PeTaL-labeller/issues/58)\n",
        "\n",
        "Databases of papers categorise their papers differently. We investigate the effect of adding Microsoft Academic Graph (MAG) fields of study and PubMed's Medical Subject Headings (MeSH) terms, when available for each paper, as additional metadata.\n",
        "\n",
        "To turn on/off including MAG fields of study and MeSH terms, use `transform_data_PetaL.py` options `--no-mag` and `--no-mesh`, respectively.\n",
        "\n",
        "To change the train-dev-test split before processing, use `PeTaL/Split.py` options `--train TRAIN --dev DEV`, where `TRAIN` and `DEV` are between 0 and 1, and so is their sum. An 80-10-10 train-dev-test split (the default) can be explicitly invoked using `python3 PeTaL/Split.py --train 0.8 --dev 0.1`.\n",
        "\n",
        "To rotate the dataset by `N` examples before processing, use `PeTaL/Split.py` option `--skip N`. This is useful for `k`-fold cross-validation."
      ],
      "metadata": {
        "id": "DYbr2McO78q7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Issue 58: Ablation study with 10-fold cross validaiton."
      ],
      "metadata": {
        "id": "pVkLBZhCdUGh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "STUDY_TITLE = \"with_mag_with_mesh\"\n",
        "\n",
        "for skip in range(0, 1000, 100):\n",
        "    print(f\"```\\n{STUDY_TITLE} skip={skip}\\n\")\n",
        "    config={\n",
        "        'train_proportion': 0.8,\n",
        "        'dev_proportion': 0.1,\n",
        "        'skip': skip,\n",
        "        'use_mag': True,\n",
        "        'use_mesh': True,\n",
        "        'use_author': True,\n",
        "        'use_venue': True,\n",
        "        'use_references': True,\n",
        "        'use_text': True,\n",
        "        'hypernymy_regularization': True,\n",
        "        'leaf_labels_only': False,\n",
        "        'other_notes': \"\",\n",
        "    }\n",
        "    group = 'issue_58_ablation'\n",
        "\n",
        "    run_trial(config, group)\n",
        "    print(\"```\\n\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "tUlbFelUu_m7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "STUDY_TITLE = \"with_mag_no_mesh\"\n",
        "\n",
        "for skip in range(0, 1000, 100):\n",
        "    print(f\"```\\n{STUDY_TITLE} skip={skip}\\n\")\n",
        "    config={\n",
        "        'train_proportion': 0.8,\n",
        "        'dev_proportion': 0.1,\n",
        "        'skip': skip,\n",
        "        'use_mag': True,\n",
        "        'use_mesh': False,\n",
        "        'use_author': True,\n",
        "        'use_venue': True,\n",
        "        'use_references': True,\n",
        "        'use_text': True,\n",
        "        'hypernymy_regularization': True,\n",
        "        'leaf_labels_only': False,\n",
        "        'other_notes': \"\",\n",
        "    }\n",
        "    group = 'issue_58_ablation'\n",
        "\n",
        "    run_trial(config, group)\n",
        "    print(\"```\\n\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "M-GVxKxIVFbY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "STUDY_TITLE = \"no_mag_with_mesh\"\n",
        "\n",
        "for skip in range(0, 1000, 100):\n",
        "    print(f\"```\\n{STUDY_TITLE} skip={skip}\\n\")\n",
        "    config={\n",
        "        'train_proportion': 0.8,\n",
        "        'dev_proportion': 0.1,\n",
        "        'skip': skip,\n",
        "        'use_mag': False,\n",
        "        'use_mesh': True,\n",
        "        'use_author': True,\n",
        "        'use_venue': True,\n",
        "        'use_references': True,\n",
        "        'use_text': True,\n",
        "        'hypernymy_regularization': True,\n",
        "        'leaf_labels_only': False,\n",
        "        'other_notes': \"\",\n",
        "    }\n",
        "    group = 'issue_58_ablation'\n",
        "\n",
        "    run_trial(config, group)\n",
        "    print(\"```\\n\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "wiQNLtsZgDO1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "STUDY_TITLE = \"no_mag_no_mesh\"\n",
        "\n",
        "for skip in range(0, 1000, 100):\n",
        "    print(f\"```\\n{STUDY_TITLE} skip={skip}\\n\")\n",
        "    config={\n",
        "        'train_proportion': 0.8,\n",
        "        'dev_proportion': 0.1,\n",
        "        'skip': skip,\n",
        "        'use_mag': False,\n",
        "        'use_mesh': False,\n",
        "        'use_author': True,\n",
        "        'use_venue': True,\n",
        "        'use_references': True,\n",
        "        'use_text': True,\n",
        "        'hypernymy_regularization': True,\n",
        "        'leaf_labels_only': False,\n",
        "        'other_notes': \"\",\n",
        "    }\n",
        "    group = 'issue_58_ablation'\n",
        "\n",
        "    run_trial(config, group)\n",
        "    print(\"```\\n\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "y8q5zfFonhoc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ablation study: Turn off hypernymy regularization\n",
        "\n",
        "We investigate the effect of the hierarachy (PeTaL/taxonomy.txt). The MATCH paper describes *hypernymy regularization*, which leverages taxonomy information to take into account the relationships between labels in training.\n",
        "\n",
        "This includes *regularization in the parameter space*, where a penalty is added to encourage the parameters of each label (e.g., `active_movement`) to be similar to its parent (e.g., `move`), and *regularization in the output space*, where a penalty is added if a child label occurs without its parent label (roughly speaking).\n",
        "\n",
        "The authors of `MATCH` were kind enough to include a CLI option, `--reg`, to toggle hypernymy regularization. `--reg 1` turns it on, and `--reg 0` turns it off."
      ],
      "metadata": {
        "id": "tsYoo4ymh-Dy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "STUDY_TITLE = \"no_hyper_reg\"\n",
        "\n",
        "for skip in range(0, 1000, 100):\n",
        "    print(f\"```\\n{STUDY_TITLE} skip={skip}\\n\")\n",
        "    config={\n",
        "        'train_proportion': 0.8,\n",
        "        'dev_proportion': 0.1,\n",
        "        'skip': skip,\n",
        "        'use_mag': True,\n",
        "        'use_mesh': True,\n",
        "        'use_author': True,\n",
        "        'use_venue': True,\n",
        "        'use_references': True,\n",
        "        'use_text': True,\n",
        "        'hypernymy_regularization': False,\n",
        "        'leaf_labels_only': False,\n",
        "        'other_notes': \"\",\n",
        "    }\n",
        "    group = 'hyper_reg_testing'\n",
        "\n",
        "    run_trial(config, group)\n",
        "    print(\"```\\n\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "z28moIQCiENw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Study: Effect of Dataset Size on MATCH Performance\n",
        "\n",
        "Warning: 70 trials are run, each of length ~5 minutes.\n",
        "\n",
        "| Train set size | P@1=nDCG@1 | P@3 | P@5 | nDCG@3 | nDCG@5 |\n",
        "| --- | --- | --- | --- | --- | --- |\n",
        "| 200 | 0.324 | 0.249 | 0.203 | 0.269 | 0.274 |\n",
        "| 300 | 0.424 | 0.337 | 0.275 | 0.362 | 0.364 |\n",
        "| 400 | 0.441 | 0.344 | 0.278 | 0.373 | 0.373 |\n",
        "| 500 | 0.547 | 0.419 | 0.328 | 0.454 | 0.447 |\n",
        "| 600 | 0.534 | 0.433 | 0.345 | 0.464 | 0.463 |\n",
        "| 700 | 0.555 | 0.434 | 0.342 | 0.466 | 0.472 |\n",
        "| 800 | 0.627 | 0.509 | 0.390 | 0.542 | 0.543 |"
      ],
      "metadata": {
        "id": "GBhMpPdH3DuI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "for train_size in range(200, 900, 100):\n",
        "    STUDY_TITLE = \"train_size\"\n",
        "\n",
        "    for skip in range(0, 1000, 100):\n",
        "        print(f\"```\\n{STUDY_TITLE} skip={skip}\\n\")\n",
        "        config={\n",
        "            'train_proportion': train_size / 1000.,\n",
        "            'dev_proportion': 0.1,\n",
        "            'skip': skip,\n",
        "            'use_mag': True,\n",
        "            'use_mesh': True,\n",
        "            'use_author': True,\n",
        "            'use_venue': True,\n",
        "            'use_references': True,\n",
        "            'use_text': True,\n",
        "            'hypernymy_regularization': True,\n",
        "            'leaf_labels_only': False,\n",
        "            'other_notes': \"\",\n",
        "        }\n",
        "        group = 'train_size_testing'\n",
        "\n",
        "        run_trial(config, group)\n",
        "        print(\"```\\n\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "ISjHCVBqKqtL"
      }
    }
  ]
}