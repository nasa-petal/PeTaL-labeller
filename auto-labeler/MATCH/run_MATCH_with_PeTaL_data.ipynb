{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "run_MATCH_with_PeTaL_data.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "EbvzgpQLdA35",
        "LwjcT9IcJG3I",
        "UB0SRyKqPGaw",
        "feVctRCvVwQV",
        "-4gUb3gaZeUJ",
        "pVkLBZhCdUGh",
        "GBhMpPdH3DuI",
        "54Ur6eGk_q3C"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mUF_9SjH8-6"
      },
      "source": [
        "# Run MATCH with PeTaL data\n",
        "\n",
        "Created by Eric Kong on 21 June 2021.\n",
        "Last modified by Eric Kong on 6 July 2021.\n",
        "\n",
        "In this notebook we run the MATCH algorithm (GitHub: https://github.com/yuzhimanhua/MATCH, arXiv: https://arxiv.org/abs/2102.07349) on Lens data labelled with PeTaL's taxonomy.\n",
        "\n",
        "This notebook was originally run in Google Colaboratory with GPU acceleration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fDodpwOeXeJ"
      },
      "source": [
        "## Setup\n",
        "\n",
        "In this section we download and install the `MATCH` directory and its requirements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HG5EtwyflIJ",
        "outputId": "1bff1681-6d09-4cae-8e4e-9600f364b321"
      },
      "source": [
        "!pip3 install gdown"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.41.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UU4GmAtqzAWd"
      },
      "source": [
        "import os\n",
        "import gdown"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxoRC0jttaC_"
      },
      "source": [
        "Check the computing devices available to this notebook using `nvidia-smi`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1o1d5YtSag3y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e164457-640a-468f-cbd2-70e3c731d7f6"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Jul  1 15:49:28 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t4vEF3kZqg9"
      },
      "source": [
        "Download the MATCH repository using gdown (thanks Paht!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04WvUlUxfpNr"
      },
      "source": [
        "if not os.path.exists('MATCH/'):\n",
        "    url = \"https://drive.google.com/uc?id=1-9oiMwjpiJCRjw9c12wQYAcC6QLhQZlj\" # MATCH_20210701\n",
        "    # url = \"https://drive.google.com/uc?id=1-2evoruw4B88P4x7EpoH45tuJ2Dfb2Xs\" # old one\n",
        "    output = \"MATCH.tar.gz\"\n",
        "    gdown.download(url, output, quiet=False)\n",
        "\n",
        "    !tar -xvf MATCH.tar.gz\n",
        "else:\n",
        "    print(\"You have already downloaded our modified MATCH repository.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVzW49Pke2a-"
      },
      "source": [
        "For the rest of the notebook, we will want to run scripts using `MATCH/` as our working directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TH4Sftq2eT5E",
        "outputId": "165358da-d73c-4d90-a9d4-ef75a11dd889"
      },
      "source": [
        "%cd ./MATCH\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/MATCH\n",
            "configure      PeTaL\t\t run_models.sh\n",
            "deepxml        predictions.txt\t transform_data_PeTaL_only_mags_and_meshes.py\n",
            "evaluation.py  preprocess.py\t transform_data_PeTaL.py\n",
            "joint\t       preprocess.sh\t transform_data_PeTaL_random_mags_and_meshes.py\n",
            "LICENSE        README.md\t transform_data.py\n",
            "main.py        requirements.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D18K9oHOZ63y"
      },
      "source": [
        "Install the MATCH requirements. NOTE: You may have to restart the runtime after installing the requirements.  This is annoying but not prohibitively so."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXEInyjhbPDy"
      },
      "source": [
        "# Install requirements in requirements.txt\n",
        "!chmod 755 -R .\n",
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vxsGKwffJ1d"
      },
      "source": [
        "## Default preprocessing, training, and testing of MATCH with PeTaL data\n",
        "\n",
        "In this section we preprocess the PeTaL data, train on MATCH on it, and evaluate it on test data.\n",
        "\n",
        "The input that MATCH expects is in newline-delimited JSON format, where each line is a JSON object with the following fields.\n",
        "\n",
        "```\n",
        "{\n",
        "  \"paper\": \"020-134-448-948-932\",\n",
        "  \"mag\": [\n",
        "    \"microtubule_polymerization\", \"microtubule\", \"tubulin\", \"guanosine_triphosphate\", \"growth_rate\", \"gtp'\", \"optical_tweezers\", \"biophysics\", \"dimer\", \"biology\"\n",
        "  ],\n",
        "  \"mesh\": [\n",
        "    \"D048429\", \"D000431\"\n",
        "  ],\n",
        "  \"venue\": \"Current biology\",\n",
        "  \"author\": [\n",
        "    \"2305659199\", \"2275630009\", \"2294310593\", \"1706693917\", \"2152058803\"\n",
        "  ],\n",
        "  \"reference\": [\n",
        "    \"020-720-960-216-820\", \"052-873-952-181-099\", \"000-849-951-902-070\"\n",
        "  ],\n",
        "  \"scholarly_citations\": [\n",
        "    \"000-393-690-357-939\", \"000-539-388-379-773\", \"002-134-932-426-244\"\n",
        "  ],\n",
        "  \"text\": \"microtubule assembly dynamics at the nanoscale background the labile nature of microtubules is critical for establishing cellular morphology and motility yet the molecular basis of assembly remains unclear here we use optical tweezers to track microtubule polymerization against microfabricated barriers permitting unprecedented spatial resolution\",\n",
        "  \"label\": [\n",
        "    \"change_size_or_color\", \"move\", \"physically_assemble/disassemble\", \"maintain_ecological_community\"\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "This file is provided as `cleaned_lens_output.json` (`https://github.com/nasa-petal/PeTaL-labeller/blob/main/scripts/lens-cleaner/cleaned_lens_output.json`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__quj3W6O1Np"
      },
      "source": [
        "DATASET = \"PeTaL\"\n",
        "MODEL = \"MATCH\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTOvPiEfhWJM"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "`PeTaL/Split.py` is a custom script which takes `cleaned_lens_output.json` and performs a training-validation-testing split (currently 80%-10%-10%), outputting `train.json`, `dev.json`, and `test.json`.\n",
        "\n",
        "`transform_data_PeTaL.py` transforms the above `json` files into plain text files, where each line is a sequence of tokens delimited by spaces. In `*_texts.txt` files, the `text` tokens are prepended by metadata tokens such as `author`, `venue`, and `references`. In `*_labels.txt` files, each line contains the PeTaL taxonomy labels for each paper.t\n",
        "\n",
        "`preprocess.py`, among other things, transforms the `*.txt` data into `numpy`-compliant `*.npy` files, using the embedding files `emb_init.npy` and `PeTaL.joint.emb`. These embeddings come from *metadata-aware embedding pre-training* (performed with PeTaL data on `hpc.grc.nasa.gov`), which embeds the text and its metadata in the same latent space in order to capture the relationships between them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXPqI8bqLnEd",
        "outputId": "b26c4e4a-6802-432c-feb2-76697d324bd7"
      },
      "source": [
        "# Slightly modified preprocess.sh\n",
        "\n",
        "%cd PeTaL/\n",
        "!python3 Split.py\n",
        "%cd ..\n",
        "\n",
        "!python3 transform_data_PeTaL.py --dataset $DATASET\n",
        "\n",
        "!python preprocess.py \\\n",
        "--text-path {DATASET}/train_texts.txt \\\n",
        "--label-path {DATASET}/train_labels.txt \\\n",
        "--vocab-path {DATASET}/vocab.npy \\\n",
        "--emb-path {DATASET}/emb_init.npy \\\n",
        "--w2v-model {DATASET}/{DATASET}.joint.emb \\\n",
        "\n",
        "!python preprocess.py \\\n",
        "--text-path {DATASET}/test_texts.txt \\\n",
        "--label-path {DATASET}/test_labels.txt \\\n",
        "--vocab-path {DATASET}/vocab.npy \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 210629 06:34:12 preprocess:28]\u001b[39m Vocab Size: 26834\n",
            "\u001b[32m[I 210629 06:34:12 preprocess:30]\u001b[39m Getting Dataset: PeTaL/train_texts.txt Max Length: 500\n",
            "\u001b[32m[I 210629 06:34:13 preprocess:32]\u001b[39m Size of Samples: 900\n",
            "\u001b[32m[I 210629 06:34:14 preprocess:28]\u001b[39m Vocab Size: 26834\n",
            "\u001b[32m[I 210629 06:34:14 preprocess:30]\u001b[39m Getting Dataset: PeTaL/test_texts.txt Max Length: 500\n",
            "\u001b[32m[I 210629 06:34:14 preprocess:32]\u001b[39m Size of Samples: 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w23xlLw2lF_p"
      },
      "source": [
        "### Training and testing\n",
        "\n",
        "`main.py` with `--mode train` performs training. During training, the model will occasionally (every `step` batches, where currently `step = 10` in the configuration file `configure/models/MATCH-PeTaL.yaml`) print out a logger line including epoch number, steps, training loss, validation loss, precisions and Normalized Discounted Cumulative Gains (nDCGs) at top `{1, 3, 5}`, and an early stopping count (currently set to interrupt training at `50`). The model is available in `PeTaL/models`.\n",
        "\n",
        "`main.py` with `--mode eval` performs testing. Precision and nDCG statistics are printed, and the results are available in `PeTaL/results`.\n",
        "\n",
        "`evaluation.py` performs inference. The top `k` (currently `k = 5`) label predictions for each paper are printed line by line in `predictions.txt`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5nUH8rtLsyP",
        "outputId": "dc6a6666-7395-49b5-ab53-9e24e7d0692a"
      },
      "source": [
        "# Slightly modified run_models.sh\n",
        "\n",
        "!PYTHONFAULTHANDLER=1 python main.py --data-cnf configure/datasets/{DATASET}.yaml --model-cnf configure/models/{MODEL}-{DATASET}.yaml --mode train --reg 1\n",
        "!PYTHONFAULTHANDLER=1 python main.py --data-cnf configure/datasets/{DATASET}.yaml --model-cnf configure/models/{MODEL}-{DATASET}.yaml --mode eval\n",
        "\n",
        "!python evaluation.py \\\n",
        "--results {DATASET}/results/{MODEL}-{DATASET}-labels.npy \\\n",
        "--targets {DATASET}/test_labels.npy \\\n",
        "--train-labels {DATASET}/train_labels.npy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 210629 06:34:16 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210629 06:34:16 main:35]\u001b[39m Loading Training and Validation Set\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['absorb_and/or_filter_solids', 'chemically_break_down_inorganic_compounds', 'detox/purify', 'protect_from_gases', 'send_vibratory_signals'] will be ignored\n",
            "  .format(sorted(unknown, key=str)))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['chemically_break_down_inorganic_compounds', 'protect_from_gases'] will be ignored\n",
            "  .format(sorted(unknown, key=str)))\n",
            "\u001b[32m[I 210629 06:34:16 main:47]\u001b[39m Number of Labels: 124\n",
            "\u001b[32m[I 210629 06:34:16 main:48]\u001b[39m Size of Training Set: 800\n",
            "\u001b[32m[I 210629 06:34:16 main:49]\u001b[39m Size of Validation Set: 100\n",
            "\u001b[32m[I 210629 06:34:16 main:66]\u001b[39m Number of Edges: 101\n",
            "\u001b[32m[I 210629 06:34:16 main:68]\u001b[39m Training\n",
            "\u001b[32m[I 210629 06:34:23 models:110]\u001b[39m 2 512 train loss: 0.2769679 valid loss: 0.1508755 P@1: 0.33000 P@3: 0.22333 P@5: 0.19400 N@3: 0.24877 N@5: 0.25615 early stop: 0\n",
            "\u001b[32m[I 210629 06:34:24 models:142]\u001b[39m SWA Initializing\n",
            "\u001b[32m[I 210629 06:34:25 models:110]\u001b[39m 4 1024 train loss: 0.1443880 valid loss: 0.1437063 P@1: 0.33000 P@3: 0.21667 P@5: 0.20400 N@3: 0.23732 N@5: 0.26094 early stop: 0\n",
            "\u001b[32m[I 210629 06:34:29 models:110]\u001b[39m 7 512 train loss: 0.1436986 valid loss: 0.1422755 P@1: 0.33000 P@3: 0.21667 P@5: 0.20400 N@3: 0.23548 N@5: 0.25954 early stop: 1\n",
            "\u001b[32m[I 210629 06:34:31 models:110]\u001b[39m 9 1024 train loss: 0.1401883 valid loss: 0.1417869 P@1: 0.33000 P@3: 0.20667 P@5: 0.20400 N@3: 0.23089 N@5: 0.26074 early stop: 2\n",
            "\u001b[32m[I 210629 06:34:35 models:110]\u001b[39m 12 512 train loss: 0.1344197 valid loss: 0.1413758 P@1: 0.33000 P@3: 0.21667 P@5: 0.20400 N@3: 0.24224 N@5: 0.26519 early stop: 0\n",
            "\u001b[32m[I 210629 06:34:37 models:110]\u001b[39m 14 1024 train loss: 0.1233254 valid loss: 0.1407958 P@1: 0.33000 P@3: 0.25667 P@5: 0.20400 N@3: 0.27224 N@5: 0.27103 early stop: 0\n",
            "\u001b[32m[I 210629 06:34:41 models:110]\u001b[39m 17 512 train loss: 0.1004521 valid loss: 0.1401767 P@1: 0.33000 P@3: 0.25000 P@5: 0.23200 N@3: 0.26877 N@5: 0.29432 early stop: 0\n",
            "\u001b[32m[I 210629 06:34:43 models:110]\u001b[39m 19 1024 train loss: 0.0865315 valid loss: 0.1395032 P@1: 0.33000 P@3: 0.27667 P@5: 0.23800 N@3: 0.28754 N@5: 0.30385 early stop: 0\n",
            "\u001b[32m[I 210629 06:34:47 models:110]\u001b[39m 22 512 train loss: 0.0652922 valid loss: 0.1387514 P@1: 0.33000 P@3: 0.29333 P@5: 0.24400 N@3: 0.29866 N@5: 0.31094 early stop: 0\n",
            "\u001b[32m[I 210629 06:34:49 models:110]\u001b[39m 24 1024 train loss: 0.0519157 valid loss: 0.1378474 P@1: 0.34000 P@3: 0.30333 P@5: 0.25600 N@3: 0.30887 N@5: 0.32418 early stop: 0\n",
            "\u001b[32m[I 210629 06:34:53 models:110]\u001b[39m 27 512 train loss: 0.0425451 valid loss: 0.1367460 P@1: 0.36000 P@3: 0.32000 P@5: 0.26400 N@3: 0.32805 N@5: 0.33809 early stop: 0\n",
            "\u001b[32m[I 210629 06:34:55 models:110]\u001b[39m 29 1024 train loss: 0.0360096 valid loss: 0.1357470 P@1: 0.36000 P@3: 0.34000 P@5: 0.27800 N@3: 0.34653 N@5: 0.35318 early stop: 0\n",
            "\u001b[32m[I 210629 06:34:59 models:110]\u001b[39m 32 512 train loss: 0.0292950 valid loss: 0.1348063 P@1: 0.43000 P@3: 0.35333 P@5: 0.28000 N@3: 0.37164 N@5: 0.36958 early stop: 0\n",
            "\u001b[32m[I 210629 06:35:01 models:110]\u001b[39m 34 1024 train loss: 0.0269935 valid loss: 0.1339283 P@1: 0.47000 P@3: 0.36667 P@5: 0.28400 N@3: 0.39306 N@5: 0.38396 early stop: 0\n",
            "\u001b[32m[I 210629 06:35:05 models:110]\u001b[39m 37 512 train loss: 0.0250641 valid loss: 0.1332465 P@1: 0.47000 P@3: 0.38000 P@5: 0.29800 N@3: 0.40562 N@5: 0.39666 early stop: 0\n",
            "\u001b[32m[I 210629 06:35:07 models:110]\u001b[39m 39 1024 train loss: 0.0207539 valid loss: 0.1329135 P@1: 0.47000 P@3: 0.39667 P@5: 0.30200 N@3: 0.42299 N@5: 0.40786 early stop: 0\n",
            "\u001b[32m[I 210629 06:35:11 models:110]\u001b[39m 42 512 train loss: 0.0171948 valid loss: 0.1324237 P@1: 0.50000 P@3: 0.40333 P@5: 0.30600 N@3: 0.43737 N@5: 0.42046 early stop: 0\n",
            "\u001b[32m[I 210629 06:35:14 models:110]\u001b[39m 44 1024 train loss: 0.0151800 valid loss: 0.1325502 P@1: 0.50000 P@3: 0.41667 P@5: 0.31400 N@3: 0.44747 N@5: 0.42960 early stop: 0\n",
            "\u001b[32m[I 210629 06:35:17 models:110]\u001b[39m 47 512 train loss: 0.0133464 valid loss: 0.1327304 P@1: 0.53000 P@3: 0.42000 P@5: 0.31200 N@3: 0.45563 N@5: 0.43272 early stop: 0\n",
            "\u001b[32m[I 210629 06:35:19 models:110]\u001b[39m 49 1024 train loss: 0.0118398 valid loss: 0.1334002 P@1: 0.54000 P@3: 0.42667 P@5: 0.31600 N@3: 0.46480 N@5: 0.44066 early stop: 0\n",
            "\u001b[32m[I 210629 06:35:23 models:110]\u001b[39m 52 512 train loss: 0.0113864 valid loss: 0.1340824 P@1: 0.54000 P@3: 0.42333 P@5: 0.32800 N@3: 0.46246 N@5: 0.45010 early stop: 0\n",
            "\u001b[32m[I 210629 06:35:26 models:110]\u001b[39m 54 1024 train loss: 0.0105990 valid loss: 0.1350189 P@1: 0.54000 P@3: 0.42667 P@5: 0.33400 N@3: 0.46542 N@5: 0.45610 early stop: 0\n",
            "\u001b[32m[I 210629 06:35:29 models:110]\u001b[39m 57 512 train loss: 0.0082764 valid loss: 0.1362618 P@1: 0.54000 P@3: 0.43333 P@5: 0.33600 N@3: 0.46950 N@5: 0.45747 early stop: 0\n",
            "\u001b[32m[I 210629 06:35:32 models:110]\u001b[39m 59 1024 train loss: 0.0087542 valid loss: 0.1374070 P@1: 0.56000 P@3: 0.44000 P@5: 0.34200 N@3: 0.47941 N@5: 0.46822 early stop: 0\n",
            "\u001b[32m[I 210629 06:35:35 models:110]\u001b[39m 62 512 train loss: 0.0088206 valid loss: 0.1386558 P@1: 0.57000 P@3: 0.45000 P@5: 0.34800 N@3: 0.48818 N@5: 0.47638 early stop: 0\n",
            "\u001b[32m[I 210629 06:35:38 models:110]\u001b[39m 64 1024 train loss: 0.0084182 valid loss: 0.1401328 P@1: 0.56000 P@3: 0.45000 P@5: 0.35600 N@3: 0.48707 N@5: 0.48305 early stop: 0\n",
            "\u001b[32m[I 210629 06:35:41 models:110]\u001b[39m 67 512 train loss: 0.0101012 valid loss: 0.1413152 P@1: 0.57000 P@3: 0.46333 P@5: 0.35400 N@3: 0.49880 N@5: 0.48532 early stop: 0\n",
            "\u001b[32m[I 210629 06:35:44 models:110]\u001b[39m 69 1024 train loss: 0.0116399 valid loss: 0.1424578 P@1: 0.57000 P@3: 0.46333 P@5: 0.35400 N@3: 0.49880 N@5: 0.48585 early stop: 0\n",
            "\u001b[32m[I 210629 06:35:47 models:110]\u001b[39m 72 512 train loss: 0.0097982 valid loss: 0.1437994 P@1: 0.57000 P@3: 0.46333 P@5: 0.36400 N@3: 0.50003 N@5: 0.49623 early stop: 0\n",
            "\u001b[32m[I 210629 06:35:50 models:110]\u001b[39m 74 1024 train loss: 0.0090265 valid loss: 0.1447047 P@1: 0.58000 P@3: 0.46667 P@5: 0.36600 N@3: 0.50288 N@5: 0.49977 early stop: 0\n",
            "\u001b[32m[I 210629 06:35:53 models:110]\u001b[39m 77 512 train loss: 0.0077268 valid loss: 0.1463462 P@1: 0.59000 P@3: 0.46667 P@5: 0.37200 N@3: 0.50522 N@5: 0.50653 early stop: 0\n",
            "\u001b[32m[I 210629 06:35:56 models:110]\u001b[39m 79 1024 train loss: 0.0067360 valid loss: 0.1477754 P@1: 0.59000 P@3: 0.47333 P@5: 0.37200 N@3: 0.51115 N@5: 0.50770 early stop: 0\n",
            "\u001b[32m[I 210629 06:35:59 models:110]\u001b[39m 82 512 train loss: 0.0047997 valid loss: 0.1493793 P@1: 0.59000 P@3: 0.47333 P@5: 0.37000 N@3: 0.51115 N@5: 0.50531 early stop: 1\n",
            "\u001b[32m[I 210629 06:36:02 models:110]\u001b[39m 84 1024 train loss: 0.0033698 valid loss: 0.1513101 P@1: 0.59000 P@3: 0.47000 P@5: 0.36800 N@3: 0.50880 N@5: 0.50404 early stop: 2\n",
            "\u001b[32m[I 210629 06:36:05 models:110]\u001b[39m 87 512 train loss: 0.0026978 valid loss: 0.1530878 P@1: 0.61000 P@3: 0.47667 P@5: 0.37000 N@3: 0.51757 N@5: 0.51079 early stop: 0\n",
            "\u001b[32m[I 210629 06:36:08 models:110]\u001b[39m 89 1024 train loss: 0.0021036 valid loss: 0.1552448 P@1: 0.63000 P@3: 0.47000 P@5: 0.37200 N@3: 0.51696 N@5: 0.51637 early stop: 0\n",
            "\u001b[32m[I 210629 06:36:11 models:110]\u001b[39m 92 512 train loss: 0.0018731 valid loss: 0.1572554 P@1: 0.63000 P@3: 0.47000 P@5: 0.37400 N@3: 0.51696 N@5: 0.51845 early stop: 0\n",
            "\u001b[32m[I 210629 06:36:14 models:110]\u001b[39m 94 1024 train loss: 0.0014456 valid loss: 0.1591772 P@1: 0.63000 P@3: 0.47667 P@5: 0.37600 N@3: 0.52184 N@5: 0.52057 early stop: 0\n",
            "\u001b[32m[I 210629 06:36:17 models:110]\u001b[39m 97 512 train loss: 0.0015021 valid loss: 0.1613131 P@1: 0.63000 P@3: 0.48000 P@5: 0.37800 N@3: 0.52480 N@5: 0.52331 early stop: 0\n",
            "\u001b[32m[I 210629 06:36:20 models:110]\u001b[39m 99 1024 train loss: 0.0026214 valid loss: 0.1634645 P@1: 0.63000 P@3: 0.48333 P@5: 0.37800 N@3: 0.52714 N@5: 0.52375 early stop: 0\n",
            "\u001b[32m[I 210629 06:36:23 models:110]\u001b[39m 102 512 train loss: 0.0031321 valid loss: 0.1657700 P@1: 0.63000 P@3: 0.48000 P@5: 0.37800 N@3: 0.52418 N@5: 0.52261 early stop: 1\n",
            "\u001b[32m[I 210629 06:36:26 models:110]\u001b[39m 104 1024 train loss: 0.0038172 valid loss: 0.1679311 P@1: 0.62000 P@3: 0.48000 P@5: 0.38000 N@3: 0.52245 N@5: 0.52282 early stop: 2\n",
            "\u001b[32m[I 210629 06:36:29 models:110]\u001b[39m 107 512 train loss: 0.0036682 valid loss: 0.1698967 P@1: 0.62000 P@3: 0.48333 P@5: 0.38200 N@3: 0.52603 N@5: 0.52543 early stop: 0\n",
            "\u001b[32m[I 210629 06:36:32 models:110]\u001b[39m 109 1024 train loss: 0.0038361 valid loss: 0.1718148 P@1: 0.60000 P@3: 0.48333 P@5: 0.38600 N@3: 0.52256 N@5: 0.52560 early stop: 0\n",
            "\u001b[32m[I 210629 06:36:35 models:110]\u001b[39m 112 512 train loss: 0.0049585 valid loss: 0.1737919 P@1: 0.61000 P@3: 0.48000 P@5: 0.38600 N@3: 0.52195 N@5: 0.52686 early stop: 0\n",
            "\u001b[32m[I 210629 06:36:38 models:110]\u001b[39m 114 1024 train loss: 0.0040183 valid loss: 0.1755492 P@1: 0.61000 P@3: 0.48667 P@5: 0.38600 N@3: 0.52736 N@5: 0.52776 early stop: 0\n",
            "\u001b[32m[I 210629 06:36:41 models:110]\u001b[39m 117 512 train loss: 0.0037468 valid loss: 0.1771896 P@1: 0.62000 P@3: 0.49333 P@5: 0.38800 N@3: 0.53379 N@5: 0.53142 early stop: 0\n",
            "\u001b[32m[I 210629 06:36:44 models:110]\u001b[39m 119 1024 train loss: 0.0027099 valid loss: 0.1790372 P@1: 0.62000 P@3: 0.49333 P@5: 0.39200 N@3: 0.53501 N@5: 0.53587 early stop: 0\n",
            "\u001b[32m[I 210629 06:36:47 models:110]\u001b[39m 122 512 train loss: 0.0022899 valid loss: 0.1808959 P@1: 0.62000 P@3: 0.49333 P@5: 0.39200 N@3: 0.53440 N@5: 0.53525 early stop: 1\n",
            "\u001b[32m[I 210629 06:36:50 models:110]\u001b[39m 124 1024 train loss: 0.0033027 valid loss: 0.1828360 P@1: 0.62000 P@3: 0.49000 P@5: 0.39200 N@3: 0.53125 N@5: 0.53384 early stop: 2\n",
            "\u001b[32m[I 210629 06:36:53 models:110]\u001b[39m 127 512 train loss: 0.0025351 valid loss: 0.1847286 P@1: 0.62000 P@3: 0.49333 P@5: 0.39200 N@3: 0.53360 N@5: 0.53406 early stop: 3\n",
            "\u001b[32m[I 210629 06:36:56 models:110]\u001b[39m 129 1024 train loss: 0.0021234 valid loss: 0.1866705 P@1: 0.62000 P@3: 0.48667 P@5: 0.39800 N@3: 0.52890 N@5: 0.53873 early stop: 0\n",
            "\u001b[32m[I 210629 06:36:59 models:110]\u001b[39m 132 512 train loss: 0.0017110 valid loss: 0.1887393 P@1: 0.62000 P@3: 0.49667 P@5: 0.40000 N@3: 0.53656 N@5: 0.54161 early stop: 0\n",
            "\u001b[32m[I 210629 06:37:02 models:110]\u001b[39m 134 1024 train loss: 0.0015104 valid loss: 0.1906260 P@1: 0.63000 P@3: 0.49667 P@5: 0.40000 N@3: 0.53829 N@5: 0.54334 early stop: 0\n",
            "\u001b[32m[I 210629 06:37:05 models:110]\u001b[39m 137 512 train loss: 0.0019338 valid loss: 0.1926509 P@1: 0.63000 P@3: 0.49667 P@5: 0.40000 N@3: 0.53829 N@5: 0.54324 early stop: 1\n",
            "\u001b[32m[I 210629 06:37:08 models:110]\u001b[39m 139 1024 train loss: 0.0026540 valid loss: 0.1944454 P@1: 0.63000 P@3: 0.50000 P@5: 0.40000 N@3: 0.54197 N@5: 0.54403 early stop: 0\n",
            "\u001b[32m[I 210629 06:37:11 models:110]\u001b[39m 142 512 train loss: 0.0033713 valid loss: 0.1961288 P@1: 0.63000 P@3: 0.49667 P@5: 0.40000 N@3: 0.53890 N@5: 0.54360 early stop: 1\n",
            "\u001b[32m[I 210629 06:37:14 models:110]\u001b[39m 144 1024 train loss: 0.0037363 valid loss: 0.1978881 P@1: 0.63000 P@3: 0.50000 P@5: 0.40200 N@3: 0.54136 N@5: 0.54501 early stop: 0\n",
            "\u001b[32m[I 210629 06:37:17 models:110]\u001b[39m 147 512 train loss: 0.0049892 valid loss: 0.1995645 P@1: 0.63000 P@3: 0.50000 P@5: 0.40200 N@3: 0.54136 N@5: 0.54481 early stop: 1\n",
            "\u001b[32m[I 210629 06:37:19 models:110]\u001b[39m 149 1024 train loss: 0.0052761 valid loss: 0.2011074 P@1: 0.63000 P@3: 0.50000 P@5: 0.40200 N@3: 0.54013 N@5: 0.54395 early stop: 2\n",
            "\u001b[32m[I 210629 06:37:23 models:110]\u001b[39m 152 512 train loss: 0.0037468 valid loss: 0.2024806 P@1: 0.63000 P@3: 0.50000 P@5: 0.40000 N@3: 0.54013 N@5: 0.54216 early stop: 3\n",
            "\u001b[32m[I 210629 06:37:25 models:110]\u001b[39m 154 1024 train loss: 0.0041646 valid loss: 0.2040120 P@1: 0.63000 P@3: 0.50333 P@5: 0.40200 N@3: 0.54186 N@5: 0.54318 early stop: 4\n",
            "\u001b[32m[I 210629 06:37:29 models:110]\u001b[39m 157 512 train loss: 0.0035324 valid loss: 0.2052497 P@1: 0.63000 P@3: 0.50667 P@5: 0.40200 N@3: 0.54543 N@5: 0.54448 early stop: 5\n",
            "\u001b[32m[I 210629 06:37:31 models:110]\u001b[39m 159 1024 train loss: 0.0028776 valid loss: 0.2064350 P@1: 0.63000 P@3: 0.51000 P@5: 0.40200 N@3: 0.54717 N@5: 0.54419 early stop: 6\n",
            "\u001b[32m[I 210629 06:37:34 models:110]\u001b[39m 162 512 train loss: 0.0015168 valid loss: 0.2078365 P@1: 0.63000 P@3: 0.50667 P@5: 0.40000 N@3: 0.54482 N@5: 0.54235 early stop: 7\n",
            "\u001b[32m[I 210629 06:37:37 models:110]\u001b[39m 164 1024 train loss: 0.0009249 valid loss: 0.2092019 P@1: 0.63000 P@3: 0.51000 P@5: 0.39800 N@3: 0.54655 N@5: 0.54092 early stop: 8\n",
            "\u001b[32m[I 210629 06:37:40 models:110]\u001b[39m 167 512 train loss: 0.0007362 valid loss: 0.2106784 P@1: 0.63000 P@3: 0.50667 P@5: 0.39800 N@3: 0.54420 N@5: 0.54069 early stop: 9\n",
            "\u001b[32m[I 210629 06:37:43 models:110]\u001b[39m 169 1024 train loss: 0.0008683 valid loss: 0.2122804 P@1: 0.64000 P@3: 0.51333 P@5: 0.39800 N@3: 0.55124 N@5: 0.54311 early stop: 10\n",
            "\u001b[32m[I 210629 06:37:46 models:110]\u001b[39m 172 512 train loss: 0.0017139 valid loss: 0.2137914 P@1: 0.64000 P@3: 0.51667 P@5: 0.40200 N@3: 0.55420 N@5: 0.54720 early stop: 0\n",
            "\u001b[32m[I 210629 06:37:49 models:110]\u001b[39m 174 1024 train loss: 0.0022226 valid loss: 0.2153457 P@1: 0.64000 P@3: 0.52000 P@5: 0.40200 N@3: 0.55594 N@5: 0.54708 early stop: 1\n",
            "\u001b[32m[I 210629 06:37:52 models:110]\u001b[39m 177 512 train loss: 0.0031180 valid loss: 0.2167652 P@1: 0.63000 P@3: 0.52333 P@5: 0.40000 N@3: 0.55655 N@5: 0.54476 early stop: 2\n",
            "\u001b[32m[I 210629 06:37:55 models:110]\u001b[39m 179 1024 train loss: 0.0037485 valid loss: 0.2180703 P@1: 0.62000 P@3: 0.52333 P@5: 0.40000 N@3: 0.55482 N@5: 0.54376 early stop: 3\n",
            "\u001b[32m[I 210629 06:37:58 models:110]\u001b[39m 182 512 train loss: 0.0084698 valid loss: 0.2191952 P@1: 0.62000 P@3: 0.52333 P@5: 0.40000 N@3: 0.55482 N@5: 0.54391 early stop: 4\n",
            "\u001b[32m[I 210629 06:38:01 models:110]\u001b[39m 184 1024 train loss: 0.0092937 valid loss: 0.2201271 P@1: 0.62000 P@3: 0.52333 P@5: 0.40000 N@3: 0.55543 N@5: 0.54453 early stop: 5\n",
            "\u001b[32m[I 210629 06:38:04 models:110]\u001b[39m 187 512 train loss: 0.0138313 valid loss: 0.2209678 P@1: 0.62000 P@3: 0.52667 P@5: 0.40000 N@3: 0.55778 N@5: 0.54493 early stop: 6\n",
            "\u001b[32m[I 210629 06:38:07 models:110]\u001b[39m 189 1024 train loss: 0.0119717 valid loss: 0.2217799 P@1: 0.62000 P@3: 0.52667 P@5: 0.40400 N@3: 0.55778 N@5: 0.54796 early stop: 0\n",
            "\u001b[32m[I 210629 06:38:10 models:110]\u001b[39m 192 512 train loss: 0.0076773 valid loss: 0.2224187 P@1: 0.62000 P@3: 0.52333 P@5: 0.40400 N@3: 0.55543 N@5: 0.54773 early stop: 1\n",
            "\u001b[32m[I 210629 06:38:13 models:110]\u001b[39m 194 1024 train loss: 0.0051064 valid loss: 0.2229349 P@1: 0.62000 P@3: 0.52333 P@5: 0.40400 N@3: 0.55543 N@5: 0.54773 early stop: 2\n",
            "\u001b[32m[I 210629 06:38:16 models:110]\u001b[39m 197 512 train loss: 0.0039323 valid loss: 0.2235675 P@1: 0.62000 P@3: 0.52667 P@5: 0.40400 N@3: 0.55839 N@5: 0.54858 early stop: 0\n",
            "\u001b[32m[I 210629 06:38:19 models:110]\u001b[39m 199 1024 train loss: 0.0029289 valid loss: 0.2241980 P@1: 0.62000 P@3: 0.52667 P@5: 0.40200 N@3: 0.55839 N@5: 0.54691 early stop: 1\n",
            "\u001b[32m[I 210629 06:38:22 models:110]\u001b[39m 202 512 train loss: 0.0023736 valid loss: 0.2249439 P@1: 0.62000 P@3: 0.52000 P@5: 0.40200 N@3: 0.55298 N@5: 0.54601 early stop: 2\n",
            "\u001b[32m[I 210629 06:38:25 models:110]\u001b[39m 204 1024 train loss: 0.0015759 valid loss: 0.2258484 P@1: 0.62000 P@3: 0.52333 P@5: 0.40200 N@3: 0.55666 N@5: 0.54695 early stop: 3\n",
            "\u001b[32m[I 210629 06:38:28 models:110]\u001b[39m 207 512 train loss: 0.0010561 valid loss: 0.2267442 P@1: 0.62000 P@3: 0.52333 P@5: 0.40200 N@3: 0.55666 N@5: 0.54680 early stop: 4\n",
            "\u001b[32m[I 210629 06:38:31 models:110]\u001b[39m 209 1024 train loss: 0.0005478 valid loss: 0.2276287 P@1: 0.62000 P@3: 0.52333 P@5: 0.40200 N@3: 0.55666 N@5: 0.54680 early stop: 5\n",
            "\u001b[32m[I 210629 06:38:34 models:110]\u001b[39m 212 512 train loss: 0.0002934 valid loss: 0.2287335 P@1: 0.62000 P@3: 0.52333 P@5: 0.40200 N@3: 0.55666 N@5: 0.54695 early stop: 6\n",
            "\u001b[32m[I 210629 06:38:37 models:110]\u001b[39m 214 1024 train loss: 0.0001750 valid loss: 0.2298121 P@1: 0.63000 P@3: 0.52000 P@5: 0.40400 N@3: 0.55605 N@5: 0.54928 early stop: 0\n",
            "\u001b[32m[I 210629 06:38:40 models:110]\u001b[39m 217 512 train loss: 0.0001152 valid loss: 0.2308799 P@1: 0.63000 P@3: 0.52000 P@5: 0.40600 N@3: 0.55605 N@5: 0.55079 early stop: 0\n",
            "\u001b[32m[I 210629 06:38:43 models:110]\u001b[39m 219 1024 train loss: 0.0000840 valid loss: 0.2319799 P@1: 0.63000 P@3: 0.51667 P@5: 0.40600 N@3: 0.55370 N@5: 0.55070 early stop: 1\n",
            "\u001b[32m[I 210629 06:38:46 models:110]\u001b[39m 222 512 train loss: 0.0000798 valid loss: 0.2330773 P@1: 0.63000 P@3: 0.51667 P@5: 0.40600 N@3: 0.55432 N@5: 0.55131 early stop: 0\n",
            "\u001b[32m[I 210629 06:38:49 models:110]\u001b[39m 224 1024 train loss: 0.0000596 valid loss: 0.2341784 P@1: 0.63000 P@3: 0.51667 P@5: 0.40600 N@3: 0.55432 N@5: 0.55131 early stop: 1\n",
            "\u001b[32m[I 210629 06:38:52 models:110]\u001b[39m 227 512 train loss: 0.0000474 valid loss: 0.2352809 P@1: 0.64000 P@3: 0.51667 P@5: 0.40600 N@3: 0.55482 N@5: 0.55144 early stop: 0\n",
            "\u001b[32m[I 210629 06:38:55 models:110]\u001b[39m 229 1024 train loss: 0.0000428 valid loss: 0.2363788 P@1: 0.64000 P@3: 0.51667 P@5: 0.40800 N@3: 0.55482 N@5: 0.55326 early stop: 0\n",
            "\u001b[32m[I 210629 06:38:58 models:110]\u001b[39m 232 512 train loss: 0.0000471 valid loss: 0.2374699 P@1: 0.64000 P@3: 0.51667 P@5: 0.40800 N@3: 0.55420 N@5: 0.55285 early stop: 1\n",
            "\u001b[32m[I 210629 06:39:01 models:110]\u001b[39m 234 1024 train loss: 0.0000440 valid loss: 0.2385371 P@1: 0.65000 P@3: 0.52000 P@5: 0.40600 N@3: 0.55828 N@5: 0.55269 early stop: 2\n",
            "\u001b[32m[I 210629 06:39:04 models:110]\u001b[39m 237 512 train loss: 0.0000377 valid loss: 0.2395967 P@1: 0.65000 P@3: 0.52000 P@5: 0.40600 N@3: 0.55828 N@5: 0.55254 early stop: 3\n",
            "\u001b[32m[I 210629 06:39:07 models:110]\u001b[39m 239 1024 train loss: 0.0000359 valid loss: 0.2406522 P@1: 0.64000 P@3: 0.52333 P@5: 0.40600 N@3: 0.55890 N@5: 0.55116 early stop: 4\n",
            "\u001b[32m[I 210629 06:39:10 models:110]\u001b[39m 242 512 train loss: 0.0000336 valid loss: 0.2417047 P@1: 0.64000 P@3: 0.52333 P@5: 0.40400 N@3: 0.55890 N@5: 0.54985 early stop: 5\n",
            "\u001b[32m[I 210629 06:39:13 models:110]\u001b[39m 244 1024 train loss: 0.0000352 valid loss: 0.2427593 P@1: 0.64000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56124 N@5: 0.55018 early stop: 6\n",
            "\u001b[32m[I 210629 06:39:16 models:110]\u001b[39m 247 512 train loss: 0.0000311 valid loss: 0.2438211 P@1: 0.64000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56124 N@5: 0.55018 early stop: 7\n",
            "\u001b[32m[I 210629 06:39:18 models:110]\u001b[39m 249 1024 train loss: 0.0000302 valid loss: 0.2448683 P@1: 0.64000 P@3: 0.53000 P@5: 0.40400 N@3: 0.56359 N@5: 0.55041 early stop: 8\n",
            "\u001b[33m[W 210629 06:39:21 models:137]\u001b[39m Clipping gradients with total norm 0.00922 and max norm 0.00138\n",
            "\u001b[32m[I 210629 06:39:22 models:110]\u001b[39m 252 512 train loss: 0.0000306 valid loss: 0.2459062 P@1: 0.65000 P@3: 0.53000 P@5: 0.40400 N@3: 0.56532 N@5: 0.55214 early stop: 9\n",
            "\u001b[32m[I 210629 06:39:24 models:110]\u001b[39m 254 1024 train loss: 0.0000262 valid loss: 0.2469355 P@1: 0.65000 P@3: 0.53000 P@5: 0.40400 N@3: 0.56532 N@5: 0.55235 early stop: 10\n",
            "\u001b[32m[I 210629 06:39:28 models:110]\u001b[39m 257 512 train loss: 0.0000253 valid loss: 0.2479525 P@1: 0.65000 P@3: 0.53000 P@5: 0.40400 N@3: 0.56532 N@5: 0.55235 early stop: 11\n",
            "\u001b[32m[I 210629 06:39:30 models:110]\u001b[39m 259 1024 train loss: 0.0000245 valid loss: 0.2489654 P@1: 0.65000 P@3: 0.53000 P@5: 0.40400 N@3: 0.56532 N@5: 0.55216 early stop: 12\n",
            "\u001b[32m[I 210629 06:39:34 models:110]\u001b[39m 262 512 train loss: 0.0000235 valid loss: 0.2499697 P@1: 0.65000 P@3: 0.53000 P@5: 0.40400 N@3: 0.56532 N@5: 0.55216 early stop: 13\n",
            "\u001b[32m[I 210629 06:39:36 models:110]\u001b[39m 264 1024 train loss: 0.0000239 valid loss: 0.2509705 P@1: 0.65000 P@3: 0.53000 P@5: 0.40600 N@3: 0.56532 N@5: 0.55347 early stop: 0\n",
            "\u001b[32m[I 210629 06:39:40 models:110]\u001b[39m 267 512 train loss: 0.0000246 valid loss: 0.2519624 P@1: 0.65000 P@3: 0.53000 P@5: 0.40600 N@3: 0.56655 N@5: 0.55470 early stop: 0\n",
            "\u001b[32m[I 210629 06:39:42 models:110]\u001b[39m 269 1024 train loss: 0.0000205 valid loss: 0.2529407 P@1: 0.65000 P@3: 0.52667 P@5: 0.40600 N@3: 0.56420 N@5: 0.55438 early stop: 1\n",
            "\u001b[32m[I 210629 06:39:45 models:110]\u001b[39m 272 512 train loss: 0.0000202 valid loss: 0.2539159 P@1: 0.65000 P@3: 0.52667 P@5: 0.40600 N@3: 0.56420 N@5: 0.55438 early stop: 2\n",
            "\u001b[32m[I 210629 06:39:48 models:110]\u001b[39m 274 1024 train loss: 0.0000201 valid loss: 0.2548863 P@1: 0.65000 P@3: 0.52667 P@5: 0.40800 N@3: 0.56420 N@5: 0.55634 early stop: 0\n",
            "\u001b[32m[I 210629 06:39:52 models:110]\u001b[39m 277 512 train loss: 0.0000191 valid loss: 0.2558550 P@1: 0.65000 P@3: 0.53000 P@5: 0.40800 N@3: 0.56655 N@5: 0.55658 early stop: 0\n",
            "\u001b[32m[I 210629 06:39:54 models:110]\u001b[39m 279 1024 train loss: 0.0000212 valid loss: 0.2568422 P@1: 0.65000 P@3: 0.53000 P@5: 0.40800 N@3: 0.56655 N@5: 0.55658 early stop: 1\n",
            "\u001b[32m[I 210629 06:39:57 models:110]\u001b[39m 282 512 train loss: 0.0000195 valid loss: 0.2578165 P@1: 0.65000 P@3: 0.53000 P@5: 0.40800 N@3: 0.56655 N@5: 0.55658 early stop: 2\n",
            "\u001b[32m[I 210629 06:40:00 models:110]\u001b[39m 284 1024 train loss: 0.0000183 valid loss: 0.2587657 P@1: 0.65000 P@3: 0.53000 P@5: 0.40800 N@3: 0.56655 N@5: 0.55658 early stop: 3\n",
            "\u001b[32m[I 210629 06:40:03 models:110]\u001b[39m 287 512 train loss: 0.0000194 valid loss: 0.2597028 P@1: 0.65000 P@3: 0.53000 P@5: 0.40600 N@3: 0.56655 N@5: 0.55476 early stop: 4\n",
            "\u001b[32m[I 210629 06:40:06 models:110]\u001b[39m 289 1024 train loss: 0.0000192 valid loss: 0.2606432 P@1: 0.65000 P@3: 0.53000 P@5: 0.40600 N@3: 0.56655 N@5: 0.55476 early stop: 5\n",
            "\u001b[32m[I 210629 06:40:09 models:110]\u001b[39m 292 512 train loss: 0.0000176 valid loss: 0.2615866 P@1: 0.65000 P@3: 0.53000 P@5: 0.40600 N@3: 0.56655 N@5: 0.55476 early stop: 6\n",
            "\u001b[32m[I 210629 06:40:12 models:110]\u001b[39m 294 1024 train loss: 0.0000157 valid loss: 0.2625213 P@1: 0.65000 P@3: 0.53000 P@5: 0.40800 N@3: 0.56717 N@5: 0.55702 early stop: 0\n",
            "\u001b[32m[I 210629 06:40:15 models:110]\u001b[39m 297 512 train loss: 0.0000146 valid loss: 0.2634431 P@1: 0.65000 P@3: 0.53000 P@5: 0.40600 N@3: 0.56717 N@5: 0.55586 early stop: 1\n",
            "\u001b[32m[I 210629 06:40:18 models:110]\u001b[39m 299 1024 train loss: 0.0000138 valid loss: 0.2643571 P@1: 0.65000 P@3: 0.53000 P@5: 0.40400 N@3: 0.56717 N@5: 0.55434 early stop: 2\n",
            "\u001b[32m[I 210629 06:40:21 models:110]\u001b[39m 302 512 train loss: 0.0000142 valid loss: 0.2652650 P@1: 0.65000 P@3: 0.53000 P@5: 0.40600 N@3: 0.56717 N@5: 0.55565 early stop: 3\n",
            "\u001b[32m[I 210629 06:40:24 models:110]\u001b[39m 304 1024 train loss: 0.0000130 valid loss: 0.2661613 P@1: 0.64000 P@3: 0.53000 P@5: 0.40600 N@3: 0.56482 N@5: 0.55331 early stop: 4\n",
            "\u001b[32m[I 210629 06:40:27 models:110]\u001b[39m 307 512 train loss: 0.0000126 valid loss: 0.2670468 P@1: 0.64000 P@3: 0.53000 P@5: 0.40600 N@3: 0.56482 N@5: 0.55331 early stop: 5\n",
            "\u001b[32m[I 210629 06:40:30 models:110]\u001b[39m 309 1024 train loss: 0.0000122 valid loss: 0.2679262 P@1: 0.64000 P@3: 0.53000 P@5: 0.40600 N@3: 0.56482 N@5: 0.55351 early stop: 6\n",
            "\u001b[32m[I 210629 06:40:33 models:110]\u001b[39m 312 512 train loss: 0.0000140 valid loss: 0.2687998 P@1: 0.64000 P@3: 0.53000 P@5: 0.40600 N@3: 0.56482 N@5: 0.55351 early stop: 7\n",
            "\u001b[32m[I 210629 06:40:36 models:110]\u001b[39m 314 1024 train loss: 0.0000119 valid loss: 0.2696662 P@1: 0.63000 P@3: 0.53000 P@5: 0.40600 N@3: 0.56309 N@5: 0.55211 early stop: 8\n",
            "\u001b[32m[I 210629 06:40:39 models:110]\u001b[39m 317 512 train loss: 0.0000126 valid loss: 0.2705292 P@1: 0.63000 P@3: 0.53000 P@5: 0.40600 N@3: 0.56309 N@5: 0.55211 early stop: 9\n",
            "\u001b[32m[I 210629 06:40:42 models:110]\u001b[39m 319 1024 train loss: 0.0000114 valid loss: 0.2713873 P@1: 0.63000 P@3: 0.53000 P@5: 0.40400 N@3: 0.56309 N@5: 0.55080 early stop: 10\n",
            "\u001b[32m[I 210629 06:40:45 models:110]\u001b[39m 322 512 train loss: 0.0000121 valid loss: 0.2722358 P@1: 0.63000 P@3: 0.53000 P@5: 0.40400 N@3: 0.56370 N@5: 0.55124 early stop: 11\n",
            "\u001b[32m[I 210629 06:40:48 models:110]\u001b[39m 324 1024 train loss: 0.0000135 valid loss: 0.2730785 P@1: 0.63000 P@3: 0.53000 P@5: 0.40400 N@3: 0.56370 N@5: 0.55124 early stop: 12\n",
            "\u001b[32m[I 210629 06:40:51 models:110]\u001b[39m 327 512 train loss: 0.0000116 valid loss: 0.2739173 P@1: 0.63000 P@3: 0.53000 P@5: 0.40400 N@3: 0.56370 N@5: 0.55104 early stop: 13\n",
            "\u001b[32m[I 210629 06:40:53 models:110]\u001b[39m 329 1024 train loss: 0.0000107 valid loss: 0.2747537 P@1: 0.63000 P@3: 0.53000 P@5: 0.40200 N@3: 0.56370 N@5: 0.54972 early stop: 14\n",
            "\u001b[32m[I 210629 06:40:57 models:110]\u001b[39m 332 512 train loss: 0.0000112 valid loss: 0.2755882 P@1: 0.63000 P@3: 0.53333 P@5: 0.40200 N@3: 0.56605 N@5: 0.54996 early stop: 15\n",
            "\u001b[32m[I 210629 06:40:59 models:110]\u001b[39m 334 1024 train loss: 0.0000152 valid loss: 0.2764133 P@1: 0.63000 P@3: 0.53333 P@5: 0.40200 N@3: 0.56605 N@5: 0.54996 early stop: 16\n",
            "\u001b[32m[I 210629 06:41:02 models:110]\u001b[39m 337 512 train loss: 0.0000118 valid loss: 0.2772252 P@1: 0.63000 P@3: 0.53333 P@5: 0.40200 N@3: 0.56605 N@5: 0.54996 early stop: 17\n",
            "\u001b[33m[W 210629 06:41:04 models:137]\u001b[39m Clipping gradients with total norm 0.00546 and max norm 0.001\n",
            "\u001b[32m[I 210629 06:41:05 models:110]\u001b[39m 339 1024 train loss: 0.0000127 valid loss: 0.2780380 P@1: 0.63000 P@3: 0.53333 P@5: 0.40200 N@3: 0.56605 N@5: 0.54996 early stop: 18\n",
            "\u001b[32m[I 210629 06:41:08 models:110]\u001b[39m 342 512 train loss: 0.0000112 valid loss: 0.2788534 P@1: 0.63000 P@3: 0.53333 P@5: 0.40200 N@3: 0.56605 N@5: 0.54996 early stop: 19\n",
            "\u001b[32m[I 210629 06:41:11 models:110]\u001b[39m 344 1024 train loss: 0.0000108 valid loss: 0.2796669 P@1: 0.63000 P@3: 0.53333 P@5: 0.40200 N@3: 0.56605 N@5: 0.54996 early stop: 20\n",
            "\u001b[32m[I 210629 06:41:14 models:110]\u001b[39m 347 512 train loss: 0.0000099 valid loss: 0.2804771 P@1: 0.63000 P@3: 0.53333 P@5: 0.40200 N@3: 0.56605 N@5: 0.54996 early stop: 21\n",
            "\u001b[33m[W 210629 06:41:15 models:137]\u001b[39m Clipping gradients with total norm 0.012 and max norm 0.00146\n",
            "\u001b[32m[I 210629 06:41:17 models:110]\u001b[39m 349 1024 train loss: 0.0000110 valid loss: 0.2812834 P@1: 0.63000 P@3: 0.53000 P@5: 0.40000 N@3: 0.56309 N@5: 0.54771 early stop: 22\n",
            "\u001b[32m[I 210629 06:41:20 models:110]\u001b[39m 352 512 train loss: 0.0000095 valid loss: 0.2820778 P@1: 0.63000 P@3: 0.53000 P@5: 0.40000 N@3: 0.56309 N@5: 0.54786 early stop: 23\n",
            "\u001b[32m[I 210629 06:41:23 models:110]\u001b[39m 354 1024 train loss: 0.0000090 valid loss: 0.2828673 P@1: 0.63000 P@3: 0.53000 P@5: 0.40000 N@3: 0.56309 N@5: 0.54786 early stop: 24\n",
            "\u001b[32m[I 210629 06:41:26 models:110]\u001b[39m 357 512 train loss: 0.0000084 valid loss: 0.2836509 P@1: 0.63000 P@3: 0.53333 P@5: 0.40000 N@3: 0.56543 N@5: 0.54818 early stop: 25\n",
            "\u001b[33m[W 210629 06:41:26 models:137]\u001b[39m Clipping gradients with total norm 0.01328 and max norm 0.00104\n",
            "\u001b[32m[I 210629 06:41:29 models:110]\u001b[39m 359 1024 train loss: 0.0000104 valid loss: 0.2844289 P@1: 0.63000 P@3: 0.53333 P@5: 0.40000 N@3: 0.56543 N@5: 0.54818 early stop: 26\n",
            "\u001b[32m[I 210629 06:41:32 models:110]\u001b[39m 362 512 train loss: 0.0000095 valid loss: 0.2852020 P@1: 0.63000 P@3: 0.53333 P@5: 0.40000 N@3: 0.56543 N@5: 0.54818 early stop: 27\n",
            "\u001b[33m[W 210629 06:41:32 models:137]\u001b[39m Clipping gradients with total norm 0.00311 and max norm 0.00055\n",
            "\u001b[32m[I 210629 06:41:35 models:110]\u001b[39m 364 1024 train loss: 0.0000103 valid loss: 0.2859709 P@1: 0.63000 P@3: 0.53333 P@5: 0.40000 N@3: 0.56543 N@5: 0.54818 early stop: 28\n",
            "\u001b[32m[I 210629 06:41:38 models:110]\u001b[39m 367 512 train loss: 0.0000079 valid loss: 0.2867353 P@1: 0.63000 P@3: 0.53000 P@5: 0.40000 N@3: 0.56309 N@5: 0.54786 early stop: 29\n",
            "\u001b[33m[W 210629 06:41:39 models:137]\u001b[39m Clipping gradients with total norm 0.00611 and max norm 0.00088\n",
            "\u001b[32m[I 210629 06:41:40 models:110]\u001b[39m 369 1024 train loss: 0.0000098 valid loss: 0.2874975 P@1: 0.63000 P@3: 0.52667 P@5: 0.40000 N@3: 0.56002 N@5: 0.54723 early stop: 30\n",
            "\u001b[32m[I 210629 06:41:44 models:110]\u001b[39m 372 512 train loss: 0.0000072 valid loss: 0.2882530 P@1: 0.63000 P@3: 0.52667 P@5: 0.40000 N@3: 0.56002 N@5: 0.54723 early stop: 31\n",
            "\u001b[32m[I 210629 06:41:46 models:110]\u001b[39m 374 1024 train loss: 0.0000068 valid loss: 0.2890036 P@1: 0.63000 P@3: 0.52667 P@5: 0.40000 N@3: 0.56002 N@5: 0.54702 early stop: 32\n",
            "\u001b[32m[I 210629 06:41:50 models:110]\u001b[39m 377 512 train loss: 0.0000067 valid loss: 0.2897513 P@1: 0.63000 P@3: 0.52667 P@5: 0.39800 N@3: 0.56002 N@5: 0.54541 early stop: 33\n",
            "\u001b[32m[I 210629 06:41:52 models:110]\u001b[39m 379 1024 train loss: 0.0000073 valid loss: 0.2904949 P@1: 0.63000 P@3: 0.53000 P@5: 0.39800 N@3: 0.56298 N@5: 0.54626 early stop: 34\n",
            "\u001b[32m[I 210629 06:41:55 models:110]\u001b[39m 382 512 train loss: 0.0000069 valid loss: 0.2912334 P@1: 0.63000 P@3: 0.53000 P@5: 0.39800 N@3: 0.56298 N@5: 0.54626 early stop: 35\n",
            "\u001b[33m[W 210629 06:41:56 models:137]\u001b[39m Clipping gradients with total norm 0.00272 and max norm 0.00048\n",
            "\u001b[32m[I 210629 06:41:58 models:110]\u001b[39m 384 1024 train loss: 0.0000074 valid loss: 0.2919679 P@1: 0.63000 P@3: 0.53000 P@5: 0.39800 N@3: 0.56360 N@5: 0.54687 early stop: 36\n",
            "\u001b[32m[I 210629 06:42:01 models:110]\u001b[39m 387 512 train loss: 0.0000069 valid loss: 0.2926961 P@1: 0.63000 P@3: 0.53000 P@5: 0.39800 N@3: 0.56360 N@5: 0.54687 early stop: 37\n",
            "\u001b[32m[I 210629 06:42:04 models:110]\u001b[39m 389 1024 train loss: 0.0000071 valid loss: 0.2934186 P@1: 0.63000 P@3: 0.53000 P@5: 0.39800 N@3: 0.56298 N@5: 0.54626 early stop: 38\n",
            "\u001b[32m[I 210629 06:42:07 models:110]\u001b[39m 392 512 train loss: 0.0000072 valid loss: 0.2941373 P@1: 0.63000 P@3: 0.53000 P@5: 0.39800 N@3: 0.56298 N@5: 0.54626 early stop: 39\n",
            "\u001b[32m[I 210629 06:42:10 models:110]\u001b[39m 394 1024 train loss: 0.0000065 valid loss: 0.2948511 P@1: 0.63000 P@3: 0.53333 P@5: 0.39600 N@3: 0.56533 N@5: 0.54468 early stop: 40\n",
            "\u001b[32m[I 210629 06:42:13 models:110]\u001b[39m 397 512 train loss: 0.0000064 valid loss: 0.2955602 P@1: 0.63000 P@3: 0.53333 P@5: 0.39600 N@3: 0.56533 N@5: 0.54468 early stop: 41\n",
            "\u001b[32m[I 210629 06:42:16 models:110]\u001b[39m 399 1024 train loss: 0.0000076 valid loss: 0.2962681 P@1: 0.63000 P@3: 0.53333 P@5: 0.39600 N@3: 0.56533 N@5: 0.54441 early stop: 42\n",
            "\u001b[33m[W 210629 06:42:17 models:137]\u001b[39m Clipping gradients with total norm 0.0047 and max norm 0.00083\n",
            "\u001b[32m[I 210629 06:42:19 models:110]\u001b[39m 402 512 train loss: 0.0000074 valid loss: 0.2969686 P@1: 0.63000 P@3: 0.53333 P@5: 0.39600 N@3: 0.56533 N@5: 0.54441 early stop: 43\n",
            "\u001b[32m[I 210629 06:42:22 models:110]\u001b[39m 404 1024 train loss: 0.0000070 valid loss: 0.2976619 P@1: 0.63000 P@3: 0.53333 P@5: 0.39600 N@3: 0.56533 N@5: 0.54441 early stop: 44\n",
            "\u001b[32m[I 210629 06:42:25 models:110]\u001b[39m 407 512 train loss: 0.0000067 valid loss: 0.2983533 P@1: 0.63000 P@3: 0.53333 P@5: 0.39600 N@3: 0.56533 N@5: 0.54441 early stop: 45\n",
            "\u001b[32m[I 210629 06:42:28 models:110]\u001b[39m 409 1024 train loss: 0.0000066 valid loss: 0.2990415 P@1: 0.63000 P@3: 0.53333 P@5: 0.39400 N@3: 0.56533 N@5: 0.54204 early stop: 46\n",
            "\u001b[32m[I 210629 06:42:31 models:110]\u001b[39m 412 512 train loss: 0.0000060 valid loss: 0.2997250 P@1: 0.63000 P@3: 0.53333 P@5: 0.39400 N@3: 0.56533 N@5: 0.54204 early stop: 47\n",
            "\u001b[32m[I 210629 06:42:34 models:110]\u001b[39m 414 1024 train loss: 0.0000055 valid loss: 0.3004060 P@1: 0.63000 P@3: 0.53333 P@5: 0.39400 N@3: 0.56533 N@5: 0.54219 early stop: 48\n",
            "\u001b[32m[I 210629 06:42:37 models:110]\u001b[39m 417 512 train loss: 0.0000057 valid loss: 0.3010855 P@1: 0.63000 P@3: 0.53000 P@5: 0.39400 N@3: 0.56298 N@5: 0.54186 early stop: 49\n",
            "\u001b[32m[I 210629 06:42:39 models:110]\u001b[39m 419 1024 train loss: 0.0000074 valid loss: 0.3017623 P@1: 0.63000 P@3: 0.52667 P@5: 0.39400 N@3: 0.56064 N@5: 0.54163 early stop: 50\n",
            "\u001b[32m[I 210629 06:42:43 main:76]\u001b[39m Finish Training\n",
            "\u001b[32m[I 210629 06:42:44 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210629 06:42:44 main:79]\u001b[39m Loading Test Set\n",
            "\u001b[32m[I 210629 06:42:44 main:83]\u001b[39m Size of Test Set: 100\n",
            "\u001b[32m[I 210629 06:42:44 main:85]\u001b[39m Predicting\n",
            "\u001b[32m[I 210629 06:42:48 main:91]\u001b[39m Finish Predicting\n",
            "Precision@1,3,5: 0.49 0.44666666666666666 0.32\n",
            "nDCG@1,3,5: 0.49 0.4654399881072021 0.449582268154886\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYbr2McO78q7"
      },
      "source": [
        "## Ablation study: Effect of adding MAG and MeSH labels to text\n",
        "\n",
        "Relevant to PeTaL Labeller Issues #53 (https://github.com/nasa-petal/PeTaL-labeller/issues/53) and #58 (https://github.com/nasa-petal/PeTaL-labeller/issues/58)\n",
        "\n",
        "Databases of papers categorise their papers differently. We investigate the effect of adding Microsoft Academic Graph (MAG) fields of study and PubMed's Medical Subject Headings (MeSH) terms, when available for each paper, as additional metadata.\n",
        "\n",
        "To turn on/off including MAG fields of study and MeSH terms, use `transform_data_PetaL.py` options `--no-mag` and `--no-mesh`, respectively.\n",
        "\n",
        "To change the train-dev-test split before processing, use `PeTaL/Split.py` options `--train TRAIN --dev DEV`, where `TRAIN` and `DEV` are between 0 and 1, and so is their sum. An 80-10-10 train-dev-test split (the default) can be explicitly invoked using `python3 PeTaL/Split.py --train 0.8 --dev 0.1`.\n",
        "\n",
        "To rotate the dataset by `N` examples before processing, use `PeTaL/Split.py` option `--skip N`. This is useful for `k`-fold cross-validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbvzgpQLdA35"
      },
      "source": [
        "### Issue 53. Smaller ablation study.\n",
        "\n",
        "This study is strictly superseded by the Issue #58 study below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwjcT9IcJG3I"
      },
      "source": [
        "#### Results with MAG labels and MeSH labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBX8dgT1JNlz"
      },
      "source": [
        "!python3 transform_data_PeTaL.py --dataset $DATASET"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TS2HGiFPJNrE",
        "outputId": "01469ab2-373b-4488-e0f6-5cdf536dc8ed"
      },
      "source": [
        "!python preprocess.py \\\n",
        "--text-path {DATASET}/train_texts.txt \\\n",
        "--label-path {DATASET}/train_labels.txt \\\n",
        "--vocab-path {DATASET}/vocab.npy \\\n",
        "--emb-path {DATASET}/emb_init.npy \\\n",
        "--w2v-model {DATASET}/{DATASET}.joint.emb \\\n",
        "\n",
        "!python preprocess.py \\\n",
        "--text-path {DATASET}/test_texts.txt \\\n",
        "--label-path {DATASET}/test_labels.txt \\\n",
        "--vocab-path {DATASET}/vocab.npy \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 210629 01:07:02 preprocess:28]\u001b[39m Vocab Size: 26834\n",
            "\u001b[32m[I 210629 01:07:02 preprocess:30]\u001b[39m Getting Dataset: PeTaL/train_texts.txt Max Length: 500\n",
            "\u001b[32m[I 210629 01:07:03 preprocess:32]\u001b[39m Size of Samples: 900\n",
            "\u001b[32m[I 210629 01:07:04 preprocess:28]\u001b[39m Vocab Size: 26834\n",
            "\u001b[32m[I 210629 01:07:04 preprocess:30]\u001b[39m Getting Dataset: PeTaL/test_texts.txt Max Length: 500\n",
            "\u001b[32m[I 210629 01:07:04 preprocess:32]\u001b[39m Size of Samples: 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnMIncviJLrO",
        "outputId": "25fffb8b-d675-40e7-992b-601c97e6c253"
      },
      "source": [
        "!PYTHONFAULTHANDLER=1 python main.py --data-cnf configure/datasets/{DATASET}.yaml --model-cnf configure/models/{MODEL}-{DATASET}.yaml --mode train --reg 1\n",
        "!PYTHONFAULTHANDLER=1 python main.py --data-cnf configure/datasets/{DATASET}.yaml --model-cnf configure/models/{MODEL}-{DATASET}.yaml --mode eval\n",
        "\n",
        "!python evaluation.py \\\n",
        "--results {DATASET}/results/{MODEL}-{DATASET}-labels.npy \\\n",
        "--targets {DATASET}/test_labels.npy \\\n",
        "--train-labels {DATASET}/train_labels.npy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 210629 01:07:08 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210629 01:07:08 main:35]\u001b[39m Loading Training and Validation Set\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['absorb_and/or_filter_solids', 'chemically_break_down_inorganic_compounds', 'detox/purify', 'manage_environmental_disturbances_in_a_community', 'protect_from_fire', 'protect_from_gases'] will be ignored\n",
            "  .format(sorted(unknown, key=str)))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['protect_from_gases'] will be ignored\n",
            "  .format(sorted(unknown, key=str)))\n",
            "\u001b[32m[I 210629 01:07:08 main:47]\u001b[39m Number of Labels: 124\n",
            "\u001b[32m[I 210629 01:07:08 main:48]\u001b[39m Size of Training Set: 800\n",
            "\u001b[32m[I 210629 01:07:08 main:49]\u001b[39m Size of Validation Set: 100\n",
            "\u001b[32m[I 210629 01:07:08 main:66]\u001b[39m Number of Edges: 101\n",
            "\u001b[32m[I 210629 01:07:08 main:68]\u001b[39m Training\n",
            "\u001b[32m[I 210629 01:07:14 models:110]\u001b[39m 2 512 train loss: 0.2721960 valid loss: 0.1478169 P@1: 0.30000 P@3: 0.17333 P@5: 0.13400 N@3: 0.20418 N@5: 0.19661 early stop: 0\n",
            "\u001b[32m[I 210629 01:07:15 models:142]\u001b[39m SWA Initializing\n",
            "\u001b[32m[I 210629 01:07:16 models:110]\u001b[39m 4 1024 train loss: 0.1486002 valid loss: 0.1413299 P@1: 0.30000 P@3: 0.18333 P@5: 0.17200 N@3: 0.20620 N@5: 0.22358 early stop: 0\n",
            "\u001b[32m[I 210629 01:07:18 models:110]\u001b[39m 7 512 train loss: 0.1418496 valid loss: 0.1394818 P@1: 0.30000 P@3: 0.22000 P@5: 0.19000 N@3: 0.23201 N@5: 0.24169 early stop: 0\n",
            "\u001b[32m[I 210629 01:07:20 models:110]\u001b[39m 9 1024 train loss: 0.1369330 valid loss: 0.1381333 P@1: 0.30000 P@3: 0.20667 P@5: 0.18200 N@3: 0.22324 N@5: 0.23560 early stop: 1\n",
            "\u001b[32m[I 210629 01:07:22 models:110]\u001b[39m 12 512 train loss: 0.1295622 valid loss: 0.1372430 P@1: 0.30000 P@3: 0.20667 P@5: 0.21200 N@3: 0.23000 N@5: 0.26558 early stop: 0\n",
            "\u001b[32m[I 210629 01:07:24 models:110]\u001b[39m 14 1024 train loss: 0.1175149 valid loss: 0.1364616 P@1: 0.37000 P@3: 0.22667 P@5: 0.21000 N@3: 0.25497 N@5: 0.27300 early stop: 0\n",
            "\u001b[32m[I 210629 01:07:26 models:110]\u001b[39m 17 512 train loss: 0.1036124 valid loss: 0.1355893 P@1: 0.40000 P@3: 0.24667 P@5: 0.21800 N@3: 0.27548 N@5: 0.28783 early stop: 0\n",
            "\u001b[32m[I 210629 01:07:28 models:110]\u001b[39m 19 1024 train loss: 0.0879972 valid loss: 0.1347124 P@1: 0.41000 P@3: 0.25667 P@5: 0.21800 N@3: 0.28671 N@5: 0.29448 early stop: 0\n",
            "\u001b[32m[I 210629 01:07:31 models:110]\u001b[39m 22 512 train loss: 0.0727948 valid loss: 0.1335702 P@1: 0.43000 P@3: 0.28000 P@5: 0.22800 N@3: 0.31212 N@5: 0.30992 early stop: 0\n",
            "\u001b[32m[I 210629 01:07:33 models:110]\u001b[39m 24 1024 train loss: 0.0600400 valid loss: 0.1324128 P@1: 0.45000 P@3: 0.28000 P@5: 0.23600 N@3: 0.31620 N@5: 0.32151 early stop: 0\n",
            "\u001b[32m[I 210629 01:07:35 models:110]\u001b[39m 27 512 train loss: 0.0512611 valid loss: 0.1313292 P@1: 0.45000 P@3: 0.29000 P@5: 0.24600 N@3: 0.32263 N@5: 0.33293 early stop: 0\n",
            "\u001b[32m[I 210629 01:07:37 models:110]\u001b[39m 29 1024 train loss: 0.0448237 valid loss: 0.1299615 P@1: 0.48000 P@3: 0.30333 P@5: 0.25600 N@3: 0.33916 N@5: 0.34801 early stop: 0\n",
            "\u001b[32m[I 210629 01:07:39 models:110]\u001b[39m 32 512 train loss: 0.0398703 valid loss: 0.1287228 P@1: 0.52000 P@3: 0.33000 P@5: 0.26400 N@3: 0.37214 N@5: 0.36809 early stop: 0\n",
            "\u001b[32m[I 210629 01:07:41 models:110]\u001b[39m 34 1024 train loss: 0.0325480 valid loss: 0.1277203 P@1: 0.55000 P@3: 0.33000 P@5: 0.27400 N@3: 0.37937 N@5: 0.38221 early stop: 0\n",
            "\u001b[32m[I 210629 01:07:43 models:110]\u001b[39m 37 512 train loss: 0.0295774 valid loss: 0.1265612 P@1: 0.54000 P@3: 0.34333 P@5: 0.29200 N@3: 0.38886 N@5: 0.40139 early stop: 0\n",
            "\u001b[32m[I 210629 01:07:45 models:110]\u001b[39m 39 1024 train loss: 0.0252028 valid loss: 0.1255425 P@1: 0.56000 P@3: 0.35333 P@5: 0.30200 N@3: 0.40201 N@5: 0.41558 early stop: 0\n",
            "\u001b[32m[I 210629 01:07:48 models:110]\u001b[39m 42 512 train loss: 0.0221400 valid loss: 0.1247626 P@1: 0.56000 P@3: 0.37000 P@5: 0.31200 N@3: 0.41417 N@5: 0.42457 early stop: 0\n",
            "\u001b[32m[I 210629 01:07:50 models:110]\u001b[39m 44 1024 train loss: 0.0198145 valid loss: 0.1240312 P@1: 0.56000 P@3: 0.37333 P@5: 0.32000 N@3: 0.41836 N@5: 0.43373 early stop: 0\n",
            "\u001b[32m[I 210629 01:07:52 models:110]\u001b[39m 47 512 train loss: 0.0197847 valid loss: 0.1235152 P@1: 0.55000 P@3: 0.38333 P@5: 0.32400 N@3: 0.42365 N@5: 0.43654 early stop: 0\n",
            "\u001b[32m[I 210629 01:07:54 models:110]\u001b[39m 49 1024 train loss: 0.0182630 valid loss: 0.1233658 P@1: 0.54000 P@3: 0.39000 P@5: 0.32400 N@3: 0.42906 N@5: 0.43816 early stop: 0\n",
            "\u001b[32m[I 210629 01:07:56 models:110]\u001b[39m 52 512 train loss: 0.0142049 valid loss: 0.1229262 P@1: 0.54000 P@3: 0.40667 P@5: 0.32800 N@3: 0.44141 N@5: 0.44328 early stop: 0\n",
            "\u001b[32m[I 210629 01:07:58 models:110]\u001b[39m 54 1024 train loss: 0.0134355 valid loss: 0.1230175 P@1: 0.54000 P@3: 0.42000 P@5: 0.33400 N@3: 0.45203 N@5: 0.44971 early stop: 0\n",
            "\u001b[32m[I 210629 01:08:00 models:110]\u001b[39m 57 512 train loss: 0.0142870 valid loss: 0.1230574 P@1: 0.54000 P@3: 0.43667 P@5: 0.33600 N@3: 0.46499 N@5: 0.45390 early stop: 0\n",
            "\u001b[32m[I 210629 01:08:02 models:110]\u001b[39m 59 1024 train loss: 0.0145089 valid loss: 0.1234713 P@1: 0.55000 P@3: 0.44333 P@5: 0.34000 N@3: 0.47520 N@5: 0.46218 early stop: 0\n",
            "\u001b[32m[I 210629 01:08:05 models:110]\u001b[39m 62 512 train loss: 0.0139847 valid loss: 0.1240110 P@1: 0.55000 P@3: 0.44667 P@5: 0.34200 N@3: 0.47816 N@5: 0.46552 early stop: 0\n",
            "\u001b[32m[I 210629 01:08:07 models:110]\u001b[39m 64 1024 train loss: 0.0111858 valid loss: 0.1247796 P@1: 0.56000 P@3: 0.44333 P@5: 0.34400 N@3: 0.47755 N@5: 0.46880 early stop: 0\n",
            "\u001b[32m[I 210629 01:08:09 models:110]\u001b[39m 67 512 train loss: 0.0090490 valid loss: 0.1254585 P@1: 0.57000 P@3: 0.44667 P@5: 0.34600 N@3: 0.48101 N@5: 0.47115 early stop: 0\n",
            "\u001b[32m[I 210629 01:08:11 models:110]\u001b[39m 69 1024 train loss: 0.0073973 valid loss: 0.1262675 P@1: 0.56000 P@3: 0.45333 P@5: 0.34400 N@3: 0.48397 N@5: 0.46775 early stop: 1\n",
            "\u001b[32m[I 210629 01:08:13 models:110]\u001b[39m 72 512 train loss: 0.0066892 valid loss: 0.1272054 P@1: 0.57000 P@3: 0.45667 P@5: 0.34600 N@3: 0.48797 N@5: 0.47118 early stop: 0\n",
            "\u001b[32m[I 210629 01:08:15 models:110]\u001b[39m 74 1024 train loss: 0.0059513 valid loss: 0.1282466 P@1: 0.57000 P@3: 0.46000 P@5: 0.34800 N@3: 0.49093 N@5: 0.47369 early stop: 0\n",
            "\u001b[32m[I 210629 01:08:17 models:110]\u001b[39m 77 512 train loss: 0.0053041 valid loss: 0.1294657 P@1: 0.57000 P@3: 0.46667 P@5: 0.35000 N@3: 0.49808 N@5: 0.47712 early stop: 0\n",
            "\u001b[32m[I 210629 01:08:19 models:110]\u001b[39m 79 1024 train loss: 0.0050685 valid loss: 0.1307840 P@1: 0.57000 P@3: 0.46667 P@5: 0.35400 N@3: 0.49808 N@5: 0.48096 early stop: 0\n",
            "\u001b[32m[I 210629 01:08:21 models:110]\u001b[39m 82 512 train loss: 0.0061452 valid loss: 0.1319605 P@1: 0.57000 P@3: 0.47000 P@5: 0.35400 N@3: 0.50043 N@5: 0.48136 early stop: 0\n",
            "\u001b[32m[I 210629 01:08:23 models:110]\u001b[39m 84 1024 train loss: 0.0111058 valid loss: 0.1333451 P@1: 0.57000 P@3: 0.47333 P@5: 0.35600 N@3: 0.50269 N@5: 0.48282 early stop: 0\n",
            "\u001b[32m[I 210629 01:08:26 models:110]\u001b[39m 87 512 train loss: 0.0116001 valid loss: 0.1343890 P@1: 0.58000 P@3: 0.47333 P@5: 0.35600 N@3: 0.50522 N@5: 0.48585 early stop: 0\n",
            "\u001b[32m[I 210629 01:08:28 models:110]\u001b[39m 89 1024 train loss: 0.0122614 valid loss: 0.1352792 P@1: 0.59000 P@3: 0.46667 P@5: 0.35800 N@3: 0.50093 N@5: 0.48766 early stop: 0\n",
            "\u001b[32m[I 210629 01:08:30 models:110]\u001b[39m 92 512 train loss: 0.0087075 valid loss: 0.1362358 P@1: 0.58000 P@3: 0.47000 P@5: 0.35800 N@3: 0.50093 N@5: 0.48575 early stop: 1\n",
            "\u001b[32m[I 210629 01:08:32 models:110]\u001b[39m 94 1024 train loss: 0.0073356 valid loss: 0.1372537 P@1: 0.58000 P@3: 0.46667 P@5: 0.36200 N@3: 0.49981 N@5: 0.48933 early stop: 0\n",
            "\u001b[32m[I 210629 01:08:34 models:110]\u001b[39m 97 512 train loss: 0.0050632 valid loss: 0.1383530 P@1: 0.59000 P@3: 0.46667 P@5: 0.36800 N@3: 0.50093 N@5: 0.49535 early stop: 0\n",
            "\u001b[32m[I 210629 01:08:36 models:110]\u001b[39m 99 1024 train loss: 0.0052619 valid loss: 0.1394829 P@1: 0.59000 P@3: 0.46333 P@5: 0.37400 N@3: 0.49797 N@5: 0.49857 early stop: 0\n",
            "\u001b[32m[I 210629 01:08:38 models:110]\u001b[39m 102 512 train loss: 0.0045676 valid loss: 0.1406048 P@1: 0.59000 P@3: 0.46000 P@5: 0.37400 N@3: 0.49501 N@5: 0.49766 early stop: 1\n",
            "\u001b[32m[I 210629 01:08:40 models:110]\u001b[39m 104 1024 train loss: 0.0038373 valid loss: 0.1418605 P@1: 0.59000 P@3: 0.46000 P@5: 0.37800 N@3: 0.49562 N@5: 0.50174 early stop: 0\n",
            "\u001b[32m[I 210629 01:08:43 models:110]\u001b[39m 107 512 train loss: 0.0036881 valid loss: 0.1433162 P@1: 0.59000 P@3: 0.45667 P@5: 0.38000 N@3: 0.49266 N@5: 0.50218 early stop: 0\n",
            "\u001b[32m[I 210629 01:08:45 models:110]\u001b[39m 109 1024 train loss: 0.0031904 valid loss: 0.1446484 P@1: 0.59000 P@3: 0.46000 P@5: 0.37800 N@3: 0.49642 N@5: 0.50255 early stop: 0\n",
            "\u001b[32m[I 210629 01:08:47 models:110]\u001b[39m 112 512 train loss: 0.0026951 valid loss: 0.1461411 P@1: 0.59000 P@3: 0.45667 P@5: 0.37400 N@3: 0.49408 N@5: 0.49939 early stop: 1\n",
            "\u001b[32m[I 210629 01:08:49 models:110]\u001b[39m 114 1024 train loss: 0.0037843 valid loss: 0.1476381 P@1: 0.59000 P@3: 0.45667 P@5: 0.37400 N@3: 0.49346 N@5: 0.49844 early stop: 2\n",
            "\u001b[32m[I 210629 01:08:51 models:110]\u001b[39m 117 512 train loss: 0.0031686 valid loss: 0.1490292 P@1: 0.59000 P@3: 0.46000 P@5: 0.37400 N@3: 0.49581 N@5: 0.49868 early stop: 3\n",
            "\u001b[32m[I 210629 01:08:53 models:110]\u001b[39m 119 1024 train loss: 0.0022820 valid loss: 0.1504841 P@1: 0.59000 P@3: 0.45667 P@5: 0.37800 N@3: 0.49346 N@5: 0.50159 early stop: 4\n",
            "\u001b[32m[I 210629 01:08:55 models:110]\u001b[39m 122 512 train loss: 0.0019533 valid loss: 0.1518091 P@1: 0.59000 P@3: 0.45667 P@5: 0.37800 N@3: 0.49346 N@5: 0.50230 early stop: 5\n",
            "\u001b[32m[I 210629 01:08:57 models:110]\u001b[39m 124 1024 train loss: 0.0021497 valid loss: 0.1531533 P@1: 0.58000 P@3: 0.46000 P@5: 0.38000 N@3: 0.49531 N@5: 0.50374 early stop: 0\n",
            "\u001b[32m[I 210629 01:08:59 models:110]\u001b[39m 127 512 train loss: 0.0016399 valid loss: 0.1545016 P@1: 0.58000 P@3: 0.46000 P@5: 0.38000 N@3: 0.49531 N@5: 0.50341 early stop: 1\n",
            "\u001b[32m[I 210629 01:09:01 models:110]\u001b[39m 129 1024 train loss: 0.0018706 valid loss: 0.1557756 P@1: 0.58000 P@3: 0.46000 P@5: 0.38000 N@3: 0.49592 N@5: 0.50300 early stop: 2\n",
            "\u001b[32m[I 210629 01:09:03 models:110]\u001b[39m 132 512 train loss: 0.0020058 valid loss: 0.1571234 P@1: 0.59000 P@3: 0.46333 P@5: 0.38200 N@3: 0.49939 N@5: 0.50594 early stop: 0\n",
            "\u001b[32m[I 210629 01:09:05 models:110]\u001b[39m 134 1024 train loss: 0.0028732 valid loss: 0.1586189 P@1: 0.59000 P@3: 0.46333 P@5: 0.38200 N@3: 0.49939 N@5: 0.50594 early stop: 1\n",
            "\u001b[32m[I 210629 01:09:08 models:110]\u001b[39m 137 512 train loss: 0.0032026 valid loss: 0.1602304 P@1: 0.59000 P@3: 0.46333 P@5: 0.38200 N@3: 0.50000 N@5: 0.50665 early stop: 0\n",
            "\u001b[32m[I 210629 01:09:10 models:110]\u001b[39m 139 1024 train loss: 0.0034705 valid loss: 0.1617533 P@1: 0.59000 P@3: 0.46333 P@5: 0.38400 N@3: 0.50000 N@5: 0.50884 early stop: 0\n",
            "\u001b[32m[I 210629 01:09:12 models:110]\u001b[39m 142 512 train loss: 0.0024447 valid loss: 0.1632381 P@1: 0.59000 P@3: 0.45667 P@5: 0.38200 N@3: 0.49592 N@5: 0.50684 early stop: 1\n",
            "\u001b[32m[I 210629 01:09:14 models:110]\u001b[39m 144 1024 train loss: 0.0062996 valid loss: 0.1645429 P@1: 0.59000 P@3: 0.46000 P@5: 0.38200 N@3: 0.49827 N@5: 0.50717 early stop: 2\n",
            "\u001b[32m[I 210629 01:09:16 models:110]\u001b[39m 147 512 train loss: 0.0096567 valid loss: 0.1657923 P@1: 0.59000 P@3: 0.45333 P@5: 0.38400 N@3: 0.49419 N@5: 0.50883 early stop: 3\n",
            "\u001b[32m[I 210629 01:09:18 models:110]\u001b[39m 149 1024 train loss: 0.0130361 valid loss: 0.1668181 P@1: 0.59000 P@3: 0.45333 P@5: 0.38400 N@3: 0.49480 N@5: 0.50909 early stop: 0\n",
            "\u001b[32m[I 210629 01:09:20 models:110]\u001b[39m 152 512 train loss: 0.0163317 valid loss: 0.1677363 P@1: 0.58000 P@3: 0.45333 P@5: 0.38400 N@3: 0.49184 N@5: 0.50667 early stop: 1\n",
            "\u001b[32m[I 210629 01:09:22 models:110]\u001b[39m 154 1024 train loss: 0.0127155 valid loss: 0.1683189 P@1: 0.58000 P@3: 0.45667 P@5: 0.38400 N@3: 0.49480 N@5: 0.50876 early stop: 2\n",
            "\u001b[32m[I 210629 01:09:24 models:110]\u001b[39m 157 512 train loss: 0.0085989 valid loss: 0.1689432 P@1: 0.58000 P@3: 0.46000 P@5: 0.38600 N@3: 0.49715 N@5: 0.51004 early stop: 0\n",
            "\u001b[32m[I 210629 01:09:26 models:110]\u001b[39m 159 1024 train loss: 0.0061947 valid loss: 0.1695132 P@1: 0.59000 P@3: 0.46333 P@5: 0.38800 N@3: 0.50123 N@5: 0.51288 early stop: 0\n",
            "\u001b[32m[I 210629 01:09:29 models:110]\u001b[39m 162 512 train loss: 0.0031598 valid loss: 0.1702041 P@1: 0.59000 P@3: 0.46333 P@5: 0.39000 N@3: 0.50123 N@5: 0.51418 early stop: 0\n",
            "\u001b[32m[I 210629 01:09:31 models:110]\u001b[39m 164 1024 train loss: 0.0018325 valid loss: 0.1709337 P@1: 0.59000 P@3: 0.46333 P@5: 0.39000 N@3: 0.50061 N@5: 0.51412 early stop: 1\n",
            "\u001b[32m[I 210629 01:09:33 models:110]\u001b[39m 167 512 train loss: 0.0008540 valid loss: 0.1717234 P@1: 0.59000 P@3: 0.46333 P@5: 0.38800 N@3: 0.50123 N@5: 0.51401 early stop: 2\n",
            "\u001b[32m[I 210629 01:09:35 models:110]\u001b[39m 169 1024 train loss: 0.0006907 valid loss: 0.1725122 P@1: 0.59000 P@3: 0.47000 P@5: 0.38600 N@3: 0.50654 N@5: 0.51343 early stop: 3\n",
            "\u001b[32m[I 210629 01:09:37 models:110]\u001b[39m 172 512 train loss: 0.0004204 valid loss: 0.1733493 P@1: 0.59000 P@3: 0.47000 P@5: 0.38400 N@3: 0.50592 N@5: 0.51132 early stop: 4\n",
            "\u001b[32m[I 210629 01:09:39 models:110]\u001b[39m 174 1024 train loss: 0.0006106 valid loss: 0.1742164 P@1: 0.58000 P@3: 0.47000 P@5: 0.38400 N@3: 0.50419 N@5: 0.50972 early stop: 5\n",
            "\u001b[32m[I 210629 01:09:41 models:110]\u001b[39m 177 512 train loss: 0.0012949 valid loss: 0.1751206 P@1: 0.58000 P@3: 0.47333 P@5: 0.38600 N@3: 0.50715 N@5: 0.51186 early stop: 6\n",
            "\u001b[32m[I 210629 01:09:43 models:110]\u001b[39m 179 1024 train loss: 0.0004925 valid loss: 0.1760050 P@1: 0.58000 P@3: 0.47333 P@5: 0.39000 N@3: 0.50715 N@5: 0.51534 early stop: 0\n",
            "\u001b[32m[I 210629 01:09:45 models:110]\u001b[39m 182 512 train loss: 0.0004140 valid loss: 0.1769011 P@1: 0.58000 P@3: 0.47333 P@5: 0.39000 N@3: 0.50715 N@5: 0.51534 early stop: 1\n",
            "\u001b[32m[I 210629 01:09:47 models:110]\u001b[39m 184 1024 train loss: 0.0003092 valid loss: 0.1778090 P@1: 0.59000 P@3: 0.47333 P@5: 0.39400 N@3: 0.50941 N@5: 0.52073 early stop: 0\n",
            "\u001b[33m[W 210629 01:09:48 models:137]\u001b[39m Clipping gradients with total norm 0.14008 and max norm 0.01449\n",
            "\u001b[32m[I 210629 01:09:49 models:110]\u001b[39m 187 512 train loss: 0.0005733 valid loss: 0.1787692 P@1: 0.59000 P@3: 0.47667 P@5: 0.39200 N@3: 0.51176 N@5: 0.51921 early stop: 1\n",
            "\u001b[32m[I 210629 01:09:51 models:110]\u001b[39m 189 1024 train loss: 0.0002945 valid loss: 0.1796935 P@1: 0.60000 P@3: 0.47667 P@5: 0.39000 N@3: 0.51288 N@5: 0.51884 early stop: 2\n",
            "\u001b[32m[I 210629 01:09:54 models:110]\u001b[39m 192 512 train loss: 0.0002609 valid loss: 0.1805854 P@1: 0.60000 P@3: 0.47667 P@5: 0.39000 N@3: 0.51288 N@5: 0.51884 early stop: 3\n",
            "\u001b[32m[I 210629 01:09:56 models:110]\u001b[39m 194 1024 train loss: 0.0002299 valid loss: 0.1815351 P@1: 0.60000 P@3: 0.47667 P@5: 0.39000 N@3: 0.51288 N@5: 0.51884 early stop: 4\n",
            "\u001b[32m[I 210629 01:09:58 models:110]\u001b[39m 197 512 train loss: 0.0001634 valid loss: 0.1824706 P@1: 0.60000 P@3: 0.47667 P@5: 0.39000 N@3: 0.51288 N@5: 0.51884 early stop: 5\n",
            "\u001b[32m[I 210629 01:10:00 models:110]\u001b[39m 199 1024 train loss: 0.0001929 valid loss: 0.1833739 P@1: 0.60000 P@3: 0.47667 P@5: 0.39000 N@3: 0.51288 N@5: 0.51901 early stop: 6\n",
            "\u001b[32m[I 210629 01:10:02 models:110]\u001b[39m 202 512 train loss: 0.0002478 valid loss: 0.1842842 P@1: 0.60000 P@3: 0.47667 P@5: 0.39000 N@3: 0.51288 N@5: 0.51918 early stop: 7\n",
            "\u001b[32m[I 210629 01:10:04 models:110]\u001b[39m 204 1024 train loss: 0.0001747 valid loss: 0.1851838 P@1: 0.60000 P@3: 0.47667 P@5: 0.39000 N@3: 0.51288 N@5: 0.51918 early stop: 8\n",
            "\u001b[32m[I 210629 01:10:06 models:110]\u001b[39m 207 512 train loss: 0.0001864 valid loss: 0.1861010 P@1: 0.60000 P@3: 0.48000 P@5: 0.39000 N@3: 0.51522 N@5: 0.51951 early stop: 9\n",
            "\u001b[32m[I 210629 01:10:08 models:110]\u001b[39m 209 1024 train loss: 0.0001561 valid loss: 0.1870067 P@1: 0.60000 P@3: 0.49000 P@5: 0.39000 N@3: 0.52226 N@5: 0.52028 early stop: 10\n",
            "\u001b[32m[I 210629 01:10:10 models:110]\u001b[39m 212 512 train loss: 0.0001546 valid loss: 0.1878867 P@1: 0.60000 P@3: 0.49000 P@5: 0.39200 N@3: 0.52226 N@5: 0.52292 early stop: 0\n",
            "\u001b[32m[I 210629 01:10:12 models:110]\u001b[39m 214 1024 train loss: 0.0001598 valid loss: 0.1887644 P@1: 0.60000 P@3: 0.49000 P@5: 0.39400 N@3: 0.52226 N@5: 0.52443 early stop: 0\n",
            "\u001b[32m[I 210629 01:10:14 models:110]\u001b[39m 217 512 train loss: 0.0001287 valid loss: 0.1896648 P@1: 0.60000 P@3: 0.49000 P@5: 0.39200 N@3: 0.52226 N@5: 0.52312 early stop: 1\n",
            "\u001b[32m[I 210629 01:10:16 models:110]\u001b[39m 219 1024 train loss: 0.0001834 valid loss: 0.1905510 P@1: 0.60000 P@3: 0.49667 P@5: 0.39200 N@3: 0.52829 N@5: 0.52433 early stop: 2\n",
            "\u001b[32m[I 210629 01:10:19 models:110]\u001b[39m 222 512 train loss: 0.0001588 valid loss: 0.1914185 P@1: 0.60000 P@3: 0.49667 P@5: 0.39200 N@3: 0.52829 N@5: 0.52433 early stop: 3\n",
            "\u001b[32m[I 210629 01:10:21 models:110]\u001b[39m 224 1024 train loss: 0.0001908 valid loss: 0.1922713 P@1: 0.60000 P@3: 0.49667 P@5: 0.39200 N@3: 0.52952 N@5: 0.52553 early stop: 0\n",
            "\u001b[33m[W 210629 01:10:21 models:137]\u001b[39m Clipping gradients with total norm 0.09081 and max norm 0.01065\n",
            "\u001b[32m[I 210629 01:10:23 models:110]\u001b[39m 227 512 train loss: 0.0005044 valid loss: 0.1931336 P@1: 0.60000 P@3: 0.49333 P@5: 0.39200 N@3: 0.52717 N@5: 0.52463 early stop: 1\n",
            "\u001b[32m[I 210629 01:10:25 models:110]\u001b[39m 229 1024 train loss: 0.0001637 valid loss: 0.1940354 P@1: 0.60000 P@3: 0.49333 P@5: 0.39200 N@3: 0.52717 N@5: 0.52454 early stop: 2\n",
            "\u001b[33m[W 210629 01:10:26 models:137]\u001b[39m Clipping gradients with total norm 0.08871 and max norm 0.01356\n",
            "\u001b[32m[I 210629 01:10:27 models:110]\u001b[39m 232 512 train loss: 0.0004192 valid loss: 0.1948990 P@1: 0.60000 P@3: 0.49333 P@5: 0.39200 N@3: 0.52717 N@5: 0.52454 early stop: 3\n",
            "\u001b[32m[I 210629 01:10:29 models:110]\u001b[39m 234 1024 train loss: 0.0002665 valid loss: 0.1957318 P@1: 0.60000 P@3: 0.49333 P@5: 0.39200 N@3: 0.52717 N@5: 0.52454 early stop: 4\n",
            "\u001b[32m[I 210629 01:10:31 models:110]\u001b[39m 237 512 train loss: 0.0002502 valid loss: 0.1965578 P@1: 0.61000 P@3: 0.49667 P@5: 0.39200 N@3: 0.53125 N@5: 0.52626 early stop: 0\n",
            "\u001b[32m[I 210629 01:10:33 models:110]\u001b[39m 239 1024 train loss: 0.0001575 valid loss: 0.1973985 P@1: 0.61000 P@3: 0.49667 P@5: 0.39000 N@3: 0.53186 N@5: 0.52525 early stop: 1\n",
            "\u001b[32m[I 210629 01:10:35 models:110]\u001b[39m 242 512 train loss: 0.0001803 valid loss: 0.1982201 P@1: 0.61000 P@3: 0.49667 P@5: 0.39400 N@3: 0.53186 N@5: 0.52838 early stop: 0\n",
            "\u001b[32m[I 210629 01:10:37 models:110]\u001b[39m 244 1024 train loss: 0.0001245 valid loss: 0.1990368 P@1: 0.61000 P@3: 0.49667 P@5: 0.39400 N@3: 0.53248 N@5: 0.52889 early stop: 0\n",
            "\u001b[32m[I 210629 01:10:39 models:110]\u001b[39m 247 512 train loss: 0.0001610 valid loss: 0.1998663 P@1: 0.61000 P@3: 0.49667 P@5: 0.39400 N@3: 0.53186 N@5: 0.52827 early stop: 1\n",
            "\u001b[33m[W 210629 01:10:40 models:137]\u001b[39m Clipping gradients with total norm 0.06792 and max norm 0.01063\n",
            "\u001b[32m[I 210629 01:10:41 models:110]\u001b[39m 249 1024 train loss: 0.0002932 valid loss: 0.2006655 P@1: 0.61000 P@3: 0.49667 P@5: 0.39400 N@3: 0.53186 N@5: 0.52827 early stop: 2\n",
            "\u001b[32m[I 210629 01:10:44 models:110]\u001b[39m 252 512 train loss: 0.0002310 valid loss: 0.2014787 P@1: 0.61000 P@3: 0.50000 P@5: 0.39400 N@3: 0.53421 N@5: 0.52851 early stop: 3\n",
            "\u001b[33m[W 210629 01:10:44 models:137]\u001b[39m Clipping gradients with total norm 0.09682 and max norm 0.00532\n",
            "\u001b[32m[I 210629 01:10:45 models:110]\u001b[39m 254 1024 train loss: 0.0007476 valid loss: 0.2023011 P@1: 0.61000 P@3: 0.50333 P@5: 0.39400 N@3: 0.53656 N@5: 0.52874 early stop: 4\n",
            "\u001b[32m[I 210629 01:10:48 models:110]\u001b[39m 257 512 train loss: 0.0001506 valid loss: 0.2031014 P@1: 0.61000 P@3: 0.50333 P@5: 0.39400 N@3: 0.53656 N@5: 0.52874 early stop: 5\n",
            "\u001b[32m[I 210629 01:10:50 models:110]\u001b[39m 259 1024 train loss: 0.0001787 valid loss: 0.2038896 P@1: 0.61000 P@3: 0.50333 P@5: 0.39600 N@3: 0.53656 N@5: 0.53056 early stop: 0\n",
            "\u001b[33m[W 210629 01:10:50 models:137]\u001b[39m Clipping gradients with total norm 0.05999 and max norm 0.0079\n",
            "\u001b[32m[I 210629 01:10:52 models:110]\u001b[39m 262 512 train loss: 0.0003190 valid loss: 0.2046659 P@1: 0.61000 P@3: 0.50667 P@5: 0.39600 N@3: 0.53952 N@5: 0.53133 early stop: 0\n",
            "\u001b[32m[I 210629 01:10:54 models:110]\u001b[39m 264 1024 train loss: 0.0001927 valid loss: 0.2054316 P@1: 0.60000 P@3: 0.50667 P@5: 0.39600 N@3: 0.53779 N@5: 0.52960 early stop: 1\n",
            "\u001b[32m[I 210629 01:10:56 models:110]\u001b[39m 267 512 train loss: 0.0001458 valid loss: 0.2061943 P@1: 0.60000 P@3: 0.50667 P@5: 0.39400 N@3: 0.53717 N@5: 0.52767 early stop: 2\n",
            "\u001b[32m[I 210629 01:10:58 models:110]\u001b[39m 269 1024 train loss: 0.0004947 valid loss: 0.2069596 P@1: 0.60000 P@3: 0.50667 P@5: 0.39600 N@3: 0.53717 N@5: 0.52898 early stop: 3\n",
            "\u001b[32m[I 210629 01:11:00 models:110]\u001b[39m 272 512 train loss: 0.0003244 valid loss: 0.2077227 P@1: 0.60000 P@3: 0.50667 P@5: 0.39600 N@3: 0.53717 N@5: 0.52898 early stop: 4\n",
            "\u001b[32m[I 210629 01:11:02 models:110]\u001b[39m 274 1024 train loss: 0.0001891 valid loss: 0.2084765 P@1: 0.60000 P@3: 0.50667 P@5: 0.39600 N@3: 0.53717 N@5: 0.52898 early stop: 5\n",
            "\u001b[32m[I 210629 01:11:04 models:110]\u001b[39m 277 512 train loss: 0.0001712 valid loss: 0.2092375 P@1: 0.60000 P@3: 0.50333 P@5: 0.39600 N@3: 0.53483 N@5: 0.52871 early stop: 6\n",
            "\u001b[32m[I 210629 01:11:06 models:110]\u001b[39m 279 1024 train loss: 0.0001022 valid loss: 0.2100069 P@1: 0.60000 P@3: 0.51000 P@5: 0.39600 N@3: 0.53952 N@5: 0.52922 early stop: 7\n",
            "\u001b[32m[I 210629 01:11:08 models:110]\u001b[39m 282 512 train loss: 0.0001480 valid loss: 0.2107314 P@1: 0.59000 P@3: 0.51000 P@5: 0.39600 N@3: 0.53779 N@5: 0.52796 early stop: 8\n",
            "\u001b[32m[I 210629 01:11:10 models:110]\u001b[39m 284 1024 train loss: 0.0002962 valid loss: 0.2114735 P@1: 0.59000 P@3: 0.51000 P@5: 0.39600 N@3: 0.53779 N@5: 0.52796 early stop: 9\n",
            "\u001b[32m[I 210629 01:11:12 models:110]\u001b[39m 287 512 train loss: 0.0002818 valid loss: 0.2122051 P@1: 0.59000 P@3: 0.51000 P@5: 0.39600 N@3: 0.53779 N@5: 0.52796 early stop: 10\n",
            "\u001b[33m[W 210629 01:11:14 models:137]\u001b[39m Clipping gradients with total norm 0.05948 and max norm 0.00792\n",
            "\u001b[32m[I 210629 01:11:14 models:110]\u001b[39m 289 1024 train loss: 0.0002754 valid loss: 0.2129074 P@1: 0.59000 P@3: 0.51000 P@5: 0.39800 N@3: 0.53779 N@5: 0.52928 early stop: 11\n",
            "\u001b[32m[I 210629 01:11:17 models:110]\u001b[39m 292 512 train loss: 0.0002026 valid loss: 0.2136131 P@1: 0.59000 P@3: 0.51000 P@5: 0.39800 N@3: 0.53779 N@5: 0.52928 early stop: 12\n",
            "\u001b[32m[I 210629 01:11:18 models:110]\u001b[39m 294 1024 train loss: 0.0001220 valid loss: 0.2143517 P@1: 0.58000 P@3: 0.51000 P@5: 0.39800 N@3: 0.53605 N@5: 0.52802 early stop: 13\n",
            "\u001b[32m[I 210629 01:11:21 models:110]\u001b[39m 297 512 train loss: 0.0001452 valid loss: 0.2150678 P@1: 0.58000 P@3: 0.51000 P@5: 0.39800 N@3: 0.53605 N@5: 0.52802 early stop: 14\n",
            "\u001b[33m[W 210629 01:11:22 models:137]\u001b[39m Clipping gradients with total norm 0.04256 and max norm 0.00605\n",
            "\u001b[32m[I 210629 01:11:23 models:110]\u001b[39m 299 1024 train loss: 0.0002457 valid loss: 0.2157607 P@1: 0.58000 P@3: 0.51000 P@5: 0.39800 N@3: 0.53605 N@5: 0.52802 early stop: 15\n",
            "\u001b[32m[I 210629 01:11:25 models:110]\u001b[39m 302 512 train loss: 0.0001677 valid loss: 0.2164612 P@1: 0.58000 P@3: 0.51000 P@5: 0.39800 N@3: 0.53605 N@5: 0.52817 early stop: 16\n",
            "\u001b[32m[I 210629 01:11:27 models:110]\u001b[39m 304 1024 train loss: 0.0001071 valid loss: 0.2171650 P@1: 0.58000 P@3: 0.51000 P@5: 0.39800 N@3: 0.53605 N@5: 0.52817 early stop: 17\n",
            "\u001b[32m[I 210629 01:11:29 models:110]\u001b[39m 307 512 train loss: 0.0001279 valid loss: 0.2178683 P@1: 0.58000 P@3: 0.51000 P@5: 0.39800 N@3: 0.53605 N@5: 0.52838 early stop: 18\n",
            "\u001b[32m[I 210629 01:11:31 models:110]\u001b[39m 309 1024 train loss: 0.0001198 valid loss: 0.2185619 P@1: 0.58000 P@3: 0.51000 P@5: 0.39800 N@3: 0.53605 N@5: 0.52838 early stop: 19\n",
            "\u001b[32m[I 210629 01:11:33 models:110]\u001b[39m 312 512 train loss: 0.0001247 valid loss: 0.2192359 P@1: 0.58000 P@3: 0.51000 P@5: 0.39800 N@3: 0.53605 N@5: 0.52809 early stop: 20\n",
            "\u001b[32m[I 210629 01:11:35 models:110]\u001b[39m 314 1024 train loss: 0.0004175 valid loss: 0.2199092 P@1: 0.58000 P@3: 0.51333 P@5: 0.39800 N@3: 0.53840 N@5: 0.52836 early stop: 21\n",
            "\u001b[33m[W 210629 01:11:36 models:137]\u001b[39m Clipping gradients with total norm 0.13928 and max norm 0.01942\n",
            "\u001b[32m[I 210629 01:11:37 models:110]\u001b[39m 317 512 train loss: 0.0013141 valid loss: 0.2205464 P@1: 0.58000 P@3: 0.51333 P@5: 0.39800 N@3: 0.53902 N@5: 0.52897 early stop: 22\n",
            "\u001b[32m[I 210629 01:11:39 models:110]\u001b[39m 319 1024 train loss: 0.0002102 valid loss: 0.2211779 P@1: 0.58000 P@3: 0.51333 P@5: 0.39800 N@3: 0.53902 N@5: 0.52897 early stop: 23\n",
            "\u001b[32m[I 210629 01:11:41 models:110]\u001b[39m 322 512 train loss: 0.0002132 valid loss: 0.2217622 P@1: 0.58000 P@3: 0.51333 P@5: 0.39800 N@3: 0.53902 N@5: 0.52897 early stop: 24\n",
            "\u001b[32m[I 210629 01:11:43 models:110]\u001b[39m 324 1024 train loss: 0.0001291 valid loss: 0.2223429 P@1: 0.58000 P@3: 0.51333 P@5: 0.39800 N@3: 0.53902 N@5: 0.52918 early stop: 25\n",
            "\u001b[32m[I 210629 01:11:45 models:110]\u001b[39m 327 512 train loss: 0.0001128 valid loss: 0.2229695 P@1: 0.58000 P@3: 0.51333 P@5: 0.39800 N@3: 0.53902 N@5: 0.52918 early stop: 26\n",
            "\u001b[32m[I 210629 01:11:47 models:110]\u001b[39m 329 1024 train loss: 0.0001480 valid loss: 0.2235932 P@1: 0.58000 P@3: 0.51333 P@5: 0.39800 N@3: 0.53840 N@5: 0.52859 early stop: 27\n",
            "\u001b[32m[I 210629 01:11:49 models:110]\u001b[39m 332 512 train loss: 0.0002496 valid loss: 0.2242001 P@1: 0.58000 P@3: 0.51000 P@5: 0.39800 N@3: 0.53605 N@5: 0.52835 early stop: 28\n",
            "\u001b[32m[I 210629 01:11:51 models:110]\u001b[39m 334 1024 train loss: 0.0003118 valid loss: 0.2248296 P@1: 0.58000 P@3: 0.51000 P@5: 0.39800 N@3: 0.53624 N@5: 0.52854 early stop: 29\n",
            "\u001b[32m[I 210629 01:11:53 models:110]\u001b[39m 337 512 train loss: 0.0002389 valid loss: 0.2254922 P@1: 0.58000 P@3: 0.51000 P@5: 0.39600 N@3: 0.53624 N@5: 0.52703 early stop: 30\n",
            "\u001b[33m[W 210629 01:11:55 models:137]\u001b[39m Clipping gradients with total norm 0.04198 and max norm 0.00759\n",
            "\u001b[32m[I 210629 01:11:55 models:110]\u001b[39m 339 1024 train loss: 0.0002552 valid loss: 0.2261469 P@1: 0.58000 P@3: 0.51333 P@5: 0.39600 N@3: 0.53859 N@5: 0.52735 early stop: 31\n",
            "\u001b[32m[I 210629 01:11:58 models:110]\u001b[39m 342 512 train loss: 0.0002390 valid loss: 0.2267885 P@1: 0.58000 P@3: 0.51333 P@5: 0.39600 N@3: 0.53920 N@5: 0.52797 early stop: 32\n",
            "\u001b[32m[I 210629 01:11:59 models:110]\u001b[39m 344 1024 train loss: 0.0001561 valid loss: 0.2274072 P@1: 0.58000 P@3: 0.51333 P@5: 0.39600 N@3: 0.53920 N@5: 0.52797 early stop: 33\n",
            "\u001b[32m[I 210629 01:12:02 models:110]\u001b[39m 347 512 train loss: 0.0002286 valid loss: 0.2280235 P@1: 0.58000 P@3: 0.51333 P@5: 0.39600 N@3: 0.53920 N@5: 0.52797 early stop: 34\n",
            "\u001b[32m[I 210629 01:12:04 models:110]\u001b[39m 349 1024 train loss: 0.0001016 valid loss: 0.2286307 P@1: 0.58000 P@3: 0.51333 P@5: 0.39600 N@3: 0.53920 N@5: 0.52797 early stop: 35\n",
            "\u001b[33m[W 210629 01:12:04 models:137]\u001b[39m Clipping gradients with total norm 0.07082 and max norm 0.00678\n",
            "\u001b[32m[I 210629 01:12:06 models:110]\u001b[39m 352 512 train loss: 0.0002811 valid loss: 0.2292410 P@1: 0.58000 P@3: 0.51000 P@5: 0.39600 N@3: 0.53686 N@5: 0.52764 early stop: 36\n",
            "\u001b[32m[I 210629 01:12:08 models:110]\u001b[39m 354 1024 train loss: 0.0001573 valid loss: 0.2298294 P@1: 0.58000 P@3: 0.51000 P@5: 0.39600 N@3: 0.53624 N@5: 0.52703 early stop: 37\n",
            "\u001b[32m[I 210629 01:12:10 models:110]\u001b[39m 357 512 train loss: 0.0001396 valid loss: 0.2304111 P@1: 0.58000 P@3: 0.50667 P@5: 0.39600 N@3: 0.53390 N@5: 0.52670 early stop: 38\n",
            "\u001b[32m[I 210629 01:12:12 models:110]\u001b[39m 359 1024 train loss: 0.0000920 valid loss: 0.2310086 P@1: 0.58000 P@3: 0.50667 P@5: 0.39600 N@3: 0.53390 N@5: 0.52670 early stop: 39\n",
            "\u001b[32m[I 210629 01:12:14 models:110]\u001b[39m 362 512 train loss: 0.0001114 valid loss: 0.2316057 P@1: 0.58000 P@3: 0.50667 P@5: 0.39600 N@3: 0.53390 N@5: 0.52670 early stop: 40\n",
            "\u001b[32m[I 210629 01:12:16 models:110]\u001b[39m 364 1024 train loss: 0.0001070 valid loss: 0.2322020 P@1: 0.58000 P@3: 0.50667 P@5: 0.39600 N@3: 0.53390 N@5: 0.52670 early stop: 41\n",
            "\u001b[32m[I 210629 01:12:18 models:110]\u001b[39m 367 512 train loss: 0.0001095 valid loss: 0.2327840 P@1: 0.58000 P@3: 0.50667 P@5: 0.39600 N@3: 0.53390 N@5: 0.52670 early stop: 42\n",
            "\u001b[32m[I 210629 01:12:20 models:110]\u001b[39m 369 1024 train loss: 0.0001123 valid loss: 0.2333659 P@1: 0.58000 P@3: 0.50667 P@5: 0.39600 N@3: 0.53390 N@5: 0.52670 early stop: 43\n",
            "\u001b[32m[I 210629 01:12:22 models:110]\u001b[39m 372 512 train loss: 0.0001019 valid loss: 0.2339559 P@1: 0.58000 P@3: 0.50667 P@5: 0.39600 N@3: 0.53390 N@5: 0.52670 early stop: 44\n",
            "\u001b[32m[I 210629 01:12:24 models:110]\u001b[39m 374 1024 train loss: 0.0001227 valid loss: 0.2345441 P@1: 0.58000 P@3: 0.50333 P@5: 0.39600 N@3: 0.53155 N@5: 0.52647 early stop: 45\n",
            "\u001b[32m[I 210629 01:12:26 models:110]\u001b[39m 377 512 train loss: 0.0000768 valid loss: 0.2351241 P@1: 0.58000 P@3: 0.50333 P@5: 0.39400 N@3: 0.53155 N@5: 0.52516 early stop: 46\n",
            "\u001b[33m[W 210629 01:12:27 models:137]\u001b[39m Clipping gradients with total norm 0.00333 and max norm 0.00062\n",
            "\u001b[32m[I 210629 01:12:28 models:110]\u001b[39m 379 1024 train loss: 0.0001194 valid loss: 0.2357043 P@1: 0.58000 P@3: 0.50333 P@5: 0.39800 N@3: 0.53155 N@5: 0.52848 early stop: 47\n",
            "\u001b[32m[I 210629 01:12:31 models:110]\u001b[39m 382 512 train loss: 0.0001246 valid loss: 0.2362788 P@1: 0.58000 P@3: 0.50333 P@5: 0.39800 N@3: 0.53155 N@5: 0.52848 early stop: 48\n",
            "\u001b[32m[I 210629 01:12:32 models:110]\u001b[39m 384 1024 train loss: 0.0000870 valid loss: 0.2368525 P@1: 0.58000 P@3: 0.50333 P@5: 0.40000 N@3: 0.53155 N@5: 0.52979 early stop: 49\n",
            "\u001b[32m[I 210629 01:12:35 models:110]\u001b[39m 387 512 train loss: 0.0001251 valid loss: 0.2374250 P@1: 0.58000 P@3: 0.50333 P@5: 0.39800 N@3: 0.53155 N@5: 0.52848 early stop: 50\n",
            "\u001b[32m[I 210629 01:12:37 main:76]\u001b[39m Finish Training\n",
            "\u001b[32m[I 210629 01:12:38 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210629 01:12:38 main:79]\u001b[39m Loading Test Set\n",
            "\u001b[32m[I 210629 01:12:38 main:83]\u001b[39m Size of Test Set: 100\n",
            "\u001b[32m[I 210629 01:12:38 main:85]\u001b[39m Predicting\n",
            "\u001b[32m[I 210629 01:12:42 main:91]\u001b[39m Finish Predicting\n",
            "Precision@1,3,5: 0.61 0.4766666666666667 0.392\n",
            "nDCG@1,3,5: 0.61 0.5213488434587253 0.5186678815322883\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UB0SRyKqPGaw"
      },
      "source": [
        "#### Results with MAG labels, without MeSH labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASAeS4SqPEmD"
      },
      "source": [
        "# !cp -r PeTaL-062315 PeTaL\n",
        "\n",
        "!python3 transform_data_PeTaL.py --dataset $DATASET --no-mesh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9pe2-41RW1H",
        "outputId": "21304b0f-a916-4c27-9d6e-5cd99608d74b"
      },
      "source": [
        "!python preprocess.py \\\n",
        "--text-path {DATASET}/train_texts.txt \\\n",
        "--label-path {DATASET}/train_labels.txt \\\n",
        "--vocab-path {DATASET}/vocab.npy \\\n",
        "--emb-path {DATASET}/emb_init.npy \\\n",
        "--w2v-model {DATASET}/{DATASET}.joint.emb \\\n",
        "\n",
        "!python preprocess.py \\\n",
        "--text-path {DATASET}/test_texts.txt \\\n",
        "--label-path {DATASET}/test_labels.txt \\\n",
        "--vocab-path {DATASET}/vocab.npy \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 210623 23:51:54 preprocess:28]\u001b[39m Vocab Size: 26834\n",
            "\u001b[32m[I 210623 23:51:54 preprocess:30]\u001b[39m Getting Dataset: PeTaL/train_texts.txt Max Length: 500\n",
            "\u001b[32m[I 210623 23:51:54 preprocess:32]\u001b[39m Size of Samples: 900\n",
            "\u001b[32m[I 210623 23:51:55 preprocess:28]\u001b[39m Vocab Size: 26834\n",
            "\u001b[32m[I 210623 23:51:55 preprocess:30]\u001b[39m Getting Dataset: PeTaL/test_texts.txt Max Length: 500\n",
            "\u001b[32m[I 210623 23:51:55 preprocess:32]\u001b[39m Size of Samples: 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZRxGf3pSddy",
        "outputId": "304cbbb0-6eb5-43b5-b1e7-4fac930bd276"
      },
      "source": [
        "!PYTHONFAULTHANDLER=1 python main.py --data-cnf configure/datasets/{DATASET}.yaml --model-cnf configure/models/{MODEL}-{DATASET}.yaml --mode train --reg 1\n",
        "!PYTHONFAULTHANDLER=1 python main.py --data-cnf configure/datasets/{DATASET}.yaml --model-cnf configure/models/{MODEL}-{DATASET}.yaml --mode eval\n",
        "\n",
        "!python evaluation.py \\\n",
        "--results {DATASET}/results/{MODEL}-{DATASET}-labels.npy \\\n",
        "--targets {DATASET}/test_labels.npy \\\n",
        "--train-labels {DATASET}/train_labels.npy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 210623 23:53:26 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210623 23:53:26 main:35]\u001b[39m Loading Training and Validation Set\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['absorb_and/or_filter_solids', 'chemically_break_down_inorganic_compounds', 'detox/purify', 'manage_environmental_disturbances_in_a_community', 'protect_from_fire', 'protect_from_gases', 'send_vibratory_signals'] will be ignored\n",
            "  .format(sorted(unknown, key=str)))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['absorb_and/or_filter_solids', 'detox/purify', 'protect_from_gases'] will be ignored\n",
            "  .format(sorted(unknown, key=str)))\n",
            "\u001b[32m[I 210623 23:53:26 main:47]\u001b[39m Number of Labels: 124\n",
            "\u001b[32m[I 210623 23:53:26 main:48]\u001b[39m Size of Training Set: 800\n",
            "\u001b[32m[I 210623 23:53:26 main:49]\u001b[39m Size of Validation Set: 100\n",
            "\u001b[32m[I 210623 23:53:26 main:66]\u001b[39m Number of Edges: 101\n",
            "\u001b[32m[I 210623 23:53:26 main:68]\u001b[39m Training\n",
            "\u001b[32m[I 210623 23:53:32 models:142]\u001b[39m SWA Initializing\n",
            "\u001b[32m[I 210623 23:53:47 models:110]\u001b[39m 24 1024 train loss: 0.1282653 valid loss: 0.1520345 P@1: 0.27000 P@3: 0.21000 P@5: 0.18600 N@3: 0.22714 N@5: 0.25315 early stop: 0\n",
            "\u001b[32m[I 210623 23:54:05 models:110]\u001b[39m 49 1024 train loss: 0.0306060 valid loss: 0.1493586 P@1: 0.40000 P@3: 0.29667 P@5: 0.24800 N@3: 0.32327 N@5: 0.33705 early stop: 0\n",
            "\u001b[32m[I 210623 23:54:23 models:110]\u001b[39m 74 1024 train loss: 0.0121740 valid loss: 0.1538170 P@1: 0.48000 P@3: 0.35333 P@5: 0.29200 N@3: 0.38439 N@5: 0.39602 early stop: 0\n",
            "\u001b[32m[I 210623 23:54:41 models:110]\u001b[39m 99 1024 train loss: 0.0036369 valid loss: 0.1629476 P@1: 0.51000 P@3: 0.39667 P@5: 0.31600 N@3: 0.42695 N@5: 0.43331 early stop: 0\n",
            "\u001b[32m[I 210623 23:54:59 models:110]\u001b[39m 124 1024 train loss: 0.0011694 valid loss: 0.1755957 P@1: 0.53000 P@3: 0.41667 P@5: 0.33000 N@3: 0.44943 N@5: 0.45384 early stop: 0\n",
            "\u001b[32m[I 210623 23:55:16 models:110]\u001b[39m 149 1024 train loss: 0.0061330 valid loss: 0.1817662 P@1: 0.54000 P@3: 0.43333 P@5: 0.35200 N@3: 0.46298 N@5: 0.47602 early stop: 0\n",
            "\u001b[32m[I 210623 23:55:34 models:110]\u001b[39m 174 1024 train loss: 0.0018362 valid loss: 0.1918411 P@1: 0.56000 P@3: 0.45667 P@5: 0.37000 N@3: 0.48409 N@5: 0.49706 early stop: 0\n",
            "\u001b[32m[I 210623 23:55:52 models:110]\u001b[39m 199 1024 train loss: 0.0015366 valid loss: 0.2025761 P@1: 0.57000 P@3: 0.46333 P@5: 0.37200 N@3: 0.49236 N@5: 0.50291 early stop: 0\n",
            "\u001b[32m[I 210623 23:56:10 models:110]\u001b[39m 224 1024 train loss: 0.0057352 valid loss: 0.2101495 P@1: 0.56000 P@3: 0.46667 P@5: 0.37400 N@3: 0.49298 N@5: 0.50486 early stop: 0\n",
            "\u001b[32m[I 210623 23:56:28 models:110]\u001b[39m 249 1024 train loss: 0.0024801 valid loss: 0.2156245 P@1: 0.58000 P@3: 0.47333 P@5: 0.38200 N@3: 0.50530 N@5: 0.51658 early stop: 0\n",
            "\u001b[32m[I 210623 23:56:46 models:110]\u001b[39m 274 1024 train loss: 0.0000528 valid loss: 0.2220434 P@1: 0.61000 P@3: 0.48333 P@5: 0.38800 N@3: 0.51754 N@5: 0.52532 early stop: 0\n",
            "\u001b[32m[I 210623 23:57:04 models:110]\u001b[39m 299 1024 train loss: 0.0000312 valid loss: 0.2285830 P@1: 0.61000 P@3: 0.49333 P@5: 0.38800 N@3: 0.52519 N@5: 0.52734 early stop: 0\n",
            "\u001b[33m[W 210623 23:57:09 models:137]\u001b[39m Clipping gradients with total norm 0.01335 and max norm 0.00178\n",
            "\u001b[32m[I 210623 23:57:21 models:110]\u001b[39m 324 1024 train loss: 0.0000222 valid loss: 0.2350696 P@1: 0.60000 P@3: 0.50000 P@5: 0.38800 N@3: 0.52876 N@5: 0.52775 early stop: 0\n",
            "\u001b[33m[W 210623 23:57:22 models:137]\u001b[39m Clipping gradients with total norm 0.01015 and max norm 0.00117\n",
            "\u001b[33m[W 210623 23:57:27 models:137]\u001b[39m Clipping gradients with total norm 0.00842 and max norm 0.00093\n",
            "\u001b[32m[I 210623 23:57:39 models:110]\u001b[39m 349 1024 train loss: 0.0000163 valid loss: 0.2413728 P@1: 0.60000 P@3: 0.50000 P@5: 0.38600 N@3: 0.52876 N@5: 0.52618 early stop: 1\n",
            "\u001b[32m[I 210623 23:57:57 models:110]\u001b[39m 374 1024 train loss: 0.0000127 valid loss: 0.2475551 P@1: 0.60000 P@3: 0.49667 P@5: 0.38400 N@3: 0.52642 N@5: 0.52406 early stop: 2\n",
            "\u001b[33m[W 210623 23:58:03 models:137]\u001b[39m Clipping gradients with total norm 0.0058 and max norm 0.00114\n",
            "\u001b[32m[I 210623 23:58:15 models:110]\u001b[39m 399 1024 train loss: 0.0000103 valid loss: 0.2535094 P@1: 0.60000 P@3: 0.49667 P@5: 0.38400 N@3: 0.52703 N@5: 0.52470 early stop: 3\n",
            "\u001b[33m[W 210623 23:58:19 models:137]\u001b[39m Clipping gradients with total norm 0.00265 and max norm 0.00053\n",
            "\u001b[32m[I 210623 23:58:33 models:110]\u001b[39m 424 1024 train loss: 0.0000086 valid loss: 0.2593060 P@1: 0.60000 P@3: 0.50000 P@5: 0.38600 N@3: 0.52938 N@5: 0.52687 early stop: 4\n",
            "\u001b[32m[I 210623 23:58:51 models:110]\u001b[39m 449 1024 train loss: 0.0000067 valid loss: 0.2648663 P@1: 0.61000 P@3: 0.50000 P@5: 0.38400 N@3: 0.53111 N@5: 0.52712 early stop: 5\n",
            "\u001b[33m[W 210623 23:58:55 models:137]\u001b[39m Clipping gradients with total norm 0.00869 and max norm 0.00126\n",
            "\u001b[33m[W 210623 23:59:01 models:137]\u001b[39m Clipping gradients with total norm 0.00712 and max norm 0.00041\n",
            "\u001b[32m[I 210623 23:59:09 models:110]\u001b[39m 474 1024 train loss: 0.0000067 valid loss: 0.2700994 P@1: 0.61000 P@3: 0.49667 P@5: 0.38600 N@3: 0.53018 N@5: 0.52936 early stop: 0\n",
            "\u001b[33m[W 210623 23:59:21 models:137]\u001b[39m Clipping gradients with total norm 0.00688 and max norm 0.00048\n",
            "\u001b[32m[I 210623 23:59:27 models:110]\u001b[39m 499 1024 train loss: 0.0000052 valid loss: 0.2752510 P@1: 0.61000 P@3: 0.49667 P@5: 0.38400 N@3: 0.52938 N@5: 0.52694 early stop: 1\n",
            "\u001b[33m[W 210623 23:59:30 models:137]\u001b[39m Clipping gradients with total norm 0.00219 and max norm 0.00033\n",
            "\u001b[32m[I 210623 23:59:44 models:110]\u001b[39m 524 1024 train loss: 0.0000045 valid loss: 0.2802268 P@1: 0.61000 P@3: 0.50000 P@5: 0.38400 N@3: 0.53173 N@5: 0.52734 early stop: 2\n",
            "\u001b[32m[I 210624 00:00:02 models:110]\u001b[39m 549 1024 train loss: 0.0000042 valid loss: 0.2849983 P@1: 0.61000 P@3: 0.49667 P@5: 0.38400 N@3: 0.52938 N@5: 0.52707 early stop: 3\n",
            "\u001b[33m[W 210624 00:00:07 models:137]\u001b[39m Clipping gradients with total norm 0.00243 and max norm 0.00044\n",
            "\u001b[32m[I 210624 00:00:20 models:110]\u001b[39m 574 1024 train loss: 0.0000035 valid loss: 0.2896004 P@1: 0.61000 P@3: 0.49667 P@5: 0.38400 N@3: 0.52938 N@5: 0.52702 early stop: 4\n",
            "\u001b[33m[W 210624 00:00:30 models:137]\u001b[39m Clipping gradients with total norm 0.00299 and max norm 0.0004\n",
            "\u001b[32m[I 210624 00:00:38 models:110]\u001b[39m 599 1024 train loss: 0.0000031 valid loss: 0.2940446 P@1: 0.61000 P@3: 0.49667 P@5: 0.38200 N@3: 0.52938 N@5: 0.52570 early stop: 5\n",
            "\u001b[32m[I 210624 00:00:55 models:110]\u001b[39m 624 1024 train loss: 0.0000027 valid loss: 0.2983665 P@1: 0.61000 P@3: 0.49333 P@5: 0.38200 N@3: 0.52703 N@5: 0.52523 early stop: 6\n",
            "\u001b[33m[W 210624 00:01:09 models:137]\u001b[39m Clipping gradients with total norm 0.00215 and max norm 0.00041\n",
            "\u001b[33m[W 210624 00:01:11 models:137]\u001b[39m Clipping gradients with total norm 0.0037 and max norm 0.00059\n",
            "\u001b[33m[W 210624 00:01:13 models:137]\u001b[39m Clipping gradients with total norm 0.00125 and max norm 0.00018\n",
            "\u001b[32m[I 210624 00:01:13 models:110]\u001b[39m 649 1024 train loss: 0.0000026 valid loss: 0.3025186 P@1: 0.60000 P@3: 0.49667 P@5: 0.38800 N@3: 0.52826 N@5: 0.52876 early stop: 7\n",
            "\u001b[33m[W 210624 00:01:19 models:137]\u001b[39m Clipping gradients with total norm 0.00428 and max norm 0.00039\n",
            "\u001b[32m[I 210624 00:01:31 models:110]\u001b[39m 674 1024 train loss: 0.0000024 valid loss: 0.3065800 P@1: 0.60000 P@3: 0.50000 P@5: 0.38800 N@3: 0.53061 N@5: 0.52892 early stop: 8\n",
            "\u001b[33m[W 210624 00:01:39 models:137]\u001b[39m Clipping gradients with total norm 0.00186 and max norm 0.0003\n",
            "\u001b[32m[I 210624 00:01:49 models:110]\u001b[39m 699 1024 train loss: 0.0000020 valid loss: 0.3104965 P@1: 0.60000 P@3: 0.50000 P@5: 0.38800 N@3: 0.53061 N@5: 0.52892 early stop: 9\n",
            "\u001b[33m[W 210624 00:01:51 models:137]\u001b[39m Clipping gradients with total norm 0.00167 and max norm 0.00015\n",
            "\u001b[33m[W 210624 00:02:02 models:137]\u001b[39m Clipping gradients with total norm 0.00106 and max norm 0.00012\n",
            "\u001b[32m[I 210624 00:02:07 models:110]\u001b[39m 724 1024 train loss: 0.0000018 valid loss: 0.3142850 P@1: 0.60000 P@3: 0.50000 P@5: 0.38800 N@3: 0.53061 N@5: 0.52892 early stop: 10\n",
            "\u001b[33m[W 210624 00:02:15 models:137]\u001b[39m Clipping gradients with total norm 0.00391 and max norm 0.00056\n",
            "\u001b[32m[I 210624 00:02:25 models:110]\u001b[39m 749 1024 train loss: 0.0000017 valid loss: 0.3179969 P@1: 0.60000 P@3: 0.50000 P@5: 0.38600 N@3: 0.53061 N@5: 0.52775 early stop: 11\n",
            "\u001b[33m[W 210624 00:02:33 models:137]\u001b[39m Clipping gradients with total norm 0.00122 and max norm 0.00022\n",
            "\u001b[33m[W 210624 00:02:37 models:137]\u001b[39m Clipping gradients with total norm 0.00176 and max norm 0.00024\n",
            "\u001b[33m[W 210624 00:02:41 models:137]\u001b[39m Clipping gradients with total norm 0.00264 and max norm 0.00022\n",
            "\u001b[32m[I 210624 00:02:42 models:110]\u001b[39m 774 1024 train loss: 0.0000017 valid loss: 0.3215601 P@1: 0.60000 P@3: 0.50000 P@5: 0.38600 N@3: 0.53061 N@5: 0.52775 early stop: 12\n",
            "\u001b[33m[W 210624 00:02:44 models:137]\u001b[39m Clipping gradients with total norm 0.00846 and max norm 0.00017\n",
            "\u001b[32m[I 210624 00:03:00 models:110]\u001b[39m 799 1024 train loss: 0.0000016 valid loss: 0.3250421 P@1: 0.60000 P@3: 0.50000 P@5: 0.38600 N@3: 0.53061 N@5: 0.52755 early stop: 13\n",
            "\u001b[33m[W 210624 00:03:10 models:137]\u001b[39m Clipping gradients with total norm 0.01381 and max norm 0.00026\n",
            "\u001b[32m[I 210624 00:03:18 models:110]\u001b[39m 824 1024 train loss: 0.0000016 valid loss: 0.3284357 P@1: 0.60000 P@3: 0.50000 P@5: 0.38400 N@3: 0.53061 N@5: 0.52574 early stop: 14\n",
            "\u001b[33m[W 210624 00:03:21 models:137]\u001b[39m Clipping gradients with total norm 0.00418 and max norm 0.00038\n",
            "\u001b[32m[I 210624 00:03:36 models:110]\u001b[39m 849 1024 train loss: 0.0000013 valid loss: 0.3317408 P@1: 0.60000 P@3: 0.50000 P@5: 0.38400 N@3: 0.53061 N@5: 0.52589 early stop: 15\n",
            "\u001b[33m[W 210624 00:03:50 models:137]\u001b[39m Clipping gradients with total norm 0.00099 and max norm 0.0002\n",
            "\u001b[33m[W 210624 00:03:52 models:137]\u001b[39m Clipping gradients with total norm 0.00103 and max norm 0.00018\n",
            "\u001b[32m[I 210624 00:03:54 models:110]\u001b[39m 874 1024 train loss: 0.0000013 valid loss: 0.3349921 P@1: 0.60000 P@3: 0.50000 P@5: 0.38400 N@3: 0.53122 N@5: 0.52671 early stop: 16\n",
            "\u001b[33m[W 210624 00:03:56 models:137]\u001b[39m Clipping gradients with total norm 0.00079 and max norm 0.00015\n",
            "\u001b[32m[I 210624 00:04:12 models:110]\u001b[39m 899 1024 train loss: 0.0000010 valid loss: 0.3381548 P@1: 0.60000 P@3: 0.50000 P@5: 0.38400 N@3: 0.53122 N@5: 0.52671 early stop: 17\n",
            "\u001b[33m[W 210624 00:04:14 models:137]\u001b[39m Clipping gradients with total norm 0.00074 and max norm 9e-05\n",
            "\u001b[32m[I 210624 00:04:30 models:110]\u001b[39m 924 1024 train loss: 0.0000010 valid loss: 0.3412432 P@1: 0.60000 P@3: 0.50000 P@5: 0.38400 N@3: 0.53122 N@5: 0.52686 early stop: 18\n",
            "\u001b[33m[W 210624 00:04:35 models:137]\u001b[39m Clipping gradients with total norm 0.00068 and max norm 0.00011\n",
            "\u001b[32m[I 210624 00:04:47 models:110]\u001b[39m 949 1024 train loss: 0.0000009 valid loss: 0.3442575 P@1: 0.61000 P@3: 0.50000 P@5: 0.38400 N@3: 0.53295 N@5: 0.52845 early stop: 19\n",
            "\u001b[33m[W 210624 00:04:55 models:137]\u001b[39m Clipping gradients with total norm 0.00127 and max norm 0.00024\n",
            "\u001b[33m[W 210624 00:04:59 models:137]\u001b[39m Clipping gradients with total norm 0.00668 and max norm 0.00013\n",
            "\u001b[32m[I 210624 00:05:05 models:110]\u001b[39m 974 1024 train loss: 0.0000009 valid loss: 0.3472100 P@1: 0.61000 P@3: 0.50000 P@5: 0.38400 N@3: 0.53295 N@5: 0.52845 early stop: 20\n",
            "\u001b[33m[W 210624 00:05:15 models:137]\u001b[39m Clipping gradients with total norm 0.00076 and max norm 0.00012\n",
            "\u001b[33m[W 210624 00:05:22 models:137]\u001b[39m Clipping gradients with total norm 0.00299 and max norm 7e-05\n",
            "\u001b[32m[I 210624 00:05:23 models:110]\u001b[39m 999 1024 train loss: 0.0000008 valid loss: 0.3501080 P@1: 0.61000 P@3: 0.50000 P@5: 0.38400 N@3: 0.53295 N@5: 0.52845 early stop: 21\n",
            "\u001b[32m[I 210624 00:05:23 main:76]\u001b[39m Finish Training\n",
            "\u001b[32m[I 210624 00:05:25 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210624 00:05:25 main:79]\u001b[39m Loading Test Set\n",
            "\u001b[32m[I 210624 00:05:25 main:83]\u001b[39m Size of Test Set: 100\n",
            "\u001b[32m[I 210624 00:05:25 main:85]\u001b[39m Predicting\n",
            "\u001b[32m[I 210624 00:05:29 main:91]\u001b[39m Finish Predicting\n",
            "Precision@1,3,5: 0.61 0.47 0.36\n",
            "nDCG@1,3,5: 0.61 0.5058103299312516 0.5092618406049622\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feVctRCvVwQV"
      },
      "source": [
        "#### Results without MAG labels, with MeSH labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFedCxzXV4LK"
      },
      "source": [
        "!python3 transform_data_PeTaL.py --dataset $DATASET"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRjbvhvHWQEH",
        "outputId": "fcfc48ee-f975-4abf-9d5f-092b62c0e3e7"
      },
      "source": [
        "!python preprocess.py \\\n",
        "--text-path {DATASET}/train_texts.txt \\\n",
        "--label-path {DATASET}/train_labels.txt \\\n",
        "--vocab-path {DATASET}/vocab.npy \\\n",
        "--emb-path {DATASET}/emb_init.npy \\\n",
        "--w2v-model {DATASET}/{DATASET}.joint.emb \\\n",
        "\n",
        "!python preprocess.py \\\n",
        "--text-path {DATASET}/test_texts.txt \\\n",
        "--label-path {DATASET}/test_labels.txt \\\n",
        "--vocab-path {DATASET}/vocab.npy \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 210629 06:30:37 preprocess:28]\u001b[39m Vocab Size: 26834\n",
            "\u001b[32m[I 210629 06:30:37 preprocess:30]\u001b[39m Getting Dataset: PeTaL/train_texts.txt Max Length: 500\n",
            "\rConverting token to id: 0it [00:00, ?it/s]\r                                          \r\rConverting labels: 0it [00:00, ?it/s]\r                                     \r\u001b[32m[I 210629 06:30:37 preprocess:32]\u001b[39m Size of Samples: 900\n",
            "Traceback (most recent call last):\n",
            "  File \"preprocess.py\", line 8, in <module>\n",
            "  File \"/content/drive/.shortcut-targets-by-id/1H34uxYzZnD3lCNKKLXPkhQj27fDosPDC/PeTaL/PeTaL Data/MATCH on PeTaL Data/MATCH/deepxml/data_utils.py\", line 7, in <module>\n",
            "    from gensim.models import KeyedVectors\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gensim/__init__.py\", line 5, in <module>\n",
            "    from gensim import parsing, corpora, matutils, interfaces, models, similarities, summarization, utils  # noqa:F401\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gensim/parsing/__init__.py\", line 4, in <module>\n",
            "    from .preprocessing import (remove_stopwords, strip_punctuation, strip_punctuation2,  # noqa:F401\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gensim/parsing/preprocessing.py\", line 42, in <module>\n",
            "    from gensim import utils\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gensim/utils.py\", line 45, in <module>\n",
            "    from smart_open import smart_open\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/smart_open/__init__.py\", line 34, in <module>\n",
            "    from .smart_open_lib import open, parse_uri, smart_open, register_compressor  # noqa: E402\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py\", line 35, in <module>\n",
            "    from smart_open import doctools\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/smart_open/doctools.py\", line 21, in <module>\n",
            "    from . import transport\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/smart_open/transport.py\", line 98, in <module>\n",
            "    register_transport('smart_open.gcs')\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/smart_open/transport.py\", line 49, in register_transport\n",
            "    submodule = importlib.import_module(submodule)\n",
            "  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/smart_open/gcs.py\", line 16, in <module>\n",
            "    import google.cloud.storage\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/google/cloud/storage/__init__.py\", line 34, in <module>\n",
            "    from pkg_resources import get_distribution\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\", line 3242, in <module>\n",
            "    @_call_aside\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\", line 3226, in _call_aside\n",
            "    f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\", line 3278, in _initialize_master_working_set\n",
            "    list(map(working_set.add_entry, sys.path))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\", line 608, in add_entry\n",
            "    for dist in find_distributions(entry, True):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\", line 2059, in find_on_path\n",
            "    path_item_entries = _by_version_descending(filtered)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\", line 2029, in _by_version_descending\n",
            "    return sorted(names, key=_by_version, reverse=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\", line 2025, in _by_version\n",
            "    name, ext = os.path.splitext(name)\n",
            "  File \"/usr/lib/python3.7/posixpath.py\", line 122, in splitext\n",
            "    p = os.fspath(p)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbXFztbYWSZp",
        "outputId": "f35792d1-0024-4cb9-c4df-41b4c91dba8e"
      },
      "source": [
        "!PYTHONFAULTHANDLER=1 python main.py --data-cnf configure/datasets/{DATASET}.yaml --model-cnf configure/models/{MODEL}-{DATASET}.yaml --mode train --reg 1\n",
        "!PYTHONFAULTHANDLER=1 python main.py --data-cnf configure/datasets/{DATASET}.yaml --model-cnf configure/models/{MODEL}-{DATASET}.yaml --mode eval\n",
        "\n",
        "!python evaluation.py \\\n",
        "--results {DATASET}/results/{MODEL}-{DATASET}-labels.npy \\\n",
        "--targets {DATASET}/test_labels.npy \\\n",
        "--train-labels {DATASET}/train_labels.npy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 210629 06:30:40 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210629 06:30:40 main:35]\u001b[39m Loading Training and Validation Set\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['absorb_and/or_filter_solids', 'chemically_break_down_inorganic_compounds', 'detox/purify', 'manage_environmental_disturbances_in_a_community', 'protect_from_fire', 'protect_from_gases', 'send_vibratory_signals'] will be ignored\n",
            "  .format(sorted(unknown, key=str)))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['absorb_and/or_filter_solids', 'detox/purify', 'protect_from_gases'] will be ignored\n",
            "  .format(sorted(unknown, key=str)))\n",
            "\u001b[32m[I 210629 06:30:40 main:47]\u001b[39m Number of Labels: 124\n",
            "\u001b[32m[I 210629 06:30:40 main:48]\u001b[39m Size of Training Set: 800\n",
            "\u001b[32m[I 210629 06:30:40 main:49]\u001b[39m Size of Validation Set: 100\n",
            "\u001b[32m[I 210629 06:30:40 main:66]\u001b[39m Number of Edges: 101\n",
            "\u001b[32m[I 210629 06:30:40 main:68]\u001b[39m Training\n",
            "^C\n",
            "\u001b[32m[I 210629 06:30:43 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210629 06:30:43 main:79]\u001b[39m Loading Test Set\n",
            "\u001b[32m[I 210629 06:30:43 main:83]\u001b[39m Size of Test Set: 100\n",
            "\u001b[32m[I 210629 06:30:43 main:85]\u001b[39m Predicting\n",
            "\u001b[32m[I 210629 06:30:47 main:91]\u001b[39m Finish Predicting\n",
            "Precision@1,3,5: 0.62 0.47 0.372\n",
            "nDCG@1,3,5: 0.62 0.5069278726022756 0.5172544503323468\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4gUb3gaZeUJ"
      },
      "source": [
        "#### Results without MAG labels, without MeSH labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rut4NXhSZnow"
      },
      "source": [
        "!python3 transform_data_PeTaL.py --dataset $DATASET"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UynO1c2Ga_Oj",
        "outputId": "3c48df05-6b35-4f73-e699-2a2154f9364f"
      },
      "source": [
        "!python preprocess.py \\\n",
        "--text-path {DATASET}/train_texts.txt \\\n",
        "--label-path {DATASET}/train_labels.txt \\\n",
        "--vocab-path {DATASET}/vocab.npy \\\n",
        "--emb-path {DATASET}/emb_init.npy \\\n",
        "--w2v-model {DATASET}/{DATASET}.joint.emb \\\n",
        "\n",
        "!python preprocess.py \\\n",
        "--text-path {DATASET}/test_texts.txt \\\n",
        "--label-path {DATASET}/test_labels.txt \\\n",
        "--vocab-path {DATASET}/vocab.npy \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"preprocess.py\", line 8, in <module>\n",
            "    from deepxml.data_utils import build_vocab, convert_to_binary\n",
            "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 724, in exec_module\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 818, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 917, in get_data\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"preprocess.py\", line 8, in <module>\n",
            "    from deepxml.data_utils import build_vocab, convert_to_binary\n",
            "  File \"/content/drive/Shareddrives/MATCH Attempt/MATCH/deepxml/data_utils.py\", line 5, in <module>\n",
            "    from sklearn.preprocessing import MultiLabelBinarizer, normalize\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/__init__.py\", line 6, in <module>\n",
            "    from ._function_transformer import FunctionTransformer\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_function_transformer.py\", line 5, in <module>\n",
            "    from ..utils.testing import assert_allclose_dense_sparse\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/testing.py\", line 21, in <module>\n",
            "    import scipy.io\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scipy/io/__init__.py\", line 97, in <module>\n",
            "    from .matlab import loadmat, savemat, whosmat, byteordercodes\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scipy/io/matlab/__init__.py\", line 13, in <module>\n",
            "    from .mio import loadmat, savemat, whosmat\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scipy/io/matlab/mio.py\", line 10, in <module>\n",
            "    from .miobase import get_matfile_version, docfiller\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scipy/io/matlab/miobase.py\", line 22, in <module>\n",
            "    from scipy.misc import doccer\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scipy/misc/__init__.py\", line 100, in <module>\n",
            "    from .pilutil import *\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scipy/misc/pilutil.py\", line 19, in <module>\n",
            "    from PIL import Image, ImageFilter\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/PIL/Image.py\", line 123, in <module>\n",
            "    import cffi\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/cffi/__init__.py\", line 4, in <module>\n",
            "    from .api import FFI\n",
            "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 724, in exec_module\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 857, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 525, in _compile_bytecode\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6u51rKSrbCIY",
        "outputId": "9e0b9e36-8ee1-423a-b088-3584a0420a58"
      },
      "source": [
        "!PYTHONFAULTHANDLER=1 python main.py --data-cnf configure/datasets/{DATASET}.yaml --model-cnf configure/models/{MODEL}-{DATASET}.yaml --mode train --reg 1\n",
        "!PYTHONFAULTHANDLER=1 python main.py --data-cnf configure/datasets/{DATASET}.yaml --model-cnf configure/models/{MODEL}-{DATASET}.yaml --mode eval\n",
        "\n",
        "!python evaluation.py \\\n",
        "--results {DATASET}/results/{MODEL}-{DATASET}-labels.npy \\\n",
        "--targets {DATASET}/test_labels.npy \\\n",
        "--train-labels {DATASET}/train_labels.npy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Aborted!\n",
            "\u001b[32m[I 210624 21:33:03 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210624 21:33:03 main:79]\u001b[39m Loading Test Set\n",
            "\u001b[32m[I 210624 21:33:03 main:83]\u001b[39m Size of Test Set: 100\n",
            "\u001b[32m[I 210624 21:33:03 main:85]\u001b[39m Predicting\n",
            "\u001b[32m[I 210624 21:33:06 main:91]\u001b[39m Finish Predicting\n",
            "Precision@1,3,5: 0.66 0.53 0.42\n",
            "nDCG@1,3,5: 0.66 0.5620467187130749 0.5699775424270964\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVkLBZhCdUGh"
      },
      "source": [
        "### Issue 58: Ablation study with 10-fold cross validaiton."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUlbFelUu_m7",
        "outputId": "d9092526-e27c-4511-bde6-613296952968"
      },
      "source": [
        "for skip in range(0, 1000, 100):\n",
        "    print(f\"\\nWITH MAG WITH MESH, skip={skip}\\n\")\n",
        "    %cd PeTaL/\n",
        "    !python3 Split.py --skip={skip}\n",
        "    %cd ..\n",
        "    !wc PeTaL/train.json\n",
        "\n",
        "    !python3 transform_data_PeTaL.py --dataset $DATASET\n",
        "\n",
        "    !python preprocess.py \\\n",
        "    --text-path {DATASET}/train_texts.txt \\\n",
        "    --label-path {DATASET}/train_labels.txt \\\n",
        "    --vocab-path {DATASET}/vocab.npy \\\n",
        "    --emb-path {DATASET}/emb_init.npy \\\n",
        "    --w2v-model {DATASET}/{DATASET}.joint.emb \\\n",
        "\n",
        "    !python preprocess.py \\\n",
        "    --text-path {DATASET}/test_texts.txt \\\n",
        "    --label-path {DATASET}/test_labels.txt \\\n",
        "    --vocab-path {DATASET}/vocab.npy \\\n",
        "\n",
        "    !PYTHONFAULTHANDLER=1 python main.py --data-cnf configure/datasets/{DATASET}.yaml --model-cnf configure/models/{MODEL}-{DATASET}.yaml --mode train --reg 1\n",
        "    !PYTHONFAULTHANDLER=1 python main.py --data-cnf configure/datasets/{DATASET}.yaml --model-cnf configure/models/{MODEL}-{DATASET}.yaml --mode eval\n",
        "\n",
        "    !python evaluation.py \\\n",
        "    --results {DATASET}/results/{MODEL}-{DATASET}-labels.npy \\\n",
        "    --targets {DATASET}/test_labels.npy \\\n",
        "    --train-labels {DATASET}/train_labels.npy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WITH MAG WITH MESH, skip=100\n",
            "\n",
            "/content/drive/Shareddrives/NASA/MATCH/PeTaL\n",
            "131\n",
            "/content/drive/Shareddrives/NASA/MATCH\n",
            "    800  274532 4635505 PeTaL/train.json\n",
            "\u001b[32m[I 210629 03:07:34 preprocess:28]\u001b[39m Vocab Size: 26834\n",
            "\u001b[32m[I 210629 03:07:34 preprocess:30]\u001b[39m Getting Dataset: PeTaL/train_texts.txt Max Length: 500\n",
            "\u001b[32m[I 210629 03:07:34 preprocess:32]\u001b[39m Size of Samples: 900\n",
            "\u001b[32m[I 210629 03:07:35 preprocess:28]\u001b[39m Vocab Size: 26834\n",
            "\u001b[32m[I 210629 03:07:35 preprocess:30]\u001b[39m Getting Dataset: PeTaL/test_texts.txt Max Length: 500\n",
            "\u001b[32m[I 210629 03:07:35 preprocess:32]\u001b[39m Size of Samples: 100\n",
            "\u001b[32m[I 210629 03:07:37 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210629 03:07:37 main:35]\u001b[39m Loading Training and Validation Set\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['absorb_and/or_filter_solids', 'chemically_break_down_inorganic_compounds', 'detox/purify', 'manage_environmental_disturbances_in_a_community', 'protect_from_fire', 'protect_from_gases', 'send_vibratory_signals'] will be ignored\n",
            "  .format(sorted(unknown, key=str)))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['detox/purify', 'protect_from_gases'] will be ignored\n",
            "  .format(sorted(unknown, key=str)))\n",
            "\u001b[32m[I 210629 03:07:37 main:47]\u001b[39m Number of Labels: 124\n",
            "\u001b[32m[I 210629 03:07:37 main:48]\u001b[39m Size of Training Set: 800\n",
            "\u001b[32m[I 210629 03:07:37 main:49]\u001b[39m Size of Validation Set: 100\n",
            "\u001b[32m[I 210629 03:07:37 main:66]\u001b[39m Number of Edges: 101\n",
            "\u001b[32m[I 210629 03:07:37 main:68]\u001b[39m Training\n",
            "\u001b[32m[I 210629 03:07:43 models:110]\u001b[39m 2 512 train loss: 0.2889029 valid loss: 0.1576317 P@1: 0.28000 P@3: 0.14000 P@5: 0.11600 N@3: 0.16855 N@5: 0.16569 early stop: 0\n",
            "\u001b[32m[I 210629 03:07:44 models:142]\u001b[39m SWA Initializing\n",
            "\u001b[32m[I 210629 03:07:45 models:110]\u001b[39m 4 1024 train loss: 0.1468785 valid loss: 0.1478182 P@1: 0.28000 P@3: 0.21000 P@5: 0.20000 N@3: 0.22335 N@5: 0.24485 early stop: 0\n",
            "\u001b[32m[I 210629 03:07:47 models:110]\u001b[39m 7 512 train loss: 0.1423314 valid loss: 0.1456265 P@1: 0.28000 P@3: 0.21000 P@5: 0.19000 N@3: 0.22581 N@5: 0.24080 early stop: 1\n",
            "\u001b[32m[I 210629 03:07:49 models:110]\u001b[39m 9 1024 train loss: 0.1348241 valid loss: 0.1448554 P@1: 0.28000 P@3: 0.21667 P@5: 0.19000 N@3: 0.22805 N@5: 0.23980 early stop: 2\n",
            "\u001b[32m[I 210629 03:07:52 models:110]\u001b[39m 12 512 train loss: 0.1319159 valid loss: 0.1444626 P@1: 0.28000 P@3: 0.21000 P@5: 0.19400 N@3: 0.22335 N@5: 0.24242 early stop: 3\n",
            "\u001b[32m[I 210629 03:07:54 models:110]\u001b[39m 14 1024 train loss: 0.1191928 valid loss: 0.1445482 P@1: 0.28000 P@3: 0.20667 P@5: 0.19600 N@3: 0.22162 N@5: 0.24266 early stop: 4\n",
            "\u001b[32m[I 210629 03:07:56 models:110]\u001b[39m 17 512 train loss: 0.1048834 valid loss: 0.1444774 P@1: 0.28000 P@3: 0.23000 P@5: 0.19400 N@3: 0.24542 N@5: 0.24894 early stop: 0\n",
            "\u001b[32m[I 210629 03:07:58 models:110]\u001b[39m 19 1024 train loss: 0.0901191 valid loss: 0.1442502 P@1: 0.33000 P@3: 0.25000 P@5: 0.19600 N@3: 0.27123 N@5: 0.26367 early stop: 0\n",
            "\u001b[32m[I 210629 03:08:01 models:110]\u001b[39m 22 512 train loss: 0.0732695 valid loss: 0.1438565 P@1: 0.36000 P@3: 0.26000 P@5: 0.21400 N@3: 0.28408 N@5: 0.28445 early stop: 0\n",
            "\u001b[32m[I 210629 03:08:03 models:110]\u001b[39m 24 1024 train loss: 0.0632292 valid loss: 0.1434788 P@1: 0.37000 P@3: 0.26667 P@5: 0.21600 N@3: 0.29213 N@5: 0.28969 early stop: 0\n",
            "\u001b[32m[I 210629 03:08:06 models:110]\u001b[39m 27 512 train loss: 0.0500342 valid loss: 0.1431763 P@1: 0.38000 P@3: 0.28000 P@5: 0.22200 N@3: 0.30581 N@5: 0.30147 early stop: 0\n",
            "\u001b[32m[I 210629 03:08:08 models:110]\u001b[39m 29 1024 train loss: 0.0421274 valid loss: 0.1425807 P@1: 0.43000 P@3: 0.28000 P@5: 0.24000 N@3: 0.31684 N@5: 0.32462 early stop: 0\n",
            "\u001b[32m[I 210629 03:08:10 models:110]\u001b[39m 32 512 train loss: 0.0368081 valid loss: 0.1418380 P@1: 0.45000 P@3: 0.29333 P@5: 0.25400 N@3: 0.33154 N@5: 0.34219 early stop: 0\n",
            "\u001b[32m[I 210629 03:08:12 models:110]\u001b[39m 34 1024 train loss: 0.0353282 valid loss: 0.1413308 P@1: 0.46000 P@3: 0.32333 P@5: 0.27000 N@3: 0.35684 N@5: 0.36291 early stop: 0\n",
            "\u001b[32m[I 210629 03:08:15 models:110]\u001b[39m 37 512 train loss: 0.0305847 valid loss: 0.1408687 P@1: 0.52000 P@3: 0.33333 P@5: 0.28200 N@3: 0.37684 N@5: 0.38491 early stop: 0\n",
            "\u001b[32m[I 210629 03:08:17 models:110]\u001b[39m 39 1024 train loss: 0.0246753 valid loss: 0.1403548 P@1: 0.55000 P@3: 0.34667 P@5: 0.28800 N@3: 0.39449 N@5: 0.39812 early stop: 0\n",
            "\u001b[32m[I 210629 03:08:19 models:110]\u001b[39m 42 512 train loss: 0.0211451 valid loss: 0.1403195 P@1: 0.56000 P@3: 0.37333 P@5: 0.31000 N@3: 0.41745 N@5: 0.42334 early stop: 0\n",
            "\u001b[32m[I 210629 03:08:21 models:110]\u001b[39m 44 1024 train loss: 0.0168626 valid loss: 0.1400763 P@1: 0.59000 P@3: 0.37333 P@5: 0.31600 N@3: 0.42345 N@5: 0.43342 early stop: 0\n",
            "\u001b[32m[I 210629 03:08:24 models:110]\u001b[39m 47 512 train loss: 0.0169494 valid loss: 0.1398344 P@1: 0.58000 P@3: 0.38000 P@5: 0.32400 N@3: 0.42887 N@5: 0.44322 early stop: 0\n",
            "\u001b[32m[I 210629 03:08:26 models:110]\u001b[39m 49 1024 train loss: 0.0155379 valid loss: 0.1404747 P@1: 0.58000 P@3: 0.38667 P@5: 0.33200 N@3: 0.43479 N@5: 0.45142 early stop: 0\n",
            "\u001b[32m[I 210629 03:08:28 models:110]\u001b[39m 52 512 train loss: 0.0166315 valid loss: 0.1405655 P@1: 0.61000 P@3: 0.40667 P@5: 0.32800 N@3: 0.45540 N@5: 0.45616 early stop: 0\n",
            "\u001b[32m[I 210629 03:08:30 models:110]\u001b[39m 54 1024 train loss: 0.0180198 valid loss: 0.1410467 P@1: 0.60000 P@3: 0.40333 P@5: 0.32200 N@3: 0.45194 N@5: 0.44981 early stop: 1\n",
            "\u001b[32m[I 210629 03:08:33 models:110]\u001b[39m 57 512 train loss: 0.0157944 valid loss: 0.1420128 P@1: 0.60000 P@3: 0.41000 P@5: 0.32800 N@3: 0.45786 N@5: 0.45583 early stop: 2\n",
            "\u001b[32m[I 210629 03:08:35 models:110]\u001b[39m 59 1024 train loss: 0.0116917 valid loss: 0.1427368 P@1: 0.62000 P@3: 0.42000 P@5: 0.33600 N@3: 0.46959 N@5: 0.46720 early stop: 0\n",
            "\u001b[32m[I 210629 03:08:37 models:110]\u001b[39m 62 512 train loss: 0.0118469 valid loss: 0.1437224 P@1: 0.61000 P@3: 0.42000 P@5: 0.33800 N@3: 0.46847 N@5: 0.46779 early stop: 0\n",
            "\u001b[32m[I 210629 03:08:40 models:110]\u001b[39m 64 1024 train loss: 0.0110992 valid loss: 0.1451294 P@1: 0.61000 P@3: 0.43000 P@5: 0.34400 N@3: 0.47631 N@5: 0.47405 early stop: 0\n",
            "\u001b[32m[I 210629 03:08:42 models:110]\u001b[39m 67 512 train loss: 0.0089455 valid loss: 0.1464316 P@1: 0.60000 P@3: 0.43333 P@5: 0.34400 N@3: 0.47816 N@5: 0.47389 early stop: 1\n",
            "\u001b[32m[I 210629 03:08:44 models:110]\u001b[39m 69 1024 train loss: 0.0070186 valid loss: 0.1478555 P@1: 0.61000 P@3: 0.43667 P@5: 0.34600 N@3: 0.48224 N@5: 0.47697 early stop: 0\n",
            "\u001b[32m[I 210629 03:08:46 models:110]\u001b[39m 72 512 train loss: 0.0052621 valid loss: 0.1493057 P@1: 0.62000 P@3: 0.43667 P@5: 0.34800 N@3: 0.48397 N@5: 0.48006 early stop: 0\n",
            "\u001b[32m[I 210629 03:08:49 models:110]\u001b[39m 74 1024 train loss: 0.0039097 valid loss: 0.1510044 P@1: 0.63000 P@3: 0.44333 P@5: 0.34800 N@3: 0.48978 N@5: 0.48083 early stop: 0\n",
            "\u001b[32m[I 210629 03:08:51 models:110]\u001b[39m 77 512 train loss: 0.0040463 valid loss: 0.1527823 P@1: 0.63000 P@3: 0.44667 P@5: 0.35200 N@3: 0.49212 N@5: 0.48436 early stop: 0\n",
            "\u001b[32m[I 210629 03:08:53 models:110]\u001b[39m 79 1024 train loss: 0.0051428 valid loss: 0.1545245 P@1: 0.63000 P@3: 0.45000 P@5: 0.35400 N@3: 0.49447 N@5: 0.48635 early stop: 0\n",
            "\u001b[32m[I 210629 03:08:56 models:110]\u001b[39m 82 512 train loss: 0.0048117 valid loss: 0.1564448 P@1: 0.63000 P@3: 0.45000 P@5: 0.35800 N@3: 0.49508 N@5: 0.48991 early stop: 0\n",
            "\u001b[32m[I 210629 03:08:58 models:110]\u001b[39m 84 1024 train loss: 0.0037121 valid loss: 0.1581750 P@1: 0.64000 P@3: 0.44333 P@5: 0.35600 N@3: 0.49212 N@5: 0.48867 early stop: 1\n",
            "\u001b[32m[I 210629 03:09:00 models:110]\u001b[39m 87 512 train loss: 0.0026297 valid loss: 0.1600195 P@1: 0.64000 P@3: 0.44667 P@5: 0.35800 N@3: 0.49447 N@5: 0.49082 early stop: 0\n",
            "\u001b[32m[I 210629 03:09:02 models:110]\u001b[39m 89 1024 train loss: 0.0020996 valid loss: 0.1620063 P@1: 0.64000 P@3: 0.45000 P@5: 0.35600 N@3: 0.49682 N@5: 0.48990 early stop: 1\n",
            "\u001b[32m[I 210629 03:09:05 models:110]\u001b[39m 92 512 train loss: 0.0024242 valid loss: 0.1640610 P@1: 0.64000 P@3: 0.44333 P@5: 0.36400 N@3: 0.49089 N@5: 0.49403 early stop: 0\n",
            "\u001b[32m[I 210629 03:09:07 models:110]\u001b[39m 94 1024 train loss: 0.0026029 valid loss: 0.1664326 P@1: 0.65000 P@3: 0.44667 P@5: 0.36400 N@3: 0.49559 N@5: 0.49636 early stop: 0\n",
            "\u001b[32m[I 210629 03:09:09 models:110]\u001b[39m 97 512 train loss: 0.0049467 valid loss: 0.1679715 P@1: 0.65000 P@3: 0.44667 P@5: 0.36400 N@3: 0.49559 N@5: 0.49677 early stop: 0\n",
            "\u001b[32m[I 210629 03:09:11 models:110]\u001b[39m 99 1024 train loss: 0.0083354 valid loss: 0.1699570 P@1: 0.65000 P@3: 0.44667 P@5: 0.36200 N@3: 0.49559 N@5: 0.49506 early stop: 1\n",
            "\u001b[32m[I 210629 03:09:14 models:110]\u001b[39m 102 512 train loss: 0.0113896 valid loss: 0.1714818 P@1: 0.65000 P@3: 0.44667 P@5: 0.36000 N@3: 0.49559 N@5: 0.49354 early stop: 2\n",
            "\u001b[32m[I 210629 03:09:16 models:110]\u001b[39m 104 1024 train loss: 0.0094396 valid loss: 0.1728521 P@1: 0.65000 P@3: 0.44667 P@5: 0.36200 N@3: 0.49682 N@5: 0.49621 early stop: 3\n",
            "\u001b[32m[I 210629 03:09:18 models:110]\u001b[39m 107 512 train loss: 0.0064256 valid loss: 0.1742841 P@1: 0.65000 P@3: 0.44333 P@5: 0.35600 N@3: 0.49386 N@5: 0.49080 early stop: 4\n",
            "\u001b[32m[I 210629 03:09:20 models:110]\u001b[39m 109 1024 train loss: 0.0055042 valid loss: 0.1756144 P@1: 0.65000 P@3: 0.44333 P@5: 0.36000 N@3: 0.49447 N@5: 0.49457 early stop: 5\n",
            "\u001b[32m[I 210629 03:09:23 models:110]\u001b[39m 112 512 train loss: 0.0034117 valid loss: 0.1771991 P@1: 0.64000 P@3: 0.44667 P@5: 0.36000 N@3: 0.49447 N@5: 0.49248 early stop: 6\n",
            "\u001b[32m[I 210629 03:09:25 models:110]\u001b[39m 114 1024 train loss: 0.0019664 valid loss: 0.1789519 P@1: 0.64000 P@3: 0.45333 P@5: 0.36000 N@3: 0.49978 N@5: 0.49364 early stop: 7\n",
            "\u001b[32m[I 210629 03:09:27 models:110]\u001b[39m 117 512 train loss: 0.0012170 valid loss: 0.1807734 P@1: 0.64000 P@3: 0.45333 P@5: 0.36000 N@3: 0.49916 N@5: 0.49319 early stop: 8\n",
            "\u001b[32m[I 210629 03:09:29 models:110]\u001b[39m 119 1024 train loss: 0.0010369 valid loss: 0.1826906 P@1: 0.64000 P@3: 0.45333 P@5: 0.36200 N@3: 0.49855 N@5: 0.49456 early stop: 9\n",
            "\u001b[32m[I 210629 03:09:32 models:110]\u001b[39m 122 512 train loss: 0.0006760 valid loss: 0.1845556 P@1: 0.63000 P@3: 0.45333 P@5: 0.36000 N@3: 0.49682 N@5: 0.49199 early stop: 10\n",
            "\u001b[32m[I 210629 03:09:34 models:110]\u001b[39m 124 1024 train loss: 0.0007387 valid loss: 0.1864716 P@1: 0.63000 P@3: 0.45333 P@5: 0.36000 N@3: 0.49682 N@5: 0.49179 early stop: 11\n",
            "\u001b[32m[I 210629 03:09:36 models:110]\u001b[39m 127 512 train loss: 0.0005668 valid loss: 0.1884220 P@1: 0.63000 P@3: 0.45333 P@5: 0.36200 N@3: 0.49682 N@5: 0.49316 early stop: 12\n",
            "\u001b[32m[I 210629 03:09:38 models:110]\u001b[39m 129 1024 train loss: 0.0012529 valid loss: 0.1905311 P@1: 0.63000 P@3: 0.45000 P@5: 0.36800 N@3: 0.49447 N@5: 0.49686 early stop: 0\n",
            "\u001b[32m[I 210629 03:09:41 models:110]\u001b[39m 132 512 train loss: 0.0009000 valid loss: 0.1923319 P@1: 0.63000 P@3: 0.44667 P@5: 0.36800 N@3: 0.49212 N@5: 0.49659 early stop: 1\n",
            "\u001b[32m[I 210629 03:09:43 models:110]\u001b[39m 134 1024 train loss: 0.0008644 valid loss: 0.1942637 P@1: 0.63000 P@3: 0.44667 P@5: 0.36800 N@3: 0.49212 N@5: 0.49644 early stop: 2\n",
            "\u001b[32m[I 210629 03:09:45 models:110]\u001b[39m 137 512 train loss: 0.0005696 valid loss: 0.1962020 P@1: 0.63000 P@3: 0.44667 P@5: 0.36800 N@3: 0.49212 N@5: 0.49629 early stop: 3\n",
            "\u001b[32m[I 210629 03:09:47 models:110]\u001b[39m 139 1024 train loss: 0.0006721 valid loss: 0.1982246 P@1: 0.63000 P@3: 0.44667 P@5: 0.37000 N@3: 0.49274 N@5: 0.49822 early stop: 0\n",
            "\u001b[32m[I 210629 03:09:50 models:110]\u001b[39m 142 512 train loss: 0.0005135 valid loss: 0.2001461 P@1: 0.63000 P@3: 0.44667 P@5: 0.37200 N@3: 0.49274 N@5: 0.49953 early stop: 0\n",
            "\u001b[32m[I 210629 03:09:52 models:110]\u001b[39m 144 1024 train loss: 0.0002375 valid loss: 0.2020745 P@1: 0.62000 P@3: 0.45333 P@5: 0.36800 N@3: 0.49693 N@5: 0.49608 early stop: 1\n",
            "\u001b[32m[I 210629 03:09:54 models:110]\u001b[39m 147 512 train loss: 0.0002489 valid loss: 0.2040560 P@1: 0.62000 P@3: 0.45333 P@5: 0.37000 N@3: 0.49693 N@5: 0.49739 early stop: 2\n",
            "\u001b[32m[I 210629 03:09:56 models:110]\u001b[39m 149 1024 train loss: 0.0002678 valid loss: 0.2059411 P@1: 0.62000 P@3: 0.45333 P@5: 0.37200 N@3: 0.49693 N@5: 0.49905 early stop: 3\n",
            "\u001b[32m[I 210629 03:09:59 models:110]\u001b[39m 152 512 train loss: 0.0002289 valid loss: 0.2078465 P@1: 0.62000 P@3: 0.45333 P@5: 0.37400 N@3: 0.49693 N@5: 0.50031 early stop: 0\n",
            "\u001b[32m[I 210629 03:10:01 models:110]\u001b[39m 154 1024 train loss: 0.0002103 valid loss: 0.2097231 P@1: 0.62000 P@3: 0.45333 P@5: 0.37400 N@3: 0.49693 N@5: 0.50031 early stop: 1\n",
            "\u001b[32m[I 210629 03:10:03 models:110]\u001b[39m 157 512 train loss: 0.0002206 valid loss: 0.2115950 P@1: 0.62000 P@3: 0.45000 P@5: 0.37400 N@3: 0.49458 N@5: 0.50034 early stop: 0\n",
            "\u001b[32m[I 210629 03:10:05 models:110]\u001b[39m 159 1024 train loss: 0.0002430 valid loss: 0.2134553 P@1: 0.62000 P@3: 0.45000 P@5: 0.37200 N@3: 0.49458 N@5: 0.49912 early stop: 1\n",
            "\u001b[32m[I 210629 03:10:08 models:110]\u001b[39m 162 512 train loss: 0.0001534 valid loss: 0.2153431 P@1: 0.62000 P@3: 0.45333 P@5: 0.37400 N@3: 0.49631 N@5: 0.50044 early stop: 0\n",
            "\u001b[32m[I 210629 03:10:10 models:110]\u001b[39m 164 1024 train loss: 0.0002793 valid loss: 0.2171570 P@1: 0.62000 P@3: 0.45333 P@5: 0.37600 N@3: 0.49631 N@5: 0.50246 early stop: 0\n",
            "\u001b[32m[I 210629 03:10:13 models:110]\u001b[39m 167 512 train loss: 0.0001400 valid loss: 0.2189746 P@1: 0.62000 P@3: 0.45000 P@5: 0.37600 N@3: 0.49397 N@5: 0.50214 early stop: 1\n",
            "\u001b[32m[I 210629 03:10:15 models:110]\u001b[39m 169 1024 train loss: 0.0001607 valid loss: 0.2207987 P@1: 0.62000 P@3: 0.45000 P@5: 0.37600 N@3: 0.49397 N@5: 0.50196 early stop: 2\n",
            "\u001b[32m[I 210629 03:10:17 models:110]\u001b[39m 172 512 train loss: 0.0002143 valid loss: 0.2225808 P@1: 0.62000 P@3: 0.45000 P@5: 0.37800 N@3: 0.49458 N@5: 0.50387 early stop: 0\n",
            "\u001b[32m[I 210629 03:10:19 models:110]\u001b[39m 174 1024 train loss: 0.0001543 valid loss: 0.2243555 P@1: 0.61000 P@3: 0.45000 P@5: 0.38000 N@3: 0.49285 N@5: 0.50409 early stop: 0\n",
            "\u001b[33m[W 210629 03:10:20 models:137]\u001b[39m Clipping gradients with total norm 0.1476 and max norm 0.01365\n",
            "\u001b[32m[I 210629 03:10:22 models:110]\u001b[39m 177 512 train loss: 0.0005322 valid loss: 0.2261105 P@1: 0.61000 P@3: 0.45000 P@5: 0.38200 N@3: 0.49285 N@5: 0.50561 early stop: 0\n",
            "\u001b[32m[I 210629 03:10:24 models:110]\u001b[39m 179 1024 train loss: 0.0003696 valid loss: 0.2278671 P@1: 0.61000 P@3: 0.45000 P@5: 0.38400 N@3: 0.49285 N@5: 0.50757 early stop: 0\n",
            "\u001b[32m[I 210629 03:10:26 models:110]\u001b[39m 182 512 train loss: 0.0004822 valid loss: 0.2295392 P@1: 0.61000 P@3: 0.45000 P@5: 0.38400 N@3: 0.49285 N@5: 0.50757 early stop: 1\n",
            "\u001b[33m[W 210629 03:10:27 models:137]\u001b[39m Clipping gradients with total norm 0.13608 and max norm 0.02007\n",
            "\u001b[32m[I 210629 03:10:29 models:110]\u001b[39m 184 1024 train loss: 0.0005434 valid loss: 0.2312472 P@1: 0.61000 P@3: 0.45000 P@5: 0.38400 N@3: 0.49408 N@5: 0.50848 early stop: 0\n",
            "\u001b[32m[I 210629 03:10:31 models:110]\u001b[39m 187 512 train loss: 0.0004915 valid loss: 0.2328342 P@1: 0.59000 P@3: 0.45000 P@5: 0.38600 N@3: 0.49000 N@5: 0.50655 early stop: 1\n",
            "\u001b[32m[I 210629 03:10:33 models:110]\u001b[39m 189 1024 train loss: 0.0003795 valid loss: 0.2344136 P@1: 0.59000 P@3: 0.45000 P@5: 0.38600 N@3: 0.49000 N@5: 0.50670 early stop: 2\n",
            "\u001b[32m[I 210629 03:10:36 models:110]\u001b[39m 192 512 train loss: 0.0005726 valid loss: 0.2358962 P@1: 0.59000 P@3: 0.44667 P@5: 0.38400 N@3: 0.48765 N@5: 0.50500 early stop: 3\n",
            "\u001b[32m[I 210629 03:10:38 models:110]\u001b[39m 194 1024 train loss: 0.0018251 valid loss: 0.2373819 P@1: 0.59000 P@3: 0.44667 P@5: 0.38400 N@3: 0.48765 N@5: 0.50500 early stop: 4\n",
            "\u001b[32m[I 210629 03:10:40 models:110]\u001b[39m 197 512 train loss: 0.0018037 valid loss: 0.2389760 P@1: 0.60000 P@3: 0.44333 P@5: 0.38200 N@3: 0.48704 N@5: 0.50457 early stop: 5\n",
            "\u001b[32m[I 210629 03:10:42 models:110]\u001b[39m 199 1024 train loss: 0.0051969 valid loss: 0.2397967 P@1: 0.60000 P@3: 0.44667 P@5: 0.38400 N@3: 0.49123 N@5: 0.50773 early stop: 6\n",
            "\u001b[32m[I 210629 03:10:44 models:110]\u001b[39m 202 512 train loss: 0.0103183 valid loss: 0.2407957 P@1: 0.60000 P@3: 0.44333 P@5: 0.38400 N@3: 0.48888 N@5: 0.50741 early stop: 7\n",
            "\u001b[32m[I 210629 03:10:46 models:110]\u001b[39m 204 1024 train loss: 0.0144469 valid loss: 0.2414329 P@1: 0.61000 P@3: 0.44333 P@5: 0.38400 N@3: 0.48939 N@5: 0.50754 early stop: 8\n",
            "\u001b[32m[I 210629 03:10:49 models:110]\u001b[39m 207 512 train loss: 0.0260758 valid loss: 0.2411997 P@1: 0.61000 P@3: 0.44333 P@5: 0.38000 N@3: 0.49000 N@5: 0.50506 early stop: 9\n",
            "\u001b[32m[I 210629 03:10:51 models:110]\u001b[39m 209 1024 train loss: 0.0226961 valid loss: 0.2404729 P@1: 0.61000 P@3: 0.45000 P@5: 0.37800 N@3: 0.49531 N@5: 0.50493 early stop: 10\n",
            "\u001b[32m[I 210629 03:10:53 models:110]\u001b[39m 212 512 train loss: 0.0151435 valid loss: 0.2397318 P@1: 0.62000 P@3: 0.45333 P@5: 0.37800 N@3: 0.49939 N@5: 0.50695 early stop: 11\n",
            "\u001b[32m[I 210629 03:10:56 models:110]\u001b[39m 214 1024 train loss: 0.0080456 valid loss: 0.2394087 P@1: 0.62000 P@3: 0.45000 P@5: 0.38400 N@3: 0.49765 N@5: 0.51177 early stop: 0\n",
            "\u001b[32m[I 210629 03:10:58 models:110]\u001b[39m 217 512 train loss: 0.0041636 valid loss: 0.2392049 P@1: 0.62000 P@3: 0.44667 P@5: 0.38400 N@3: 0.49531 N@5: 0.51113 early stop: 1\n",
            "\u001b[32m[I 210629 03:11:00 models:110]\u001b[39m 219 1024 train loss: 0.0021867 valid loss: 0.2393015 P@1: 0.62000 P@3: 0.44667 P@5: 0.38400 N@3: 0.49531 N@5: 0.51163 early stop: 2\n",
            "\u001b[32m[I 210629 03:11:03 models:110]\u001b[39m 222 512 train loss: 0.0014617 valid loss: 0.2396305 P@1: 0.62000 P@3: 0.44667 P@5: 0.38600 N@3: 0.49531 N@5: 0.51331 early stop: 0\n",
            "\u001b[32m[I 210629 03:11:05 models:110]\u001b[39m 224 1024 train loss: 0.0008514 valid loss: 0.2400428 P@1: 0.62000 P@3: 0.44667 P@5: 0.38600 N@3: 0.49531 N@5: 0.51331 early stop: 1\n",
            "\u001b[32m[I 210629 03:11:07 models:110]\u001b[39m 227 512 train loss: 0.0005196 valid loss: 0.2405502 P@1: 0.62000 P@3: 0.44667 P@5: 0.38600 N@3: 0.49531 N@5: 0.51327 early stop: 2\n",
            "\u001b[32m[I 210629 03:11:09 models:110]\u001b[39m 229 1024 train loss: 0.0003656 valid loss: 0.2410894 P@1: 0.61000 P@3: 0.44667 P@5: 0.38600 N@3: 0.49358 N@5: 0.51185 early stop: 3\n",
            "\u001b[32m[I 210629 03:11:12 models:110]\u001b[39m 232 512 train loss: 0.0002868 valid loss: 0.2416681 P@1: 0.61000 P@3: 0.45333 P@5: 0.38000 N@3: 0.49827 N@5: 0.50797 early stop: 4\n",
            "\u001b[32m[I 210629 03:11:14 models:110]\u001b[39m 234 1024 train loss: 0.0003097 valid loss: 0.2423196 P@1: 0.61000 P@3: 0.45333 P@5: 0.38200 N@3: 0.49827 N@5: 0.50928 early stop: 5\n",
            "\u001b[32m[I 210629 03:11:16 models:110]\u001b[39m 237 512 train loss: 0.0002508 valid loss: 0.2429160 P@1: 0.61000 P@3: 0.45333 P@5: 0.38000 N@3: 0.49765 N@5: 0.50717 early stop: 6\n",
            "\u001b[32m[I 210629 03:11:18 models:110]\u001b[39m 239 1024 train loss: 0.0002295 valid loss: 0.2435312 P@1: 0.61000 P@3: 0.45333 P@5: 0.38200 N@3: 0.49704 N@5: 0.50804 early stop: 7\n",
            "\u001b[32m[I 210629 03:11:21 models:110]\u001b[39m 242 512 train loss: 0.0002351 valid loss: 0.2441846 P@1: 0.61000 P@3: 0.45333 P@5: 0.37800 N@3: 0.49704 N@5: 0.50522 early stop: 8\n",
            "\u001b[33m[W 210629 03:11:21 models:137]\u001b[39m Clipping gradients with total norm 0.06652 and max norm 0.01247\n",
            "\u001b[32m[I 210629 03:11:23 models:110]\u001b[39m 244 1024 train loss: 0.0003638 valid loss: 0.2448366 P@1: 0.61000 P@3: 0.45333 P@5: 0.37800 N@3: 0.49704 N@5: 0.50522 early stop: 9\n",
            "\u001b[32m[I 210629 03:11:25 models:110]\u001b[39m 247 512 train loss: 0.0003411 valid loss: 0.2454964 P@1: 0.61000 P@3: 0.45333 P@5: 0.37800 N@3: 0.49704 N@5: 0.50522 early stop: 10\n",
            "\u001b[33m[W 210629 03:11:26 models:137]\u001b[39m Clipping gradients with total norm 0.10968 and max norm 0.01103\n",
            "\u001b[32m[I 210629 03:11:27 models:110]\u001b[39m 249 1024 train loss: 0.0005720 valid loss: 0.2462144 P@1: 0.61000 P@3: 0.45000 P@5: 0.37800 N@3: 0.49469 N@5: 0.50498 early stop: 11\n",
            "\u001b[32m[I 210629 03:11:30 models:110]\u001b[39m 252 512 train loss: 0.0002957 valid loss: 0.2469316 P@1: 0.62000 P@3: 0.45333 P@5: 0.37800 N@3: 0.49877 N@5: 0.50647 early stop: 12\n",
            "\u001b[32m[I 210629 03:11:32 models:110]\u001b[39m 254 1024 train loss: 0.0001476 valid loss: 0.2476281 P@1: 0.61000 P@3: 0.45667 P@5: 0.37800 N@3: 0.50000 N@5: 0.50590 early stop: 13\n",
            "\u001b[32m[I 210629 03:11:34 models:110]\u001b[39m 257 512 train loss: 0.0002558 valid loss: 0.2483398 P@1: 0.61000 P@3: 0.45667 P@5: 0.37600 N@3: 0.50000 N@5: 0.50444 early stop: 14\n",
            "\u001b[32m[I 210629 03:11:36 models:110]\u001b[39m 259 1024 train loss: 0.0001381 valid loss: 0.2490756 P@1: 0.61000 P@3: 0.45667 P@5: 0.37600 N@3: 0.50000 N@5: 0.50444 early stop: 15\n",
            "\u001b[32m[I 210629 03:11:38 models:110]\u001b[39m 262 512 train loss: 0.0001635 valid loss: 0.2498107 P@1: 0.61000 P@3: 0.45667 P@5: 0.37600 N@3: 0.50000 N@5: 0.50444 early stop: 16\n",
            "\u001b[32m[I 210629 03:11:40 models:110]\u001b[39m 264 1024 train loss: 0.0001758 valid loss: 0.2505449 P@1: 0.61000 P@3: 0.45333 P@5: 0.37800 N@3: 0.49765 N@5: 0.50551 early stop: 17\n",
            "\u001b[32m[I 210629 03:11:43 models:110]\u001b[39m 267 512 train loss: 0.0001474 valid loss: 0.2512825 P@1: 0.61000 P@3: 0.46000 P@5: 0.37800 N@3: 0.50235 N@5: 0.50623 early stop: 18\n",
            "\u001b[32m[I 210629 03:11:45 models:110]\u001b[39m 269 1024 train loss: 0.0001720 valid loss: 0.2520390 P@1: 0.61000 P@3: 0.46000 P@5: 0.37800 N@3: 0.50235 N@5: 0.50623 early stop: 19\n",
            "\u001b[32m[I 210629 03:11:47 models:110]\u001b[39m 272 512 train loss: 0.0001416 valid loss: 0.2527793 P@1: 0.61000 P@3: 0.46000 P@5: 0.37800 N@3: 0.50235 N@5: 0.50643 early stop: 20\n",
            "\u001b[32m[I 210629 03:11:49 models:110]\u001b[39m 274 1024 train loss: 0.0001566 valid loss: 0.2535285 P@1: 0.61000 P@3: 0.46000 P@5: 0.37800 N@3: 0.50235 N@5: 0.50658 early stop: 21\n",
            "\u001b[33m[W 210629 03:11:50 models:137]\u001b[39m Clipping gradients with total norm 0.07337 and max norm 0.00975\n",
            "\u001b[32m[I 210629 03:11:52 models:110]\u001b[39m 277 512 train loss: 0.0004181 valid loss: 0.2542594 P@1: 0.61000 P@3: 0.46000 P@5: 0.37800 N@3: 0.50235 N@5: 0.50658 early stop: 22\n",
            "\u001b[32m[I 210629 03:11:54 models:110]\u001b[39m 279 1024 train loss: 0.0002200 valid loss: 0.2550193 P@1: 0.61000 P@3: 0.45667 P@5: 0.37400 N@3: 0.50000 N@5: 0.50363 early stop: 23\n",
            "\u001b[32m[I 210629 03:11:56 models:110]\u001b[39m 282 512 train loss: 0.0001537 valid loss: 0.2557780 P@1: 0.61000 P@3: 0.45667 P@5: 0.37400 N@3: 0.49939 N@5: 0.50302 early stop: 24\n",
            "\u001b[33m[W 210629 03:11:56 models:137]\u001b[39m Clipping gradients with total norm 0.06992 and max norm 0.00875\n",
            "\u001b[32m[I 210629 03:11:58 models:110]\u001b[39m 284 1024 train loss: 0.0004465 valid loss: 0.2565430 P@1: 0.61000 P@3: 0.45667 P@5: 0.37400 N@3: 0.49939 N@5: 0.50302 early stop: 25\n",
            "\u001b[32m[I 210629 03:12:01 models:110]\u001b[39m 287 512 train loss: 0.0002304 valid loss: 0.2572610 P@1: 0.61000 P@3: 0.45667 P@5: 0.37600 N@3: 0.49939 N@5: 0.50418 early stop: 26\n",
            "\u001b[32m[I 210629 03:12:03 models:110]\u001b[39m 289 1024 train loss: 0.0001240 valid loss: 0.2580333 P@1: 0.61000 P@3: 0.46000 P@5: 0.37600 N@3: 0.50235 N@5: 0.50512 early stop: 27\n",
            "\u001b[33m[W 210629 03:12:04 models:137]\u001b[39m Clipping gradients with total norm 0.04291 and max norm 0.00604\n",
            "\u001b[32m[I 210629 03:12:05 models:110]\u001b[39m 292 512 train loss: 0.0002986 valid loss: 0.2588018 P@1: 0.61000 P@3: 0.46000 P@5: 0.37600 N@3: 0.50235 N@5: 0.50512 early stop: 28\n",
            "\u001b[32m[I 210629 03:12:07 models:110]\u001b[39m 294 1024 train loss: 0.0001686 valid loss: 0.2595654 P@1: 0.61000 P@3: 0.46000 P@5: 0.37600 N@3: 0.50235 N@5: 0.50512 early stop: 29\n",
            "\u001b[32m[I 210629 03:12:10 models:110]\u001b[39m 297 512 train loss: 0.0001258 valid loss: 0.2603521 P@1: 0.61000 P@3: 0.46000 P@5: 0.37600 N@3: 0.50235 N@5: 0.50512 early stop: 30\n",
            "\u001b[32m[I 210629 03:12:12 models:110]\u001b[39m 299 1024 train loss: 0.0001502 valid loss: 0.2611286 P@1: 0.61000 P@3: 0.46000 P@5: 0.37600 N@3: 0.50235 N@5: 0.50512 early stop: 31\n",
            "\u001b[32m[I 210629 03:12:14 models:110]\u001b[39m 302 512 train loss: 0.0001318 valid loss: 0.2618903 P@1: 0.61000 P@3: 0.46667 P@5: 0.37600 N@3: 0.50642 N@5: 0.50524 early stop: 32\n",
            "\u001b[32m[I 210629 03:12:16 models:110]\u001b[39m 304 1024 train loss: 0.0001285 valid loss: 0.2626359 P@1: 0.61000 P@3: 0.46667 P@5: 0.37600 N@3: 0.50642 N@5: 0.50524 early stop: 33\n",
            "\u001b[32m[I 210629 03:12:19 models:110]\u001b[39m 307 512 train loss: 0.0000912 valid loss: 0.2633810 P@1: 0.61000 P@3: 0.46667 P@5: 0.37600 N@3: 0.50642 N@5: 0.50524 early stop: 34\n",
            "\u001b[32m[I 210629 03:12:21 models:110]\u001b[39m 309 1024 train loss: 0.0001583 valid loss: 0.2641213 P@1: 0.62000 P@3: 0.46667 P@5: 0.37600 N@3: 0.50877 N@5: 0.50741 early stop: 35\n",
            "\u001b[32m[I 210629 03:12:23 models:110]\u001b[39m 312 512 train loss: 0.0001426 valid loss: 0.2648665 P@1: 0.62000 P@3: 0.46667 P@5: 0.37600 N@3: 0.50877 N@5: 0.50741 early stop: 36\n",
            "\u001b[33m[W 210629 03:12:24 models:137]\u001b[39m Clipping gradients with total norm 0.04149 and max norm 0.00587\n",
            "\u001b[32m[I 210629 03:12:25 models:110]\u001b[39m 314 1024 train loss: 0.0002547 valid loss: 0.2656115 P@1: 0.62000 P@3: 0.46667 P@5: 0.37600 N@3: 0.50877 N@5: 0.50741 early stop: 37\n",
            "\u001b[32m[I 210629 03:12:28 models:110]\u001b[39m 317 512 train loss: 0.0002167 valid loss: 0.2663592 P@1: 0.62000 P@3: 0.46667 P@5: 0.37800 N@3: 0.50877 N@5: 0.50872 early stop: 38\n",
            "\u001b[32m[I 210629 03:12:30 models:110]\u001b[39m 319 1024 train loss: 0.0000962 valid loss: 0.2671005 P@1: 0.62000 P@3: 0.46667 P@5: 0.37800 N@3: 0.50877 N@5: 0.50887 early stop: 39\n",
            "\u001b[32m[I 210629 03:12:32 models:110]\u001b[39m 322 512 train loss: 0.0000997 valid loss: 0.2678510 P@1: 0.62000 P@3: 0.46667 P@5: 0.37800 N@3: 0.50877 N@5: 0.50887 early stop: 40\n",
            "\u001b[33m[W 210629 03:12:33 models:137]\u001b[39m Clipping gradients with total norm 0.05621 and max norm 0.00451\n",
            "\u001b[32m[I 210629 03:12:34 models:110]\u001b[39m 324 1024 train loss: 0.0003625 valid loss: 0.2686021 P@1: 0.62000 P@3: 0.46667 P@5: 0.37800 N@3: 0.50877 N@5: 0.50887 early stop: 41\n",
            "\u001b[32m[I 210629 03:12:37 models:110]\u001b[39m 327 512 train loss: 0.0001284 valid loss: 0.2693352 P@1: 0.62000 P@3: 0.46667 P@5: 0.37800 N@3: 0.50877 N@5: 0.50887 early stop: 42\n",
            "\u001b[32m[I 210629 03:12:39 models:110]\u001b[39m 329 1024 train loss: 0.0001207 valid loss: 0.2700659 P@1: 0.62000 P@3: 0.46667 P@5: 0.38000 N@3: 0.50877 N@5: 0.51069 early stop: 43\n",
            "\u001b[32m[I 210629 03:12:41 models:110]\u001b[39m 332 512 train loss: 0.0001397 valid loss: 0.2707922 P@1: 0.62000 P@3: 0.46667 P@5: 0.38000 N@3: 0.50877 N@5: 0.51069 early stop: 44\n",
            "\u001b[32m[I 210629 03:12:43 models:110]\u001b[39m 334 1024 train loss: 0.0001124 valid loss: 0.2715004 P@1: 0.62000 P@3: 0.46667 P@5: 0.38000 N@3: 0.50877 N@5: 0.51069 early stop: 45\n",
            "\u001b[32m[I 210629 03:12:46 models:110]\u001b[39m 337 512 train loss: 0.0001246 valid loss: 0.2722195 P@1: 0.62000 P@3: 0.46667 P@5: 0.37800 N@3: 0.50877 N@5: 0.50887 early stop: 46\n",
            "\u001b[33m[W 210629 03:12:48 models:137]\u001b[39m Clipping gradients with total norm 0.03494 and max norm 0.00689\n",
            "\u001b[32m[I 210629 03:12:48 models:110]\u001b[39m 339 1024 train loss: 0.0002476 valid loss: 0.2729383 P@1: 0.62000 P@3: 0.46667 P@5: 0.37800 N@3: 0.50877 N@5: 0.50887 early stop: 47\n",
            "\u001b[32m[I 210629 03:12:50 models:110]\u001b[39m 342 512 train loss: 0.0002577 valid loss: 0.2736435 P@1: 0.62000 P@3: 0.46667 P@5: 0.37800 N@3: 0.50877 N@5: 0.50887 early stop: 48\n",
            "\u001b[32m[I 210629 03:12:52 models:110]\u001b[39m 344 1024 train loss: 0.0001499 valid loss: 0.2743684 P@1: 0.62000 P@3: 0.46333 P@5: 0.37800 N@3: 0.50642 N@5: 0.50864 early stop: 49\n",
            "\u001b[33m[W 210629 03:12:54 models:137]\u001b[39m Clipping gradients with total norm 0.02832 and max norm 0.00561\n",
            "\u001b[32m[I 210629 03:12:55 models:110]\u001b[39m 347 512 train loss: 0.0002298 valid loss: 0.2750919 P@1: 0.62000 P@3: 0.46333 P@5: 0.37800 N@3: 0.50642 N@5: 0.50864 early stop: 50\n",
            "\u001b[32m[I 210629 03:12:57 main:76]\u001b[39m Finish Training\n",
            "\u001b[32m[I 210629 03:12:59 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210629 03:12:59 main:79]\u001b[39m Loading Test Set\n",
            "\u001b[32m[I 210629 03:12:59 main:83]\u001b[39m Size of Test Set: 100\n",
            "\u001b[32m[I 210629 03:12:59 main:85]\u001b[39m Predicting\n",
            "\u001b[32m[I 210629 03:13:02 main:91]\u001b[39m Finish Predicting\n",
            "Precision@1,3,5: 0.63 0.4766666666666667 0.388\n",
            "nDCG@1,3,5: 0.63 0.5156990216431859 0.5247429975633092\n",
            "\n",
            "WITH MAG WITH MESH, skip=300\n",
            "\n",
            "/content/drive/Shareddrives/NASA/MATCH/PeTaL\n",
            "130\n",
            "/content/drive/Shareddrives/NASA/MATCH\n",
            "    800  288986 4967896 PeTaL/train.json\n",
            "\u001b[32m[I 210629 03:13:05 preprocess:28]\u001b[39m Vocab Size: 26834\n",
            "\u001b[32m[I 210629 03:13:05 preprocess:30]\u001b[39m Getting Dataset: PeTaL/train_texts.txt Max Length: 500\n",
            "\u001b[32m[I 210629 03:13:05 preprocess:32]\u001b[39m Size of Samples: 900\n",
            "\u001b[32m[I 210629 03:13:06 preprocess:28]\u001b[39m Vocab Size: 26834\n",
            "\u001b[32m[I 210629 03:13:06 preprocess:30]\u001b[39m Getting Dataset: PeTaL/test_texts.txt Max Length: 500\n",
            "\u001b[32m[I 210629 03:13:06 preprocess:32]\u001b[39m Size of Samples: 100\n",
            "\u001b[32m[I 210629 03:13:08 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210629 03:13:08 main:35]\u001b[39m Loading Training and Validation Set\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['absorb_and/or_filter_solids', 'chemically_break_down_inorganic_compounds', 'detox/purify', 'protect_from_fire', 'protect_from_gases', 'send_vibratory_signals'] will be ignored\n",
            "  .format(sorted(unknown, key=str)))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['detox/purify', 'protect_from_gases'] will be ignored\n",
            "  .format(sorted(unknown, key=str)))\n",
            "\u001b[32m[I 210629 03:13:08 main:47]\u001b[39m Number of Labels: 124\n",
            "\u001b[32m[I 210629 03:13:08 main:48]\u001b[39m Size of Training Set: 800\n",
            "\u001b[32m[I 210629 03:13:08 main:49]\u001b[39m Size of Validation Set: 100\n",
            "\u001b[32m[I 210629 03:13:08 main:66]\u001b[39m Number of Edges: 101\n",
            "\u001b[32m[I 210629 03:13:08 main:68]\u001b[39m Training\n",
            "\u001b[32m[I 210629 03:13:14 models:110]\u001b[39m 2 512 train loss: 0.2752265 valid loss: 0.1590435 P@1: 0.37000 P@3: 0.19667 P@5: 0.15600 N@3: 0.23580 N@5: 0.22545 early stop: 0\n",
            "\u001b[32m[I 210629 03:13:15 models:142]\u001b[39m SWA Initializing\n",
            "\u001b[32m[I 210629 03:13:16 models:110]\u001b[39m 4 1024 train loss: 0.1436258 valid loss: 0.1479441 P@1: 0.37000 P@3: 0.24333 P@5: 0.22800 N@3: 0.27366 N@5: 0.28833 early stop: 0\n",
            "\u001b[32m[I 210629 03:13:18 models:110]\u001b[39m 7 512 train loss: 0.1416691 valid loss: 0.1466857 P@1: 0.37000 P@3: 0.25333 P@5: 0.22800 N@3: 0.27682 N@5: 0.28775 early stop: 1\n",
            "\u001b[32m[I 210629 03:13:20 models:110]\u001b[39m 9 1024 train loss: 0.1379219 valid loss: 0.1460398 P@1: 0.37000 P@3: 0.26000 P@5: 0.22800 N@3: 0.28662 N@5: 0.29239 early stop: 0\n",
            "\u001b[32m[I 210629 03:13:23 models:110]\u001b[39m 12 512 train loss: 0.1377144 valid loss: 0.1459345 P@1: 0.37000 P@3: 0.27667 P@5: 0.22800 N@3: 0.29897 N@5: 0.29363 early stop: 0\n",
            "\u001b[32m[I 210629 03:13:25 models:110]\u001b[39m 14 1024 train loss: 0.1267773 valid loss: 0.1452516 P@1: 0.37000 P@3: 0.28000 P@5: 0.23000 N@3: 0.30131 N@5: 0.29589 early stop: 0\n",
            "\u001b[32m[I 210629 03:13:27 models:110]\u001b[39m 17 512 train loss: 0.1078124 valid loss: 0.1447463 P@1: 0.37000 P@3: 0.28333 P@5: 0.26000 N@3: 0.30366 N@5: 0.32001 early stop: 0\n",
            "\u001b[32m[I 210629 03:13:29 models:110]\u001b[39m 19 1024 train loss: 0.0875846 valid loss: 0.1443909 P@1: 0.37000 P@3: 0.28333 P@5: 0.26200 N@3: 0.30673 N@5: 0.32613 early stop: 0\n",
            "\u001b[32m[I 210629 03:13:32 models:110]\u001b[39m 22 512 train loss: 0.0712230 valid loss: 0.1440294 P@1: 0.37000 P@3: 0.30667 P@5: 0.27200 N@3: 0.32746 N@5: 0.33959 early stop: 0\n",
            "\u001b[32m[I 210629 03:13:34 models:110]\u001b[39m 24 1024 train loss: 0.0632170 valid loss: 0.1433855 P@1: 0.44000 P@3: 0.32333 P@5: 0.27200 N@3: 0.35388 N@5: 0.35249 early stop: 0\n",
            "\u001b[32m[I 210629 03:13:37 models:110]\u001b[39m 27 512 train loss: 0.0532202 valid loss: 0.1426716 P@1: 0.50000 P@3: 0.33333 P@5: 0.28200 N@3: 0.37590 N@5: 0.37444 early stop: 0\n",
            "\u001b[32m[I 210629 03:13:39 models:110]\u001b[39m 29 1024 train loss: 0.0441620 valid loss: 0.1419749 P@1: 0.55000 P@3: 0.35000 P@5: 0.27600 N@3: 0.39966 N@5: 0.37910 early stop: 0\n",
            "\u001b[32m[I 210629 03:13:41 models:110]\u001b[39m 32 512 train loss: 0.0365958 valid loss: 0.1411311 P@1: 0.56000 P@3: 0.34667 P@5: 0.28400 N@3: 0.40029 N@5: 0.38864 early stop: 0\n",
            "\u001b[32m[I 210629 03:13:43 models:110]\u001b[39m 34 1024 train loss: 0.0276591 valid loss: 0.1404938 P@1: 0.54000 P@3: 0.35667 P@5: 0.27800 N@3: 0.40448 N@5: 0.38470 early stop: 1\n",
            "\u001b[32m[I 210629 03:13:46 models:110]\u001b[39m 37 512 train loss: 0.0234968 valid loss: 0.1400886 P@1: 0.56000 P@3: 0.37333 P@5: 0.28000 N@3: 0.42029 N@5: 0.39150 early stop: 0\n",
            "\u001b[32m[I 210629 03:13:48 models:110]\u001b[39m 39 1024 train loss: 0.0247167 valid loss: 0.1400170 P@1: 0.53000 P@3: 0.39333 P@5: 0.29000 N@3: 0.42795 N@5: 0.39506 early stop: 0\n",
            "\u001b[32m[I 210629 03:13:50 models:110]\u001b[39m 42 512 train loss: 0.0217201 valid loss: 0.1402453 P@1: 0.54000 P@3: 0.39333 P@5: 0.29200 N@3: 0.43152 N@5: 0.39976 early stop: 0\n",
            "\u001b[32m[I 210629 03:13:52 models:110]\u001b[39m 44 1024 train loss: 0.0216022 valid loss: 0.1404344 P@1: 0.54000 P@3: 0.39667 P@5: 0.29800 N@3: 0.43430 N@5: 0.40474 early stop: 0\n",
            "\u001b[32m[I 210629 03:13:55 models:110]\u001b[39m 47 512 train loss: 0.0184414 valid loss: 0.1405478 P@1: 0.54000 P@3: 0.40000 P@5: 0.30400 N@3: 0.43715 N@5: 0.41224 early stop: 0\n",
            "\u001b[32m[I 210629 03:13:57 models:110]\u001b[39m 49 1024 train loss: 0.0166510 valid loss: 0.1408834 P@1: 0.58000 P@3: 0.40333 P@5: 0.31000 N@3: 0.44888 N@5: 0.42639 early stop: 0\n",
            "\u001b[32m[I 210629 03:13:59 models:110]\u001b[39m 52 512 train loss: 0.0121416 valid loss: 0.1415878 P@1: 0.58000 P@3: 0.40000 P@5: 0.32200 N@3: 0.44511 N@5: 0.43554 early stop: 0\n",
            "\u001b[32m[I 210629 03:14:02 models:110]\u001b[39m 54 1024 train loss: 0.0098884 valid loss: 0.1420703 P@1: 0.59000 P@3: 0.40333 P@5: 0.33400 N@3: 0.44919 N@5: 0.44703 early stop: 0\n",
            "\u001b[32m[I 210629 03:14:04 models:110]\u001b[39m 57 512 train loss: 0.0082838 valid loss: 0.1433135 P@1: 0.60000 P@3: 0.40000 P@5: 0.33000 N@3: 0.44858 N@5: 0.44642 early stop: 1\n",
            "\u001b[32m[I 210629 03:14:06 models:110]\u001b[39m 59 1024 train loss: 0.0095596 valid loss: 0.1443319 P@1: 0.61000 P@3: 0.39667 P@5: 0.33200 N@3: 0.44930 N@5: 0.44737 early stop: 0\n",
            "\u001b[32m[I 210629 03:14:09 models:110]\u001b[39m 62 512 train loss: 0.0099014 valid loss: 0.1456453 P@1: 0.62000 P@3: 0.40000 P@5: 0.34000 N@3: 0.45337 N@5: 0.45531 early stop: 0\n",
            "\u001b[32m[I 210629 03:14:11 models:110]\u001b[39m 64 1024 train loss: 0.0127706 valid loss: 0.1462070 P@1: 0.62000 P@3: 0.41000 P@5: 0.35200 N@3: 0.46226 N@5: 0.46793 early stop: 0\n",
            "\u001b[32m[I 210629 03:14:13 models:110]\u001b[39m 67 512 train loss: 0.0162026 valid loss: 0.1476569 P@1: 0.62000 P@3: 0.41667 P@5: 0.35400 N@3: 0.46695 N@5: 0.47044 early stop: 0\n",
            "\u001b[32m[I 210629 03:14:15 models:110]\u001b[39m 69 1024 train loss: 0.0120936 valid loss: 0.1482649 P@1: 0.61000 P@3: 0.42667 P@5: 0.35800 N@3: 0.47287 N@5: 0.47376 early stop: 0\n",
            "\u001b[32m[I 210629 03:14:18 models:110]\u001b[39m 72 512 train loss: 0.0117653 valid loss: 0.1493619 P@1: 0.62000 P@3: 0.42667 P@5: 0.36200 N@3: 0.47460 N@5: 0.47842 early stop: 0\n",
            "\u001b[32m[I 210629 03:14:20 models:110]\u001b[39m 74 1024 train loss: 0.0074488 valid loss: 0.1505912 P@1: 0.61000 P@3: 0.43000 P@5: 0.36600 N@3: 0.47645 N@5: 0.48086 early stop: 0\n",
            "\u001b[32m[I 210629 03:14:23 models:110]\u001b[39m 77 512 train loss: 0.0049816 valid loss: 0.1518785 P@1: 0.61000 P@3: 0.44000 P@5: 0.36800 N@3: 0.48642 N@5: 0.48523 early stop: 0\n",
            "\u001b[32m[I 210629 03:14:25 models:110]\u001b[39m 79 1024 train loss: 0.0049989 valid loss: 0.1533180 P@1: 0.61000 P@3: 0.44333 P@5: 0.37200 N@3: 0.49000 N@5: 0.48992 early stop: 0\n",
            "\u001b[32m[I 210629 03:14:27 models:110]\u001b[39m 82 512 train loss: 0.0041581 valid loss: 0.1547447 P@1: 0.62000 P@3: 0.44667 P@5: 0.37800 N@3: 0.49408 N@5: 0.49612 early stop: 0\n",
            "\u001b[32m[I 210629 03:14:29 models:110]\u001b[39m 84 1024 train loss: 0.0027679 valid loss: 0.1563439 P@1: 0.61000 P@3: 0.45000 P@5: 0.37800 N@3: 0.49469 N@5: 0.49596 early stop: 1\n",
            "\u001b[32m[I 210629 03:14:32 models:110]\u001b[39m 87 512 train loss: 0.0018721 valid loss: 0.1580095 P@1: 0.60000 P@3: 0.45000 P@5: 0.37600 N@3: 0.49235 N@5: 0.49230 early stop: 2\n",
            "\u001b[32m[I 210629 03:14:34 models:110]\u001b[39m 89 1024 train loss: 0.0016913 valid loss: 0.1597189 P@1: 0.61000 P@3: 0.46000 P@5: 0.37600 N@3: 0.50173 N@5: 0.49591 early stop: 3\n",
            "\u001b[32m[I 210629 03:14:36 models:110]\u001b[39m 92 512 train loss: 0.0022455 valid loss: 0.1614586 P@1: 0.61000 P@3: 0.45667 P@5: 0.37800 N@3: 0.49877 N@5: 0.49637 early stop: 0\n",
            "\u001b[32m[I 210629 03:14:38 models:110]\u001b[39m 94 1024 train loss: 0.0021982 valid loss: 0.1632911 P@1: 0.61000 P@3: 0.45667 P@5: 0.38200 N@3: 0.49877 N@5: 0.49915 early stop: 0\n",
            "\u001b[32m[I 210629 03:14:41 models:110]\u001b[39m 97 512 train loss: 0.0013355 valid loss: 0.1652512 P@1: 0.61000 P@3: 0.47000 P@5: 0.38200 N@3: 0.50877 N@5: 0.50159 early stop: 0\n",
            "\u001b[32m[I 210629 03:14:43 models:110]\u001b[39m 99 1024 train loss: 0.0014444 valid loss: 0.1672391 P@1: 0.61000 P@3: 0.47000 P@5: 0.38800 N@3: 0.50877 N@5: 0.50469 early stop: 0\n",
            "\u001b[32m[I 210629 03:14:45 models:110]\u001b[39m 102 512 train loss: 0.0015957 valid loss: 0.1693076 P@1: 0.60000 P@3: 0.47667 P@5: 0.38800 N@3: 0.51112 N@5: 0.50382 early stop: 1\n",
            "\u001b[32m[I 210629 03:14:47 models:110]\u001b[39m 104 1024 train loss: 0.0028020 valid loss: 0.1710030 P@1: 0.61000 P@3: 0.48000 P@5: 0.38600 N@3: 0.51458 N@5: 0.50355 early stop: 2\n",
            "\u001b[32m[I 210629 03:14:50 models:110]\u001b[39m 107 512 train loss: 0.0056580 valid loss: 0.1727684 P@1: 0.60000 P@3: 0.48000 P@5: 0.38600 N@3: 0.51285 N@5: 0.50267 early stop: 3\n",
            "\u001b[32m[I 210629 03:14:52 models:110]\u001b[39m 109 1024 train loss: 0.0079876 valid loss: 0.1744749 P@1: 0.60000 P@3: 0.48000 P@5: 0.38800 N@3: 0.51346 N@5: 0.50508 early stop: 0\n",
            "\u001b[32m[I 210629 03:14:54 models:110]\u001b[39m 112 512 train loss: 0.0087265 valid loss: 0.1759101 P@1: 0.60000 P@3: 0.48000 P@5: 0.39000 N@3: 0.51346 N@5: 0.50720 early stop: 0\n",
            "\u001b[32m[I 210629 03:14:57 models:110]\u001b[39m 114 1024 train loss: 0.0116288 valid loss: 0.1772905 P@1: 0.60000 P@3: 0.47667 P@5: 0.39400 N@3: 0.51112 N@5: 0.50958 early stop: 0\n",
            "\u001b[32m[I 210629 03:14:59 models:110]\u001b[39m 117 512 train loss: 0.0097659 valid loss: 0.1787042 P@1: 0.60000 P@3: 0.47667 P@5: 0.39400 N@3: 0.51173 N@5: 0.51003 early stop: 0\n",
            "\u001b[32m[I 210629 03:15:01 models:110]\u001b[39m 119 1024 train loss: 0.0093297 valid loss: 0.1799689 P@1: 0.60000 P@3: 0.47667 P@5: 0.39200 N@3: 0.51173 N@5: 0.50857 early stop: 1\n",
            "\u001b[32m[I 210629 03:15:04 models:110]\u001b[39m 122 512 train loss: 0.0064745 valid loss: 0.1811516 P@1: 0.60000 P@3: 0.47667 P@5: 0.39400 N@3: 0.51173 N@5: 0.51008 early stop: 0\n",
            "\u001b[32m[I 210629 03:15:06 models:110]\u001b[39m 124 1024 train loss: 0.0031669 valid loss: 0.1824818 P@1: 0.60000 P@3: 0.48667 P@5: 0.39200 N@3: 0.51877 N@5: 0.50945 early stop: 1\n",
            "\u001b[32m[I 210629 03:15:08 models:110]\u001b[39m 127 512 train loss: 0.0017105 valid loss: 0.1839390 P@1: 0.62000 P@3: 0.49000 P@5: 0.39400 N@3: 0.52520 N@5: 0.51422 early stop: 0\n",
            "\u001b[32m[I 210629 03:15:10 models:110]\u001b[39m 129 1024 train loss: 0.0019500 valid loss: 0.1854889 P@1: 0.62000 P@3: 0.49333 P@5: 0.40000 N@3: 0.52754 N@5: 0.51981 early stop: 0\n",
            "\u001b[32m[I 210629 03:15:13 models:110]\u001b[39m 132 512 train loss: 0.0014690 valid loss: 0.1871034 P@1: 0.63000 P@3: 0.49333 P@5: 0.40200 N@3: 0.52927 N@5: 0.52294 early stop: 0\n",
            "\u001b[32m[I 210629 03:15:15 models:110]\u001b[39m 134 1024 train loss: 0.0010044 valid loss: 0.1885739 P@1: 0.63000 P@3: 0.48667 P@5: 0.40000 N@3: 0.52458 N@5: 0.52036 early stop: 1\n",
            "\u001b[32m[I 210629 03:15:17 models:110]\u001b[39m 137 512 train loss: 0.0008548 valid loss: 0.1901780 P@1: 0.63000 P@3: 0.49333 P@5: 0.40200 N@3: 0.52847 N@5: 0.52154 early stop: 2\n",
            "\u001b[32m[I 210629 03:15:19 models:110]\u001b[39m 139 1024 train loss: 0.0007108 valid loss: 0.1917726 P@1: 0.63000 P@3: 0.49333 P@5: 0.40600 N@3: 0.52909 N@5: 0.52489 early stop: 0\n",
            "\u001b[32m[I 210629 03:15:22 models:110]\u001b[39m 142 512 train loss: 0.0005472 valid loss: 0.1933496 P@1: 0.63000 P@3: 0.49333 P@5: 0.40400 N@3: 0.52970 N@5: 0.52334 early stop: 1\n",
            "\u001b[32m[I 210629 03:15:24 models:110]\u001b[39m 144 1024 train loss: 0.0003520 valid loss: 0.1950122 P@1: 0.63000 P@3: 0.49333 P@5: 0.40400 N@3: 0.52970 N@5: 0.52278 early stop: 2\n",
            "\u001b[32m[I 210629 03:15:26 models:110]\u001b[39m 147 512 train loss: 0.0003471 valid loss: 0.1966376 P@1: 0.63000 P@3: 0.49333 P@5: 0.40400 N@3: 0.52970 N@5: 0.52278 early stop: 3\n",
            "\u001b[32m[I 210629 03:15:28 models:110]\u001b[39m 149 1024 train loss: 0.0002101 valid loss: 0.1982678 P@1: 0.63000 P@3: 0.49667 P@5: 0.40400 N@3: 0.53205 N@5: 0.52337 early stop: 4\n",
            "\u001b[32m[I 210629 03:15:31 models:110]\u001b[39m 152 512 train loss: 0.0002112 valid loss: 0.1999176 P@1: 0.63000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53205 N@5: 0.52453 early stop: 5\n",
            "\u001b[32m[I 210629 03:15:33 models:110]\u001b[39m 154 1024 train loss: 0.0001896 valid loss: 0.2014668 P@1: 0.63000 P@3: 0.50667 P@5: 0.40200 N@3: 0.53909 N@5: 0.52300 early stop: 6\n",
            "\u001b[32m[I 210629 03:15:35 models:110]\u001b[39m 157 512 train loss: 0.0001755 valid loss: 0.2030505 P@1: 0.63000 P@3: 0.50667 P@5: 0.40200 N@3: 0.53909 N@5: 0.52320 early stop: 7\n",
            "\u001b[32m[I 210629 03:15:37 models:110]\u001b[39m 159 1024 train loss: 0.0001938 valid loss: 0.2046759 P@1: 0.63000 P@3: 0.50667 P@5: 0.40200 N@3: 0.53909 N@5: 0.52273 early stop: 8\n",
            "\u001b[32m[I 210629 03:15:40 models:110]\u001b[39m 162 512 train loss: 0.0001607 valid loss: 0.2062595 P@1: 0.63000 P@3: 0.51000 P@5: 0.40200 N@3: 0.54205 N@5: 0.52341 early stop: 9\n",
            "\u001b[32m[I 210629 03:15:42 models:110]\u001b[39m 164 1024 train loss: 0.0001879 valid loss: 0.2078176 P@1: 0.63000 P@3: 0.51000 P@5: 0.40000 N@3: 0.54205 N@5: 0.52103 early stop: 10\n",
            "\u001b[32m[I 210629 03:15:44 models:110]\u001b[39m 167 512 train loss: 0.0001847 valid loss: 0.2093842 P@1: 0.63000 P@3: 0.50667 P@5: 0.39600 N@3: 0.53970 N@5: 0.51788 early stop: 11\n",
            "\u001b[33m[W 210629 03:15:45 models:137]\u001b[39m Clipping gradients with total norm 0.05034 and max norm 0.00897\n",
            "\u001b[32m[I 210629 03:15:46 models:110]\u001b[39m 169 1024 train loss: 0.0002401 valid loss: 0.2109207 P@1: 0.63000 P@3: 0.51000 P@5: 0.39400 N@3: 0.54205 N@5: 0.51680 early stop: 12\n",
            "\u001b[32m[I 210629 03:15:49 models:110]\u001b[39m 172 512 train loss: 0.0003174 valid loss: 0.2124383 P@1: 0.62000 P@3: 0.51000 P@5: 0.39600 N@3: 0.54031 N@5: 0.51638 early stop: 13\n",
            "\u001b[33m[W 210629 03:15:50 models:137]\u001b[39m Clipping gradients with total norm 0.15551 and max norm 0.01266\n",
            "\u001b[32m[I 210629 03:15:51 models:110]\u001b[39m 174 1024 train loss: 0.0002212 valid loss: 0.2139440 P@1: 0.62000 P@3: 0.51333 P@5: 0.39600 N@3: 0.54266 N@5: 0.51653 early stop: 14\n",
            "\u001b[32m[I 210629 03:15:53 models:110]\u001b[39m 177 512 train loss: 0.0002368 valid loss: 0.2154741 P@1: 0.62000 P@3: 0.51000 P@5: 0.39800 N@3: 0.54031 N@5: 0.51780 early stop: 15\n",
            "\u001b[32m[I 210629 03:15:55 models:110]\u001b[39m 179 1024 train loss: 0.0003598 valid loss: 0.2169047 P@1: 0.62000 P@3: 0.51000 P@5: 0.39800 N@3: 0.54031 N@5: 0.51780 early stop: 16\n",
            "\u001b[32m[I 210629 03:15:58 models:110]\u001b[39m 182 512 train loss: 0.0002420 valid loss: 0.2183630 P@1: 0.62000 P@3: 0.51000 P@5: 0.39800 N@3: 0.54031 N@5: 0.51780 early stop: 17\n",
            "\u001b[32m[I 210629 03:16:00 models:110]\u001b[39m 184 1024 train loss: 0.0003776 valid loss: 0.2197924 P@1: 0.62000 P@3: 0.51000 P@5: 0.39800 N@3: 0.54031 N@5: 0.51780 early stop: 18\n",
            "\u001b[32m[I 210629 03:16:02 models:110]\u001b[39m 187 512 train loss: 0.0004038 valid loss: 0.2212715 P@1: 0.62000 P@3: 0.51333 P@5: 0.39800 N@3: 0.54266 N@5: 0.51807 early stop: 19\n",
            "\u001b[32m[I 210629 03:16:04 models:110]\u001b[39m 189 1024 train loss: 0.0001892 valid loss: 0.2227576 P@1: 0.62000 P@3: 0.51333 P@5: 0.39800 N@3: 0.54266 N@5: 0.51787 early stop: 20\n",
            "\u001b[32m[I 210629 03:16:07 models:110]\u001b[39m 192 512 train loss: 0.0001804 valid loss: 0.2241762 P@1: 0.62000 P@3: 0.51333 P@5: 0.39800 N@3: 0.54266 N@5: 0.51787 early stop: 21\n",
            "\u001b[32m[I 210629 03:16:09 models:110]\u001b[39m 194 1024 train loss: 0.0001768 valid loss: 0.2255774 P@1: 0.62000 P@3: 0.51333 P@5: 0.39800 N@3: 0.54266 N@5: 0.51804 early stop: 22\n",
            "\u001b[33m[W 210629 03:16:10 models:137]\u001b[39m Clipping gradients with total norm 0.12568 and max norm 0.01347\n",
            "\u001b[32m[I 210629 03:16:11 models:110]\u001b[39m 197 512 train loss: 0.0004381 valid loss: 0.2269773 P@1: 0.62000 P@3: 0.51333 P@5: 0.39800 N@3: 0.54328 N@5: 0.51865 early stop: 23\n",
            "\u001b[32m[I 210629 03:16:13 models:110]\u001b[39m 199 1024 train loss: 0.0003161 valid loss: 0.2283404 P@1: 0.62000 P@3: 0.51333 P@5: 0.39800 N@3: 0.54328 N@5: 0.51865 early stop: 24\n",
            "\u001b[32m[I 210629 03:16:16 models:110]\u001b[39m 202 512 train loss: 0.0002875 valid loss: 0.2295356 P@1: 0.62000 P@3: 0.51333 P@5: 0.39800 N@3: 0.54266 N@5: 0.51850 early stop: 25\n",
            "\u001b[32m[I 210629 03:16:18 models:110]\u001b[39m 204 1024 train loss: 0.0004457 valid loss: 0.2308924 P@1: 0.62000 P@3: 0.51333 P@5: 0.39800 N@3: 0.54266 N@5: 0.51850 early stop: 26\n",
            "\u001b[32m[I 210629 03:16:20 models:110]\u001b[39m 207 512 train loss: 0.0002921 valid loss: 0.2321912 P@1: 0.62000 P@3: 0.51333 P@5: 0.40200 N@3: 0.54266 N@5: 0.52163 early stop: 27\n",
            "\u001b[32m[I 210629 03:16:22 models:110]\u001b[39m 209 1024 train loss: 0.0001398 valid loss: 0.2335082 P@1: 0.63000 P@3: 0.51333 P@5: 0.40200 N@3: 0.54439 N@5: 0.52322 early stop: 28\n",
            "\u001b[33m[W 210629 03:16:23 models:137]\u001b[39m Clipping gradients with total norm 0.09316 and max norm 0.01193\n",
            "\u001b[32m[I 210629 03:16:25 models:110]\u001b[39m 212 512 train loss: 0.0004686 valid loss: 0.2348206 P@1: 0.63000 P@3: 0.51333 P@5: 0.40200 N@3: 0.54439 N@5: 0.52325 early stop: 29\n",
            "\u001b[32m[I 210629 03:16:27 models:110]\u001b[39m 214 1024 train loss: 0.0002696 valid loss: 0.2360770 P@1: 0.63000 P@3: 0.51333 P@5: 0.40400 N@3: 0.54439 N@5: 0.52456 early stop: 30\n",
            "\u001b[32m[I 210629 03:16:29 models:110]\u001b[39m 217 512 train loss: 0.0002068 valid loss: 0.2373560 P@1: 0.63000 P@3: 0.51667 P@5: 0.40200 N@3: 0.54920 N@5: 0.52530 early stop: 0\n",
            "\u001b[32m[I 210629 03:16:31 models:110]\u001b[39m 219 1024 train loss: 0.0001342 valid loss: 0.2386066 P@1: 0.63000 P@3: 0.51667 P@5: 0.40200 N@3: 0.54920 N@5: 0.52530 early stop: 1\n",
            "\u001b[32m[I 210629 03:16:34 models:110]\u001b[39m 222 512 train loss: 0.0001552 valid loss: 0.2398585 P@1: 0.63000 P@3: 0.51667 P@5: 0.40200 N@3: 0.54981 N@5: 0.52575 early stop: 0\n",
            "\u001b[32m[I 210629 03:16:36 models:110]\u001b[39m 224 1024 train loss: 0.0001174 valid loss: 0.2411343 P@1: 0.63000 P@3: 0.51667 P@5: 0.40200 N@3: 0.54981 N@5: 0.52554 early stop: 1\n",
            "\u001b[33m[W 210629 03:16:38 models:137]\u001b[39m Clipping gradients with total norm 0.06827 and max norm 0.00707\n",
            "\u001b[32m[I 210629 03:16:39 models:110]\u001b[39m 227 512 train loss: 0.0003332 valid loss: 0.2423924 P@1: 0.63000 P@3: 0.51667 P@5: 0.40400 N@3: 0.54981 N@5: 0.52685 early stop: 0\n",
            "\u001b[32m[I 210629 03:16:41 models:110]\u001b[39m 229 1024 train loss: 0.0001897 valid loss: 0.2436087 P@1: 0.63000 P@3: 0.51667 P@5: 0.40400 N@3: 0.54981 N@5: 0.52685 early stop: 1\n",
            "\u001b[32m[I 210629 03:16:43 models:110]\u001b[39m 232 512 train loss: 0.0001347 valid loss: 0.2448222 P@1: 0.63000 P@3: 0.51667 P@5: 0.40400 N@3: 0.54981 N@5: 0.52685 early stop: 2\n",
            "\u001b[33m[W 210629 03:16:45 models:137]\u001b[39m Clipping gradients with total norm 0.04189 and max norm 0.00687\n",
            "\u001b[32m[I 210629 03:16:45 models:110]\u001b[39m 234 1024 train loss: 0.0002635 valid loss: 0.2460488 P@1: 0.63000 P@3: 0.51667 P@5: 0.40400 N@3: 0.54981 N@5: 0.52685 early stop: 3\n",
            "\u001b[32m[I 210629 03:16:47 models:110]\u001b[39m 237 512 train loss: 0.0001824 valid loss: 0.2472633 P@1: 0.63000 P@3: 0.51667 P@5: 0.40400 N@3: 0.54981 N@5: 0.52685 early stop: 4\n",
            "\u001b[32m[I 210629 03:16:50 models:110]\u001b[39m 239 1024 train loss: 0.0001305 valid loss: 0.2484712 P@1: 0.63000 P@3: 0.51667 P@5: 0.40400 N@3: 0.54981 N@5: 0.52700 early stop: 0\n",
            "\u001b[32m[I 210629 03:16:52 models:110]\u001b[39m 242 512 train loss: 0.0002050 valid loss: 0.2496372 P@1: 0.63000 P@3: 0.52000 P@5: 0.40400 N@3: 0.55216 N@5: 0.52718 early stop: 0\n",
            "\u001b[32m[I 210629 03:16:54 models:110]\u001b[39m 244 1024 train loss: 0.0001302 valid loss: 0.2508067 P@1: 0.64000 P@3: 0.52000 P@5: 0.40400 N@3: 0.55389 N@5: 0.52858 early stop: 0\n",
            "\u001b[32m[I 210629 03:16:57 models:110]\u001b[39m 247 512 train loss: 0.0001046 valid loss: 0.2519771 P@1: 0.64000 P@3: 0.52000 P@5: 0.40400 N@3: 0.55389 N@5: 0.52858 early stop: 1\n",
            "\u001b[32m[I 210629 03:16:59 models:110]\u001b[39m 249 1024 train loss: 0.0001164 valid loss: 0.2531543 P@1: 0.65000 P@3: 0.52000 P@5: 0.40400 N@3: 0.55562 N@5: 0.52983 early stop: 0\n",
            "\u001b[32m[I 210629 03:17:01 models:110]\u001b[39m 252 512 train loss: 0.0001151 valid loss: 0.2543250 P@1: 0.65000 P@3: 0.52000 P@5: 0.40600 N@3: 0.55562 N@5: 0.53114 early stop: 0\n",
            "\u001b[32m[I 210629 03:17:03 models:110]\u001b[39m 254 1024 train loss: 0.0001206 valid loss: 0.2554871 P@1: 0.65000 P@3: 0.52000 P@5: 0.40600 N@3: 0.55562 N@5: 0.53114 early stop: 1\n",
            "\u001b[32m[I 210629 03:17:06 models:110]\u001b[39m 257 512 train loss: 0.0000910 valid loss: 0.2566380 P@1: 0.65000 P@3: 0.52000 P@5: 0.40600 N@3: 0.55562 N@5: 0.53114 early stop: 2\n",
            "\u001b[32m[I 210629 03:17:08 models:110]\u001b[39m 259 1024 train loss: 0.0001280 valid loss: 0.2577943 P@1: 0.65000 P@3: 0.52333 P@5: 0.40800 N@3: 0.55797 N@5: 0.53304 early stop: 0\n",
            "\u001b[32m[I 210629 03:17:10 models:110]\u001b[39m 262 512 train loss: 0.0001371 valid loss: 0.2589541 P@1: 0.65000 P@3: 0.52333 P@5: 0.40800 N@3: 0.55797 N@5: 0.53290 early stop: 1\n",
            "\u001b[33m[W 210629 03:17:11 models:137]\u001b[39m Clipping gradients with total norm 0.04964 and max norm 0.00522\n",
            "\u001b[32m[I 210629 03:17:12 models:110]\u001b[39m 264 1024 train loss: 0.0002577 valid loss: 0.2600929 P@1: 0.65000 P@3: 0.52667 P@5: 0.40600 N@3: 0.56093 N@5: 0.53176 early stop: 2\n",
            "\u001b[32m[I 210629 03:17:15 models:110]\u001b[39m 267 512 train loss: 0.0001350 valid loss: 0.2612333 P@1: 0.65000 P@3: 0.52667 P@5: 0.40800 N@3: 0.56093 N@5: 0.53307 early stop: 0\n",
            "\u001b[32m[I 210629 03:17:17 models:110]\u001b[39m 269 1024 train loss: 0.0001492 valid loss: 0.2623693 P@1: 0.65000 P@3: 0.52667 P@5: 0.40800 N@3: 0.56093 N@5: 0.53322 early stop: 0\n",
            "\u001b[32m[I 210629 03:17:20 models:110]\u001b[39m 272 512 train loss: 0.0001268 valid loss: 0.2634842 P@1: 0.65000 P@3: 0.52667 P@5: 0.40800 N@3: 0.56093 N@5: 0.53322 early stop: 1\n",
            "\u001b[32m[I 210629 03:17:22 models:110]\u001b[39m 274 1024 train loss: 0.0000822 valid loss: 0.2645792 P@1: 0.65000 P@3: 0.52667 P@5: 0.40800 N@3: 0.56093 N@5: 0.53322 early stop: 2\n",
            "\u001b[33m[W 210629 03:17:23 models:137]\u001b[39m Clipping gradients with total norm 0.07048 and max norm 0.00407\n",
            "\u001b[32m[I 210629 03:17:24 models:110]\u001b[39m 277 512 train loss: 0.0003653 valid loss: 0.2656741 P@1: 0.65000 P@3: 0.52667 P@5: 0.40800 N@3: 0.56031 N@5: 0.53271 early stop: 3\n",
            "\u001b[32m[I 210629 03:17:26 models:110]\u001b[39m 279 1024 train loss: 0.0001023 valid loss: 0.2667795 P@1: 0.65000 P@3: 0.52667 P@5: 0.40800 N@3: 0.56031 N@5: 0.53271 early stop: 4\n",
            "\u001b[32m[I 210629 03:17:29 models:110]\u001b[39m 282 512 train loss: 0.0000988 valid loss: 0.2678606 P@1: 0.65000 P@3: 0.52667 P@5: 0.41000 N@3: 0.56031 N@5: 0.53402 early stop: 0\n",
            "\u001b[32m[I 210629 03:17:31 models:110]\u001b[39m 284 1024 train loss: 0.0001301 valid loss: 0.2689346 P@1: 0.65000 P@3: 0.52667 P@5: 0.41000 N@3: 0.56031 N@5: 0.53381 early stop: 1\n",
            "\u001b[32m[I 210629 03:17:33 models:110]\u001b[39m 287 512 train loss: 0.0001107 valid loss: 0.2700018 P@1: 0.65000 P@3: 0.52667 P@5: 0.40800 N@3: 0.56031 N@5: 0.53250 early stop: 2\n",
            "\u001b[32m[I 210629 03:17:35 models:110]\u001b[39m 289 1024 train loss: 0.0001085 valid loss: 0.2710546 P@1: 0.65000 P@3: 0.52667 P@5: 0.41000 N@3: 0.56031 N@5: 0.53381 early stop: 3\n",
            "\u001b[32m[I 210629 03:17:38 models:110]\u001b[39m 292 512 train loss: 0.0001025 valid loss: 0.2721021 P@1: 0.65000 P@3: 0.52667 P@5: 0.41000 N@3: 0.56031 N@5: 0.53381 early stop: 4\n",
            "\u001b[33m[W 210629 03:17:39 models:137]\u001b[39m Clipping gradients with total norm 0.05008 and max norm 0.00748\n",
            "\u001b[32m[I 210629 03:17:40 models:110]\u001b[39m 294 1024 train loss: 0.0003118 valid loss: 0.2731338 P@1: 0.65000 P@3: 0.52667 P@5: 0.41000 N@3: 0.56031 N@5: 0.53381 early stop: 5\n",
            "\u001b[32m[I 210629 03:17:42 models:110]\u001b[39m 297 512 train loss: 0.0003086 valid loss: 0.2741972 P@1: 0.65000 P@3: 0.52667 P@5: 0.41000 N@3: 0.56031 N@5: 0.53396 early stop: 6\n",
            "\u001b[32m[I 210629 03:17:44 models:110]\u001b[39m 299 1024 train loss: 0.0001321 valid loss: 0.2752475 P@1: 0.65000 P@3: 0.52667 P@5: 0.41000 N@3: 0.56031 N@5: 0.53396 early stop: 7\n",
            "\u001b[32m[I 210629 03:17:47 models:110]\u001b[39m 302 512 train loss: 0.0001250 valid loss: 0.2762417 P@1: 0.65000 P@3: 0.52667 P@5: 0.41200 N@3: 0.56031 N@5: 0.53578 early stop: 0\n",
            "\u001b[33m[W 210629 03:17:47 models:137]\u001b[39m Clipping gradients with total norm 0.06031 and max norm 0.00601\n",
            "\u001b[32m[I 210629 03:17:49 models:110]\u001b[39m 304 1024 train loss: 0.0003567 valid loss: 0.2772611 P@1: 0.65000 P@3: 0.52667 P@5: 0.41200 N@3: 0.56031 N@5: 0.53578 early stop: 1\n",
            "\u001b[33m[W 210629 03:17:50 models:137]\u001b[39m Clipping gradients with total norm 0.05228 and max norm 0.00326\n",
            "\u001b[32m[I 210629 03:17:51 models:110]\u001b[39m 307 512 train loss: 0.0004649 valid loss: 0.2782746 P@1: 0.65000 P@3: 0.52667 P@5: 0.41200 N@3: 0.56031 N@5: 0.53578 early stop: 2\n",
            "\u001b[32m[I 210629 03:17:53 models:110]\u001b[39m 309 1024 train loss: 0.0000904 valid loss: 0.2792660 P@1: 0.65000 P@3: 0.52667 P@5: 0.41200 N@3: 0.56031 N@5: 0.53578 early stop: 3\n",
            "\u001b[32m[I 210629 03:17:56 models:110]\u001b[39m 312 512 train loss: 0.0001412 valid loss: 0.2802476 P@1: 0.65000 P@3: 0.52667 P@5: 0.41200 N@3: 0.56031 N@5: 0.53578 early stop: 4\n",
            "\u001b[32m[I 210629 03:17:58 models:110]\u001b[39m 314 1024 train loss: 0.0000952 valid loss: 0.2812280 P@1: 0.65000 P@3: 0.52667 P@5: 0.41200 N@3: 0.55970 N@5: 0.53533 early stop: 5\n",
            "\u001b[32m[I 210629 03:18:00 models:110]\u001b[39m 317 512 train loss: 0.0001081 valid loss: 0.2822056 P@1: 0.65000 P@3: 0.52667 P@5: 0.41200 N@3: 0.55970 N@5: 0.53533 early stop: 6\n",
            "\u001b[32m[I 210629 03:18:02 models:110]\u001b[39m 319 1024 train loss: 0.0001066 valid loss: 0.2831752 P@1: 0.65000 P@3: 0.52667 P@5: 0.41200 N@3: 0.55970 N@5: 0.53533 early stop: 7\n",
            "\u001b[32m[I 210629 03:18:05 models:110]\u001b[39m 322 512 train loss: 0.0001199 valid loss: 0.2841401 P@1: 0.65000 P@3: 0.52667 P@5: 0.41200 N@3: 0.55970 N@5: 0.53533 early stop: 8\n",
            "\u001b[32m[I 210629 03:18:07 models:110]\u001b[39m 324 1024 train loss: 0.0000801 valid loss: 0.2851098 P@1: 0.65000 P@3: 0.52667 P@5: 0.41200 N@3: 0.55970 N@5: 0.53533 early stop: 9\n",
            "\u001b[32m[I 210629 03:18:09 models:110]\u001b[39m 327 512 train loss: 0.0001124 valid loss: 0.2860710 P@1: 0.65000 P@3: 0.52667 P@5: 0.41200 N@3: 0.55970 N@5: 0.53533 early stop: 10\n",
            "\u001b[32m[I 210629 03:18:11 models:110]\u001b[39m 329 1024 train loss: 0.0002481 valid loss: 0.2870156 P@1: 0.65000 P@3: 0.52667 P@5: 0.41200 N@3: 0.55970 N@5: 0.53533 early stop: 11\n",
            "\u001b[32m[I 210629 03:18:14 models:110]\u001b[39m 332 512 train loss: 0.0002127 valid loss: 0.2879543 P@1: 0.65000 P@3: 0.52667 P@5: 0.41200 N@3: 0.55970 N@5: 0.53533 early stop: 12\n",
            "\u001b[32m[I 210629 03:18:16 models:110]\u001b[39m 334 1024 train loss: 0.0001355 valid loss: 0.2888607 P@1: 0.65000 P@3: 0.52667 P@5: 0.41200 N@3: 0.55970 N@5: 0.53533 early stop: 13\n",
            "\u001b[32m[I 210629 03:18:19 models:110]\u001b[39m 337 512 train loss: 0.0001012 valid loss: 0.2897412 P@1: 0.65000 P@3: 0.52667 P@5: 0.41200 N@3: 0.55970 N@5: 0.53554 early stop: 14\n",
            "\u001b[32m[I 210629 03:18:21 models:110]\u001b[39m 339 1024 train loss: 0.0001098 valid loss: 0.2906418 P@1: 0.65000 P@3: 0.52667 P@5: 0.41200 N@3: 0.55970 N@5: 0.53554 early stop: 15\n",
            "\u001b[32m[I 210629 03:18:23 models:110]\u001b[39m 342 512 train loss: 0.0000814 valid loss: 0.2915621 P@1: 0.65000 P@3: 0.52667 P@5: 0.41200 N@3: 0.55970 N@5: 0.53554 early stop: 16\n",
            "\u001b[33m[W 210629 03:18:23 models:137]\u001b[39m Clipping gradients with total norm 0.06941 and max norm 0.01022\n",
            "\u001b[32m[I 210629 03:18:25 models:110]\u001b[39m 344 1024 train loss: 0.0004568 valid loss: 0.2924847 P@1: 0.65000 P@3: 0.52667 P@5: 0.41200 N@3: 0.55970 N@5: 0.53554 early stop: 17\n",
            "\u001b[33m[W 210629 03:18:27 models:137]\u001b[39m Clipping gradients with total norm 0.07863 and max norm 0.01058\n",
            "\u001b[32m[I 210629 03:18:28 models:110]\u001b[39m 347 512 train loss: 0.0007126 valid loss: 0.2934307 P@1: 0.65000 P@3: 0.52667 P@5: 0.41200 N@3: 0.55970 N@5: 0.53575 early stop: 18\n",
            "\u001b[33m[W 210629 03:18:28 models:137]\u001b[39m Clipping gradients with total norm 0.15573 and max norm 0.02115\n",
            "\u001b[33m[W 210629 03:18:29 models:137]\u001b[39m Clipping gradients with total norm 0.10646 and max norm 0.0201\n",
            "\u001b[32m[I 210629 03:18:30 models:110]\u001b[39m 349 1024 train loss: 0.0007207 valid loss: 0.2942802 P@1: 0.65000 P@3: 0.52667 P@5: 0.41200 N@3: 0.55970 N@5: 0.53575 early stop: 19\n",
            "\u001b[32m[I 210629 03:18:32 models:110]\u001b[39m 352 512 train loss: 0.0011985 valid loss: 0.2950490 P@1: 0.65000 P@3: 0.52667 P@5: 0.41200 N@3: 0.55970 N@5: 0.53575 early stop: 20\n",
            "\u001b[32m[I 210629 03:18:34 models:110]\u001b[39m 354 1024 train loss: 0.0022108 valid loss: 0.2957843 P@1: 0.65000 P@3: 0.52333 P@5: 0.41200 N@3: 0.55797 N@5: 0.53623 early stop: 0\n",
            "\u001b[32m[I 210629 03:18:37 models:110]\u001b[39m 357 512 train loss: 0.0060448 valid loss: 0.2959329 P@1: 0.65000 P@3: 0.52667 P@5: 0.41200 N@3: 0.56031 N@5: 0.53646 early stop: 0\n",
            "\u001b[32m[I 210629 03:18:39 models:110]\u001b[39m 359 1024 train loss: 0.0288433 valid loss: 0.2955817 P@1: 0.65000 P@3: 0.52333 P@5: 0.41200 N@3: 0.55797 N@5: 0.53623 early stop: 1\n",
            "\u001b[32m[I 210629 03:18:42 models:110]\u001b[39m 362 512 train loss: 0.0428767 valid loss: 0.2936120 P@1: 0.66000 P@3: 0.52667 P@5: 0.41200 N@3: 0.56205 N@5: 0.53771 early stop: 0\n",
            "\u001b[32m[I 210629 03:18:44 models:110]\u001b[39m 364 1024 train loss: 0.0326336 valid loss: 0.2915005 P@1: 0.66000 P@3: 0.53000 P@5: 0.41200 N@3: 0.56439 N@5: 0.53810 early stop: 0\n",
            "\u001b[32m[I 210629 03:18:46 models:110]\u001b[39m 367 512 train loss: 0.0205821 valid loss: 0.2899418 P@1: 0.67000 P@3: 0.53000 P@5: 0.41000 N@3: 0.56613 N@5: 0.53852 early stop: 0\n",
            "\u001b[32m[I 210629 03:18:48 models:110]\u001b[39m 369 1024 train loss: 0.0133874 valid loss: 0.2886184 P@1: 0.67000 P@3: 0.53000 P@5: 0.41200 N@3: 0.56674 N@5: 0.54027 early stop: 0\n",
            "\u001b[32m[I 210629 03:18:51 models:110]\u001b[39m 372 512 train loss: 0.0084983 valid loss: 0.2875882 P@1: 0.67000 P@3: 0.53000 P@5: 0.41200 N@3: 0.56674 N@5: 0.54027 early stop: 1\n",
            "\u001b[32m[I 210629 03:18:53 models:110]\u001b[39m 374 1024 train loss: 0.0044796 valid loss: 0.2868235 P@1: 0.67000 P@3: 0.53000 P@5: 0.41200 N@3: 0.56613 N@5: 0.53976 early stop: 2\n",
            "\u001b[32m[I 210629 03:18:55 models:110]\u001b[39m 377 512 train loss: 0.0027300 valid loss: 0.2863581 P@1: 0.68000 P@3: 0.53000 P@5: 0.41200 N@3: 0.56786 N@5: 0.54150 early stop: 0\n",
            "\u001b[32m[I 210629 03:18:58 models:110]\u001b[39m 379 1024 train loss: 0.0020411 valid loss: 0.2860098 P@1: 0.68000 P@3: 0.53000 P@5: 0.41200 N@3: 0.56847 N@5: 0.54194 early stop: 0\n",
            "\u001b[32m[I 210629 03:19:00 models:110]\u001b[39m 382 512 train loss: 0.0021611 valid loss: 0.2856058 P@1: 0.68000 P@3: 0.53000 P@5: 0.41200 N@3: 0.56847 N@5: 0.54194 early stop: 1\n",
            "\u001b[32m[I 210629 03:19:02 models:110]\u001b[39m 384 1024 train loss: 0.0022814 valid loss: 0.2854507 P@1: 0.67000 P@3: 0.53333 P@5: 0.41200 N@3: 0.56970 N@5: 0.54135 early stop: 2\n",
            "\u001b[32m[I 210629 03:19:05 models:110]\u001b[39m 387 512 train loss: 0.0013207 valid loss: 0.2851841 P@1: 0.67000 P@3: 0.53333 P@5: 0.41200 N@3: 0.57031 N@5: 0.54196 early stop: 0\n",
            "\u001b[32m[I 210629 03:19:07 models:110]\u001b[39m 389 1024 train loss: 0.0009759 valid loss: 0.2850195 P@1: 0.67000 P@3: 0.53333 P@5: 0.41400 N@3: 0.56970 N@5: 0.54286 early stop: 0\n",
            "\u001b[32m[I 210629 03:19:09 models:110]\u001b[39m 392 512 train loss: 0.0009870 valid loss: 0.2849179 P@1: 0.67000 P@3: 0.53333 P@5: 0.41400 N@3: 0.56970 N@5: 0.54286 early stop: 1\n",
            "\u001b[32m[I 210629 03:19:11 models:110]\u001b[39m 394 1024 train loss: 0.0008931 valid loss: 0.2848982 P@1: 0.67000 P@3: 0.53333 P@5: 0.41400 N@3: 0.56970 N@5: 0.54286 early stop: 2\n",
            "\u001b[32m[I 210629 03:19:14 models:110]\u001b[39m 397 512 train loss: 0.0005495 valid loss: 0.2848336 P@1: 0.67000 P@3: 0.53333 P@5: 0.41600 N@3: 0.57031 N@5: 0.54512 early stop: 0\n",
            "\u001b[32m[I 210629 03:19:16 models:110]\u001b[39m 399 1024 train loss: 0.0004315 valid loss: 0.2848217 P@1: 0.67000 P@3: 0.53333 P@5: 0.41600 N@3: 0.57093 N@5: 0.54556 early stop: 0\n",
            "\u001b[32m[I 210629 03:19:18 models:110]\u001b[39m 402 512 train loss: 0.0002362 valid loss: 0.2848492 P@1: 0.67000 P@3: 0.53333 P@5: 0.41600 N@3: 0.57093 N@5: 0.54541 early stop: 1\n",
            "\u001b[32m[I 210629 03:19:20 models:110]\u001b[39m 404 1024 train loss: 0.0001505 valid loss: 0.2849088 P@1: 0.67000 P@3: 0.53333 P@5: 0.41600 N@3: 0.57093 N@5: 0.54541 early stop: 2\n",
            "\u001b[32m[I 210629 03:19:23 models:110]\u001b[39m 407 512 train loss: 0.0002321 valid loss: 0.2849928 P@1: 0.67000 P@3: 0.53333 P@5: 0.42000 N@3: 0.57093 N@5: 0.54804 early stop: 0\n",
            "\u001b[32m[I 210629 03:19:25 models:110]\u001b[39m 409 1024 train loss: 0.0001424 valid loss: 0.2850828 P@1: 0.67000 P@3: 0.53333 P@5: 0.42000 N@3: 0.57093 N@5: 0.54819 early stop: 0\n",
            "\u001b[32m[I 210629 03:19:28 models:110]\u001b[39m 412 512 train loss: 0.0001826 valid loss: 0.2851876 P@1: 0.67000 P@3: 0.53333 P@5: 0.42000 N@3: 0.57093 N@5: 0.54819 early stop: 1\n",
            "\u001b[32m[I 210629 03:19:30 models:110]\u001b[39m 414 1024 train loss: 0.0001112 valid loss: 0.2853048 P@1: 0.67000 P@3: 0.53333 P@5: 0.42000 N@3: 0.57093 N@5: 0.54833 early stop: 0\n",
            "\u001b[32m[I 210629 03:19:33 models:110]\u001b[39m 417 512 train loss: 0.0001759 valid loss: 0.2854404 P@1: 0.67000 P@3: 0.53333 P@5: 0.42000 N@3: 0.57093 N@5: 0.54833 early stop: 1\n",
            "\u001b[32m[I 210629 03:19:35 models:110]\u001b[39m 419 1024 train loss: 0.0001098 valid loss: 0.2855898 P@1: 0.67000 P@3: 0.53333 P@5: 0.42000 N@3: 0.57093 N@5: 0.54854 early stop: 0\n",
            "\u001b[32m[I 210629 03:19:38 models:110]\u001b[39m 422 512 train loss: 0.0001410 valid loss: 0.2857473 P@1: 0.67000 P@3: 0.53333 P@5: 0.42000 N@3: 0.57093 N@5: 0.54854 early stop: 1\n",
            "\u001b[32m[I 210629 03:19:40 models:110]\u001b[39m 424 1024 train loss: 0.0001184 valid loss: 0.2858964 P@1: 0.67000 P@3: 0.53333 P@5: 0.42200 N@3: 0.57093 N@5: 0.55000 early stop: 0\n",
            "\u001b[32m[I 210629 03:19:42 models:110]\u001b[39m 427 512 train loss: 0.0001279 valid loss: 0.2860580 P@1: 0.67000 P@3: 0.53333 P@5: 0.42200 N@3: 0.57093 N@5: 0.54985 early stop: 1\n",
            "\u001b[32m[I 210629 03:19:44 models:110]\u001b[39m 429 1024 train loss: 0.0001156 valid loss: 0.2862322 P@1: 0.67000 P@3: 0.53333 P@5: 0.42200 N@3: 0.57093 N@5: 0.54985 early stop: 2\n",
            "\u001b[32m[I 210629 03:19:47 models:110]\u001b[39m 432 512 train loss: 0.0001360 valid loss: 0.2864138 P@1: 0.67000 P@3: 0.53333 P@5: 0.42200 N@3: 0.57093 N@5: 0.54985 early stop: 3\n",
            "\u001b[32m[I 210629 03:19:49 models:110]\u001b[39m 434 1024 train loss: 0.0001276 valid loss: 0.2866215 P@1: 0.67000 P@3: 0.53333 P@5: 0.42200 N@3: 0.57093 N@5: 0.54970 early stop: 4\n",
            "\u001b[32m[I 210629 03:19:51 models:110]\u001b[39m 437 512 train loss: 0.0000992 valid loss: 0.2868322 P@1: 0.67000 P@3: 0.53333 P@5: 0.42200 N@3: 0.57093 N@5: 0.54970 early stop: 5\n",
            "\u001b[33m[W 210629 03:19:52 models:137]\u001b[39m Clipping gradients with total norm 0.07291 and max norm 0.00448\n",
            "\u001b[32m[I 210629 03:19:53 models:110]\u001b[39m 439 1024 train loss: 0.0003182 valid loss: 0.2870442 P@1: 0.67000 P@3: 0.53333 P@5: 0.42400 N@3: 0.57093 N@5: 0.55102 early stop: 0\n",
            "\u001b[32m[I 210629 03:19:56 models:110]\u001b[39m 442 512 train loss: 0.0001502 valid loss: 0.2872607 P@1: 0.67000 P@3: 0.53667 P@5: 0.42600 N@3: 0.57328 N@5: 0.55256 early stop: 0\n",
            "\u001b[32m[I 210629 03:19:58 models:110]\u001b[39m 444 1024 train loss: 0.0001034 valid loss: 0.2874809 P@1: 0.67000 P@3: 0.53667 P@5: 0.42600 N@3: 0.57389 N@5: 0.55318 early stop: 0\n",
            "\u001b[32m[I 210629 03:20:00 models:110]\u001b[39m 447 512 train loss: 0.0001502 valid loss: 0.2876956 P@1: 0.67000 P@3: 0.53667 P@5: 0.42400 N@3: 0.57450 N@5: 0.55231 early stop: 1\n",
            "\u001b[33m[W 210629 03:20:02 models:137]\u001b[39m Clipping gradients with total norm 0.04791 and max norm 0.00888\n",
            "\u001b[32m[I 210629 03:20:03 models:110]\u001b[39m 449 1024 train loss: 0.0002395 valid loss: 0.2879190 P@1: 0.67000 P@3: 0.54000 P@5: 0.42400 N@3: 0.57685 N@5: 0.55254 early stop: 2\n",
            "\u001b[32m[I 210629 03:20:05 models:110]\u001b[39m 452 512 train loss: 0.0001950 valid loss: 0.2881536 P@1: 0.67000 P@3: 0.54333 P@5: 0.42400 N@3: 0.57920 N@5: 0.55287 early stop: 3\n",
            "\u001b[32m[I 210629 03:20:07 models:110]\u001b[39m 454 1024 train loss: 0.0001713 valid loss: 0.2883809 P@1: 0.67000 P@3: 0.54333 P@5: 0.42200 N@3: 0.57920 N@5: 0.55105 early stop: 4\n",
            "\u001b[32m[I 210629 03:20:10 models:110]\u001b[39m 457 512 train loss: 0.0001530 valid loss: 0.2885935 P@1: 0.67000 P@3: 0.54333 P@5: 0.42200 N@3: 0.57920 N@5: 0.55126 early stop: 5\n",
            "\u001b[32m[I 210629 03:20:12 models:110]\u001b[39m 459 1024 train loss: 0.0000999 valid loss: 0.2888159 P@1: 0.67000 P@3: 0.54333 P@5: 0.42200 N@3: 0.57920 N@5: 0.55126 early stop: 6\n",
            "\u001b[32m[I 210629 03:20:14 models:110]\u001b[39m 462 512 train loss: 0.0001228 valid loss: 0.2890554 P@1: 0.67000 P@3: 0.54333 P@5: 0.42200 N@3: 0.57920 N@5: 0.55126 early stop: 7\n",
            "\u001b[32m[I 210629 03:20:16 models:110]\u001b[39m 464 1024 train loss: 0.0001057 valid loss: 0.2893069 P@1: 0.67000 P@3: 0.54333 P@5: 0.42000 N@3: 0.57920 N@5: 0.54995 early stop: 8\n",
            "\u001b[32m[I 210629 03:20:19 models:110]\u001b[39m 467 512 train loss: 0.0001449 valid loss: 0.2895675 P@1: 0.67000 P@3: 0.54333 P@5: 0.42000 N@3: 0.57920 N@5: 0.54995 early stop: 9\n",
            "\u001b[32m[I 210629 03:20:21 models:110]\u001b[39m 469 1024 train loss: 0.0000961 valid loss: 0.2898329 P@1: 0.67000 P@3: 0.54333 P@5: 0.42000 N@3: 0.57920 N@5: 0.54978 early stop: 10\n",
            "\u001b[32m[I 210629 03:20:23 models:110]\u001b[39m 472 512 train loss: 0.0001344 valid loss: 0.2901007 P@1: 0.67000 P@3: 0.54333 P@5: 0.42000 N@3: 0.57920 N@5: 0.54978 early stop: 11\n",
            "\u001b[32m[I 210629 03:20:25 models:110]\u001b[39m 474 1024 train loss: 0.0000815 valid loss: 0.2903658 P@1: 0.67000 P@3: 0.54333 P@5: 0.42000 N@3: 0.57920 N@5: 0.54998 early stop: 12\n",
            "\u001b[32m[I 210629 03:20:28 models:110]\u001b[39m 477 512 train loss: 0.0001070 valid loss: 0.2906404 P@1: 0.67000 P@3: 0.54333 P@5: 0.42000 N@3: 0.57920 N@5: 0.54983 early stop: 13\n",
            "\u001b[32m[I 210629 03:20:30 models:110]\u001b[39m 479 1024 train loss: 0.0001355 valid loss: 0.2909225 P@1: 0.67000 P@3: 0.54333 P@5: 0.42000 N@3: 0.57920 N@5: 0.54983 early stop: 14\n",
            "\u001b[32m[I 210629 03:20:32 models:110]\u001b[39m 482 512 train loss: 0.0001333 valid loss: 0.2912013 P@1: 0.67000 P@3: 0.54333 P@5: 0.42000 N@3: 0.57920 N@5: 0.54983 early stop: 15\n",
            "\u001b[32m[I 210629 03:20:34 models:110]\u001b[39m 484 1024 train loss: 0.0001027 valid loss: 0.2914753 P@1: 0.67000 P@3: 0.54333 P@5: 0.42000 N@3: 0.57920 N@5: 0.54983 early stop: 16\n",
            "\u001b[32m[I 210629 03:20:37 models:110]\u001b[39m 487 512 train loss: 0.0001239 valid loss: 0.2917491 P@1: 0.67000 P@3: 0.54333 P@5: 0.42000 N@3: 0.57920 N@5: 0.54983 early stop: 17\n",
            "\u001b[32m[I 210629 03:20:39 models:110]\u001b[39m 489 1024 train loss: 0.0000921 valid loss: 0.2920220 P@1: 0.67000 P@3: 0.54333 P@5: 0.41800 N@3: 0.57920 N@5: 0.54867 early stop: 18\n",
            "\u001b[32m[I 210629 03:20:41 models:110]\u001b[39m 492 512 train loss: 0.0000802 valid loss: 0.2922955 P@1: 0.67000 P@3: 0.54667 P@5: 0.41800 N@3: 0.58154 N@5: 0.54900 early stop: 19\n",
            "\u001b[33m[W 210629 03:20:41 models:137]\u001b[39m Clipping gradients with total norm 0.03617 and max norm 0.00487\n",
            "\u001b[32m[I 210629 03:20:43 models:110]\u001b[39m 494 1024 train loss: 0.0003291 valid loss: 0.2925798 P@1: 0.67000 P@3: 0.54667 P@5: 0.41800 N@3: 0.58154 N@5: 0.54879 early stop: 20\n",
            "\u001b[32m[I 210629 03:20:46 models:110]\u001b[39m 497 512 train loss: 0.0001396 valid loss: 0.2928738 P@1: 0.67000 P@3: 0.54667 P@5: 0.41800 N@3: 0.58154 N@5: 0.54894 early stop: 21\n",
            "\u001b[32m[I 210629 03:20:48 models:110]\u001b[39m 499 1024 train loss: 0.0001259 valid loss: 0.2931683 P@1: 0.67000 P@3: 0.54667 P@5: 0.41800 N@3: 0.58154 N@5: 0.54894 early stop: 22\n",
            "\u001b[32m[I 210629 03:20:50 models:110]\u001b[39m 502 512 train loss: 0.0001254 valid loss: 0.2934624 P@1: 0.67000 P@3: 0.54667 P@5: 0.41800 N@3: 0.58154 N@5: 0.54894 early stop: 23\n",
            "\u001b[32m[I 210629 03:20:52 models:110]\u001b[39m 504 1024 train loss: 0.0000806 valid loss: 0.2937604 P@1: 0.67000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58154 N@5: 0.54763 early stop: 24\n",
            "\u001b[32m[I 210629 03:20:55 models:110]\u001b[39m 507 512 train loss: 0.0001236 valid loss: 0.2940596 P@1: 0.67000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58154 N@5: 0.54763 early stop: 25\n",
            "\u001b[32m[I 210629 03:20:57 models:110]\u001b[39m 509 1024 train loss: 0.0000932 valid loss: 0.2943567 P@1: 0.67000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58154 N@5: 0.54631 early stop: 26\n",
            "\u001b[33m[W 210629 03:20:58 models:137]\u001b[39m Clipping gradients with total norm 0.04238 and max norm 0.00401\n",
            "\u001b[32m[I 210629 03:20:59 models:110]\u001b[39m 512 512 train loss: 0.0002718 valid loss: 0.2946550 P@1: 0.67000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58154 N@5: 0.54631 early stop: 27\n",
            "\u001b[32m[I 210629 03:21:01 models:110]\u001b[39m 514 1024 train loss: 0.0001417 valid loss: 0.2949542 P@1: 0.67000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58154 N@5: 0.54631 early stop: 28\n",
            "\u001b[32m[I 210629 03:21:04 models:110]\u001b[39m 517 512 train loss: 0.0001590 valid loss: 0.2952588 P@1: 0.67000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58154 N@5: 0.54646 early stop: 29\n",
            "\u001b[32m[I 210629 03:21:06 models:110]\u001b[39m 519 1024 train loss: 0.0000866 valid loss: 0.2955615 P@1: 0.67000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58154 N@5: 0.54646 early stop: 30\n",
            "\u001b[32m[I 210629 03:21:08 models:110]\u001b[39m 522 512 train loss: 0.0001039 valid loss: 0.2958621 P@1: 0.67000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58154 N@5: 0.54646 early stop: 31\n",
            "\u001b[33m[W 210629 03:21:09 models:137]\u001b[39m Clipping gradients with total norm 0.03277 and max norm 0.00467\n",
            "\u001b[32m[I 210629 03:21:10 models:110]\u001b[39m 524 1024 train loss: 0.0002596 valid loss: 0.2961714 P@1: 0.67000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58154 N@5: 0.54646 early stop: 32\n",
            "\u001b[32m[I 210629 03:21:13 models:110]\u001b[39m 527 512 train loss: 0.0002244 valid loss: 0.2964854 P@1: 0.67000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58154 N@5: 0.54646 early stop: 33\n",
            "\u001b[32m[I 210629 03:21:15 models:110]\u001b[39m 529 1024 train loss: 0.0001029 valid loss: 0.2967994 P@1: 0.67000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58154 N@5: 0.54646 early stop: 34\n",
            "\u001b[32m[I 210629 03:21:17 models:110]\u001b[39m 532 512 train loss: 0.0001194 valid loss: 0.2971131 P@1: 0.67000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58154 N@5: 0.54646 early stop: 35\n",
            "\u001b[32m[I 210629 03:21:19 models:110]\u001b[39m 534 1024 train loss: 0.0002460 valid loss: 0.2974288 P@1: 0.66000 P@3: 0.54667 P@5: 0.41600 N@3: 0.57981 N@5: 0.54720 early stop: 36\n",
            "\u001b[32m[I 210629 03:21:22 models:110]\u001b[39m 537 512 train loss: 0.0002097 valid loss: 0.2977509 P@1: 0.66000 P@3: 0.54667 P@5: 0.41600 N@3: 0.57981 N@5: 0.54720 early stop: 37\n",
            "\u001b[32m[I 210629 03:21:24 models:110]\u001b[39m 539 1024 train loss: 0.0000879 valid loss: 0.2980724 P@1: 0.66000 P@3: 0.54667 P@5: 0.41600 N@3: 0.57981 N@5: 0.54720 early stop: 38\n",
            "\u001b[32m[I 210629 03:21:26 models:110]\u001b[39m 542 512 train loss: 0.0001374 valid loss: 0.2983889 P@1: 0.66000 P@3: 0.54667 P@5: 0.41600 N@3: 0.57981 N@5: 0.54720 early stop: 39\n",
            "\u001b[33m[W 210629 03:21:27 models:137]\u001b[39m Clipping gradients with total norm 0.04297 and max norm 0.00386\n",
            "\u001b[32m[I 210629 03:21:28 models:110]\u001b[39m 544 1024 train loss: 0.0002663 valid loss: 0.2987031 P@1: 0.66000 P@3: 0.54667 P@5: 0.41600 N@3: 0.57981 N@5: 0.54720 early stop: 40\n",
            "\u001b[32m[I 210629 03:21:31 models:110]\u001b[39m 547 512 train loss: 0.0001497 valid loss: 0.2990186 P@1: 0.66000 P@3: 0.55000 P@5: 0.41600 N@3: 0.58216 N@5: 0.54743 early stop: 41\n",
            "\u001b[32m[I 210629 03:21:33 models:110]\u001b[39m 549 1024 train loss: 0.0000839 valid loss: 0.2993285 P@1: 0.66000 P@3: 0.55000 P@5: 0.41600 N@3: 0.58216 N@5: 0.54743 early stop: 42\n",
            "\u001b[32m[I 210629 03:21:35 models:110]\u001b[39m 552 512 train loss: 0.0001277 valid loss: 0.2996309 P@1: 0.66000 P@3: 0.55000 P@5: 0.41600 N@3: 0.58216 N@5: 0.54743 early stop: 43\n",
            "\u001b[32m[I 210629 03:21:37 models:110]\u001b[39m 554 1024 train loss: 0.0000793 valid loss: 0.2999319 P@1: 0.66000 P@3: 0.55000 P@5: 0.41600 N@3: 0.58216 N@5: 0.54743 early stop: 44\n",
            "\u001b[32m[I 210629 03:21:40 models:110]\u001b[39m 557 512 train loss: 0.0001208 valid loss: 0.3002346 P@1: 0.66000 P@3: 0.55000 P@5: 0.41600 N@3: 0.58216 N@5: 0.54743 early stop: 45\n",
            "\u001b[32m[I 210629 03:21:42 models:110]\u001b[39m 559 1024 train loss: 0.0000820 valid loss: 0.3005447 P@1: 0.66000 P@3: 0.55000 P@5: 0.41600 N@3: 0.58216 N@5: 0.54743 early stop: 46\n",
            "\u001b[32m[I 210629 03:21:44 models:110]\u001b[39m 562 512 train loss: 0.0001033 valid loss: 0.3008594 P@1: 0.66000 P@3: 0.55000 P@5: 0.41600 N@3: 0.58216 N@5: 0.54743 early stop: 47\n",
            "\u001b[32m[I 210629 03:21:46 models:110]\u001b[39m 564 1024 train loss: 0.0001061 valid loss: 0.3011756 P@1: 0.66000 P@3: 0.55000 P@5: 0.41600 N@3: 0.58216 N@5: 0.54743 early stop: 48\n",
            "\u001b[32m[I 210629 03:21:49 models:110]\u001b[39m 567 512 train loss: 0.0001146 valid loss: 0.3014920 P@1: 0.66000 P@3: 0.55000 P@5: 0.41600 N@3: 0.58216 N@5: 0.54743 early stop: 49\n",
            "\u001b[32m[I 210629 03:21:51 models:110]\u001b[39m 569 1024 train loss: 0.0000773 valid loss: 0.3018104 P@1: 0.66000 P@3: 0.55333 P@5: 0.41600 N@3: 0.58450 N@5: 0.54761 early stop: 50\n",
            "\u001b[32m[I 210629 03:21:53 main:76]\u001b[39m Finish Training\n",
            "\u001b[32m[I 210629 03:21:55 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210629 03:21:55 main:79]\u001b[39m Loading Test Set\n",
            "\u001b[32m[I 210629 03:21:55 main:83]\u001b[39m Size of Test Set: 100\n",
            "\u001b[32m[I 210629 03:21:55 main:85]\u001b[39m Predicting\n",
            "\u001b[32m[I 210629 03:21:59 main:91]\u001b[39m Finish Predicting\n",
            "Precision@1,3,5: 0.57 0.44 0.37\n",
            "nDCG@1,3,5: 0.57 0.47581032993125155 0.47558288396912396\n",
            "\n",
            "WITH MAG WITH MESH, skip=500\n",
            "\n",
            "/content/drive/Shareddrives/NASA/MATCH/PeTaL\n",
            "131\n",
            "/content/drive/Shareddrives/NASA/MATCH\n",
            "    800  260480 4295970 PeTaL/train.json\n",
            "\u001b[32m[I 210629 03:22:01 preprocess:28]\u001b[39m Vocab Size: 26834\n",
            "\u001b[32m[I 210629 03:22:01 preprocess:30]\u001b[39m Getting Dataset: PeTaL/train_texts.txt Max Length: 500\n",
            "\u001b[32m[I 210629 03:22:01 preprocess:32]\u001b[39m Size of Samples: 900\n",
            "\u001b[32m[I 210629 03:22:03 preprocess:28]\u001b[39m Vocab Size: 26834\n",
            "\u001b[32m[I 210629 03:22:03 preprocess:30]\u001b[39m Getting Dataset: PeTaL/test_texts.txt Max Length: 500\n",
            "\u001b[32m[I 210629 03:22:03 preprocess:32]\u001b[39m Size of Samples: 100\n",
            "\u001b[32m[I 210629 03:22:04 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210629 03:22:04 main:35]\u001b[39m Loading Training and Validation Set\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['absorb_and/or_filter_solids', 'chemically_break_down_inorganic_compounds', 'detox/purify', 'manage_environmental_disturbances_in_a_community', 'protect_from_fire', 'protect_from_gases', 'send_vibratory_signals'] will be ignored\n",
            "  .format(sorted(unknown, key=str)))\n",
            "\u001b[32m[I 210629 03:22:04 main:47]\u001b[39m Number of Labels: 124\n",
            "\u001b[32m[I 210629 03:22:04 main:48]\u001b[39m Size of Training Set: 800\n",
            "\u001b[32m[I 210629 03:22:04 main:49]\u001b[39m Size of Validation Set: 100\n",
            "\u001b[32m[I 210629 03:22:04 main:66]\u001b[39m Number of Edges: 101\n",
            "\u001b[32m[I 210629 03:22:04 main:68]\u001b[39m Training\n",
            "\u001b[32m[I 210629 03:22:10 models:110]\u001b[39m 2 512 train loss: 0.2721736 valid loss: 0.1496854 P@1: 0.33000 P@3: 0.18333 P@5: 0.14200 N@3: 0.21631 N@5: 0.20122 early stop: 0\n",
            "\u001b[32m[I 210629 03:22:11 models:142]\u001b[39m SWA Initializing\n",
            "\u001b[32m[I 210629 03:22:12 models:110]\u001b[39m 4 1024 train loss: 0.1464667 valid loss: 0.1383201 P@1: 0.33000 P@3: 0.21000 P@5: 0.19600 N@3: 0.23324 N@5: 0.24631 early stop: 0\n",
            "\u001b[32m[I 210629 03:22:14 models:110]\u001b[39m 7 512 train loss: 0.1415387 valid loss: 0.1366958 P@1: 0.33000 P@3: 0.20667 P@5: 0.20000 N@3: 0.23274 N@5: 0.25088 early stop: 0\n",
            "\u001b[32m[I 210629 03:22:17 models:110]\u001b[39m 9 1024 train loss: 0.1405319 valid loss: 0.1360932 P@1: 0.33000 P@3: 0.23000 P@5: 0.19800 N@3: 0.25101 N@5: 0.25362 early stop: 0\n",
            "\u001b[32m[I 210629 03:22:19 models:110]\u001b[39m 12 512 train loss: 0.1303796 valid loss: 0.1357028 P@1: 0.33000 P@3: 0.24000 P@5: 0.19400 N@3: 0.25866 N@5: 0.25243 early stop: 1\n",
            "\u001b[32m[I 210629 03:22:21 models:110]\u001b[39m 14 1024 train loss: 0.1218717 valid loss: 0.1354543 P@1: 0.37000 P@3: 0.23333 P@5: 0.18800 N@3: 0.26520 N@5: 0.25817 early stop: 0\n",
            "\u001b[32m[I 210629 03:22:24 models:110]\u001b[39m 17 512 train loss: 0.0993615 valid loss: 0.1352886 P@1: 0.36000 P@3: 0.23333 P@5: 0.20200 N@3: 0.26346 N@5: 0.26709 early stop: 0\n",
            "\u001b[32m[I 210629 03:22:26 models:110]\u001b[39m 19 1024 train loss: 0.0871942 valid loss: 0.1350129 P@1: 0.32000 P@3: 0.24000 P@5: 0.22000 N@3: 0.25570 N@5: 0.27537 early stop: 0\n",
            "\u001b[32m[I 210629 03:22:28 models:110]\u001b[39m 22 512 train loss: 0.0721410 valid loss: 0.1345897 P@1: 0.32000 P@3: 0.25333 P@5: 0.23800 N@3: 0.26703 N@5: 0.29692 early stop: 0\n",
            "\u001b[32m[I 210629 03:22:30 models:110]\u001b[39m 24 1024 train loss: 0.0628829 valid loss: 0.1345560 P@1: 0.34000 P@3: 0.26667 P@5: 0.23200 N@3: 0.28487 N@5: 0.30301 early stop: 0\n",
            "\u001b[32m[I 210629 03:22:33 models:110]\u001b[39m 27 512 train loss: 0.0518448 valid loss: 0.1346259 P@1: 0.36000 P@3: 0.27667 P@5: 0.22200 N@3: 0.29991 N@5: 0.30176 early stop: 1\n",
            "\u001b[32m[I 210629 03:22:35 models:110]\u001b[39m 29 1024 train loss: 0.0469519 valid loss: 0.1341494 P@1: 0.37000 P@3: 0.28667 P@5: 0.22800 N@3: 0.31290 N@5: 0.31471 early stop: 0\n",
            "\u001b[32m[I 210629 03:22:37 models:110]\u001b[39m 32 512 train loss: 0.0378179 valid loss: 0.1336855 P@1: 0.42000 P@3: 0.30000 P@5: 0.24800 N@3: 0.33484 N@5: 0.34289 early stop: 0\n",
            "\u001b[32m[I 210629 03:22:39 models:110]\u001b[39m 34 1024 train loss: 0.0319009 valid loss: 0.1332464 P@1: 0.42000 P@3: 0.32333 P@5: 0.25600 N@3: 0.35137 N@5: 0.35210 early stop: 0\n",
            "\u001b[32m[I 210629 03:22:42 models:110]\u001b[39m 37 512 train loss: 0.0275612 valid loss: 0.1328336 P@1: 0.43000 P@3: 0.32667 P@5: 0.26600 N@3: 0.35962 N@5: 0.36640 early stop: 0\n",
            "\u001b[32m[I 210629 03:22:44 models:110]\u001b[39m 39 1024 train loss: 0.0255278 valid loss: 0.1324180 P@1: 0.46000 P@3: 0.34333 P@5: 0.27200 N@3: 0.37777 N@5: 0.37830 early stop: 0\n",
            "\u001b[32m[I 210629 03:22:47 models:110]\u001b[39m 42 512 train loss: 0.0221646 valid loss: 0.1322007 P@1: 0.46000 P@3: 0.35000 P@5: 0.27400 N@3: 0.38714 N@5: 0.38559 early stop: 0\n",
            "\u001b[32m[I 210629 03:22:49 models:110]\u001b[39m 44 1024 train loss: 0.0183226 valid loss: 0.1320142 P@1: 0.47000 P@3: 0.35667 P@5: 0.28200 N@3: 0.39418 N@5: 0.39356 early stop: 0\n",
            "\u001b[32m[I 210629 03:22:51 models:110]\u001b[39m 47 512 train loss: 0.0165800 valid loss: 0.1319878 P@1: 0.49000 P@3: 0.37667 P@5: 0.28600 N@3: 0.41367 N@5: 0.40308 early stop: 0\n",
            "\u001b[32m[I 210629 03:22:53 models:110]\u001b[39m 49 1024 train loss: 0.0176173 valid loss: 0.1319087 P@1: 0.51000 P@3: 0.37000 P@5: 0.29800 N@3: 0.41306 N@5: 0.41555 early stop: 0\n",
            "\u001b[32m[I 210629 03:22:56 models:110]\u001b[39m 52 512 train loss: 0.0173090 valid loss: 0.1322392 P@1: 0.52000 P@3: 0.37667 P@5: 0.30800 N@3: 0.41938 N@5: 0.42696 early stop: 0\n",
            "\u001b[32m[I 210629 03:22:58 models:110]\u001b[39m 54 1024 train loss: 0.0145197 valid loss: 0.1329018 P@1: 0.53000 P@3: 0.38000 P@5: 0.30800 N@3: 0.42346 N@5: 0.42941 early stop: 0\n",
            "\u001b[32m[I 210629 03:23:00 models:110]\u001b[39m 57 512 train loss: 0.0124937 valid loss: 0.1338911 P@1: 0.53000 P@3: 0.38333 P@5: 0.31200 N@3: 0.42519 N@5: 0.43301 early stop: 0\n",
            "\u001b[32m[I 210629 03:23:03 models:110]\u001b[39m 59 1024 train loss: 0.0103530 valid loss: 0.1346016 P@1: 0.53000 P@3: 0.39333 P@5: 0.31400 N@3: 0.43346 N@5: 0.43657 early stop: 0\n",
            "\u001b[32m[I 210629 03:23:05 models:110]\u001b[39m 62 512 train loss: 0.0077053 valid loss: 0.1357895 P@1: 0.54000 P@3: 0.40667 P@5: 0.32000 N@3: 0.44458 N@5: 0.44352 early stop: 0\n",
            "\u001b[32m[I 210629 03:23:07 models:110]\u001b[39m 64 1024 train loss: 0.0065008 valid loss: 0.1371721 P@1: 0.54000 P@3: 0.41000 P@5: 0.32000 N@3: 0.44692 N@5: 0.44402 early stop: 0\n",
            "\u001b[32m[I 210629 03:23:10 models:110]\u001b[39m 67 512 train loss: 0.0075591 valid loss: 0.1386036 P@1: 0.54000 P@3: 0.42000 P@5: 0.32000 N@3: 0.45396 N@5: 0.44406 early stop: 0\n",
            "\u001b[32m[I 210629 03:23:12 models:110]\u001b[39m 69 1024 train loss: 0.0071812 valid loss: 0.1404832 P@1: 0.54000 P@3: 0.41000 P@5: 0.32400 N@3: 0.44692 N@5: 0.44691 early stop: 0\n",
            "\u001b[32m[I 210629 03:23:14 models:110]\u001b[39m 72 512 train loss: 0.0106579 valid loss: 0.1423257 P@1: 0.54000 P@3: 0.41333 P@5: 0.32600 N@3: 0.44988 N@5: 0.44929 early stop: 0\n",
            "\u001b[32m[I 210629 03:23:16 models:110]\u001b[39m 74 1024 train loss: 0.0129585 valid loss: 0.1439735 P@1: 0.54000 P@3: 0.41667 P@5: 0.32600 N@3: 0.45223 N@5: 0.44932 early stop: 0\n",
            "\u001b[32m[I 210629 03:23:19 models:110]\u001b[39m 77 512 train loss: 0.0107099 valid loss: 0.1453512 P@1: 0.54000 P@3: 0.41000 P@5: 0.33400 N@3: 0.44682 N@5: 0.45467 early stop: 0\n",
            "\u001b[32m[I 210629 03:23:21 models:110]\u001b[39m 79 1024 train loss: 0.0083485 valid loss: 0.1469238 P@1: 0.54000 P@3: 0.40333 P@5: 0.33600 N@3: 0.44212 N@5: 0.45608 early stop: 0\n",
            "\u001b[32m[I 210629 03:23:23 models:110]\u001b[39m 82 512 train loss: 0.0059143 valid loss: 0.1486553 P@1: 0.53000 P@3: 0.41333 P@5: 0.33600 N@3: 0.44743 N@5: 0.45493 early stop: 1\n",
            "\u001b[32m[I 210629 03:23:26 models:110]\u001b[39m 84 1024 train loss: 0.0040203 valid loss: 0.1504611 P@1: 0.53000 P@3: 0.41333 P@5: 0.34200 N@3: 0.44743 N@5: 0.45896 early stop: 0\n",
            "\u001b[32m[I 210629 03:23:28 models:110]\u001b[39m 87 512 train loss: 0.0036698 valid loss: 0.1523730 P@1: 0.53000 P@3: 0.41333 P@5: 0.34000 N@3: 0.44743 N@5: 0.45756 early stop: 1\n",
            "\u001b[32m[I 210629 03:23:30 models:110]\u001b[39m 89 1024 train loss: 0.0023850 valid loss: 0.1544813 P@1: 0.51000 P@3: 0.41000 P@5: 0.34400 N@3: 0.44101 N@5: 0.45689 early stop: 2\n",
            "\u001b[32m[I 210629 03:23:33 models:110]\u001b[39m 92 512 train loss: 0.0018253 valid loss: 0.1566286 P@1: 0.51000 P@3: 0.41000 P@5: 0.34000 N@3: 0.44101 N@5: 0.45400 early stop: 3\n",
            "\u001b[32m[I 210629 03:23:35 models:110]\u001b[39m 94 1024 train loss: 0.0022040 valid loss: 0.1584452 P@1: 0.52000 P@3: 0.41000 P@5: 0.34000 N@3: 0.44388 N@5: 0.45698 early stop: 4\n",
            "\u001b[32m[I 210629 03:23:37 models:110]\u001b[39m 97 512 train loss: 0.0022605 valid loss: 0.1604054 P@1: 0.52000 P@3: 0.41333 P@5: 0.34400 N@3: 0.44562 N@5: 0.45910 early stop: 0\n",
            "\u001b[32m[I 210629 03:23:39 models:110]\u001b[39m 99 1024 train loss: 0.0029507 valid loss: 0.1624428 P@1: 0.51000 P@3: 0.41667 P@5: 0.34000 N@3: 0.44623 N@5: 0.45570 early stop: 1\n",
            "\u001b[32m[I 210629 03:23:42 models:110]\u001b[39m 102 512 train loss: 0.0031709 valid loss: 0.1649251 P@1: 0.51000 P@3: 0.41667 P@5: 0.33000 N@3: 0.44695 N@5: 0.44792 early stop: 2\n",
            "\u001b[32m[I 210629 03:23:44 models:110]\u001b[39m 104 1024 train loss: 0.0052881 valid loss: 0.1672234 P@1: 0.50000 P@3: 0.41667 P@5: 0.33200 N@3: 0.44469 N@5: 0.44737 early stop: 3\n",
            "\u001b[32m[I 210629 03:23:46 models:110]\u001b[39m 107 512 train loss: 0.0062259 valid loss: 0.1693856 P@1: 0.50000 P@3: 0.42000 P@5: 0.33000 N@3: 0.44703 N@5: 0.44612 early stop: 4\n",
            "\u001b[32m[I 210629 03:23:48 models:110]\u001b[39m 109 1024 train loss: 0.0064990 valid loss: 0.1715499 P@1: 0.50000 P@3: 0.41667 P@5: 0.33000 N@3: 0.44469 N@5: 0.44560 early stop: 5\n",
            "\u001b[32m[I 210629 03:23:51 models:110]\u001b[39m 112 512 train loss: 0.0086607 valid loss: 0.1730177 P@1: 0.49000 P@3: 0.41667 P@5: 0.32600 N@3: 0.44295 N@5: 0.44046 early stop: 6\n",
            "\u001b[32m[I 210629 03:23:53 models:110]\u001b[39m 114 1024 train loss: 0.0081550 valid loss: 0.1745263 P@1: 0.49000 P@3: 0.41667 P@5: 0.32800 N@3: 0.44295 N@5: 0.44289 early stop: 7\n",
            "\u001b[32m[I 210629 03:23:55 models:110]\u001b[39m 117 512 train loss: 0.0061461 valid loss: 0.1760298 P@1: 0.49000 P@3: 0.41667 P@5: 0.32600 N@3: 0.44295 N@5: 0.44051 early stop: 8\n",
            "\u001b[32m[I 210629 03:23:57 models:110]\u001b[39m 119 1024 train loss: 0.0041534 valid loss: 0.1775166 P@1: 0.50000 P@3: 0.41667 P@5: 0.32800 N@3: 0.44407 N@5: 0.44341 early stop: 9\n",
            "\u001b[32m[I 210629 03:24:00 models:110]\u001b[39m 122 512 train loss: 0.0031644 valid loss: 0.1791835 P@1: 0.50000 P@3: 0.41667 P@5: 0.33000 N@3: 0.44407 N@5: 0.44487 early stop: 10\n",
            "\u001b[32m[I 210629 03:24:02 models:110]\u001b[39m 124 1024 train loss: 0.0034372 valid loss: 0.1806715 P@1: 0.50000 P@3: 0.41667 P@5: 0.33400 N@3: 0.44530 N@5: 0.44975 early stop: 11\n",
            "\u001b[32m[I 210629 03:24:04 models:110]\u001b[39m 127 512 train loss: 0.0031482 valid loss: 0.1820557 P@1: 0.50000 P@3: 0.41667 P@5: 0.33400 N@3: 0.44530 N@5: 0.45010 early stop: 12\n",
            "\u001b[32m[I 210629 03:24:06 models:110]\u001b[39m 129 1024 train loss: 0.0039287 valid loss: 0.1837029 P@1: 0.49000 P@3: 0.41667 P@5: 0.33400 N@3: 0.44295 N@5: 0.44841 early stop: 13\n",
            "\u001b[32m[I 210629 03:24:09 models:110]\u001b[39m 132 512 train loss: 0.0014688 valid loss: 0.1853559 P@1: 0.49000 P@3: 0.41667 P@5: 0.33200 N@3: 0.44295 N@5: 0.44680 early stop: 14\n",
            "\u001b[32m[I 210629 03:24:11 models:110]\u001b[39m 134 1024 train loss: 0.0008122 valid loss: 0.1869225 P@1: 0.49000 P@3: 0.42000 P@5: 0.33000 N@3: 0.44469 N@5: 0.44474 early stop: 15\n",
            "\u001b[32m[I 210629 03:24:13 models:110]\u001b[39m 137 512 train loss: 0.0007541 valid loss: 0.1885799 P@1: 0.49000 P@3: 0.42000 P@5: 0.33200 N@3: 0.44610 N@5: 0.44747 early stop: 16\n",
            "\u001b[32m[I 210629 03:24:15 models:110]\u001b[39m 139 1024 train loss: 0.0005179 valid loss: 0.1901717 P@1: 0.49000 P@3: 0.42333 P@5: 0.33200 N@3: 0.44784 N@5: 0.44718 early stop: 17\n",
            "\u001b[32m[I 210629 03:24:18 models:110]\u001b[39m 142 512 train loss: 0.0003350 valid loss: 0.1919284 P@1: 0.49000 P@3: 0.42333 P@5: 0.33400 N@3: 0.44784 N@5: 0.44849 early stop: 18\n",
            "\u001b[32m[I 210629 03:24:20 models:110]\u001b[39m 144 1024 train loss: 0.0003048 valid loss: 0.1936970 P@1: 0.49000 P@3: 0.42333 P@5: 0.33400 N@3: 0.44784 N@5: 0.44849 early stop: 19\n",
            "\u001b[32m[I 210629 03:24:22 models:110]\u001b[39m 147 512 train loss: 0.0002204 valid loss: 0.1954367 P@1: 0.49000 P@3: 0.42333 P@5: 0.33400 N@3: 0.44784 N@5: 0.44849 early stop: 20\n",
            "\u001b[32m[I 210629 03:24:24 models:110]\u001b[39m 149 1024 train loss: 0.0001268 valid loss: 0.1971279 P@1: 0.49000 P@3: 0.42667 P@5: 0.33400 N@3: 0.45090 N@5: 0.44924 early stop: 21\n",
            "\u001b[32m[I 210629 03:24:26 models:110]\u001b[39m 152 512 train loss: 0.0001258 valid loss: 0.1988529 P@1: 0.49000 P@3: 0.42667 P@5: 0.33600 N@3: 0.45090 N@5: 0.45054 early stop: 22\n",
            "\u001b[32m[I 210629 03:24:28 models:110]\u001b[39m 154 1024 train loss: 0.0000891 valid loss: 0.2005672 P@1: 0.49000 P@3: 0.42667 P@5: 0.33600 N@3: 0.45090 N@5: 0.45080 early stop: 23\n",
            "\u001b[32m[I 210629 03:24:31 models:110]\u001b[39m 157 512 train loss: 0.0000679 valid loss: 0.2023300 P@1: 0.49000 P@3: 0.42333 P@5: 0.33600 N@3: 0.44855 N@5: 0.45027 early stop: 24\n",
            "\u001b[32m[I 210629 03:24:33 models:110]\u001b[39m 159 1024 train loss: 0.0000485 valid loss: 0.2040927 P@1: 0.49000 P@3: 0.42333 P@5: 0.33600 N@3: 0.44855 N@5: 0.45000 early stop: 25\n",
            "\u001b[32m[I 210629 03:24:35 models:110]\u001b[39m 162 512 train loss: 0.0000454 valid loss: 0.2058250 P@1: 0.49000 P@3: 0.42333 P@5: 0.33800 N@3: 0.44855 N@5: 0.45131 early stop: 26\n",
            "\u001b[32m[I 210629 03:24:37 models:110]\u001b[39m 164 1024 train loss: 0.0000528 valid loss: 0.2075257 P@1: 0.49000 P@3: 0.42333 P@5: 0.33800 N@3: 0.44794 N@5: 0.45087 early stop: 27\n",
            "\u001b[32m[I 210629 03:24:40 models:110]\u001b[39m 167 512 train loss: 0.0000419 valid loss: 0.2092232 P@1: 0.49000 P@3: 0.42333 P@5: 0.33800 N@3: 0.44794 N@5: 0.45087 early stop: 28\n",
            "\u001b[32m[I 210629 03:24:42 models:110]\u001b[39m 169 1024 train loss: 0.0000450 valid loss: 0.2109292 P@1: 0.49000 P@3: 0.42667 P@5: 0.33800 N@3: 0.45029 N@5: 0.45119 early stop: 29\n",
            "\u001b[32m[I 210629 03:24:44 models:110]\u001b[39m 172 512 train loss: 0.0000413 valid loss: 0.2126333 P@1: 0.49000 P@3: 0.42667 P@5: 0.33800 N@3: 0.45029 N@5: 0.45119 early stop: 30\n",
            "\u001b[32m[I 210629 03:24:46 models:110]\u001b[39m 174 1024 train loss: 0.0000360 valid loss: 0.2143157 P@1: 0.49000 P@3: 0.42667 P@5: 0.33800 N@3: 0.45029 N@5: 0.45119 early stop: 31\n",
            "\u001b[32m[I 210629 03:24:49 models:110]\u001b[39m 177 512 train loss: 0.0000348 valid loss: 0.2159730 P@1: 0.49000 P@3: 0.42667 P@5: 0.34000 N@3: 0.45029 N@5: 0.45271 early stop: 32\n",
            "\u001b[32m[I 210629 03:24:51 models:110]\u001b[39m 179 1024 train loss: 0.0000371 valid loss: 0.2176134 P@1: 0.49000 P@3: 0.42667 P@5: 0.34200 N@3: 0.45029 N@5: 0.45402 early stop: 33\n",
            "\u001b[32m[I 210629 03:24:53 models:110]\u001b[39m 182 512 train loss: 0.0000283 valid loss: 0.2192459 P@1: 0.49000 P@3: 0.42667 P@5: 0.34200 N@3: 0.45029 N@5: 0.45407 early stop: 34\n",
            "\u001b[32m[I 210629 03:24:55 models:110]\u001b[39m 184 1024 train loss: 0.0000285 valid loss: 0.2208755 P@1: 0.49000 P@3: 0.42333 P@5: 0.34200 N@3: 0.44794 N@5: 0.45384 early stop: 35\n",
            "\u001b[32m[I 210629 03:24:58 models:110]\u001b[39m 187 512 train loss: 0.0000294 valid loss: 0.2224966 P@1: 0.49000 P@3: 0.42333 P@5: 0.34400 N@3: 0.44855 N@5: 0.45612 early stop: 36\n",
            "\u001b[32m[I 210629 03:25:00 models:110]\u001b[39m 189 1024 train loss: 0.0000255 valid loss: 0.2241006 P@1: 0.49000 P@3: 0.42333 P@5: 0.34200 N@3: 0.44855 N@5: 0.45375 early stop: 37\n",
            "\u001b[33m[W 210629 03:25:02 models:137]\u001b[39m Clipping gradients with total norm 0.01675 and max norm 0.00331\n",
            "\u001b[32m[I 210629 03:25:02 models:110]\u001b[39m 192 512 train loss: 0.0000307 valid loss: 0.2256893 P@1: 0.49000 P@3: 0.42333 P@5: 0.34200 N@3: 0.44855 N@5: 0.45395 early stop: 38\n",
            "\u001b[32m[I 210629 03:25:04 models:110]\u001b[39m 194 1024 train loss: 0.0000285 valid loss: 0.2272681 P@1: 0.50000 P@3: 0.42333 P@5: 0.34200 N@3: 0.45082 N@5: 0.45621 early stop: 39\n",
            "\u001b[32m[I 210629 03:25:07 models:110]\u001b[39m 197 512 train loss: 0.0000319 valid loss: 0.2288403 P@1: 0.50000 P@3: 0.42000 P@5: 0.34200 N@3: 0.44847 N@5: 0.45597 early stop: 40\n",
            "\u001b[32m[I 210629 03:25:09 models:110]\u001b[39m 199 1024 train loss: 0.0000271 valid loss: 0.2303998 P@1: 0.50000 P@3: 0.42000 P@5: 0.34200 N@3: 0.44847 N@5: 0.45592 early stop: 41\n",
            "\u001b[32m[I 210629 03:25:11 models:110]\u001b[39m 202 512 train loss: 0.0000236 valid loss: 0.2319590 P@1: 0.50000 P@3: 0.42000 P@5: 0.34200 N@3: 0.44927 N@5: 0.45651 early stop: 42\n",
            "\u001b[32m[I 210629 03:25:13 models:110]\u001b[39m 204 1024 train loss: 0.0000231 valid loss: 0.2335009 P@1: 0.50000 P@3: 0.42000 P@5: 0.34200 N@3: 0.44927 N@5: 0.45651 early stop: 43\n",
            "\u001b[33m[W 210629 03:25:15 models:137]\u001b[39m Clipping gradients with total norm 0.0228 and max norm 0.00329\n",
            "\u001b[32m[I 210629 03:25:16 models:110]\u001b[39m 207 512 train loss: 0.0000255 valid loss: 0.2349978 P@1: 0.50000 P@3: 0.42000 P@5: 0.34000 N@3: 0.44927 N@5: 0.45478 early stop: 44\n",
            "\u001b[32m[I 210629 03:25:18 models:110]\u001b[39m 209 1024 train loss: 0.0000199 valid loss: 0.2364867 P@1: 0.50000 P@3: 0.42000 P@5: 0.34000 N@3: 0.44927 N@5: 0.45478 early stop: 45\n",
            "\u001b[32m[I 210629 03:25:20 models:110]\u001b[39m 212 512 train loss: 0.0000277 valid loss: 0.2379764 P@1: 0.50000 P@3: 0.41667 P@5: 0.34000 N@3: 0.44693 N@5: 0.45445 early stop: 46\n",
            "\u001b[32m[I 210629 03:25:22 models:110]\u001b[39m 214 1024 train loss: 0.0000216 valid loss: 0.2394642 P@1: 0.50000 P@3: 0.41667 P@5: 0.34000 N@3: 0.44693 N@5: 0.45445 early stop: 47\n",
            "\u001b[32m[I 210629 03:25:25 models:110]\u001b[39m 217 512 train loss: 0.0000180 valid loss: 0.2409442 P@1: 0.50000 P@3: 0.41667 P@5: 0.34000 N@3: 0.44693 N@5: 0.45445 early stop: 48\n",
            "\u001b[32m[I 210629 03:25:27 models:110]\u001b[39m 219 1024 train loss: 0.0000162 valid loss: 0.2424064 P@1: 0.50000 P@3: 0.41667 P@5: 0.33800 N@3: 0.44693 N@5: 0.45243 early stop: 49\n",
            "\u001b[32m[I 210629 03:25:29 models:110]\u001b[39m 222 512 train loss: 0.0000177 valid loss: 0.2438523 P@1: 0.50000 P@3: 0.41667 P@5: 0.33800 N@3: 0.44693 N@5: 0.45243 early stop: 50\n",
            "\u001b[32m[I 210629 03:25:31 main:76]\u001b[39m Finish Training\n",
            "\u001b[32m[I 210629 03:25:33 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210629 03:25:33 main:79]\u001b[39m Loading Test Set\n",
            "\u001b[32m[I 210629 03:25:33 main:83]\u001b[39m Size of Test Set: 100\n",
            "\u001b[32m[I 210629 03:25:33 main:85]\u001b[39m Predicting\n",
            "\u001b[32m[I 210629 03:25:36 main:91]\u001b[39m Finish Predicting\n",
            "Precision@1,3,5: 0.57 0.4666666666666667 0.37\n",
            "nDCG@1,3,5: 0.57 0.4970606479697054 0.48336418108219337\n",
            "\n",
            "WITH MAG WITH MESH, skip=700\n",
            "\n",
            "/content/drive/Shareddrives/NASA/MATCH/PeTaL\n",
            "129\n",
            "/content/drive/Shareddrives/NASA/MATCH\n",
            "    800  275675 4658257 PeTaL/train.json\n",
            "\u001b[32m[I 210629 03:25:39 preprocess:28]\u001b[39m Vocab Size: 26834\n",
            "\u001b[32m[I 210629 03:25:39 preprocess:30]\u001b[39m Getting Dataset: PeTaL/train_texts.txt Max Length: 500\n",
            "\u001b[32m[I 210629 03:25:39 preprocess:32]\u001b[39m Size of Samples: 900\n",
            "\u001b[32m[I 210629 03:25:40 preprocess:28]\u001b[39m Vocab Size: 26834\n",
            "\u001b[32m[I 210629 03:25:40 preprocess:30]\u001b[39m Getting Dataset: PeTaL/test_texts.txt Max Length: 500\n",
            "\u001b[32m[I 210629 03:25:40 preprocess:32]\u001b[39m Size of Samples: 100\n",
            "\u001b[32m[I 210629 03:25:42 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210629 03:25:42 main:35]\u001b[39m Loading Training and Validation Set\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['absorb_and/or_filter_solids', 'chemically_break_down_inorganic_compounds', 'detox/purify', 'manage_environmental_disturbances_in_a_community', 'protect_from_fire', 'protect_from_gases', 'send_vibratory_signals'] will be ignored\n",
            "  .format(sorted(unknown, key=str)))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['chemically_break_down_inorganic_compounds', 'detox/purify'] will be ignored\n",
            "  .format(sorted(unknown, key=str)))\n",
            "\u001b[32m[I 210629 03:25:42 main:47]\u001b[39m Number of Labels: 124\n",
            "\u001b[32m[I 210629 03:25:42 main:48]\u001b[39m Size of Training Set: 800\n",
            "\u001b[32m[I 210629 03:25:42 main:49]\u001b[39m Size of Validation Set: 100\n",
            "\u001b[32m[I 210629 03:25:42 main:66]\u001b[39m Number of Edges: 101\n",
            "\u001b[32m[I 210629 03:25:42 main:68]\u001b[39m Training\n",
            "\u001b[32m[I 210629 03:25:48 models:110]\u001b[39m 2 512 train loss: 0.2684738 valid loss: 0.1539717 P@1: 0.28000 P@3: 0.16000 P@5: 0.10800 N@3: 0.18140 N@5: 0.15660 early stop: 0\n",
            "\u001b[32m[I 210629 03:25:49 models:142]\u001b[39m SWA Initializing\n",
            "\u001b[32m[I 210629 03:25:50 models:110]\u001b[39m 4 1024 train loss: 0.1455726 valid loss: 0.1449327 P@1: 0.28000 P@3: 0.21333 P@5: 0.18800 N@3: 0.23000 N@5: 0.23351 early stop: 0\n",
            "\u001b[32m[I 210629 03:25:52 models:110]\u001b[39m 7 512 train loss: 0.1410984 valid loss: 0.1433296 P@1: 0.28000 P@3: 0.22000 P@5: 0.19200 N@3: 0.23469 N@5: 0.24141 early stop: 0\n",
            "\u001b[32m[I 210629 03:25:54 models:110]\u001b[39m 9 1024 train loss: 0.1422220 valid loss: 0.1429029 P@1: 0.28000 P@3: 0.22000 P@5: 0.19000 N@3: 0.23531 N@5: 0.24132 early stop: 1\n",
            "\u001b[32m[I 210629 03:25:57 models:110]\u001b[39m 12 512 train loss: 0.1378752 valid loss: 0.1422803 P@1: 0.28000 P@3: 0.22000 P@5: 0.20200 N@3: 0.22978 N@5: 0.24219 early stop: 0\n",
            "\u001b[32m[I 210629 03:25:59 models:110]\u001b[39m 14 1024 train loss: 0.1282798 valid loss: 0.1418056 P@1: 0.28000 P@3: 0.21000 P@5: 0.20400 N@3: 0.22274 N@5: 0.24445 early stop: 0\n",
            "\u001b[32m[I 210629 03:26:01 models:110]\u001b[39m 17 512 train loss: 0.1084663 valid loss: 0.1412566 P@1: 0.28000 P@3: 0.21000 P@5: 0.21000 N@3: 0.22520 N@5: 0.25063 early stop: 0\n",
            "\u001b[32m[I 210629 03:26:04 models:110]\u001b[39m 19 1024 train loss: 0.0849964 valid loss: 0.1405888 P@1: 0.28000 P@3: 0.23000 P@5: 0.22000 N@3: 0.24296 N@5: 0.26256 early stop: 0\n",
            "\u001b[32m[I 210629 03:26:06 models:110]\u001b[39m 22 512 train loss: 0.0684826 valid loss: 0.1396276 P@1: 0.33000 P@3: 0.25333 P@5: 0.22400 N@3: 0.27296 N@5: 0.28054 early stop: 0\n",
            "\u001b[32m[I 210629 03:26:08 models:110]\u001b[39m 24 1024 train loss: 0.0585537 valid loss: 0.1385499 P@1: 0.39000 P@3: 0.27000 P@5: 0.23400 N@3: 0.29631 N@5: 0.30081 early stop: 0\n",
            "\u001b[32m[I 210629 03:26:11 models:110]\u001b[39m 27 512 train loss: 0.0495577 valid loss: 0.1374226 P@1: 0.43000 P@3: 0.29000 P@5: 0.26400 N@3: 0.32234 N@5: 0.33430 early stop: 0\n",
            "\u001b[32m[I 210629 03:26:13 models:110]\u001b[39m 29 1024 train loss: 0.0444224 valid loss: 0.1363321 P@1: 0.42000 P@3: 0.32333 P@5: 0.29400 N@3: 0.34551 N@5: 0.36163 early stop: 0\n",
            "\u001b[32m[I 210629 03:26:15 models:110]\u001b[39m 32 512 train loss: 0.0388626 valid loss: 0.1351571 P@1: 0.43000 P@3: 0.35667 P@5: 0.30400 N@3: 0.37661 N@5: 0.37902 early stop: 0\n",
            "\u001b[32m[I 210629 03:26:17 models:110]\u001b[39m 34 1024 train loss: 0.0315635 valid loss: 0.1339614 P@1: 0.43000 P@3: 0.37333 P@5: 0.31400 N@3: 0.39099 N@5: 0.39419 early stop: 0\n",
            "\u001b[32m[I 210629 03:26:20 models:110]\u001b[39m 37 512 train loss: 0.0257214 valid loss: 0.1329100 P@1: 0.45000 P@3: 0.38667 P@5: 0.32200 N@3: 0.40630 N@5: 0.40807 early stop: 0\n",
            "\u001b[32m[I 210629 03:26:22 models:110]\u001b[39m 39 1024 train loss: 0.0247657 valid loss: 0.1320233 P@1: 0.48000 P@3: 0.39333 P@5: 0.33400 N@3: 0.41814 N@5: 0.42250 early stop: 0\n",
            "\u001b[32m[I 210629 03:26:24 models:110]\u001b[39m 42 512 train loss: 0.0240730 valid loss: 0.1313454 P@1: 0.51000 P@3: 0.39333 P@5: 0.35000 N@3: 0.42651 N@5: 0.44103 early stop: 0\n",
            "\u001b[32m[I 210629 03:26:27 models:110]\u001b[39m 44 1024 train loss: 0.0257853 valid loss: 0.1309716 P@1: 0.51000 P@3: 0.40333 P@5: 0.35800 N@3: 0.43435 N@5: 0.44946 early stop: 0\n",
            "\u001b[32m[I 210629 03:26:29 models:110]\u001b[39m 47 512 train loss: 0.0212952 valid loss: 0.1306419 P@1: 0.50000 P@3: 0.41333 P@5: 0.36600 N@3: 0.43885 N@5: 0.45376 early stop: 0\n",
            "\u001b[32m[I 210629 03:26:31 models:110]\u001b[39m 49 1024 train loss: 0.0191196 valid loss: 0.1305901 P@1: 0.54000 P@3: 0.42333 P@5: 0.36600 N@3: 0.45416 N@5: 0.46394 early stop: 0\n",
            "\u001b[32m[I 210629 03:26:34 models:110]\u001b[39m 52 512 train loss: 0.0148907 valid loss: 0.1306350 P@1: 0.54000 P@3: 0.41667 P@5: 0.37200 N@3: 0.45069 N@5: 0.46902 early stop: 0\n",
            "\u001b[32m[I 210629 03:26:36 models:110]\u001b[39m 54 1024 train loss: 0.0155053 valid loss: 0.1305982 P@1: 0.54000 P@3: 0.43333 P@5: 0.37000 N@3: 0.46295 N@5: 0.46938 early stop: 0\n",
            "\u001b[32m[I 210629 03:26:38 models:110]\u001b[39m 57 512 train loss: 0.0145403 valid loss: 0.1311837 P@1: 0.54000 P@3: 0.44000 P@5: 0.37200 N@3: 0.46826 N@5: 0.47176 early stop: 0\n",
            "\u001b[32m[I 210629 03:26:40 models:110]\u001b[39m 59 1024 train loss: 0.0143440 valid loss: 0.1318029 P@1: 0.55000 P@3: 0.44333 P@5: 0.38000 N@3: 0.47234 N@5: 0.48030 early stop: 0\n",
            "\u001b[32m[I 210629 03:26:43 models:110]\u001b[39m 62 512 train loss: 0.0118588 valid loss: 0.1327407 P@1: 0.57000 P@3: 0.44000 P@5: 0.38200 N@3: 0.47388 N@5: 0.48570 early stop: 0\n",
            "\u001b[32m[I 210629 03:26:45 models:110]\u001b[39m 64 1024 train loss: 0.0099939 valid loss: 0.1336251 P@1: 0.57000 P@3: 0.44667 P@5: 0.38600 N@3: 0.47876 N@5: 0.49041 early stop: 0\n",
            "\u001b[32m[I 210629 03:26:47 models:110]\u001b[39m 67 512 train loss: 0.0075746 valid loss: 0.1345265 P@1: 0.57000 P@3: 0.45000 P@5: 0.38400 N@3: 0.48253 N@5: 0.48988 early stop: 1\n",
            "\u001b[32m[I 210629 03:26:49 models:110]\u001b[39m 69 1024 train loss: 0.0053624 valid loss: 0.1358433 P@1: 0.57000 P@3: 0.46000 P@5: 0.38400 N@3: 0.49141 N@5: 0.49226 early stop: 0\n",
            "\u001b[32m[I 210629 03:26:52 models:110]\u001b[39m 72 512 train loss: 0.0048459 valid loss: 0.1372130 P@1: 0.57000 P@3: 0.46000 P@5: 0.38600 N@3: 0.49018 N@5: 0.49254 early stop: 0\n",
            "\u001b[32m[I 210629 03:26:54 models:110]\u001b[39m 74 1024 train loss: 0.0049556 valid loss: 0.1390042 P@1: 0.57000 P@3: 0.46000 P@5: 0.38600 N@3: 0.48938 N@5: 0.49191 early stop: 1\n",
            "\u001b[32m[I 210629 03:26:56 models:110]\u001b[39m 77 512 train loss: 0.0053166 valid loss: 0.1405915 P@1: 0.57000 P@3: 0.44667 P@5: 0.38600 N@3: 0.48050 N@5: 0.49206 early stop: 2\n",
            "\u001b[32m[I 210629 03:26:59 models:110]\u001b[39m 79 1024 train loss: 0.0055308 valid loss: 0.1424259 P@1: 0.58000 P@3: 0.44333 P@5: 0.38600 N@3: 0.47927 N@5: 0.49279 early stop: 0\n",
            "\u001b[32m[I 210629 03:27:01 models:110]\u001b[39m 82 512 train loss: 0.0064096 valid loss: 0.1438642 P@1: 0.59000 P@3: 0.44000 P@5: 0.38600 N@3: 0.47805 N@5: 0.49273 early stop: 1\n",
            "\u001b[32m[I 210629 03:27:03 models:110]\u001b[39m 84 1024 train loss: 0.0074689 valid loss: 0.1453087 P@1: 0.59000 P@3: 0.44000 P@5: 0.38800 N@3: 0.47805 N@5: 0.49368 early stop: 0\n",
            "\u001b[32m[I 210629 03:27:06 models:110]\u001b[39m 87 512 train loss: 0.0063648 valid loss: 0.1468885 P@1: 0.59000 P@3: 0.44333 P@5: 0.39200 N@3: 0.48101 N@5: 0.49758 early stop: 0\n",
            "\u001b[32m[I 210629 03:27:08 models:110]\u001b[39m 89 1024 train loss: 0.0064965 valid loss: 0.1482689 P@1: 0.60000 P@3: 0.44333 P@5: 0.39400 N@3: 0.48274 N@5: 0.50058 early stop: 0\n",
            "\u001b[32m[I 210629 03:27:10 models:110]\u001b[39m 92 512 train loss: 0.0080051 valid loss: 0.1499469 P@1: 0.60000 P@3: 0.44333 P@5: 0.39000 N@3: 0.48274 N@5: 0.49690 early stop: 1\n",
            "\u001b[32m[I 210629 03:27:12 models:110]\u001b[39m 94 1024 train loss: 0.0108856 valid loss: 0.1513738 P@1: 0.60000 P@3: 0.44667 P@5: 0.39000 N@3: 0.48508 N@5: 0.49849 early stop: 2\n",
            "\u001b[32m[I 210629 03:27:15 models:110]\u001b[39m 97 512 train loss: 0.0080911 valid loss: 0.1526938 P@1: 0.60000 P@3: 0.45000 P@5: 0.38600 N@3: 0.48805 N@5: 0.49578 early stop: 3\n",
            "\u001b[32m[I 210629 03:27:17 models:110]\u001b[39m 99 1024 train loss: 0.0060958 valid loss: 0.1537013 P@1: 0.60000 P@3: 0.45000 P@5: 0.38400 N@3: 0.48805 N@5: 0.49397 early stop: 4\n",
            "\u001b[32m[I 210629 03:27:19 models:110]\u001b[39m 102 512 train loss: 0.0059208 valid loss: 0.1549065 P@1: 0.60000 P@3: 0.45000 P@5: 0.38400 N@3: 0.48866 N@5: 0.49421 early stop: 5\n",
            "\u001b[32m[I 210629 03:27:21 models:110]\u001b[39m 104 1024 train loss: 0.0046896 valid loss: 0.1561093 P@1: 0.60000 P@3: 0.46000 P@5: 0.38000 N@3: 0.49570 N@5: 0.49193 early stop: 6\n",
            "\u001b[32m[I 210629 03:27:24 models:110]\u001b[39m 107 512 train loss: 0.0043915 valid loss: 0.1574951 P@1: 0.60000 P@3: 0.46000 P@5: 0.37800 N@3: 0.49570 N@5: 0.49010 early stop: 7\n",
            "\u001b[32m[I 210629 03:27:26 models:110]\u001b[39m 109 1024 train loss: 0.0027175 valid loss: 0.1590965 P@1: 0.60000 P@3: 0.45667 P@5: 0.37600 N@3: 0.49335 N@5: 0.48758 early stop: 8\n",
            "\u001b[32m[I 210629 03:27:28 models:110]\u001b[39m 112 512 train loss: 0.0020382 valid loss: 0.1605929 P@1: 0.60000 P@3: 0.46000 P@5: 0.37600 N@3: 0.49570 N@5: 0.48811 early stop: 9\n",
            "\u001b[32m[I 210629 03:27:30 models:110]\u001b[39m 114 1024 train loss: 0.0020230 valid loss: 0.1622187 P@1: 0.60000 P@3: 0.46000 P@5: 0.37600 N@3: 0.49631 N@5: 0.48856 early stop: 10\n",
            "\u001b[32m[I 210629 03:27:33 models:110]\u001b[39m 117 512 train loss: 0.0020873 valid loss: 0.1638730 P@1: 0.60000 P@3: 0.46333 P@5: 0.37600 N@3: 0.49866 N@5: 0.48773 early stop: 11\n",
            "\u001b[32m[I 210629 03:27:35 models:110]\u001b[39m 119 1024 train loss: 0.0019422 valid loss: 0.1657072 P@1: 0.60000 P@3: 0.46333 P@5: 0.37400 N@3: 0.49866 N@5: 0.48627 early stop: 12\n",
            "\u001b[32m[I 210629 03:27:37 models:110]\u001b[39m 122 512 train loss: 0.0009496 valid loss: 0.1674794 P@1: 0.61000 P@3: 0.46333 P@5: 0.37800 N@3: 0.50039 N@5: 0.49169 early stop: 13\n",
            "\u001b[32m[I 210629 03:27:39 models:110]\u001b[39m 124 1024 train loss: 0.0006176 valid loss: 0.1692019 P@1: 0.61000 P@3: 0.46333 P@5: 0.37800 N@3: 0.50101 N@5: 0.49230 early stop: 14\n",
            "\u001b[32m[I 210629 03:27:42 models:110]\u001b[39m 127 512 train loss: 0.0003718 valid loss: 0.1709321 P@1: 0.60000 P@3: 0.46333 P@5: 0.37800 N@3: 0.49866 N@5: 0.49057 early stop: 15\n",
            "\u001b[32m[I 210629 03:27:44 models:110]\u001b[39m 129 1024 train loss: 0.0004494 valid loss: 0.1728466 P@1: 0.60000 P@3: 0.46333 P@5: 0.37600 N@3: 0.49866 N@5: 0.48875 early stop: 16\n",
            "\u001b[32m[I 210629 03:27:46 models:110]\u001b[39m 132 512 train loss: 0.0005839 valid loss: 0.1746992 P@1: 0.60000 P@3: 0.46000 P@5: 0.37600 N@3: 0.49631 N@5: 0.48852 early stop: 17\n",
            "\u001b[32m[I 210629 03:27:48 models:110]\u001b[39m 134 1024 train loss: 0.0002866 valid loss: 0.1764728 P@1: 0.60000 P@3: 0.46333 P@5: 0.37800 N@3: 0.49866 N@5: 0.49030 early stop: 18\n",
            "\u001b[32m[I 210629 03:27:51 models:110]\u001b[39m 137 512 train loss: 0.0002001 valid loss: 0.1783799 P@1: 0.60000 P@3: 0.46333 P@5: 0.37600 N@3: 0.49866 N@5: 0.48884 early stop: 19\n",
            "\u001b[32m[I 210629 03:27:53 models:110]\u001b[39m 139 1024 train loss: 0.0001293 valid loss: 0.1802772 P@1: 0.60000 P@3: 0.46000 P@5: 0.37600 N@3: 0.49631 N@5: 0.48861 early stop: 20\n",
            "\u001b[32m[I 210629 03:27:55 models:110]\u001b[39m 142 512 train loss: 0.0001052 valid loss: 0.1821313 P@1: 0.60000 P@3: 0.45667 P@5: 0.37400 N@3: 0.49397 N@5: 0.48686 early stop: 21\n",
            "\u001b[32m[I 210629 03:27:57 models:110]\u001b[39m 144 1024 train loss: 0.0001048 valid loss: 0.1839049 P@1: 0.60000 P@3: 0.45667 P@5: 0.37400 N@3: 0.49397 N@5: 0.48718 early stop: 22\n",
            "\u001b[32m[I 210629 03:28:00 models:110]\u001b[39m 147 512 train loss: 0.0000848 valid loss: 0.1857466 P@1: 0.60000 P@3: 0.45667 P@5: 0.37200 N@3: 0.49397 N@5: 0.48587 early stop: 23\n",
            "\u001b[32m[I 210629 03:28:02 models:110]\u001b[39m 149 1024 train loss: 0.0000596 valid loss: 0.1875574 P@1: 0.60000 P@3: 0.46000 P@5: 0.37200 N@3: 0.49631 N@5: 0.48610 early stop: 24\n",
            "\u001b[32m[I 210629 03:28:04 models:110]\u001b[39m 152 512 train loss: 0.0000568 valid loss: 0.1893309 P@1: 0.60000 P@3: 0.46000 P@5: 0.36800 N@3: 0.49570 N@5: 0.48301 early stop: 25\n",
            "\u001b[32m[I 210629 03:28:06 models:110]\u001b[39m 154 1024 train loss: 0.0000531 valid loss: 0.1910726 P@1: 0.60000 P@3: 0.46000 P@5: 0.37000 N@3: 0.49570 N@5: 0.48433 early stop: 26\n",
            "\u001b[32m[I 210629 03:28:09 models:110]\u001b[39m 157 512 train loss: 0.0000397 valid loss: 0.1927980 P@1: 0.61000 P@3: 0.46000 P@5: 0.36800 N@3: 0.49743 N@5: 0.48326 early stop: 27\n",
            "\u001b[33m[W 210629 03:28:09 models:137]\u001b[39m Clipping gradients with total norm 0.05704 and max norm 0.00475\n",
            "\u001b[32m[I 210629 03:28:11 models:110]\u001b[39m 159 1024 train loss: 0.0000593 valid loss: 0.1945493 P@1: 0.61000 P@3: 0.46000 P@5: 0.36800 N@3: 0.49743 N@5: 0.48326 early stop: 28\n",
            "\u001b[32m[I 210629 03:28:13 models:110]\u001b[39m 162 512 train loss: 0.0000351 valid loss: 0.1963054 P@1: 0.60000 P@3: 0.46000 P@5: 0.36800 N@3: 0.49631 N@5: 0.48246 early stop: 29\n",
            "\u001b[32m[I 210629 03:28:15 models:110]\u001b[39m 164 1024 train loss: 0.0000341 valid loss: 0.1980475 P@1: 0.60000 P@3: 0.46000 P@5: 0.36800 N@3: 0.49570 N@5: 0.48169 early stop: 30\n",
            "\u001b[32m[I 210629 03:28:18 models:110]\u001b[39m 167 512 train loss: 0.0000361 valid loss: 0.1997849 P@1: 0.60000 P@3: 0.46333 P@5: 0.36800 N@3: 0.49805 N@5: 0.48193 early stop: 31\n",
            "\u001b[32m[I 210629 03:28:20 models:110]\u001b[39m 169 1024 train loss: 0.0000284 valid loss: 0.2014970 P@1: 0.60000 P@3: 0.46667 P@5: 0.37000 N@3: 0.50039 N@5: 0.48356 early stop: 32\n",
            "\u001b[32m[I 210629 03:28:22 models:110]\u001b[39m 172 512 train loss: 0.0000267 valid loss: 0.2031925 P@1: 0.60000 P@3: 0.46667 P@5: 0.37000 N@3: 0.50039 N@5: 0.48356 early stop: 33\n",
            "\u001b[32m[I 210629 03:28:24 models:110]\u001b[39m 174 1024 train loss: 0.0000261 valid loss: 0.2048771 P@1: 0.60000 P@3: 0.46667 P@5: 0.37000 N@3: 0.50039 N@5: 0.48356 early stop: 34\n",
            "\u001b[32m[I 210629 03:28:26 models:110]\u001b[39m 177 512 train loss: 0.0000314 valid loss: 0.2065438 P@1: 0.60000 P@3: 0.46667 P@5: 0.37000 N@3: 0.49978 N@5: 0.48305 early stop: 35\n",
            "\u001b[32m[I 210629 03:28:28 models:110]\u001b[39m 179 1024 train loss: 0.0000294 valid loss: 0.2082018 P@1: 0.60000 P@3: 0.46667 P@5: 0.37000 N@3: 0.49978 N@5: 0.48322 early stop: 36\n",
            "\u001b[33m[W 210629 03:28:29 models:137]\u001b[39m Clipping gradients with total norm 0.0373 and max norm 0.00608\n",
            "\u001b[32m[I 210629 03:28:31 models:110]\u001b[39m 182 512 train loss: 0.0000318 valid loss: 0.2098503 P@1: 0.60000 P@3: 0.46667 P@5: 0.37000 N@3: 0.49978 N@5: 0.48322 early stop: 37\n",
            "\u001b[32m[I 210629 03:28:33 models:110]\u001b[39m 184 1024 train loss: 0.0000272 valid loss: 0.2114827 P@1: 0.60000 P@3: 0.46667 P@5: 0.37200 N@3: 0.49978 N@5: 0.48454 early stop: 38\n",
            "\u001b[32m[I 210629 03:28:35 models:110]\u001b[39m 187 512 train loss: 0.0000273 valid loss: 0.2130727 P@1: 0.60000 P@3: 0.46333 P@5: 0.37200 N@3: 0.49743 N@5: 0.48427 early stop: 39\n",
            "\u001b[33m[W 210629 03:28:35 models:137]\u001b[39m Clipping gradients with total norm 0.04189 and max norm 0.00465\n",
            "\u001b[32m[I 210629 03:28:37 models:110]\u001b[39m 189 1024 train loss: 0.0000331 valid loss: 0.2146981 P@1: 0.60000 P@3: 0.46333 P@5: 0.37200 N@3: 0.49743 N@5: 0.48427 early stop: 40\n",
            "\u001b[32m[I 210629 03:28:40 models:110]\u001b[39m 192 512 train loss: 0.0000247 valid loss: 0.2162944 P@1: 0.60000 P@3: 0.46667 P@5: 0.37400 N@3: 0.49978 N@5: 0.48585 early stop: 41\n",
            "\u001b[33m[W 210629 03:28:42 models:137]\u001b[39m Clipping gradients with total norm 0.12592 and max norm 0.00665\n",
            "\u001b[32m[I 210629 03:28:42 models:110]\u001b[39m 194 1024 train loss: 0.0000336 valid loss: 0.2178604 P@1: 0.60000 P@3: 0.46667 P@5: 0.37400 N@3: 0.49978 N@5: 0.48585 early stop: 42\n",
            "\u001b[32m[I 210629 03:28:44 models:110]\u001b[39m 197 512 train loss: 0.0000190 valid loss: 0.2194047 P@1: 0.59000 P@3: 0.46667 P@5: 0.37400 N@3: 0.49690 N@5: 0.48297 early stop: 43\n",
            "\u001b[33m[W 210629 03:28:44 models:137]\u001b[39m Clipping gradients with total norm 0.01368 and max norm 0.00231\n",
            "\u001b[32m[I 210629 03:28:46 models:110]\u001b[39m 199 1024 train loss: 0.0000242 valid loss: 0.2209470 P@1: 0.59000 P@3: 0.46667 P@5: 0.37400 N@3: 0.49690 N@5: 0.48282 early stop: 44\n",
            "\u001b[32m[I 210629 03:28:49 models:110]\u001b[39m 202 512 train loss: 0.0000229 valid loss: 0.2224690 P@1: 0.59000 P@3: 0.46667 P@5: 0.37400 N@3: 0.49690 N@5: 0.48282 early stop: 45\n",
            "\u001b[32m[I 210629 03:28:51 models:110]\u001b[39m 204 1024 train loss: 0.0000179 valid loss: 0.2239834 P@1: 0.59000 P@3: 0.46667 P@5: 0.37400 N@3: 0.49690 N@5: 0.48282 early stop: 46\n",
            "\u001b[32m[I 210629 03:28:53 models:110]\u001b[39m 207 512 train loss: 0.0000193 valid loss: 0.2254790 P@1: 0.60000 P@3: 0.46667 P@5: 0.37400 N@3: 0.49863 N@5: 0.48455 early stop: 47\n",
            "\u001b[32m[I 210629 03:28:55 models:110]\u001b[39m 209 1024 train loss: 0.0000198 valid loss: 0.2269747 P@1: 0.60000 P@3: 0.46667 P@5: 0.37400 N@3: 0.49863 N@5: 0.48455 early stop: 48\n",
            "\u001b[32m[I 210629 03:28:58 models:110]\u001b[39m 212 512 train loss: 0.0000172 valid loss: 0.2284708 P@1: 0.60000 P@3: 0.46667 P@5: 0.37400 N@3: 0.49863 N@5: 0.48455 early stop: 49\n",
            "\u001b[33m[W 210629 03:28:59 models:137]\u001b[39m Clipping gradients with total norm 0.01877 and max norm 0.00245\n",
            "\u001b[32m[I 210629 03:29:00 models:110]\u001b[39m 214 1024 train loss: 0.0000260 valid loss: 0.2299523 P@1: 0.60000 P@3: 0.47000 P@5: 0.37400 N@3: 0.50036 N@5: 0.48428 early stop: 50\n",
            "\u001b[32m[I 210629 03:29:02 main:76]\u001b[39m Finish Training\n",
            "\u001b[32m[I 210629 03:29:04 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210629 03:29:04 main:79]\u001b[39m Loading Test Set\n",
            "\u001b[32m[I 210629 03:29:04 main:83]\u001b[39m Size of Test Set: 100\n",
            "\u001b[32m[I 210629 03:29:04 main:85]\u001b[39m Predicting\n",
            "\u001b[32m[I 210629 03:29:08 main:91]\u001b[39m Finish Predicting\n",
            "Precision@1,3,5: 0.51 0.4033333333333333 0.33\n",
            "nDCG@1,3,5: 0.51 0.43345754486724075 0.4362850302815957\n",
            "\n",
            "WITH MAG WITH MESH, skip=900\n",
            "\n",
            "/content/drive/Shareddrives/NASA/MATCH/PeTaL\n",
            "130\n",
            "/content/drive/Shareddrives/NASA/MATCH\n",
            "    800  286051 4914820 PeTaL/train.json\n",
            "\u001b[32m[I 210629 03:29:10 preprocess:28]\u001b[39m Vocab Size: 26834\n",
            "\u001b[32m[I 210629 03:29:10 preprocess:30]\u001b[39m Getting Dataset: PeTaL/train_texts.txt Max Length: 500\n",
            "\u001b[32m[I 210629 03:29:11 preprocess:32]\u001b[39m Size of Samples: 900\n",
            "\u001b[32m[I 210629 03:29:12 preprocess:28]\u001b[39m Vocab Size: 26834\n",
            "\u001b[32m[I 210629 03:29:12 preprocess:30]\u001b[39m Getting Dataset: PeTaL/test_texts.txt Max Length: 500\n",
            "\u001b[32m[I 210629 03:29:12 preprocess:32]\u001b[39m Size of Samples: 100\n",
            "\u001b[32m[I 210629 03:29:13 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210629 03:29:13 main:35]\u001b[39m Loading Training and Validation Set\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['absorb_and/or_filter_solids', 'chemically_break_down_inorganic_compounds', 'detox/purify', 'manage_environmental_disturbances_in_a_community', 'protect_from_fire', 'protect_from_gases'] will be ignored\n",
            "  .format(sorted(unknown, key=str)))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['chemically_break_down_inorganic_compounds', 'protect_from_fire'] will be ignored\n",
            "  .format(sorted(unknown, key=str)))\n",
            "\u001b[32m[I 210629 03:29:13 main:47]\u001b[39m Number of Labels: 124\n",
            "\u001b[32m[I 210629 03:29:13 main:48]\u001b[39m Size of Training Set: 800\n",
            "\u001b[32m[I 210629 03:29:13 main:49]\u001b[39m Size of Validation Set: 100\n",
            "\u001b[32m[I 210629 03:29:13 main:66]\u001b[39m Number of Edges: 101\n",
            "\u001b[32m[I 210629 03:29:13 main:68]\u001b[39m Training\n",
            "\u001b[32m[I 210629 03:29:19 models:110]\u001b[39m 2 512 train loss: 0.2702575 valid loss: 0.1554075 P@1: 0.29000 P@3: 0.17000 P@5: 0.14000 N@3: 0.19570 N@5: 0.19892 early stop: 0\n",
            "\u001b[32m[I 210629 03:29:20 models:142]\u001b[39m SWA Initializing\n",
            "\u001b[32m[I 210629 03:29:21 models:110]\u001b[39m 4 1024 train loss: 0.1467399 valid loss: 0.1458711 P@1: 0.29000 P@3: 0.23000 P@5: 0.18200 N@3: 0.24285 N@5: 0.24336 early stop: 0\n",
            "\u001b[32m[I 210629 03:29:24 models:110]\u001b[39m 7 512 train loss: 0.1405401 valid loss: 0.1440092 P@1: 0.29000 P@3: 0.22333 P@5: 0.18200 N@3: 0.23754 N@5: 0.24226 early stop: 1\n",
            "\u001b[32m[I 210629 03:29:26 models:110]\u001b[39m 9 1024 train loss: 0.1385632 valid loss: 0.1432843 P@1: 0.29000 P@3: 0.22333 P@5: 0.18400 N@3: 0.23754 N@5: 0.24448 early stop: 0\n",
            "\u001b[32m[I 210629 03:29:28 models:110]\u001b[39m 12 512 train loss: 0.1322481 valid loss: 0.1426681 P@1: 0.29000 P@3: 0.22667 P@5: 0.21000 N@3: 0.24358 N@5: 0.26813 early stop: 0\n",
            "\u001b[32m[I 210629 03:29:31 models:110]\u001b[39m 14 1024 train loss: 0.1150035 valid loss: 0.1419141 P@1: 0.29000 P@3: 0.23000 P@5: 0.19400 N@3: 0.24899 N@5: 0.25846 early stop: 1\n",
            "\u001b[32m[I 210629 03:29:33 models:110]\u001b[39m 17 512 train loss: 0.0941633 valid loss: 0.1408121 P@1: 0.30000 P@3: 0.25667 P@5: 0.20200 N@3: 0.26827 N@5: 0.26849 early stop: 0\n",
            "\u001b[32m[I 210629 03:29:35 models:110]\u001b[39m 19 1024 train loss: 0.0815967 valid loss: 0.1395946 P@1: 0.35000 P@3: 0.25333 P@5: 0.22200 N@3: 0.27520 N@5: 0.28995 early stop: 0\n",
            "\u001b[32m[I 210629 03:29:38 models:110]\u001b[39m 22 512 train loss: 0.0652824 valid loss: 0.1380961 P@1: 0.37000 P@3: 0.25667 P@5: 0.22200 N@3: 0.28285 N@5: 0.29424 early stop: 0\n",
            "\u001b[32m[I 210629 03:29:40 models:110]\u001b[39m 24 1024 train loss: 0.0524902 valid loss: 0.1364634 P@1: 0.42000 P@3: 0.29333 P@5: 0.22400 N@3: 0.32173 N@5: 0.31207 early stop: 0\n",
            "\u001b[32m[I 210629 03:29:42 models:110]\u001b[39m 27 512 train loss: 0.0454958 valid loss: 0.1351160 P@1: 0.42000 P@3: 0.29667 P@5: 0.24000 N@3: 0.32682 N@5: 0.32842 early stop: 0\n",
            "\u001b[32m[I 210629 03:29:45 models:110]\u001b[39m 29 1024 train loss: 0.0396431 valid loss: 0.1341106 P@1: 0.42000 P@3: 0.32000 P@5: 0.24200 N@3: 0.34571 N@5: 0.33491 early stop: 0\n",
            "\u001b[32m[I 210629 03:29:47 models:110]\u001b[39m 32 512 train loss: 0.0344424 valid loss: 0.1328947 P@1: 0.46000 P@3: 0.33333 P@5: 0.25400 N@3: 0.36509 N@5: 0.35442 early stop: 0\n",
            "\u001b[32m[I 210629 03:29:49 models:110]\u001b[39m 34 1024 train loss: 0.0288468 valid loss: 0.1317064 P@1: 0.47000 P@3: 0.33667 P@5: 0.27000 N@3: 0.36898 N@5: 0.37005 early stop: 0\n",
            "\u001b[32m[I 210629 03:29:52 models:110]\u001b[39m 37 512 train loss: 0.0241056 valid loss: 0.1310504 P@1: 0.50000 P@3: 0.35000 P@5: 0.27000 N@3: 0.38508 N@5: 0.37850 early stop: 0\n",
            "\u001b[32m[I 210629 03:29:54 models:110]\u001b[39m 39 1024 train loss: 0.0230351 valid loss: 0.1308309 P@1: 0.50000 P@3: 0.36000 P@5: 0.27000 N@3: 0.39458 N@5: 0.38085 early stop: 0\n",
            "\u001b[32m[I 210629 03:29:56 models:110]\u001b[39m 42 512 train loss: 0.0210080 valid loss: 0.1304356 P@1: 0.54000 P@3: 0.36333 P@5: 0.27800 N@3: 0.40621 N@5: 0.39545 early stop: 0\n",
            "\u001b[32m[I 210629 03:29:58 models:110]\u001b[39m 44 1024 train loss: 0.0165069 valid loss: 0.1306654 P@1: 0.53000 P@3: 0.36667 P@5: 0.28600 N@3: 0.40663 N@5: 0.40102 early stop: 0\n",
            "\u001b[32m[I 210629 03:30:01 models:110]\u001b[39m 47 512 train loss: 0.0163013 valid loss: 0.1305965 P@1: 0.55000 P@3: 0.36667 P@5: 0.29400 N@3: 0.40876 N@5: 0.40719 early stop: 0\n",
            "\u001b[32m[I 210629 03:30:03 models:110]\u001b[39m 49 1024 train loss: 0.0152613 valid loss: 0.1309690 P@1: 0.55000 P@3: 0.37000 P@5: 0.29600 N@3: 0.40978 N@5: 0.40854 early stop: 0\n",
            "\u001b[32m[I 210629 03:30:06 models:110]\u001b[39m 52 512 train loss: 0.0140227 valid loss: 0.1314069 P@1: 0.55000 P@3: 0.37667 P@5: 0.29800 N@3: 0.41508 N@5: 0.41256 early stop: 0\n",
            "\u001b[32m[I 210629 03:30:08 models:110]\u001b[39m 54 1024 train loss: 0.0115642 valid loss: 0.1313133 P@1: 0.56000 P@3: 0.38667 P@5: 0.31000 N@3: 0.42570 N@5: 0.42513 early stop: 0\n",
            "\u001b[32m[I 210629 03:30:10 models:110]\u001b[39m 57 512 train loss: 0.0143710 valid loss: 0.1317635 P@1: 0.55000 P@3: 0.38667 P@5: 0.32200 N@3: 0.42335 N@5: 0.43364 early stop: 0\n",
            "\u001b[32m[I 210629 03:30:12 models:110]\u001b[39m 59 1024 train loss: 0.0172001 valid loss: 0.1324961 P@1: 0.55000 P@3: 0.38667 P@5: 0.32800 N@3: 0.42530 N@5: 0.44056 early stop: 0\n",
            "\u001b[32m[I 210629 03:30:15 models:110]\u001b[39m 62 512 train loss: 0.0156191 valid loss: 0.1329641 P@1: 0.55000 P@3: 0.40000 P@5: 0.33000 N@3: 0.43530 N@5: 0.44372 early stop: 0\n",
            "\u001b[32m[I 210629 03:30:17 models:110]\u001b[39m 64 1024 train loss: 0.0148070 valid loss: 0.1334489 P@1: 0.55000 P@3: 0.41667 P@5: 0.33400 N@3: 0.44722 N@5: 0.44856 early stop: 0\n",
            "\u001b[32m[I 210629 03:30:20 models:110]\u001b[39m 67 512 train loss: 0.0132481 valid loss: 0.1343323 P@1: 0.55000 P@3: 0.41000 P@5: 0.33800 N@3: 0.44499 N@5: 0.45300 early stop: 0\n",
            "\u001b[32m[I 210629 03:30:22 models:110]\u001b[39m 69 1024 train loss: 0.0090998 valid loss: 0.1349528 P@1: 0.55000 P@3: 0.41667 P@5: 0.34600 N@3: 0.44968 N@5: 0.46117 early stop: 0\n",
            "\u001b[32m[I 210629 03:30:24 models:110]\u001b[39m 72 512 train loss: 0.0080152 valid loss: 0.1359618 P@1: 0.55000 P@3: 0.42000 P@5: 0.35200 N@3: 0.45203 N@5: 0.46554 early stop: 0\n",
            "\u001b[32m[I 210629 03:30:26 models:110]\u001b[39m 74 1024 train loss: 0.0054768 valid loss: 0.1370752 P@1: 0.57000 P@3: 0.42000 P@5: 0.35400 N@3: 0.45549 N@5: 0.47052 early stop: 0\n",
            "\u001b[32m[I 210629 03:30:29 models:110]\u001b[39m 77 512 train loss: 0.0039981 valid loss: 0.1382971 P@1: 0.57000 P@3: 0.43000 P@5: 0.36200 N@3: 0.46253 N@5: 0.47745 early stop: 0\n",
            "\u001b[32m[I 210629 03:30:31 models:110]\u001b[39m 79 1024 train loss: 0.0035330 valid loss: 0.1396397 P@1: 0.57000 P@3: 0.42667 P@5: 0.36600 N@3: 0.46141 N@5: 0.48197 early stop: 0\n",
            "\u001b[32m[I 210629 03:30:34 models:110]\u001b[39m 82 512 train loss: 0.0027301 valid loss: 0.1412112 P@1: 0.58000 P@3: 0.43000 P@5: 0.36600 N@3: 0.46487 N@5: 0.48324 early stop: 0\n",
            "\u001b[32m[I 210629 03:30:36 models:110]\u001b[39m 84 1024 train loss: 0.0034049 valid loss: 0.1427352 P@1: 0.58000 P@3: 0.42667 P@5: 0.36400 N@3: 0.46376 N@5: 0.48220 early stop: 1\n",
            "\u001b[32m[I 210629 03:30:38 models:110]\u001b[39m 87 512 train loss: 0.0031564 valid loss: 0.1442380 P@1: 0.58000 P@3: 0.44000 P@5: 0.36600 N@3: 0.47253 N@5: 0.48400 early stop: 0\n",
            "\u001b[32m[I 210629 03:30:40 models:110]\u001b[39m 89 1024 train loss: 0.0031612 valid loss: 0.1456410 P@1: 0.58000 P@3: 0.44333 P@5: 0.37000 N@3: 0.47426 N@5: 0.48765 early stop: 0\n",
            "\u001b[32m[I 210629 03:30:43 models:110]\u001b[39m 92 512 train loss: 0.0053189 valid loss: 0.1469289 P@1: 0.57000 P@3: 0.44333 P@5: 0.37600 N@3: 0.47314 N@5: 0.49078 early stop: 0\n",
            "\u001b[32m[I 210629 03:30:45 models:110]\u001b[39m 94 1024 train loss: 0.0057579 valid loss: 0.1482317 P@1: 0.57000 P@3: 0.45000 P@5: 0.37600 N@3: 0.47784 N@5: 0.49111 early stop: 0\n",
            "\u001b[32m[I 210629 03:30:47 models:110]\u001b[39m 97 512 train loss: 0.0055963 valid loss: 0.1497784 P@1: 0.57000 P@3: 0.45000 P@5: 0.37400 N@3: 0.47784 N@5: 0.49034 early stop: 1\n",
            "\u001b[32m[I 210629 03:30:50 models:110]\u001b[39m 99 1024 train loss: 0.0060742 valid loss: 0.1512357 P@1: 0.58000 P@3: 0.44667 P@5: 0.37200 N@3: 0.47661 N@5: 0.48955 early stop: 2\n",
            "\u001b[32m[I 210629 03:30:52 models:110]\u001b[39m 102 512 train loss: 0.0083423 valid loss: 0.1526044 P@1: 0.58000 P@3: 0.44333 P@5: 0.37600 N@3: 0.47426 N@5: 0.49149 early stop: 0\n",
            "\u001b[32m[I 210629 03:30:54 models:110]\u001b[39m 104 1024 train loss: 0.0066756 valid loss: 0.1536092 P@1: 0.57000 P@3: 0.44000 P@5: 0.37600 N@3: 0.47018 N@5: 0.49038 early stop: 1\n",
            "\u001b[32m[I 210629 03:30:57 models:110]\u001b[39m 107 512 train loss: 0.0085074 valid loss: 0.1550414 P@1: 0.57000 P@3: 0.44667 P@5: 0.37400 N@3: 0.47426 N@5: 0.48913 early stop: 2\n",
            "\u001b[32m[I 210629 03:30:59 models:110]\u001b[39m 109 1024 train loss: 0.0098319 valid loss: 0.1563523 P@1: 0.58000 P@3: 0.44667 P@5: 0.37800 N@3: 0.47661 N@5: 0.49427 early stop: 0\n",
            "\u001b[32m[I 210629 03:31:01 models:110]\u001b[39m 112 512 train loss: 0.0077756 valid loss: 0.1575407 P@1: 0.58000 P@3: 0.45333 P@5: 0.38000 N@3: 0.48130 N@5: 0.49706 early stop: 0\n",
            "\u001b[32m[I 210629 03:31:03 models:110]\u001b[39m 114 1024 train loss: 0.0070531 valid loss: 0.1586143 P@1: 0.59000 P@3: 0.45333 P@5: 0.37800 N@3: 0.48365 N@5: 0.49791 early stop: 0\n",
            "\u001b[32m[I 210629 03:31:06 models:110]\u001b[39m 117 512 train loss: 0.0059099 valid loss: 0.1594810 P@1: 0.59000 P@3: 0.45667 P@5: 0.38000 N@3: 0.48661 N@5: 0.50020 early stop: 0\n",
            "\u001b[32m[I 210629 03:31:08 models:110]\u001b[39m 119 1024 train loss: 0.0050492 valid loss: 0.1605679 P@1: 0.59000 P@3: 0.45667 P@5: 0.38600 N@3: 0.48599 N@5: 0.50443 early stop: 0\n",
            "\u001b[32m[I 210629 03:31:10 models:110]\u001b[39m 122 512 train loss: 0.0038051 valid loss: 0.1617249 P@1: 0.59000 P@3: 0.45333 P@5: 0.38600 N@3: 0.48365 N@5: 0.50442 early stop: 1\n",
            "\u001b[32m[I 210629 03:31:12 models:110]\u001b[39m 124 1024 train loss: 0.0040928 valid loss: 0.1629694 P@1: 0.60000 P@3: 0.45000 P@5: 0.38800 N@3: 0.48365 N@5: 0.50774 early stop: 0\n",
            "\u001b[32m[I 210629 03:31:15 models:110]\u001b[39m 127 512 train loss: 0.0029517 valid loss: 0.1641413 P@1: 0.60000 P@3: 0.45000 P@5: 0.39000 N@3: 0.48365 N@5: 0.50942 early stop: 0\n",
            "\u001b[32m[I 210629 03:31:17 models:110]\u001b[39m 129 1024 train loss: 0.0027886 valid loss: 0.1653208 P@1: 0.60000 P@3: 0.45000 P@5: 0.38800 N@3: 0.48365 N@5: 0.50796 early stop: 1\n",
            "\u001b[32m[I 210629 03:31:20 models:110]\u001b[39m 132 512 train loss: 0.0012988 valid loss: 0.1665849 P@1: 0.60000 P@3: 0.45667 P@5: 0.38800 N@3: 0.48978 N@5: 0.50952 early stop: 0\n",
            "\u001b[32m[I 210629 03:31:22 models:110]\u001b[39m 134 1024 train loss: 0.0008564 valid loss: 0.1679044 P@1: 0.60000 P@3: 0.46000 P@5: 0.38800 N@3: 0.49212 N@5: 0.50956 early stop: 0\n",
            "\u001b[33m[W 210629 03:31:23 models:137]\u001b[39m Clipping gradients with total norm 0.32862 and max norm 0.03949\n",
            "\u001b[32m[I 210629 03:31:24 models:110]\u001b[39m 137 512 train loss: 0.0007952 valid loss: 0.1691391 P@1: 0.60000 P@3: 0.46333 P@5: 0.38600 N@3: 0.49447 N@5: 0.50872 early stop: 1\n",
            "\u001b[32m[I 210629 03:31:26 models:110]\u001b[39m 139 1024 train loss: 0.0007122 valid loss: 0.1704749 P@1: 0.59000 P@3: 0.46333 P@5: 0.39200 N@3: 0.49274 N@5: 0.51299 early stop: 0\n",
            "\u001b[32m[I 210629 03:31:29 models:110]\u001b[39m 142 512 train loss: 0.0008231 valid loss: 0.1718388 P@1: 0.59000 P@3: 0.46333 P@5: 0.39400 N@3: 0.49274 N@5: 0.51471 early stop: 0\n",
            "\u001b[32m[I 210629 03:31:31 models:110]\u001b[39m 144 1024 train loss: 0.0006180 valid loss: 0.1731509 P@1: 0.59000 P@3: 0.46667 P@5: 0.39400 N@3: 0.49508 N@5: 0.51483 early stop: 0\n",
            "\u001b[32m[I 210629 03:31:33 models:110]\u001b[39m 147 512 train loss: 0.0007869 valid loss: 0.1744705 P@1: 0.59000 P@3: 0.46000 P@5: 0.39400 N@3: 0.49039 N@5: 0.51467 early stop: 1\n",
            "\u001b[32m[I 210629 03:31:36 models:110]\u001b[39m 149 1024 train loss: 0.0005104 valid loss: 0.1758248 P@1: 0.59000 P@3: 0.46000 P@5: 0.39400 N@3: 0.49039 N@5: 0.51467 early stop: 2\n",
            "\u001b[32m[I 210629 03:31:38 models:110]\u001b[39m 152 512 train loss: 0.0003722 valid loss: 0.1771783 P@1: 0.59000 P@3: 0.45667 P@5: 0.39400 N@3: 0.48805 N@5: 0.51419 early stop: 3\n",
            "\u001b[32m[I 210629 03:31:40 models:110]\u001b[39m 154 1024 train loss: 0.0002706 valid loss: 0.1785234 P@1: 0.59000 P@3: 0.46333 P@5: 0.39400 N@3: 0.49274 N@5: 0.51458 early stop: 4\n",
            "\u001b[32m[I 210629 03:31:42 models:110]\u001b[39m 157 512 train loss: 0.0002493 valid loss: 0.1798887 P@1: 0.59000 P@3: 0.46333 P@5: 0.39200 N@3: 0.49274 N@5: 0.51307 early stop: 5\n",
            "\u001b[32m[I 210629 03:31:45 models:110]\u001b[39m 159 1024 train loss: 0.0002477 valid loss: 0.1812637 P@1: 0.58000 P@3: 0.46667 P@5: 0.39200 N@3: 0.49274 N@5: 0.51170 early stop: 6\n",
            "\u001b[32m[I 210629 03:31:47 models:110]\u001b[39m 162 512 train loss: 0.0001925 valid loss: 0.1826530 P@1: 0.58000 P@3: 0.46667 P@5: 0.39000 N@3: 0.49354 N@5: 0.51082 early stop: 7\n",
            "\u001b[32m[I 210629 03:31:49 models:110]\u001b[39m 164 1024 train loss: 0.0002292 valid loss: 0.1840523 P@1: 0.57000 P@3: 0.47000 P@5: 0.39000 N@3: 0.49416 N@5: 0.51001 early stop: 8\n",
            "\u001b[32m[I 210629 03:31:51 models:110]\u001b[39m 167 512 train loss: 0.0001993 valid loss: 0.1854043 P@1: 0.58000 P@3: 0.47333 P@5: 0.39200 N@3: 0.49876 N@5: 0.51406 early stop: 9\n",
            "\u001b[32m[I 210629 03:31:54 models:110]\u001b[39m 169 1024 train loss: 0.0002117 valid loss: 0.1867645 P@1: 0.57000 P@3: 0.47000 P@5: 0.39200 N@3: 0.49477 N@5: 0.51197 early stop: 10\n",
            "\u001b[33m[W 210629 03:31:54 models:137]\u001b[39m Clipping gradients with total norm 0.11276 and max norm 0.02015\n",
            "\u001b[32m[I 210629 03:31:56 models:110]\u001b[39m 172 512 train loss: 0.0005892 valid loss: 0.1881568 P@1: 0.57000 P@3: 0.47000 P@5: 0.39200 N@3: 0.49477 N@5: 0.51197 early stop: 11\n",
            "\u001b[32m[I 210629 03:31:58 models:110]\u001b[39m 174 1024 train loss: 0.0005953 valid loss: 0.1894352 P@1: 0.57000 P@3: 0.47000 P@5: 0.39200 N@3: 0.49538 N@5: 0.51258 early stop: 12\n",
            "\u001b[32m[I 210629 03:32:01 models:110]\u001b[39m 177 512 train loss: 0.0003312 valid loss: 0.1906473 P@1: 0.57000 P@3: 0.47333 P@5: 0.39000 N@3: 0.49712 N@5: 0.51039 early stop: 13\n",
            "\u001b[32m[I 210629 03:32:03 models:110]\u001b[39m 179 1024 train loss: 0.0002433 valid loss: 0.1918710 P@1: 0.57000 P@3: 0.46667 P@5: 0.39000 N@3: 0.49242 N@5: 0.50974 early stop: 14\n",
            "\u001b[32m[I 210629 03:32:05 models:110]\u001b[39m 182 512 train loss: 0.0001682 valid loss: 0.1931523 P@1: 0.57000 P@3: 0.46667 P@5: 0.39000 N@3: 0.49242 N@5: 0.50974 early stop: 15\n",
            "\u001b[32m[I 210629 03:32:07 models:110]\u001b[39m 184 1024 train loss: 0.0001714 valid loss: 0.1944300 P@1: 0.57000 P@3: 0.46667 P@5: 0.39000 N@3: 0.49242 N@5: 0.50974 early stop: 16\n",
            "\u001b[33m[W 210629 03:32:08 models:137]\u001b[39m Clipping gradients with total norm 0.06637 and max norm 0.0109\n",
            "\u001b[32m[I 210629 03:32:10 models:110]\u001b[39m 187 512 train loss: 0.0003618 valid loss: 0.1956868 P@1: 0.57000 P@3: 0.46667 P@5: 0.38800 N@3: 0.49242 N@5: 0.50837 early stop: 17\n",
            "\u001b[32m[I 210629 03:32:12 models:110]\u001b[39m 189 1024 train loss: 0.0002516 valid loss: 0.1969495 P@1: 0.57000 P@3: 0.46667 P@5: 0.38800 N@3: 0.49242 N@5: 0.50837 early stop: 18\n",
            "\u001b[32m[I 210629 03:32:14 models:110]\u001b[39m 192 512 train loss: 0.0002299 valid loss: 0.1982168 P@1: 0.57000 P@3: 0.47000 P@5: 0.39000 N@3: 0.49538 N@5: 0.51082 early stop: 19\n",
            "\u001b[32m[I 210629 03:32:16 models:110]\u001b[39m 194 1024 train loss: 0.0001266 valid loss: 0.1994656 P@1: 0.57000 P@3: 0.47000 P@5: 0.39000 N@3: 0.49538 N@5: 0.51052 early stop: 20\n",
            "\u001b[32m[I 210629 03:32:19 models:110]\u001b[39m 197 512 train loss: 0.0001215 valid loss: 0.2007045 P@1: 0.57000 P@3: 0.47333 P@5: 0.39000 N@3: 0.49773 N@5: 0.51076 early stop: 21\n",
            "\u001b[32m[I 210629 03:32:21 models:110]\u001b[39m 199 1024 train loss: 0.0001724 valid loss: 0.2019506 P@1: 0.57000 P@3: 0.47333 P@5: 0.39000 N@3: 0.49773 N@5: 0.51076 early stop: 22\n",
            "\u001b[32m[I 210629 03:32:23 models:110]\u001b[39m 202 512 train loss: 0.0001558 valid loss: 0.2031862 P@1: 0.57000 P@3: 0.47333 P@5: 0.39000 N@3: 0.49773 N@5: 0.51076 early stop: 23\n",
            "\u001b[32m[I 210629 03:32:25 models:110]\u001b[39m 204 1024 train loss: 0.0003762 valid loss: 0.2043815 P@1: 0.57000 P@3: 0.47333 P@5: 0.39000 N@3: 0.49773 N@5: 0.51076 early stop: 24\n",
            "\u001b[32m[I 210629 03:32:28 models:110]\u001b[39m 207 512 train loss: 0.0002864 valid loss: 0.2055631 P@1: 0.57000 P@3: 0.47333 P@5: 0.39000 N@3: 0.49712 N@5: 0.51035 early stop: 25\n",
            "\u001b[32m[I 210629 03:32:30 models:110]\u001b[39m 209 1024 train loss: 0.0002288 valid loss: 0.2067429 P@1: 0.57000 P@3: 0.47333 P@5: 0.39000 N@3: 0.49712 N@5: 0.51035 early stop: 26\n",
            "\u001b[32m[I 210629 03:32:32 models:110]\u001b[39m 212 512 train loss: 0.0001808 valid loss: 0.2079399 P@1: 0.57000 P@3: 0.47667 P@5: 0.39200 N@3: 0.49946 N@5: 0.51198 early stop: 27\n",
            "\u001b[32m[I 210629 03:32:34 models:110]\u001b[39m 214 1024 train loss: 0.0006034 valid loss: 0.2090851 P@1: 0.57000 P@3: 0.47667 P@5: 0.39000 N@3: 0.49946 N@5: 0.51027 early stop: 28\n",
            "\u001b[32m[I 210629 03:32:37 models:110]\u001b[39m 217 512 train loss: 0.0004665 valid loss: 0.2102778 P@1: 0.58000 P@3: 0.47667 P@5: 0.38800 N@3: 0.50119 N@5: 0.51069 early stop: 29\n",
            "\u001b[32m[I 210629 03:32:39 models:110]\u001b[39m 219 1024 train loss: 0.0002657 valid loss: 0.2114309 P@1: 0.57000 P@3: 0.47667 P@5: 0.38800 N@3: 0.49946 N@5: 0.50896 early stop: 30\n",
            "\u001b[32m[I 210629 03:32:41 models:110]\u001b[39m 222 512 train loss: 0.0002542 valid loss: 0.2125792 P@1: 0.58000 P@3: 0.47667 P@5: 0.39000 N@3: 0.50119 N@5: 0.51185 early stop: 31\n",
            "\u001b[32m[I 210629 03:32:43 models:110]\u001b[39m 224 1024 train loss: 0.0001587 valid loss: 0.2137482 P@1: 0.58000 P@3: 0.47667 P@5: 0.39000 N@3: 0.50119 N@5: 0.51185 early stop: 32\n",
            "\u001b[33m[W 210629 03:32:44 models:137]\u001b[39m Clipping gradients with total norm 0.06002 and max norm 0.01117\n",
            "\u001b[32m[I 210629 03:32:46 models:110]\u001b[39m 227 512 train loss: 0.0002210 valid loss: 0.2148742 P@1: 0.58000 P@3: 0.47667 P@5: 0.39000 N@3: 0.50119 N@5: 0.51236 early stop: 33\n",
            "\u001b[32m[I 210629 03:32:48 models:110]\u001b[39m 229 1024 train loss: 0.0002129 valid loss: 0.2159866 P@1: 0.58000 P@3: 0.47667 P@5: 0.38600 N@3: 0.50119 N@5: 0.50858 early stop: 34\n",
            "\u001b[33m[W 210629 03:32:48 models:137]\u001b[39m Clipping gradients with total norm 0.1544 and max norm 0.02258\n",
            "\u001b[32m[I 210629 03:32:50 models:110]\u001b[39m 232 512 train loss: 0.0003097 valid loss: 0.2170796 P@1: 0.58000 P@3: 0.47667 P@5: 0.39000 N@3: 0.50119 N@5: 0.51241 early stop: 35\n",
            "\u001b[32m[I 210629 03:32:52 models:110]\u001b[39m 234 1024 train loss: 0.0002497 valid loss: 0.2182184 P@1: 0.58000 P@3: 0.47667 P@5: 0.38800 N@3: 0.50119 N@5: 0.51108 early stop: 36\n",
            "\u001b[32m[I 210629 03:32:55 models:110]\u001b[39m 237 512 train loss: 0.0002269 valid loss: 0.2192391 P@1: 0.58000 P@3: 0.47667 P@5: 0.38800 N@3: 0.50119 N@5: 0.51108 early stop: 37\n",
            "\u001b[32m[I 210629 03:32:57 models:110]\u001b[39m 239 1024 train loss: 0.0001352 valid loss: 0.2202684 P@1: 0.58000 P@3: 0.47667 P@5: 0.38800 N@3: 0.50119 N@5: 0.51108 early stop: 38\n",
            "\u001b[33m[W 210629 03:32:57 models:137]\u001b[39m Clipping gradients with total norm 0.06589 and max norm 0.01283\n",
            "\u001b[32m[I 210629 03:32:59 models:110]\u001b[39m 242 512 train loss: 0.0004381 valid loss: 0.2213295 P@1: 0.58000 P@3: 0.47667 P@5: 0.38600 N@3: 0.50119 N@5: 0.50926 early stop: 39\n",
            "\u001b[32m[I 210629 03:33:01 models:110]\u001b[39m 244 1024 train loss: 0.0003916 valid loss: 0.2224315 P@1: 0.58000 P@3: 0.48000 P@5: 0.38800 N@3: 0.50354 N@5: 0.51069 early stop: 40\n",
            "\u001b[32m[I 210629 03:33:04 models:110]\u001b[39m 247 512 train loss: 0.0002944 valid loss: 0.2235601 P@1: 0.57000 P@3: 0.48000 P@5: 0.38800 N@3: 0.50181 N@5: 0.50896 early stop: 41\n",
            "\u001b[32m[I 210629 03:33:06 models:110]\u001b[39m 249 1024 train loss: 0.0002079 valid loss: 0.2246622 P@1: 0.57000 P@3: 0.48000 P@5: 0.38800 N@3: 0.50181 N@5: 0.50896 early stop: 42\n",
            "\u001b[32m[I 210629 03:33:08 models:110]\u001b[39m 252 512 train loss: 0.0001546 valid loss: 0.2257523 P@1: 0.57000 P@3: 0.48000 P@5: 0.38800 N@3: 0.50181 N@5: 0.50896 early stop: 43\n",
            "\u001b[32m[I 210629 03:33:10 models:110]\u001b[39m 254 1024 train loss: 0.0001613 valid loss: 0.2268198 P@1: 0.57000 P@3: 0.48333 P@5: 0.38800 N@3: 0.50416 N@5: 0.50979 early stop: 44\n",
            "\u001b[33m[W 210629 03:33:12 models:137]\u001b[39m Clipping gradients with total norm 0.08045 and max norm 0.01096\n",
            "\u001b[32m[I 210629 03:33:13 models:110]\u001b[39m 257 512 train loss: 0.0003045 valid loss: 0.2278860 P@1: 0.56000 P@3: 0.48333 P@5: 0.38800 N@3: 0.50242 N@5: 0.50835 early stop: 45\n",
            "\u001b[32m[I 210629 03:33:15 models:110]\u001b[39m 259 1024 train loss: 0.0002821 valid loss: 0.2289275 P@1: 0.56000 P@3: 0.48333 P@5: 0.38800 N@3: 0.50242 N@5: 0.50835 early stop: 46\n",
            "\u001b[32m[I 210629 03:33:17 models:110]\u001b[39m 262 512 train loss: 0.0002369 valid loss: 0.2299141 P@1: 0.56000 P@3: 0.49000 P@5: 0.38800 N@3: 0.50773 N@5: 0.50926 early stop: 47\n",
            "\u001b[32m[I 210629 03:33:19 models:110]\u001b[39m 264 1024 train loss: 0.0004819 valid loss: 0.2307854 P@1: 0.56000 P@3: 0.49000 P@5: 0.39000 N@3: 0.50835 N@5: 0.51102 early stop: 48\n",
            "\u001b[32m[I 210629 03:33:22 models:110]\u001b[39m 267 512 train loss: 0.0004097 valid loss: 0.2317555 P@1: 0.56000 P@3: 0.49000 P@5: 0.39000 N@3: 0.50896 N@5: 0.51117 early stop: 49\n",
            "\u001b[32m[I 210629 03:33:24 models:110]\u001b[39m 269 1024 train loss: 0.0013750 valid loss: 0.2326272 P@1: 0.57000 P@3: 0.49000 P@5: 0.39000 N@3: 0.51008 N@5: 0.51231 early stop: 50\n",
            "\u001b[32m[I 210629 03:33:26 main:76]\u001b[39m Finish Training\n",
            "\u001b[32m[I 210629 03:33:28 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210629 03:33:28 main:79]\u001b[39m Loading Test Set\n",
            "\u001b[32m[I 210629 03:33:28 main:83]\u001b[39m Size of Test Set: 100\n",
            "\u001b[32m[I 210629 03:33:28 main:85]\u001b[39m Predicting\n",
            "\u001b[32m[I 210629 03:33:32 main:91]\u001b[39m Finish Predicting\n",
            "Precision@1,3,5: 0.57 0.4633333333333333 0.36\n",
            "nDCG@1,3,5: 0.57 0.49511884611079915 0.49234584548907356\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-GVxKxIVFbY",
        "outputId": "c2b263f2-dd0e-412d-cc7c-b51d3faff6d4"
      },
      "source": [
        "for skip in range(0, 1000, 100):\n",
        "    print(f\"\\nWITH MAG NO MESH, skip={skip}\\n\")\n",
        "    %cd PeTaL/\n",
        "    !python3 Split.py --skip={skip}\n",
        "    %cd ..\n",
        "    !wc PeTaL/train.json\n",
        "\n",
        "    !python3 transform_data_PeTaL.py --dataset $DATASET --no-mesh\n",
        "\n",
        "    !python preprocess.py \\\n",
        "    --text-path {DATASET}/train_texts.txt \\\n",
        "    --label-path {DATASET}/train_labels.txt \\\n",
        "    --vocab-path {DATASET}/vocab.npy \\\n",
        "    --emb-path {DATASET}/emb_init.npy \\\n",
        "    --w2v-model {DATASET}/{DATASET}.joint.emb \\\n",
        "\n",
        "    !python preprocess.py \\\n",
        "    --text-path {DATASET}/test_texts.txt \\\n",
        "    --label-path {DATASET}/test_labels.txt \\\n",
        "    --vocab-path {DATASET}/vocab.npy \\\n",
        "\n",
        "    !PYTHONFAULTHANDLER=1 python main.py --data-cnf configure/datasets/{DATASET}.yaml --model-cnf configure/models/{MODEL}-{DATASET}.yaml --mode train --reg 1\n",
        "    !PYTHONFAULTHANDLER=1 python main.py --data-cnf configure/datasets/{DATASET}.yaml --model-cnf configure/models/{MODEL}-{DATASET}.yaml --mode eval\n",
        "\n",
        "    !python evaluation.py \\\n",
        "    --results {DATASET}/results/{MODEL}-{DATASET}-labels.npy \\\n",
        "    --targets {DATASET}/test_labels.npy \\\n",
        "    --train-labels {DATASET}/train_labels.npy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WITH MAG NO MESH, skip=100\n",
            "\n",
            "/content/drive/Shareddrives/NASA/MATCH/PeTaL\n",
            "131\n",
            "/content/drive/Shareddrives/NASA/MATCH\n",
            "    800  274532 4635505 PeTaL/train.json\n",
            "\u001b[32m[I 210629 03:40:20 preprocess:28]\u001b[39m Vocab Size: 26834\n",
            "\u001b[32m[I 210629 03:40:20 preprocess:30]\u001b[39m Getting Dataset: PeTaL/train_texts.txt Max Length: 500\n",
            "\u001b[32m[I 210629 03:40:20 preprocess:32]\u001b[39m Size of Samples: 900\n",
            "\u001b[32m[I 210629 03:40:22 preprocess:28]\u001b[39m Vocab Size: 26834\n",
            "\u001b[32m[I 210629 03:40:22 preprocess:30]\u001b[39m Getting Dataset: PeTaL/test_texts.txt Max Length: 500\n",
            "\u001b[32m[I 210629 03:40:22 preprocess:32]\u001b[39m Size of Samples: 100\n",
            "\u001b[32m[I 210629 03:40:23 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210629 03:40:23 main:35]\u001b[39m Loading Training and Validation Set\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['absorb_and/or_filter_solids', 'chemically_break_down_inorganic_compounds', 'detox/purify', 'manage_environmental_disturbances_in_a_community', 'protect_from_fire', 'protect_from_gases', 'send_vibratory_signals'] will be ignored\n",
            "  .format(sorted(unknown, key=str)))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['detox/purify', 'protect_from_gases'] will be ignored\n",
            "  .format(sorted(unknown, key=str)))\n",
            "\u001b[32m[I 210629 03:40:23 main:47]\u001b[39m Number of Labels: 124\n",
            "\u001b[32m[I 210629 03:40:23 main:48]\u001b[39m Size of Training Set: 800\n",
            "\u001b[32m[I 210629 03:40:23 main:49]\u001b[39m Size of Validation Set: 100\n",
            "\u001b[32m[I 210629 03:40:23 main:66]\u001b[39m Number of Edges: 101\n",
            "\u001b[32m[I 210629 03:40:23 main:68]\u001b[39m Training\n",
            "\u001b[32m[I 210629 03:40:29 models:110]\u001b[39m 2 512 train loss: 0.2852952 valid loss: 0.1581773 P@1: 0.28000 P@3: 0.14667 P@5: 0.12600 N@3: 0.17447 N@5: 0.17787 early stop: 0\n",
            "\u001b[32m[I 210629 03:40:30 models:142]\u001b[39m SWA Initializing\n",
            "\u001b[32m[I 210629 03:40:31 models:110]\u001b[39m 4 1024 train loss: 0.1486503 valid loss: 0.1469015 P@1: 0.28000 P@3: 0.22333 P@5: 0.18800 N@3: 0.23581 N@5: 0.23876 early stop: 0\n",
            "\u001b[32m[I 210629 03:40:34 models:110]\u001b[39m 7 512 train loss: 0.1425977 valid loss: 0.1452393 P@1: 0.28000 P@3: 0.23667 P@5: 0.19000 N@3: 0.24520 N@5: 0.24298 early stop: 0\n",
            "\u001b[32m[I 210629 03:40:36 models:110]\u001b[39m 9 1024 train loss: 0.1385385 valid loss: 0.1446109 P@1: 0.28000 P@3: 0.23000 P@5: 0.19000 N@3: 0.24050 N@5: 0.24260 early stop: 1\n",
            "\u001b[32m[I 210629 03:40:38 models:110]\u001b[39m 12 512 train loss: 0.1340294 valid loss: 0.1445466 P@1: 0.28000 P@3: 0.22667 P@5: 0.19000 N@3: 0.23816 N@5: 0.24237 early stop: 2\n",
            "\u001b[32m[I 210629 03:40:40 models:110]\u001b[39m 14 1024 train loss: 0.1229535 valid loss: 0.1444615 P@1: 0.28000 P@3: 0.23000 P@5: 0.19000 N@3: 0.24173 N@5: 0.24409 early stop: 0\n",
            "\u001b[32m[I 210629 03:40:43 models:110]\u001b[39m 17 512 train loss: 0.1063492 valid loss: 0.1441184 P@1: 0.28000 P@3: 0.23667 P@5: 0.20800 N@3: 0.24520 N@5: 0.25869 early stop: 0\n",
            "\u001b[32m[I 210629 03:40:45 models:110]\u001b[39m 19 1024 train loss: 0.0830719 valid loss: 0.1440600 P@1: 0.27000 P@3: 0.25000 P@5: 0.22400 N@3: 0.25295 N@5: 0.27151 early stop: 0\n",
            "\u001b[32m[I 210629 03:40:47 models:110]\u001b[39m 22 512 train loss: 0.0657539 valid loss: 0.1437310 P@1: 0.29000 P@3: 0.24667 P@5: 0.23000 N@3: 0.25541 N@5: 0.28104 early stop: 0\n",
            "\u001b[32m[I 210629 03:40:49 models:110]\u001b[39m 24 1024 train loss: 0.0545976 valid loss: 0.1435542 P@1: 0.32000 P@3: 0.27000 P@5: 0.23800 N@3: 0.28170 N@5: 0.30109 early stop: 0\n",
            "\u001b[32m[I 210629 03:40:52 models:110]\u001b[39m 27 512 train loss: 0.0423276 valid loss: 0.1429667 P@1: 0.34000 P@3: 0.29000 P@5: 0.25600 N@3: 0.30416 N@5: 0.32623 early stop: 0\n",
            "\u001b[32m[I 210629 03:40:54 models:110]\u001b[39m 29 1024 train loss: 0.0345371 valid loss: 0.1421307 P@1: 0.40000 P@3: 0.31333 P@5: 0.27200 N@3: 0.33528 N@5: 0.35465 early stop: 0\n",
            "\u001b[32m[I 210629 03:40:57 models:110]\u001b[39m 32 512 train loss: 0.0312082 valid loss: 0.1413361 P@1: 0.38000 P@3: 0.33333 P@5: 0.28600 N@3: 0.35091 N@5: 0.37087 early stop: 0\n",
            "\u001b[32m[I 210629 03:40:59 models:110]\u001b[39m 34 1024 train loss: 0.0257733 valid loss: 0.1402931 P@1: 0.43000 P@3: 0.35667 P@5: 0.29800 N@3: 0.37723 N@5: 0.39152 early stop: 0\n",
            "\u001b[32m[I 210629 03:41:01 models:110]\u001b[39m 37 512 train loss: 0.0201908 valid loss: 0.1396408 P@1: 0.45000 P@3: 0.38667 P@5: 0.30600 N@3: 0.40242 N@5: 0.40589 early stop: 0\n",
            "\u001b[32m[I 210629 03:41:03 models:110]\u001b[39m 39 1024 train loss: 0.0176310 valid loss: 0.1389314 P@1: 0.45000 P@3: 0.38667 P@5: 0.31600 N@3: 0.40427 N@5: 0.41637 early stop: 0\n",
            "\u001b[32m[I 210629 03:41:06 models:110]\u001b[39m 42 512 train loss: 0.0153930 valid loss: 0.1385825 P@1: 0.50000 P@3: 0.39667 P@5: 0.32400 N@3: 0.42376 N@5: 0.43491 early stop: 0\n",
            "\u001b[32m[I 210629 03:41:08 models:110]\u001b[39m 44 1024 train loss: 0.0178281 valid loss: 0.1383635 P@1: 0.55000 P@3: 0.41000 P@5: 0.32400 N@3: 0.44487 N@5: 0.44615 early stop: 0\n",
            "\u001b[32m[I 210629 03:41:11 models:110]\u001b[39m 47 512 train loss: 0.0165849 valid loss: 0.1389412 P@1: 0.56000 P@3: 0.42000 P@5: 0.33400 N@3: 0.45303 N@5: 0.45554 early stop: 0\n",
            "\u001b[32m[I 210629 03:41:13 models:110]\u001b[39m 49 1024 train loss: 0.0133439 valid loss: 0.1388539 P@1: 0.57000 P@3: 0.42000 P@5: 0.34400 N@3: 0.45874 N@5: 0.46644 early stop: 0\n",
            "\u001b[32m[I 210629 03:41:15 models:110]\u001b[39m 52 512 train loss: 0.0105775 valid loss: 0.1387981 P@1: 0.61000 P@3: 0.43667 P@5: 0.35200 N@3: 0.48058 N@5: 0.48202 early stop: 0\n",
            "\u001b[32m[I 210629 03:41:17 models:110]\u001b[39m 54 1024 train loss: 0.0077829 valid loss: 0.1392304 P@1: 0.62000 P@3: 0.44333 P@5: 0.35400 N@3: 0.48762 N@5: 0.48710 early stop: 0\n",
            "\u001b[32m[I 210629 03:41:20 models:110]\u001b[39m 57 512 train loss: 0.0078362 valid loss: 0.1398438 P@1: 0.61000 P@3: 0.46000 P@5: 0.36000 N@3: 0.49762 N@5: 0.49300 early stop: 0\n",
            "\u001b[32m[I 210629 03:41:22 models:110]\u001b[39m 59 1024 train loss: 0.0096888 valid loss: 0.1401172 P@1: 0.62000 P@3: 0.47000 P@5: 0.36400 N@3: 0.50754 N@5: 0.50132 early stop: 0\n",
            "\u001b[32m[I 210629 03:41:24 models:110]\u001b[39m 62 512 train loss: 0.0106150 valid loss: 0.1406883 P@1: 0.63000 P@3: 0.48000 P@5: 0.37200 N@3: 0.51745 N@5: 0.51074 early stop: 0\n",
            "\u001b[32m[I 210629 03:41:26 models:110]\u001b[39m 64 1024 train loss: 0.0123852 valid loss: 0.1420823 P@1: 0.63000 P@3: 0.49000 P@5: 0.37200 N@3: 0.52511 N@5: 0.51311 early stop: 0\n",
            "\u001b[32m[I 210629 03:41:29 models:110]\u001b[39m 67 512 train loss: 0.0126673 valid loss: 0.1428199 P@1: 0.63000 P@3: 0.49000 P@5: 0.37200 N@3: 0.52572 N@5: 0.51390 early stop: 0\n",
            "\u001b[32m[I 210629 03:41:31 models:110]\u001b[39m 69 1024 train loss: 0.0094270 valid loss: 0.1434407 P@1: 0.63000 P@3: 0.49333 P@5: 0.37200 N@3: 0.52868 N@5: 0.51497 early stop: 0\n",
            "\u001b[32m[I 210629 03:41:34 models:110]\u001b[39m 72 512 train loss: 0.0062261 valid loss: 0.1444902 P@1: 0.64000 P@3: 0.49333 P@5: 0.37400 N@3: 0.53287 N@5: 0.52016 early stop: 0\n",
            "\u001b[32m[I 210629 03:41:36 models:110]\u001b[39m 74 1024 train loss: 0.0039027 valid loss: 0.1456046 P@1: 0.65000 P@3: 0.49333 P@5: 0.37800 N@3: 0.53337 N@5: 0.52333 early stop: 0\n",
            "\u001b[32m[I 210629 03:41:38 models:110]\u001b[39m 77 512 train loss: 0.0028212 valid loss: 0.1468264 P@1: 0.65000 P@3: 0.50333 P@5: 0.38600 N@3: 0.54236 N@5: 0.53073 early stop: 0\n",
            "\u001b[32m[I 210629 03:41:40 models:110]\u001b[39m 79 1024 train loss: 0.0028890 valid loss: 0.1482922 P@1: 0.65000 P@3: 0.51000 P@5: 0.38800 N@3: 0.54633 N@5: 0.53335 early stop: 0\n",
            "\u001b[32m[I 210629 03:41:43 models:110]\u001b[39m 82 512 train loss: 0.0028612 valid loss: 0.1498455 P@1: 0.65000 P@3: 0.51667 P@5: 0.38800 N@3: 0.55103 N@5: 0.53374 early stop: 0\n",
            "\u001b[32m[I 210629 03:41:45 models:110]\u001b[39m 84 1024 train loss: 0.0021572 valid loss: 0.1513474 P@1: 0.66000 P@3: 0.51667 P@5: 0.38600 N@3: 0.55276 N@5: 0.53380 early stop: 0\n",
            "\u001b[32m[I 210629 03:41:47 models:110]\u001b[39m 87 512 train loss: 0.0015454 valid loss: 0.1529202 P@1: 0.66000 P@3: 0.51000 P@5: 0.39000 N@3: 0.54735 N@5: 0.53579 early stop: 0\n",
            "\u001b[32m[I 210629 03:41:50 models:110]\u001b[39m 89 1024 train loss: 0.0013230 valid loss: 0.1546257 P@1: 0.66000 P@3: 0.51000 P@5: 0.39400 N@3: 0.54815 N@5: 0.53971 early stop: 0\n",
            "\u001b[32m[I 210629 03:41:52 models:110]\u001b[39m 92 512 train loss: 0.0011481 valid loss: 0.1562554 P@1: 0.66000 P@3: 0.50667 P@5: 0.39400 N@3: 0.54580 N@5: 0.53968 early stop: 1\n",
            "\u001b[32m[I 210629 03:41:54 models:110]\u001b[39m 94 1024 train loss: 0.0015391 valid loss: 0.1579707 P@1: 0.66000 P@3: 0.51000 P@5: 0.39400 N@3: 0.54876 N@5: 0.54077 early stop: 0\n",
            "\u001b[32m[I 210629 03:41:57 models:110]\u001b[39m 97 512 train loss: 0.0026626 valid loss: 0.1596252 P@1: 0.67000 P@3: 0.50667 P@5: 0.40200 N@3: 0.54876 N@5: 0.54901 early stop: 0\n",
            "\u001b[32m[I 210629 03:41:59 models:110]\u001b[39m 99 1024 train loss: 0.0050910 valid loss: 0.1616631 P@1: 0.67000 P@3: 0.50667 P@5: 0.40400 N@3: 0.54876 N@5: 0.55169 early stop: 0\n",
            "\u001b[32m[I 210629 03:42:01 models:110]\u001b[39m 102 512 train loss: 0.0100416 valid loss: 0.1633029 P@1: 0.67000 P@3: 0.50333 P@5: 0.40000 N@3: 0.54642 N@5: 0.54848 early stop: 1\n",
            "\u001b[32m[I 210629 03:42:03 models:110]\u001b[39m 104 1024 train loss: 0.0160227 valid loss: 0.1644151 P@1: 0.66000 P@3: 0.50333 P@5: 0.40400 N@3: 0.54549 N@5: 0.55089 early stop: 2\n",
            "\u001b[32m[I 210629 03:42:06 models:110]\u001b[39m 107 512 train loss: 0.0122016 valid loss: 0.1653061 P@1: 0.66000 P@3: 0.50333 P@5: 0.40400 N@3: 0.54610 N@5: 0.55168 early stop: 3\n",
            "\u001b[32m[I 210629 03:42:08 models:110]\u001b[39m 109 1024 train loss: 0.0100619 valid loss: 0.1660565 P@1: 0.66000 P@3: 0.50333 P@5: 0.40200 N@3: 0.54549 N@5: 0.54960 early stop: 4\n",
            "\u001b[32m[I 210629 03:42:10 models:110]\u001b[39m 112 512 train loss: 0.0066562 valid loss: 0.1672743 P@1: 0.66000 P@3: 0.50333 P@5: 0.40200 N@3: 0.54549 N@5: 0.55030 early stop: 5\n",
            "\u001b[32m[I 210629 03:42:12 models:110]\u001b[39m 114 1024 train loss: 0.0044368 valid loss: 0.1685581 P@1: 0.66000 P@3: 0.51333 P@5: 0.40200 N@3: 0.55437 N@5: 0.55338 early stop: 0\n",
            "\u001b[32m[I 210629 03:42:15 models:110]\u001b[39m 117 512 train loss: 0.0029526 valid loss: 0.1697511 P@1: 0.68000 P@3: 0.51667 P@5: 0.40200 N@3: 0.56018 N@5: 0.55606 early stop: 0\n",
            "\u001b[32m[I 210629 03:42:17 models:110]\u001b[39m 119 1024 train loss: 0.0018417 valid loss: 0.1712828 P@1: 0.68000 P@3: 0.52333 P@5: 0.40400 N@3: 0.56487 N@5: 0.55832 early stop: 0\n",
            "\u001b[32m[I 210629 03:42:19 models:110]\u001b[39m 122 512 train loss: 0.0012566 valid loss: 0.1727297 P@1: 0.68000 P@3: 0.52333 P@5: 0.40600 N@3: 0.56487 N@5: 0.55964 early stop: 0\n",
            "\u001b[32m[I 210629 03:42:22 models:110]\u001b[39m 124 1024 train loss: 0.0009459 valid loss: 0.1743218 P@1: 0.68000 P@3: 0.52000 P@5: 0.40600 N@3: 0.56253 N@5: 0.55963 early stop: 1\n",
            "\u001b[32m[I 210629 03:42:24 models:110]\u001b[39m 127 512 train loss: 0.0006414 valid loss: 0.1757244 P@1: 0.68000 P@3: 0.52000 P@5: 0.40600 N@3: 0.56253 N@5: 0.55993 early stop: 0\n",
            "\u001b[32m[I 210629 03:42:26 models:110]\u001b[39m 129 1024 train loss: 0.0004609 valid loss: 0.1772789 P@1: 0.68000 P@3: 0.52000 P@5: 0.41000 N@3: 0.56253 N@5: 0.56255 early stop: 0\n",
            "\u001b[32m[I 210629 03:42:29 models:110]\u001b[39m 132 512 train loss: 0.0003291 valid loss: 0.1788488 P@1: 0.68000 P@3: 0.52000 P@5: 0.41000 N@3: 0.56253 N@5: 0.56238 early stop: 1\n",
            "\u001b[33m[W 210629 03:42:29 models:137]\u001b[39m Clipping gradients with total norm 0.1332 and max norm 0.01582\n",
            "\u001b[32m[I 210629 03:42:31 models:110]\u001b[39m 134 1024 train loss: 0.0007493 valid loss: 0.1803733 P@1: 0.68000 P@3: 0.52333 P@5: 0.41000 N@3: 0.56487 N@5: 0.56286 early stop: 0\n",
            "\u001b[32m[I 210629 03:42:33 models:110]\u001b[39m 137 512 train loss: 0.0003417 valid loss: 0.1819313 P@1: 0.68000 P@3: 0.52333 P@5: 0.41000 N@3: 0.56487 N@5: 0.56336 early stop: 0\n",
            "\u001b[32m[I 210629 03:42:35 models:110]\u001b[39m 139 1024 train loss: 0.0002307 valid loss: 0.1834944 P@1: 0.68000 P@3: 0.52333 P@5: 0.41000 N@3: 0.56487 N@5: 0.56316 early stop: 1\n",
            "\u001b[32m[I 210629 03:42:38 models:110]\u001b[39m 142 512 train loss: 0.0002164 valid loss: 0.1850152 P@1: 0.68000 P@3: 0.52333 P@5: 0.41000 N@3: 0.56487 N@5: 0.56301 early stop: 2\n",
            "\u001b[32m[I 210629 03:42:40 models:110]\u001b[39m 144 1024 train loss: 0.0001910 valid loss: 0.1865662 P@1: 0.68000 P@3: 0.52333 P@5: 0.41200 N@3: 0.56487 N@5: 0.56452 early stop: 0\n",
            "\u001b[33m[W 210629 03:42:41 models:137]\u001b[39m Clipping gradients with total norm 0.09896 and max norm 0.01289\n",
            "\u001b[32m[I 210629 03:42:42 models:110]\u001b[39m 147 512 train loss: 0.0005017 valid loss: 0.1880701 P@1: 0.68000 P@3: 0.52333 P@5: 0.41200 N@3: 0.56487 N@5: 0.56432 early stop: 1\n",
            "\u001b[32m[I 210629 03:42:44 models:110]\u001b[39m 149 1024 train loss: 0.0003314 valid loss: 0.1896369 P@1: 0.68000 P@3: 0.52333 P@5: 0.41200 N@3: 0.56487 N@5: 0.56432 early stop: 2\n",
            "\u001b[32m[I 210629 03:42:47 models:110]\u001b[39m 152 512 train loss: 0.0002631 valid loss: 0.1910802 P@1: 0.68000 P@3: 0.52000 P@5: 0.41400 N@3: 0.56253 N@5: 0.56530 early stop: 0\n",
            "\u001b[32m[I 210629 03:42:49 models:110]\u001b[39m 154 1024 train loss: 0.0004436 valid loss: 0.1925126 P@1: 0.68000 P@3: 0.52000 P@5: 0.41200 N@3: 0.56253 N@5: 0.56399 early stop: 1\n",
            "\u001b[32m[I 210629 03:42:52 models:110]\u001b[39m 157 512 train loss: 0.0003448 valid loss: 0.1939954 P@1: 0.68000 P@3: 0.52333 P@5: 0.41200 N@3: 0.56487 N@5: 0.56449 early stop: 2\n",
            "\u001b[32m[I 210629 03:42:54 models:110]\u001b[39m 159 1024 train loss: 0.0001797 valid loss: 0.1954698 P@1: 0.68000 P@3: 0.52333 P@5: 0.41000 N@3: 0.56487 N@5: 0.56333 early stop: 3\n",
            "\u001b[32m[I 210629 03:42:56 models:110]\u001b[39m 162 512 train loss: 0.0002578 valid loss: 0.1969489 P@1: 0.68000 P@3: 0.52333 P@5: 0.41000 N@3: 0.56487 N@5: 0.56282 early stop: 4\n",
            "\u001b[32m[I 210629 03:42:58 models:110]\u001b[39m 164 1024 train loss: 0.0001584 valid loss: 0.1983465 P@1: 0.68000 P@3: 0.52333 P@5: 0.40800 N@3: 0.56549 N@5: 0.56216 early stop: 5\n",
            "\u001b[32m[I 210629 03:43:01 models:110]\u001b[39m 167 512 train loss: 0.0001809 valid loss: 0.1998047 P@1: 0.68000 P@3: 0.52333 P@5: 0.40800 N@3: 0.56549 N@5: 0.56266 early stop: 6\n",
            "\u001b[33m[W 210629 03:43:01 models:137]\u001b[39m Clipping gradients with total norm 0.12134 and max norm 0.01054\n",
            "\u001b[32m[I 210629 03:43:03 models:110]\u001b[39m 169 1024 train loss: 0.0004805 valid loss: 0.2012149 P@1: 0.68000 P@3: 0.52333 P@5: 0.40800 N@3: 0.56549 N@5: 0.56252 early stop: 7\n",
            "\u001b[32m[I 210629 03:43:05 models:110]\u001b[39m 172 512 train loss: 0.0002320 valid loss: 0.2026161 P@1: 0.68000 P@3: 0.52333 P@5: 0.40800 N@3: 0.56549 N@5: 0.56257 early stop: 8\n",
            "\u001b[32m[I 210629 03:43:07 models:110]\u001b[39m 174 1024 train loss: 0.0001582 valid loss: 0.2039783 P@1: 0.68000 P@3: 0.52333 P@5: 0.40800 N@3: 0.56610 N@5: 0.56319 early stop: 9\n",
            "\u001b[32m[I 210629 03:43:10 models:110]\u001b[39m 177 512 train loss: 0.0001833 valid loss: 0.2053534 P@1: 0.68000 P@3: 0.52333 P@5: 0.40600 N@3: 0.56610 N@5: 0.56187 early stop: 10\n",
            "\u001b[32m[I 210629 03:43:12 models:110]\u001b[39m 179 1024 train loss: 0.0001575 valid loss: 0.2066999 P@1: 0.67000 P@3: 0.52333 P@5: 0.40600 N@3: 0.56437 N@5: 0.56062 early stop: 11\n",
            "\u001b[33m[W 210629 03:43:12 models:137]\u001b[39m Clipping gradients with total norm 0.09958 and max norm 0.01604\n",
            "\u001b[32m[I 210629 03:43:14 models:110]\u001b[39m 182 512 train loss: 0.0006205 valid loss: 0.2080416 P@1: 0.68000 P@3: 0.52333 P@5: 0.40600 N@3: 0.56610 N@5: 0.56187 early stop: 12\n",
            "\u001b[32m[I 210629 03:43:16 models:110]\u001b[39m 184 1024 train loss: 0.0002567 valid loss: 0.2092967 P@1: 0.68000 P@3: 0.52333 P@5: 0.40600 N@3: 0.56610 N@5: 0.56155 early stop: 13\n",
            "\u001b[32m[I 210629 03:43:19 models:110]\u001b[39m 187 512 train loss: 0.0002268 valid loss: 0.2105936 P@1: 0.67000 P@3: 0.52333 P@5: 0.40600 N@3: 0.56437 N@5: 0.56039 early stop: 14\n",
            "\u001b[32m[I 210629 03:43:21 models:110]\u001b[39m 189 1024 train loss: 0.0004369 valid loss: 0.2119168 P@1: 0.68000 P@3: 0.52333 P@5: 0.40800 N@3: 0.56610 N@5: 0.56316 early stop: 15\n",
            "\u001b[32m[I 210629 03:43:23 models:110]\u001b[39m 192 512 train loss: 0.0003122 valid loss: 0.2131705 P@1: 0.68000 P@3: 0.52333 P@5: 0.40600 N@3: 0.56672 N@5: 0.56246 early stop: 16\n",
            "\u001b[32m[I 210629 03:43:25 models:110]\u001b[39m 194 1024 train loss: 0.0004379 valid loss: 0.2144204 P@1: 0.68000 P@3: 0.52333 P@5: 0.40800 N@3: 0.56672 N@5: 0.56427 early stop: 17\n",
            "\u001b[32m[I 210629 03:43:28 models:110]\u001b[39m 197 512 train loss: 0.0003436 valid loss: 0.2156338 P@1: 0.67000 P@3: 0.52667 P@5: 0.40800 N@3: 0.56733 N@5: 0.56346 early stop: 18\n",
            "\u001b[33m[W 210629 03:43:28 models:137]\u001b[39m Clipping gradients with total norm 0.10015 and max norm 0.01815\n",
            "\u001b[32m[I 210629 03:43:30 models:110]\u001b[39m 199 1024 train loss: 0.0009763 valid loss: 0.2168888 P@1: 0.66000 P@3: 0.52667 P@5: 0.41000 N@3: 0.56560 N@5: 0.56403 early stop: 19\n",
            "\u001b[32m[I 210629 03:43:32 models:110]\u001b[39m 202 512 train loss: 0.0005099 valid loss: 0.2181101 P@1: 0.66000 P@3: 0.52333 P@5: 0.41000 N@3: 0.56325 N@5: 0.56379 early stop: 20\n",
            "\u001b[32m[I 210629 03:43:34 models:110]\u001b[39m 204 1024 train loss: 0.0001553 valid loss: 0.2192829 P@1: 0.66000 P@3: 0.52333 P@5: 0.41000 N@3: 0.56325 N@5: 0.56379 early stop: 21\n",
            "\u001b[32m[I 210629 03:43:37 models:110]\u001b[39m 207 512 train loss: 0.0002770 valid loss: 0.2205343 P@1: 0.66000 P@3: 0.52333 P@5: 0.41000 N@3: 0.56325 N@5: 0.56379 early stop: 22\n",
            "\u001b[32m[I 210629 03:43:39 models:110]\u001b[39m 209 1024 train loss: 0.0002034 valid loss: 0.2216911 P@1: 0.66000 P@3: 0.52333 P@5: 0.41000 N@3: 0.56325 N@5: 0.56379 early stop: 23\n",
            "\u001b[32m[I 210629 03:43:41 models:110]\u001b[39m 212 512 train loss: 0.0001833 valid loss: 0.2228702 P@1: 0.66000 P@3: 0.52333 P@5: 0.41000 N@3: 0.56325 N@5: 0.56385 early stop: 24\n",
            "\u001b[32m[I 210629 03:43:43 models:110]\u001b[39m 214 1024 train loss: 0.0001207 valid loss: 0.2240422 P@1: 0.66000 P@3: 0.52333 P@5: 0.41000 N@3: 0.56264 N@5: 0.56340 early stop: 25\n",
            "\u001b[32m[I 210629 03:43:46 models:110]\u001b[39m 217 512 train loss: 0.0001424 valid loss: 0.2251972 P@1: 0.66000 P@3: 0.52333 P@5: 0.41000 N@3: 0.56264 N@5: 0.56340 early stop: 26\n",
            "\u001b[32m[I 210629 03:43:48 models:110]\u001b[39m 219 1024 train loss: 0.0001145 valid loss: 0.2263376 P@1: 0.66000 P@3: 0.52333 P@5: 0.41000 N@3: 0.56264 N@5: 0.56340 early stop: 27\n",
            "\u001b[32m[I 210629 03:43:50 models:110]\u001b[39m 222 512 train loss: 0.0001457 valid loss: 0.2274606 P@1: 0.66000 P@3: 0.52333 P@5: 0.41000 N@3: 0.56264 N@5: 0.56355 early stop: 28\n",
            "\u001b[33m[W 210629 03:43:50 models:137]\u001b[39m Clipping gradients with total norm 0.10933 and max norm 0.01063\n",
            "\u001b[33m[W 210629 03:43:52 models:137]\u001b[39m Clipping gradients with total norm 0.10984 and max norm 0.0059\n",
            "\u001b[32m[I 210629 03:43:52 models:110]\u001b[39m 224 1024 train loss: 0.0010785 valid loss: 0.2285801 P@1: 0.66000 P@3: 0.52333 P@5: 0.41000 N@3: 0.56264 N@5: 0.56355 early stop: 29\n",
            "\u001b[32m[I 210629 03:43:55 models:110]\u001b[39m 227 512 train loss: 0.0001678 valid loss: 0.2297039 P@1: 0.66000 P@3: 0.52333 P@5: 0.40600 N@3: 0.56264 N@5: 0.56023 early stop: 30\n",
            "\u001b[32m[I 210629 03:43:57 models:110]\u001b[39m 229 1024 train loss: 0.0001916 valid loss: 0.2307954 P@1: 0.66000 P@3: 0.52000 P@5: 0.40400 N@3: 0.56029 N@5: 0.55868 early stop: 31\n",
            "\u001b[32m[I 210629 03:43:59 models:110]\u001b[39m 232 512 train loss: 0.0001477 valid loss: 0.2318451 P@1: 0.66000 P@3: 0.52000 P@5: 0.40600 N@3: 0.56029 N@5: 0.55999 early stop: 32\n",
            "\u001b[32m[I 210629 03:44:01 models:110]\u001b[39m 234 1024 train loss: 0.0001274 valid loss: 0.2328913 P@1: 0.66000 P@3: 0.52000 P@5: 0.40800 N@3: 0.56029 N@5: 0.56150 early stop: 33\n",
            "\u001b[32m[I 210629 03:44:04 models:110]\u001b[39m 237 512 train loss: 0.0001131 valid loss: 0.2339483 P@1: 0.66000 P@3: 0.52000 P@5: 0.41000 N@3: 0.56029 N@5: 0.56301 early stop: 34\n",
            "\u001b[32m[I 210629 03:44:06 models:110]\u001b[39m 239 1024 train loss: 0.0001176 valid loss: 0.2349980 P@1: 0.66000 P@3: 0.52000 P@5: 0.41000 N@3: 0.56029 N@5: 0.56301 early stop: 35\n",
            "\u001b[33m[W 210629 03:44:07 models:137]\u001b[39m Clipping gradients with total norm 0.096 and max norm 0.01638\n",
            "\u001b[32m[I 210629 03:44:08 models:110]\u001b[39m 242 512 train loss: 0.0003121 valid loss: 0.2360056 P@1: 0.66000 P@3: 0.52000 P@5: 0.41000 N@3: 0.55968 N@5: 0.56257 early stop: 36\n",
            "\u001b[32m[I 210629 03:44:10 models:110]\u001b[39m 244 1024 train loss: 0.0003875 valid loss: 0.2370036 P@1: 0.66000 P@3: 0.52000 P@5: 0.41000 N@3: 0.55968 N@5: 0.56242 early stop: 37\n",
            "\u001b[32m[I 210629 03:44:13 models:110]\u001b[39m 247 512 train loss: 0.0001773 valid loss: 0.2380678 P@1: 0.66000 P@3: 0.52000 P@5: 0.41000 N@3: 0.55968 N@5: 0.56242 early stop: 38\n",
            "\u001b[32m[I 210629 03:44:15 models:110]\u001b[39m 249 1024 train loss: 0.0001699 valid loss: 0.2391188 P@1: 0.66000 P@3: 0.52000 P@5: 0.41000 N@3: 0.55968 N@5: 0.56242 early stop: 39\n",
            "\u001b[32m[I 210629 03:44:17 models:110]\u001b[39m 252 512 train loss: 0.0001373 valid loss: 0.2401025 P@1: 0.66000 P@3: 0.52000 P@5: 0.41000 N@3: 0.55826 N@5: 0.56117 early stop: 40\n",
            "\u001b[33m[W 210629 03:44:17 models:137]\u001b[39m Clipping gradients with total norm 0.08072 and max norm 0.007\n",
            "\u001b[32m[I 210629 03:44:19 models:110]\u001b[39m 254 1024 train loss: 0.0004680 valid loss: 0.2410837 P@1: 0.66000 P@3: 0.52000 P@5: 0.41000 N@3: 0.55826 N@5: 0.56132 early stop: 41\n",
            "\u001b[32m[I 210629 03:44:22 models:110]\u001b[39m 257 512 train loss: 0.0001779 valid loss: 0.2420235 P@1: 0.66000 P@3: 0.52000 P@5: 0.41000 N@3: 0.55826 N@5: 0.56105 early stop: 42\n",
            "\u001b[33m[W 210629 03:44:23 models:137]\u001b[39m Clipping gradients with total norm 0.13564 and max norm 0.01328\n",
            "\u001b[32m[I 210629 03:44:24 models:110]\u001b[39m 259 1024 train loss: 0.0004347 valid loss: 0.2430048 P@1: 0.66000 P@3: 0.52000 P@5: 0.41000 N@3: 0.55826 N@5: 0.56120 early stop: 43\n",
            "\u001b[32m[I 210629 03:44:26 models:110]\u001b[39m 262 512 train loss: 0.0002417 valid loss: 0.2439890 P@1: 0.66000 P@3: 0.52000 P@5: 0.41000 N@3: 0.55826 N@5: 0.56147 early stop: 44\n",
            "\u001b[32m[I 210629 03:44:28 models:110]\u001b[39m 264 1024 train loss: 0.0001235 valid loss: 0.2449534 P@1: 0.66000 P@3: 0.52000 P@5: 0.41000 N@3: 0.55826 N@5: 0.56132 early stop: 45\n",
            "\u001b[32m[I 210629 03:44:31 models:110]\u001b[39m 267 512 train loss: 0.0001577 valid loss: 0.2458742 P@1: 0.66000 P@3: 0.52000 P@5: 0.41000 N@3: 0.55826 N@5: 0.56132 early stop: 46\n",
            "\u001b[32m[I 210629 03:44:33 models:110]\u001b[39m 269 1024 train loss: 0.0001147 valid loss: 0.2467498 P@1: 0.66000 P@3: 0.52333 P@5: 0.41200 N@3: 0.56061 N@5: 0.56346 early stop: 47\n",
            "\u001b[32m[I 210629 03:44:35 models:110]\u001b[39m 272 512 train loss: 0.0001466 valid loss: 0.2476450 P@1: 0.66000 P@3: 0.52333 P@5: 0.41200 N@3: 0.56061 N@5: 0.56319 early stop: 48\n",
            "\u001b[32m[I 210629 03:44:37 models:110]\u001b[39m 274 1024 train loss: 0.0000921 valid loss: 0.2485477 P@1: 0.66000 P@3: 0.52333 P@5: 0.41200 N@3: 0.56061 N@5: 0.56319 early stop: 49\n",
            "\u001b[32m[I 210629 03:44:40 models:110]\u001b[39m 277 512 train loss: 0.0001348 valid loss: 0.2494423 P@1: 0.66000 P@3: 0.52333 P@5: 0.41200 N@3: 0.56061 N@5: 0.56319 early stop: 50\n",
            "\u001b[32m[I 210629 03:44:42 main:76]\u001b[39m Finish Training\n",
            "\u001b[32m[I 210629 03:44:44 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210629 03:44:44 main:79]\u001b[39m Loading Test Set\n",
            "\u001b[32m[I 210629 03:44:44 main:83]\u001b[39m Size of Test Set: 100\n",
            "\u001b[32m[I 210629 03:44:44 main:85]\u001b[39m Predicting\n",
            "\u001b[32m[I 210629 03:44:47 main:91]\u001b[39m Finish Predicting\n",
            "Precision@1,3,5: 0.61 0.5133333333333333 0.402\n",
            "nDCG@1,3,5: 0.61 0.5403147505625059 0.5373872122630345\n",
            "\n",
            "WITH MAG NO MESH, skip=300\n",
            "\n",
            "/content/drive/Shareddrives/NASA/MATCH/PeTaL\n",
            "130\n",
            "/content/drive/Shareddrives/NASA/MATCH\n",
            "    800  288986 4967896 PeTaL/train.json\n",
            "\u001b[32m[I 210629 03:44:50 preprocess:28]\u001b[39m Vocab Size: 26834\n",
            "\u001b[32m[I 210629 03:44:50 preprocess:30]\u001b[39m Getting Dataset: PeTaL/train_texts.txt Max Length: 500\n",
            "\u001b[32m[I 210629 03:44:50 preprocess:32]\u001b[39m Size of Samples: 900\n",
            "\u001b[32m[I 210629 03:44:51 preprocess:28]\u001b[39m Vocab Size: 26834\n",
            "\u001b[32m[I 210629 03:44:51 preprocess:30]\u001b[39m Getting Dataset: PeTaL/test_texts.txt Max Length: 500\n",
            "\u001b[32m[I 210629 03:44:51 preprocess:32]\u001b[39m Size of Samples: 100\n",
            "\u001b[32m[I 210629 03:44:53 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210629 03:44:53 main:35]\u001b[39m Loading Training and Validation Set\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['absorb_and/or_filter_solids', 'chemically_break_down_inorganic_compounds', 'detox/purify', 'protect_from_fire', 'protect_from_gases', 'send_vibratory_signals'] will be ignored\n",
            "  .format(sorted(unknown, key=str)))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['detox/purify', 'protect_from_gases'] will be ignored\n",
            "  .format(sorted(unknown, key=str)))\n",
            "\u001b[32m[I 210629 03:44:53 main:47]\u001b[39m Number of Labels: 124\n",
            "\u001b[32m[I 210629 03:44:53 main:48]\u001b[39m Size of Training Set: 800\n",
            "\u001b[32m[I 210629 03:44:53 main:49]\u001b[39m Size of Validation Set: 100\n",
            "\u001b[32m[I 210629 03:44:53 main:66]\u001b[39m Number of Edges: 101\n",
            "\u001b[32m[I 210629 03:44:53 main:68]\u001b[39m Training\n",
            "\u001b[32m[I 210629 03:44:59 models:110]\u001b[39m 2 512 train loss: 0.2719438 valid loss: 0.1589094 P@1: 0.37000 P@3: 0.16000 P@5: 0.16000 N@3: 0.20252 N@5: 0.21776 early stop: 0\n",
            "\u001b[32m[I 210629 03:45:00 models:142]\u001b[39m SWA Initializing\n",
            "\u001b[32m[I 210629 03:45:01 models:110]\u001b[39m 4 1024 train loss: 0.1490195 valid loss: 0.1474459 P@1: 0.37000 P@3: 0.26333 P@5: 0.22200 N@3: 0.28693 N@5: 0.28295 early stop: 0\n",
            "\u001b[32m[I 210629 03:45:03 models:110]\u001b[39m 7 512 train loss: 0.1443645 valid loss: 0.1461683 P@1: 0.37000 P@3: 0.27000 P@5: 0.22800 N@3: 0.29427 N@5: 0.29307 early stop: 0\n",
            "\u001b[32m[I 210629 03:45:05 models:110]\u001b[39m 9 1024 train loss: 0.1414224 valid loss: 0.1458282 P@1: 0.37000 P@3: 0.25667 P@5: 0.22800 N@3: 0.28366 N@5: 0.29174 early stop: 1\n",
            "\u001b[32m[I 210629 03:45:08 models:110]\u001b[39m 12 512 train loss: 0.1348949 valid loss: 0.1455507 P@1: 0.37000 P@3: 0.26667 P@5: 0.22800 N@3: 0.29131 N@5: 0.29317 early stop: 0\n",
            "\u001b[32m[I 210629 03:45:10 models:110]\u001b[39m 14 1024 train loss: 0.1206212 valid loss: 0.1450603 P@1: 0.37000 P@3: 0.29333 P@5: 0.23000 N@3: 0.31561 N@5: 0.30091 early stop: 0\n",
            "\u001b[32m[I 210629 03:45:12 models:110]\u001b[39m 17 512 train loss: 0.1001854 valid loss: 0.1450205 P@1: 0.36000 P@3: 0.29667 P@5: 0.24000 N@3: 0.31262 N@5: 0.30673 early stop: 0\n",
            "\u001b[32m[I 210629 03:45:14 models:110]\u001b[39m 19 1024 train loss: 0.0812741 valid loss: 0.1456495 P@1: 0.40000 P@3: 0.29333 P@5: 0.24600 N@3: 0.31731 N@5: 0.31731 early stop: 0\n",
            "\u001b[32m[I 210629 03:45:17 models:110]\u001b[39m 22 512 train loss: 0.0720844 valid loss: 0.1462285 P@1: 0.41000 P@3: 0.28000 P@5: 0.23200 N@3: 0.31179 N@5: 0.30562 early stop: 1\n",
            "\u001b[32m[I 210629 03:45:19 models:110]\u001b[39m 24 1024 train loss: 0.0582390 valid loss: 0.1463740 P@1: 0.42000 P@3: 0.28667 P@5: 0.23400 N@3: 0.32121 N@5: 0.31361 early stop: 2\n",
            "\u001b[32m[I 210629 03:45:21 models:110]\u001b[39m 27 512 train loss: 0.0485638 valid loss: 0.1462893 P@1: 0.44000 P@3: 0.28667 P@5: 0.22600 N@3: 0.32774 N@5: 0.31478 early stop: 3\n",
            "\u001b[32m[I 210629 03:45:23 models:110]\u001b[39m 29 1024 train loss: 0.0414089 valid loss: 0.1461308 P@1: 0.42000 P@3: 0.28667 P@5: 0.23800 N@3: 0.32457 N@5: 0.31906 early stop: 0\n",
            "\u001b[32m[I 210629 03:45:26 models:110]\u001b[39m 32 512 train loss: 0.0347965 valid loss: 0.1458173 P@1: 0.45000 P@3: 0.31333 P@5: 0.24000 N@3: 0.35303 N@5: 0.33223 early stop: 0\n",
            "\u001b[32m[I 210629 03:45:28 models:110]\u001b[39m 34 1024 train loss: 0.0308214 valid loss: 0.1458228 P@1: 0.46000 P@3: 0.33333 P@5: 0.25200 N@3: 0.37068 N@5: 0.34638 early stop: 0\n",
            "\u001b[32m[I 210629 03:45:30 models:110]\u001b[39m 37 512 train loss: 0.0271207 valid loss: 0.1462584 P@1: 0.47000 P@3: 0.33333 P@5: 0.26000 N@3: 0.37303 N@5: 0.35594 early stop: 0\n",
            "\u001b[32m[I 210629 03:45:33 models:110]\u001b[39m 39 1024 train loss: 0.0249681 valid loss: 0.1463316 P@1: 0.46000 P@3: 0.34333 P@5: 0.27400 N@3: 0.37711 N@5: 0.36456 early stop: 0\n",
            "\u001b[32m[I 210629 03:45:35 models:110]\u001b[39m 42 512 train loss: 0.0239254 valid loss: 0.1467563 P@1: 0.49000 P@3: 0.35000 P@5: 0.27600 N@3: 0.38699 N@5: 0.37323 early stop: 0\n",
            "\u001b[32m[I 210629 03:45:37 models:110]\u001b[39m 44 1024 train loss: 0.0210808 valid loss: 0.1474816 P@1: 0.51000 P@3: 0.36000 P@5: 0.28800 N@3: 0.39926 N@5: 0.39042 early stop: 0\n",
            "\u001b[32m[I 210629 03:45:40 models:110]\u001b[39m 47 512 train loss: 0.0188151 valid loss: 0.1479613 P@1: 0.51000 P@3: 0.36667 P@5: 0.29400 N@3: 0.40641 N@5: 0.39804 early stop: 0\n",
            "\u001b[32m[I 210629 03:45:42 models:110]\u001b[39m 49 1024 train loss: 0.0195932 valid loss: 0.1486356 P@1: 0.54000 P@3: 0.39333 P@5: 0.29600 N@3: 0.43490 N@5: 0.41048 early stop: 0\n",
            "\u001b[32m[I 210629 03:45:44 models:110]\u001b[39m 52 512 train loss: 0.0177699 valid loss: 0.1492175 P@1: 0.55000 P@3: 0.40000 P@5: 0.29400 N@3: 0.44255 N@5: 0.41172 early stop: 0\n",
            "\u001b[32m[I 210629 03:45:47 models:110]\u001b[39m 54 1024 train loss: 0.0148353 valid loss: 0.1499969 P@1: 0.55000 P@3: 0.40000 P@5: 0.29400 N@3: 0.44378 N@5: 0.41206 early stop: 0\n",
            "\u001b[32m[I 210629 03:45:49 models:110]\u001b[39m 57 512 train loss: 0.0130459 valid loss: 0.1508207 P@1: 0.54000 P@3: 0.40000 P@5: 0.30600 N@3: 0.44266 N@5: 0.42114 early stop: 0\n",
            "\u001b[32m[I 210629 03:45:51 models:110]\u001b[39m 59 1024 train loss: 0.0114890 valid loss: 0.1522422 P@1: 0.54000 P@3: 0.40667 P@5: 0.31000 N@3: 0.44981 N@5: 0.42596 early stop: 0\n",
            "\u001b[32m[I 210629 03:45:54 models:110]\u001b[39m 62 512 train loss: 0.0100504 valid loss: 0.1533807 P@1: 0.53000 P@3: 0.41333 P@5: 0.31400 N@3: 0.45480 N@5: 0.43054 early stop: 0\n",
            "\u001b[32m[I 210629 03:45:56 models:110]\u001b[39m 64 1024 train loss: 0.0079011 valid loss: 0.1548042 P@1: 0.56000 P@3: 0.41667 P@5: 0.31600 N@3: 0.46358 N@5: 0.43794 early stop: 0\n",
            "\u001b[32m[I 210629 03:45:58 models:110]\u001b[39m 67 512 train loss: 0.0081380 valid loss: 0.1563233 P@1: 0.55000 P@3: 0.42000 P@5: 0.32000 N@3: 0.46358 N@5: 0.44105 early stop: 0\n",
            "\u001b[32m[I 210629 03:46:00 models:110]\u001b[39m 69 1024 train loss: 0.0086085 valid loss: 0.1582688 P@1: 0.57000 P@3: 0.42667 P@5: 0.32000 N@3: 0.47296 N@5: 0.44519 early stop: 0\n",
            "\u001b[32m[I 210629 03:46:03 models:110]\u001b[39m 72 512 train loss: 0.0084074 valid loss: 0.1597695 P@1: 0.57000 P@3: 0.42667 P@5: 0.32600 N@3: 0.47296 N@5: 0.45078 early stop: 0\n",
            "\u001b[32m[I 210629 03:46:05 models:110]\u001b[39m 74 1024 train loss: 0.0068099 valid loss: 0.1618107 P@1: 0.57000 P@3: 0.43333 P@5: 0.32200 N@3: 0.47765 N@5: 0.44860 early stop: 1\n",
            "\u001b[32m[I 210629 03:46:08 models:110]\u001b[39m 77 512 train loss: 0.0063027 valid loss: 0.1633497 P@1: 0.57000 P@3: 0.43333 P@5: 0.33000 N@3: 0.47765 N@5: 0.45457 early stop: 0\n",
            "\u001b[32m[I 210629 03:46:10 models:110]\u001b[39m 79 1024 train loss: 0.0055694 valid loss: 0.1650068 P@1: 0.57000 P@3: 0.43667 P@5: 0.34400 N@3: 0.48000 N@5: 0.46398 early stop: 0\n",
            "\u001b[32m[I 210629 03:46:12 models:110]\u001b[39m 82 512 train loss: 0.0079878 valid loss: 0.1665571 P@1: 0.58000 P@3: 0.43000 P@5: 0.34200 N@3: 0.47704 N@5: 0.46394 early stop: 1\n",
            "\u001b[32m[I 210629 03:46:14 models:110]\u001b[39m 84 1024 train loss: 0.0068069 valid loss: 0.1679241 P@1: 0.58000 P@3: 0.44000 P@5: 0.35000 N@3: 0.48408 N@5: 0.47113 early stop: 0\n",
            "\u001b[32m[I 210629 03:46:17 models:110]\u001b[39m 87 512 train loss: 0.0058880 valid loss: 0.1692631 P@1: 0.58000 P@3: 0.44333 P@5: 0.35200 N@3: 0.48642 N@5: 0.47241 early stop: 0\n",
            "\u001b[32m[I 210629 03:46:19 models:110]\u001b[39m 89 1024 train loss: 0.0055876 valid loss: 0.1709031 P@1: 0.58000 P@3: 0.44333 P@5: 0.34800 N@3: 0.48765 N@5: 0.47078 early stop: 1\n",
            "\u001b[32m[I 210629 03:46:21 models:110]\u001b[39m 92 512 train loss: 0.0038831 valid loss: 0.1725641 P@1: 0.58000 P@3: 0.44667 P@5: 0.35200 N@3: 0.49123 N@5: 0.47432 early stop: 0\n",
            "\u001b[32m[I 210629 03:46:24 models:110]\u001b[39m 94 1024 train loss: 0.0033200 valid loss: 0.1742649 P@1: 0.58000 P@3: 0.45000 P@5: 0.35400 N@3: 0.49358 N@5: 0.47587 early stop: 0\n",
            "\u001b[32m[I 210629 03:46:26 models:110]\u001b[39m 97 512 train loss: 0.0025535 valid loss: 0.1760344 P@1: 0.59000 P@3: 0.45000 P@5: 0.35600 N@3: 0.49654 N@5: 0.47966 early stop: 0\n",
            "\u001b[32m[I 210629 03:46:28 models:110]\u001b[39m 99 1024 train loss: 0.0016786 valid loss: 0.1778220 P@1: 0.60000 P@3: 0.45333 P@5: 0.35600 N@3: 0.50184 N@5: 0.48269 early stop: 0\n",
            "\u001b[32m[I 210629 03:46:31 models:110]\u001b[39m 102 512 train loss: 0.0012255 valid loss: 0.1796080 P@1: 0.62000 P@3: 0.45667 P@5: 0.35400 N@3: 0.50765 N@5: 0.48426 early stop: 0\n",
            "\u001b[32m[I 210629 03:46:33 models:110]\u001b[39m 104 1024 train loss: 0.0009800 valid loss: 0.1816125 P@1: 0.62000 P@3: 0.45667 P@5: 0.35600 N@3: 0.50888 N@5: 0.48697 early stop: 0\n",
            "\u001b[32m[I 210629 03:46:35 models:110]\u001b[39m 107 512 train loss: 0.0006908 valid loss: 0.1834965 P@1: 0.62000 P@3: 0.46333 P@5: 0.35800 N@3: 0.51419 N@5: 0.48958 early stop: 0\n",
            "\u001b[32m[I 210629 03:46:37 models:110]\u001b[39m 109 1024 train loss: 0.0009972 valid loss: 0.1854318 P@1: 0.62000 P@3: 0.46333 P@5: 0.35600 N@3: 0.51419 N@5: 0.48832 early stop: 1\n",
            "\u001b[32m[I 210629 03:46:40 models:110]\u001b[39m 112 512 train loss: 0.0011130 valid loss: 0.1871528 P@1: 0.63000 P@3: 0.46333 P@5: 0.36000 N@3: 0.51592 N@5: 0.49404 early stop: 0\n",
            "\u001b[32m[I 210629 03:46:42 models:110]\u001b[39m 114 1024 train loss: 0.0045248 valid loss: 0.1889007 P@1: 0.63000 P@3: 0.46333 P@5: 0.36200 N@3: 0.51531 N@5: 0.49491 early stop: 0\n",
            "\u001b[32m[I 210629 03:46:45 models:110]\u001b[39m 117 512 train loss: 0.0039539 valid loss: 0.1905335 P@1: 0.63000 P@3: 0.46667 P@5: 0.36200 N@3: 0.51704 N@5: 0.49472 early stop: 1\n",
            "\u001b[32m[I 210629 03:46:47 models:110]\u001b[39m 119 1024 train loss: 0.0069401 valid loss: 0.1921683 P@1: 0.62000 P@3: 0.46667 P@5: 0.36600 N@3: 0.51592 N@5: 0.49745 early stop: 0\n",
            "\u001b[32m[I 210629 03:46:49 models:110]\u001b[39m 122 512 train loss: 0.0067947 valid loss: 0.1932078 P@1: 0.63000 P@3: 0.47000 P@5: 0.36400 N@3: 0.52000 N@5: 0.49771 early stop: 0\n",
            "\u001b[32m[I 210629 03:46:51 models:110]\u001b[39m 124 1024 train loss: 0.0084837 valid loss: 0.1944030 P@1: 0.62000 P@3: 0.47333 P@5: 0.36400 N@3: 0.52061 N@5: 0.49621 early stop: 1\n",
            "\u001b[32m[I 210629 03:46:54 models:110]\u001b[39m 127 512 train loss: 0.0089160 valid loss: 0.1956202 P@1: 0.63000 P@3: 0.47667 P@5: 0.36600 N@3: 0.52469 N@5: 0.49961 early stop: 0\n",
            "\u001b[32m[I 210629 03:46:56 models:110]\u001b[39m 129 1024 train loss: 0.0102864 valid loss: 0.1967767 P@1: 0.63000 P@3: 0.47667 P@5: 0.36600 N@3: 0.52408 N@5: 0.49916 early stop: 1\n",
            "\u001b[32m[I 210629 03:46:58 models:110]\u001b[39m 132 512 train loss: 0.0084827 valid loss: 0.1977302 P@1: 0.62000 P@3: 0.47667 P@5: 0.36600 N@3: 0.52235 N@5: 0.49808 early stop: 2\n",
            "\u001b[32m[I 210629 03:47:00 models:110]\u001b[39m 134 1024 train loss: 0.0057171 valid loss: 0.1989205 P@1: 0.62000 P@3: 0.47667 P@5: 0.36600 N@3: 0.52235 N@5: 0.49829 early stop: 3\n",
            "\u001b[32m[I 210629 03:47:03 models:110]\u001b[39m 137 512 train loss: 0.0052892 valid loss: 0.2000444 P@1: 0.62000 P@3: 0.48000 P@5: 0.36200 N@3: 0.52531 N@5: 0.49619 early stop: 4\n",
            "\u001b[32m[I 210629 03:47:05 models:110]\u001b[39m 139 1024 train loss: 0.0035301 valid loss: 0.2013164 P@1: 0.62000 P@3: 0.48000 P@5: 0.36400 N@3: 0.52531 N@5: 0.49751 early stop: 5\n",
            "\u001b[32m[I 210629 03:47:07 models:110]\u001b[39m 142 512 train loss: 0.0033115 valid loss: 0.2020954 P@1: 0.63000 P@3: 0.48000 P@5: 0.36400 N@3: 0.52765 N@5: 0.49905 early stop: 6\n",
            "\u001b[32m[I 210629 03:47:10 models:110]\u001b[39m 144 1024 train loss: 0.0062959 valid loss: 0.2034340 P@1: 0.62000 P@3: 0.48000 P@5: 0.36600 N@3: 0.52592 N@5: 0.49911 early stop: 7\n",
            "\u001b[32m[I 210629 03:47:12 models:110]\u001b[39m 147 512 train loss: 0.0044417 valid loss: 0.2045569 P@1: 0.61000 P@3: 0.48000 P@5: 0.36600 N@3: 0.52419 N@5: 0.49786 early stop: 8\n",
            "\u001b[32m[I 210629 03:47:14 models:110]\u001b[39m 149 1024 train loss: 0.0029199 valid loss: 0.2058570 P@1: 0.62000 P@3: 0.47667 P@5: 0.36800 N@3: 0.52419 N@5: 0.50151 early stop: 0\n",
            "\u001b[32m[I 210629 03:47:17 models:110]\u001b[39m 152 512 train loss: 0.0020399 valid loss: 0.2069762 P@1: 0.62000 P@3: 0.48667 P@5: 0.36600 N@3: 0.53184 N@5: 0.50100 early stop: 1\n",
            "\u001b[32m[I 210629 03:47:19 models:110]\u001b[39m 154 1024 train loss: 0.0018672 valid loss: 0.2082226 P@1: 0.63000 P@3: 0.48667 P@5: 0.36600 N@3: 0.53358 N@5: 0.50204 early stop: 0\n",
            "\u001b[32m[I 210629 03:47:21 models:110]\u001b[39m 157 512 train loss: 0.0016414 valid loss: 0.2095896 P@1: 0.63000 P@3: 0.49000 P@5: 0.36800 N@3: 0.53531 N@5: 0.50374 early stop: 0\n",
            "\u001b[32m[I 210629 03:47:23 models:110]\u001b[39m 159 1024 train loss: 0.0015435 valid loss: 0.2106441 P@1: 0.63000 P@3: 0.50000 P@5: 0.37000 N@3: 0.54235 N@5: 0.50613 early stop: 0\n",
            "\u001b[32m[I 210629 03:47:26 models:110]\u001b[39m 162 512 train loss: 0.0008869 valid loss: 0.2120537 P@1: 0.63000 P@3: 0.50000 P@5: 0.37000 N@3: 0.54296 N@5: 0.50660 early stop: 0\n",
            "\u001b[32m[I 210629 03:47:28 models:110]\u001b[39m 164 1024 train loss: 0.0008798 valid loss: 0.2133674 P@1: 0.63000 P@3: 0.50000 P@5: 0.36800 N@3: 0.54296 N@5: 0.50499 early stop: 1\n",
            "\u001b[32m[I 210629 03:47:31 models:110]\u001b[39m 167 512 train loss: 0.0025146 valid loss: 0.2147427 P@1: 0.63000 P@3: 0.49667 P@5: 0.36800 N@3: 0.54123 N@5: 0.50542 early stop: 2\n",
            "\u001b[32m[I 210629 03:47:33 models:110]\u001b[39m 169 1024 train loss: 0.0022036 valid loss: 0.2160705 P@1: 0.64000 P@3: 0.49667 P@5: 0.36400 N@3: 0.54296 N@5: 0.50375 early stop: 3\n",
            "\u001b[32m[I 210629 03:47:35 models:110]\u001b[39m 172 512 train loss: 0.0025174 valid loss: 0.2171385 P@1: 0.64000 P@3: 0.49667 P@5: 0.36200 N@3: 0.54358 N@5: 0.50305 early stop: 4\n",
            "\u001b[32m[I 210629 03:47:37 models:110]\u001b[39m 174 1024 train loss: 0.0025786 valid loss: 0.2183238 P@1: 0.64000 P@3: 0.49667 P@5: 0.36800 N@3: 0.54358 N@5: 0.50784 early stop: 0\n",
            "\u001b[32m[I 210629 03:47:40 models:110]\u001b[39m 177 512 train loss: 0.0024333 valid loss: 0.2193037 P@1: 0.64000 P@3: 0.49667 P@5: 0.37000 N@3: 0.54358 N@5: 0.50915 early stop: 0\n",
            "\u001b[32m[I 210629 03:47:42 models:110]\u001b[39m 179 1024 train loss: 0.0021703 valid loss: 0.2203623 P@1: 0.64000 P@3: 0.49333 P@5: 0.36800 N@3: 0.54123 N@5: 0.50752 early stop: 1\n",
            "\u001b[32m[I 210629 03:47:44 models:110]\u001b[39m 182 512 train loss: 0.0014194 valid loss: 0.2215524 P@1: 0.64000 P@3: 0.49333 P@5: 0.36800 N@3: 0.54123 N@5: 0.50752 early stop: 2\n",
            "\u001b[32m[I 210629 03:47:46 models:110]\u001b[39m 184 1024 train loss: 0.0010517 valid loss: 0.2227320 P@1: 0.64000 P@3: 0.49333 P@5: 0.36800 N@3: 0.54123 N@5: 0.50752 early stop: 3\n",
            "\u001b[32m[I 210629 03:47:49 models:110]\u001b[39m 187 512 train loss: 0.0008665 valid loss: 0.2240099 P@1: 0.64000 P@3: 0.49333 P@5: 0.36800 N@3: 0.54123 N@5: 0.50731 early stop: 4\n",
            "\u001b[32m[I 210629 03:47:51 models:110]\u001b[39m 189 1024 train loss: 0.0010386 valid loss: 0.2250870 P@1: 0.64000 P@3: 0.49667 P@5: 0.36800 N@3: 0.54419 N@5: 0.50793 early stop: 5\n",
            "\u001b[32m[I 210629 03:47:53 models:110]\u001b[39m 192 512 train loss: 0.0007037 valid loss: 0.2262346 P@1: 0.64000 P@3: 0.49667 P@5: 0.36800 N@3: 0.54419 N@5: 0.50793 early stop: 6\n",
            "\u001b[32m[I 210629 03:47:55 models:110]\u001b[39m 194 1024 train loss: 0.0005716 valid loss: 0.2273427 P@1: 0.64000 P@3: 0.49667 P@5: 0.36800 N@3: 0.54419 N@5: 0.50743 early stop: 7\n",
            "\u001b[32m[I 210629 03:47:58 models:110]\u001b[39m 197 512 train loss: 0.0003516 valid loss: 0.2284879 P@1: 0.64000 P@3: 0.49667 P@5: 0.37000 N@3: 0.54480 N@5: 0.50904 early stop: 8\n",
            "\u001b[33m[W 210629 03:47:59 models:137]\u001b[39m Clipping gradients with total norm 0.12867 and max norm 0.02246\n",
            "\u001b[32m[I 210629 03:48:00 models:110]\u001b[39m 199 1024 train loss: 0.0003884 valid loss: 0.2296334 P@1: 0.64000 P@3: 0.49667 P@5: 0.37000 N@3: 0.54419 N@5: 0.50857 early stop: 9\n",
            "\u001b[32m[I 210629 03:48:02 models:110]\u001b[39m 202 512 train loss: 0.0007828 valid loss: 0.2306371 P@1: 0.64000 P@3: 0.50333 P@5: 0.37200 N@3: 0.54888 N@5: 0.51039 early stop: 0\n",
            "\u001b[32m[I 210629 03:48:04 models:110]\u001b[39m 204 1024 train loss: 0.0011616 valid loss: 0.2315587 P@1: 0.63000 P@3: 0.50333 P@5: 0.38000 N@3: 0.54396 N@5: 0.51109 early stop: 0\n",
            "\u001b[32m[I 210629 03:48:07 models:110]\u001b[39m 207 512 train loss: 0.0013853 valid loss: 0.2326010 P@1: 0.63000 P@3: 0.50333 P@5: 0.38000 N@3: 0.54396 N@5: 0.51109 early stop: 1\n",
            "\u001b[32m[I 210629 03:48:09 models:110]\u001b[39m 209 1024 train loss: 0.0008671 valid loss: 0.2336892 P@1: 0.63000 P@3: 0.50000 P@5: 0.38000 N@3: 0.54162 N@5: 0.51076 early stop: 2\n",
            "\u001b[32m[I 210629 03:48:12 models:110]\u001b[39m 212 512 train loss: 0.0005420 valid loss: 0.2348290 P@1: 0.63000 P@3: 0.50000 P@5: 0.37800 N@3: 0.54162 N@5: 0.50945 early stop: 3\n",
            "\u001b[32m[I 210629 03:48:14 models:110]\u001b[39m 214 1024 train loss: 0.0007559 valid loss: 0.2359242 P@1: 0.63000 P@3: 0.50333 P@5: 0.37800 N@3: 0.54396 N@5: 0.50983 early stop: 4\n",
            "\u001b[32m[I 210629 03:48:16 models:110]\u001b[39m 217 512 train loss: 0.0004736 valid loss: 0.2371671 P@1: 0.63000 P@3: 0.50333 P@5: 0.37800 N@3: 0.54335 N@5: 0.50939 early stop: 5\n",
            "\u001b[32m[I 210629 03:48:18 models:110]\u001b[39m 219 1024 train loss: 0.0002482 valid loss: 0.2383734 P@1: 0.63000 P@3: 0.50333 P@5: 0.38000 N@3: 0.54335 N@5: 0.51070 early stop: 6\n",
            "\u001b[32m[I 210629 03:48:21 models:110]\u001b[39m 222 512 train loss: 0.0001918 valid loss: 0.2395484 P@1: 0.64000 P@3: 0.50000 P@5: 0.38200 N@3: 0.54273 N@5: 0.51303 early stop: 0\n",
            "\u001b[32m[I 210629 03:48:23 models:110]\u001b[39m 224 1024 train loss: 0.0001433 valid loss: 0.2407519 P@1: 0.64000 P@3: 0.50000 P@5: 0.38200 N@3: 0.54212 N@5: 0.51252 early stop: 1\n",
            "\u001b[32m[I 210629 03:48:25 models:110]\u001b[39m 227 512 train loss: 0.0001476 valid loss: 0.2419428 P@1: 0.64000 P@3: 0.50000 P@5: 0.38200 N@3: 0.54212 N@5: 0.51252 early stop: 2\n",
            "\u001b[32m[I 210629 03:48:27 models:110]\u001b[39m 229 1024 train loss: 0.0001417 valid loss: 0.2431082 P@1: 0.64000 P@3: 0.50000 P@5: 0.38200 N@3: 0.54212 N@5: 0.51252 early stop: 3\n",
            "\u001b[32m[I 210629 03:48:30 models:110]\u001b[39m 232 512 train loss: 0.0001563 valid loss: 0.2442755 P@1: 0.64000 P@3: 0.50000 P@5: 0.38200 N@3: 0.54212 N@5: 0.51269 early stop: 4\n",
            "\u001b[32m[I 210629 03:48:32 models:110]\u001b[39m 234 1024 train loss: 0.0001044 valid loss: 0.2454201 P@1: 0.64000 P@3: 0.50000 P@5: 0.38200 N@3: 0.54212 N@5: 0.51269 early stop: 5\n",
            "\u001b[32m[I 210629 03:48:34 models:110]\u001b[39m 237 512 train loss: 0.0001020 valid loss: 0.2465425 P@1: 0.64000 P@3: 0.50000 P@5: 0.38000 N@3: 0.54212 N@5: 0.51138 early stop: 6\n",
            "\u001b[32m[I 210629 03:48:36 models:110]\u001b[39m 239 1024 train loss: 0.0001425 valid loss: 0.2476550 P@1: 0.64000 P@3: 0.50000 P@5: 0.38000 N@3: 0.54212 N@5: 0.51188 early stop: 7\n",
            "\u001b[32m[I 210629 03:48:39 models:110]\u001b[39m 242 512 train loss: 0.0001163 valid loss: 0.2487621 P@1: 0.64000 P@3: 0.50000 P@5: 0.37600 N@3: 0.54212 N@5: 0.50875 early stop: 8\n",
            "\u001b[32m[I 210629 03:48:41 models:110]\u001b[39m 244 1024 train loss: 0.0001580 valid loss: 0.2498633 P@1: 0.63000 P@3: 0.49667 P@5: 0.38200 N@3: 0.53804 N@5: 0.51156 early stop: 9\n",
            "\u001b[32m[I 210629 03:48:43 models:110]\u001b[39m 247 512 train loss: 0.0001469 valid loss: 0.2509603 P@1: 0.63000 P@3: 0.50000 P@5: 0.38200 N@3: 0.54039 N@5: 0.51188 early stop: 10\n",
            "\u001b[32m[I 210629 03:48:45 models:110]\u001b[39m 249 1024 train loss: 0.0001067 valid loss: 0.2520409 P@1: 0.63000 P@3: 0.50000 P@5: 0.38200 N@3: 0.54039 N@5: 0.51188 early stop: 11\n",
            "\u001b[32m[I 210629 03:48:48 models:110]\u001b[39m 252 512 train loss: 0.0001509 valid loss: 0.2531103 P@1: 0.63000 P@3: 0.50333 P@5: 0.38200 N@3: 0.54273 N@5: 0.51212 early stop: 12\n",
            "\u001b[32m[I 210629 03:48:50 models:110]\u001b[39m 254 1024 train loss: 0.0001200 valid loss: 0.2541612 P@1: 0.63000 P@3: 0.50333 P@5: 0.38200 N@3: 0.54273 N@5: 0.51212 early stop: 13\n",
            "\u001b[32m[I 210629 03:48:52 models:110]\u001b[39m 257 512 train loss: 0.0001642 valid loss: 0.2552184 P@1: 0.63000 P@3: 0.50333 P@5: 0.38200 N@3: 0.54273 N@5: 0.51212 early stop: 14\n",
            "\u001b[33m[W 210629 03:48:54 models:137]\u001b[39m Clipping gradients with total norm 0.08185 and max norm 0.00785\n",
            "\u001b[32m[I 210629 03:48:54 models:110]\u001b[39m 259 1024 train loss: 0.0004269 valid loss: 0.2562608 P@1: 0.63000 P@3: 0.50333 P@5: 0.38400 N@3: 0.54273 N@5: 0.51343 early stop: 0\n",
            "\u001b[32m[I 210629 03:48:57 models:110]\u001b[39m 262 512 train loss: 0.0002422 valid loss: 0.2572803 P@1: 0.63000 P@3: 0.50333 P@5: 0.38400 N@3: 0.54273 N@5: 0.51343 early stop: 1\n",
            "\u001b[32m[I 210629 03:48:59 models:110]\u001b[39m 264 1024 train loss: 0.0001607 valid loss: 0.2582949 P@1: 0.63000 P@3: 0.50333 P@5: 0.38400 N@3: 0.54151 N@5: 0.51254 early stop: 2\n",
            "\u001b[32m[I 210629 03:49:01 models:110]\u001b[39m 267 512 train loss: 0.0001671 valid loss: 0.2593226 P@1: 0.63000 P@3: 0.50000 P@5: 0.38400 N@3: 0.53916 N@5: 0.51231 early stop: 3\n",
            "\u001b[32m[I 210629 03:49:03 models:110]\u001b[39m 269 1024 train loss: 0.0001184 valid loss: 0.2603332 P@1: 0.63000 P@3: 0.50000 P@5: 0.38400 N@3: 0.53916 N@5: 0.51231 early stop: 4\n",
            "\u001b[33m[W 210629 03:49:05 models:137]\u001b[39m Clipping gradients with total norm 0.05304 and max norm 0.00743\n",
            "\u001b[32m[I 210629 03:49:06 models:110]\u001b[39m 272 512 train loss: 0.0003030 valid loss: 0.2613339 P@1: 0.63000 P@3: 0.50000 P@5: 0.38400 N@3: 0.53854 N@5: 0.51186 early stop: 5\n",
            "\u001b[32m[I 210629 03:49:08 models:110]\u001b[39m 274 1024 train loss: 0.0001859 valid loss: 0.2623154 P@1: 0.63000 P@3: 0.50000 P@5: 0.38400 N@3: 0.53854 N@5: 0.51186 early stop: 6\n",
            "\u001b[32m[I 210629 03:49:10 models:110]\u001b[39m 277 512 train loss: 0.0001699 valid loss: 0.2632530 P@1: 0.63000 P@3: 0.50000 P@5: 0.38400 N@3: 0.53854 N@5: 0.51186 early stop: 7\n",
            "\u001b[32m[I 210629 03:49:12 models:110]\u001b[39m 279 1024 train loss: 0.0001342 valid loss: 0.2642066 P@1: 0.63000 P@3: 0.50000 P@5: 0.38400 N@3: 0.53854 N@5: 0.51201 early stop: 8\n",
            "\u001b[32m[I 210629 03:49:15 models:110]\u001b[39m 282 512 train loss: 0.0001378 valid loss: 0.2651766 P@1: 0.63000 P@3: 0.50000 P@5: 0.38400 N@3: 0.53854 N@5: 0.51186 early stop: 9\n",
            "\u001b[32m[I 210629 03:49:17 models:110]\u001b[39m 284 1024 train loss: 0.0001001 valid loss: 0.2661355 P@1: 0.63000 P@3: 0.50000 P@5: 0.38400 N@3: 0.53854 N@5: 0.51201 early stop: 10\n",
            "\u001b[33m[W 210629 03:49:18 models:137]\u001b[39m Clipping gradients with total norm 0.05303 and max norm 0.00939\n",
            "\u001b[32m[I 210629 03:49:19 models:110]\u001b[39m 287 512 train loss: 0.0003434 valid loss: 0.2670919 P@1: 0.63000 P@3: 0.50000 P@5: 0.38400 N@3: 0.53854 N@5: 0.51201 early stop: 11\n",
            "\u001b[33m[W 210629 03:49:21 models:137]\u001b[39m Clipping gradients with total norm 0.06487 and max norm 0.01208\n",
            "\u001b[32m[I 210629 03:49:21 models:110]\u001b[39m 289 1024 train loss: 0.0002812 valid loss: 0.2680163 P@1: 0.62000 P@3: 0.50333 P@5: 0.38400 N@3: 0.53916 N@5: 0.51055 early stop: 12\n",
            "\u001b[32m[I 210629 03:49:24 models:110]\u001b[39m 292 512 train loss: 0.0002706 valid loss: 0.2689258 P@1: 0.62000 P@3: 0.50333 P@5: 0.38400 N@3: 0.53916 N@5: 0.51055 early stop: 13\n",
            "\u001b[32m[I 210629 03:49:26 models:110]\u001b[39m 294 1024 train loss: 0.0001127 valid loss: 0.2698559 P@1: 0.63000 P@3: 0.50667 P@5: 0.38400 N@3: 0.54324 N@5: 0.51261 early stop: 14\n",
            "\u001b[33m[W 210629 03:49:28 models:137]\u001b[39m Clipping gradients with total norm 0.10027 and max norm 0.01092\n",
            "\u001b[32m[I 210629 03:49:28 models:110]\u001b[39m 297 512 train loss: 0.0001893 valid loss: 0.2707725 P@1: 0.63000 P@3: 0.50667 P@5: 0.38400 N@3: 0.54324 N@5: 0.51246 early stop: 15\n",
            "\u001b[33m[W 210629 03:49:29 models:137]\u001b[39m Clipping gradients with total norm 0.23048 and max norm 0.02184\n",
            "\u001b[32m[I 210629 03:49:31 models:110]\u001b[39m 299 1024 train loss: 0.0009701 valid loss: 0.2716368 P@1: 0.63000 P@3: 0.50667 P@5: 0.38400 N@3: 0.54324 N@5: 0.51246 early stop: 16\n",
            "\u001b[32m[I 210629 03:49:33 models:110]\u001b[39m 302 512 train loss: 0.0006672 valid loss: 0.2725931 P@1: 0.63000 P@3: 0.51000 P@5: 0.38400 N@3: 0.54558 N@5: 0.51269 early stop: 17\n",
            "\u001b[32m[I 210629 03:49:35 models:110]\u001b[39m 304 1024 train loss: 0.0005729 valid loss: 0.2733178 P@1: 0.62000 P@3: 0.51000 P@5: 0.38800 N@3: 0.54385 N@5: 0.51358 early stop: 0\n",
            "\u001b[32m[I 210629 03:49:38 models:110]\u001b[39m 307 512 train loss: 0.0011422 valid loss: 0.2740467 P@1: 0.62000 P@3: 0.51000 P@5: 0.38800 N@3: 0.54385 N@5: 0.51373 early stop: 0\n",
            "\u001b[32m[I 210629 03:49:40 models:110]\u001b[39m 309 1024 train loss: 0.0025270 valid loss: 0.2748092 P@1: 0.62000 P@3: 0.51000 P@5: 0.38800 N@3: 0.54385 N@5: 0.51373 early stop: 1\n",
            "\u001b[32m[I 210629 03:49:42 models:110]\u001b[39m 312 512 train loss: 0.0044531 valid loss: 0.2753586 P@1: 0.62000 P@3: 0.50667 P@5: 0.38800 N@3: 0.54151 N@5: 0.51350 early stop: 2\n",
            "\u001b[32m[I 210629 03:49:44 models:110]\u001b[39m 314 1024 train loss: 0.0240292 valid loss: 0.2752380 P@1: 0.62000 P@3: 0.50333 P@5: 0.38800 N@3: 0.53916 N@5: 0.51340 early stop: 3\n",
            "\u001b[32m[I 210629 03:49:47 models:110]\u001b[39m 317 512 train loss: 0.0405888 valid loss: 0.2746806 P@1: 0.62000 P@3: 0.50333 P@5: 0.38800 N@3: 0.53977 N@5: 0.51369 early stop: 4\n",
            "\u001b[32m[I 210629 03:49:49 models:110]\u001b[39m 319 1024 train loss: 0.0321613 valid loss: 0.2734875 P@1: 0.62000 P@3: 0.50333 P@5: 0.38800 N@3: 0.53977 N@5: 0.51405 early stop: 0\n",
            "\u001b[32m[I 210629 03:49:51 models:110]\u001b[39m 322 512 train loss: 0.0188510 valid loss: 0.2723654 P@1: 0.62000 P@3: 0.50333 P@5: 0.38800 N@3: 0.53977 N@5: 0.51420 early stop: 0\n",
            "\u001b[32m[I 210629 03:49:53 models:110]\u001b[39m 324 1024 train loss: 0.0110923 valid loss: 0.2713918 P@1: 0.62000 P@3: 0.50333 P@5: 0.38600 N@3: 0.53977 N@5: 0.51288 early stop: 1\n",
            "\u001b[32m[I 210629 03:49:56 models:110]\u001b[39m 327 512 train loss: 0.0057107 valid loss: 0.2707233 P@1: 0.63000 P@3: 0.50333 P@5: 0.38600 N@3: 0.54151 N@5: 0.51462 early stop: 0\n",
            "\u001b[32m[I 210629 03:49:58 models:110]\u001b[39m 329 1024 train loss: 0.0030251 valid loss: 0.2702015 P@1: 0.62000 P@3: 0.50333 P@5: 0.38600 N@3: 0.54039 N@5: 0.51365 early stop: 1\n",
            "\u001b[32m[I 210629 03:50:00 models:110]\u001b[39m 332 512 train loss: 0.0017661 valid loss: 0.2698562 P@1: 0.63000 P@3: 0.50333 P@5: 0.38800 N@3: 0.54212 N@5: 0.51720 early stop: 0\n",
            "\u001b[32m[I 210629 03:50:02 models:110]\u001b[39m 334 1024 train loss: 0.0008109 valid loss: 0.2696308 P@1: 0.63000 P@3: 0.50333 P@5: 0.38600 N@3: 0.54212 N@5: 0.51588 early stop: 1\n",
            "\u001b[32m[I 210629 03:50:05 models:110]\u001b[39m 337 512 train loss: 0.0005571 valid loss: 0.2694751 P@1: 0.63000 P@3: 0.50667 P@5: 0.38800 N@3: 0.54447 N@5: 0.51802 early stop: 0\n",
            "\u001b[32m[I 210629 03:50:07 models:110]\u001b[39m 339 1024 train loss: 0.0003754 valid loss: 0.2693839 P@1: 0.63000 P@3: 0.50667 P@5: 0.38800 N@3: 0.54447 N@5: 0.51802 early stop: 1\n",
            "\u001b[32m[I 210629 03:50:10 models:110]\u001b[39m 342 512 train loss: 0.0003199 valid loss: 0.2693437 P@1: 0.63000 P@3: 0.50667 P@5: 0.38800 N@3: 0.54447 N@5: 0.51802 early stop: 2\n",
            "\u001b[32m[I 210629 03:50:12 models:110]\u001b[39m 344 1024 train loss: 0.0002584 valid loss: 0.2693423 P@1: 0.63000 P@3: 0.50667 P@5: 0.38800 N@3: 0.54447 N@5: 0.51802 early stop: 3\n",
            "\u001b[32m[I 210629 03:50:14 models:110]\u001b[39m 347 512 train loss: 0.0002423 valid loss: 0.2693780 P@1: 0.63000 P@3: 0.50667 P@5: 0.38800 N@3: 0.54447 N@5: 0.51793 early stop: 4\n",
            "\u001b[32m[I 210629 03:50:16 models:110]\u001b[39m 349 1024 train loss: 0.0003393 valid loss: 0.2694111 P@1: 0.63000 P@3: 0.50667 P@5: 0.38800 N@3: 0.54447 N@5: 0.51793 early stop: 5\n",
            "\u001b[32m[I 210629 03:50:19 models:110]\u001b[39m 352 512 train loss: 0.0003509 valid loss: 0.2694952 P@1: 0.63000 P@3: 0.50667 P@5: 0.38800 N@3: 0.54447 N@5: 0.51793 early stop: 6\n",
            "\u001b[32m[I 210629 03:50:21 models:110]\u001b[39m 354 1024 train loss: 0.0002214 valid loss: 0.2696097 P@1: 0.63000 P@3: 0.50667 P@5: 0.38800 N@3: 0.54385 N@5: 0.51732 early stop: 7\n",
            "\u001b[32m[I 210629 03:50:23 models:110]\u001b[39m 357 512 train loss: 0.0001705 valid loss: 0.2697223 P@1: 0.63000 P@3: 0.50667 P@5: 0.38800 N@3: 0.54447 N@5: 0.51791 early stop: 8\n",
            "\u001b[32m[I 210629 03:50:25 models:110]\u001b[39m 359 1024 train loss: 0.0002072 valid loss: 0.2698626 P@1: 0.63000 P@3: 0.50667 P@5: 0.39000 N@3: 0.54447 N@5: 0.51922 early stop: 0\n",
            "\u001b[33m[W 210629 03:50:26 models:137]\u001b[39m Clipping gradients with total norm 0.07892 and max norm 0.0082\n",
            "\u001b[32m[I 210629 03:50:28 models:110]\u001b[39m 362 512 train loss: 0.0003922 valid loss: 0.2700156 P@1: 0.63000 P@3: 0.50667 P@5: 0.39000 N@3: 0.54447 N@5: 0.51937 early stop: 0\n",
            "\u001b[32m[I 210629 03:50:30 models:110]\u001b[39m 364 1024 train loss: 0.0002429 valid loss: 0.2701700 P@1: 0.63000 P@3: 0.50667 P@5: 0.39000 N@3: 0.54447 N@5: 0.51937 early stop: 1\n",
            "\u001b[32m[I 210629 03:50:32 models:110]\u001b[39m 367 512 train loss: 0.0002126 valid loss: 0.2703428 P@1: 0.63000 P@3: 0.50667 P@5: 0.39000 N@3: 0.54447 N@5: 0.51937 early stop: 2\n",
            "\u001b[32m[I 210629 03:50:34 models:110]\u001b[39m 369 1024 train loss: 0.0001699 valid loss: 0.2705404 P@1: 0.63000 P@3: 0.50667 P@5: 0.39000 N@3: 0.54447 N@5: 0.51937 early stop: 3\n",
            "\u001b[32m[I 210629 03:50:37 models:110]\u001b[39m 372 512 train loss: 0.0001643 valid loss: 0.2707516 P@1: 0.63000 P@3: 0.50667 P@5: 0.39000 N@3: 0.54447 N@5: 0.51917 early stop: 4\n",
            "\u001b[33m[W 210629 03:50:37 models:137]\u001b[39m Clipping gradients with total norm 0.04512 and max norm 0.00792\n",
            "\u001b[32m[I 210629 03:50:39 models:110]\u001b[39m 374 1024 train loss: 0.0006970 valid loss: 0.2709594 P@1: 0.63000 P@3: 0.50667 P@5: 0.39000 N@3: 0.54447 N@5: 0.51917 early stop: 5\n",
            "\u001b[32m[I 210629 03:50:42 models:110]\u001b[39m 377 512 train loss: 0.0003560 valid loss: 0.2711793 P@1: 0.63000 P@3: 0.50667 P@5: 0.39000 N@3: 0.54447 N@5: 0.51917 early stop: 6\n",
            "\u001b[32m[I 210629 03:50:44 models:110]\u001b[39m 379 1024 train loss: 0.0002388 valid loss: 0.2714110 P@1: 0.63000 P@3: 0.51000 P@5: 0.39000 N@3: 0.54681 N@5: 0.51940 early stop: 0\n",
            "\u001b[33m[W 210629 03:50:44 models:137]\u001b[39m Clipping gradients with total norm 0.05761 and max norm 0.00393\n",
            "\u001b[32m[I 210629 03:50:46 models:110]\u001b[39m 382 512 train loss: 0.0004625 valid loss: 0.2716417 P@1: 0.63000 P@3: 0.51000 P@5: 0.39000 N@3: 0.54681 N@5: 0.51955 early stop: 0\n",
            "\u001b[32m[I 210629 03:50:48 models:110]\u001b[39m 384 1024 train loss: 0.0001712 valid loss: 0.2718690 P@1: 0.63000 P@3: 0.51000 P@5: 0.39000 N@3: 0.54743 N@5: 0.52017 early stop: 0\n",
            "\u001b[32m[I 210629 03:50:51 models:110]\u001b[39m 387 512 train loss: 0.0001547 valid loss: 0.2721195 P@1: 0.63000 P@3: 0.51000 P@5: 0.39000 N@3: 0.54743 N@5: 0.52017 early stop: 1\n",
            "\u001b[32m[I 210629 03:50:53 models:110]\u001b[39m 389 1024 train loss: 0.0001220 valid loss: 0.2723884 P@1: 0.63000 P@3: 0.51000 P@5: 0.39200 N@3: 0.54743 N@5: 0.52148 early stop: 0\n",
            "\u001b[32m[I 210629 03:50:55 models:110]\u001b[39m 392 512 train loss: 0.0001319 valid loss: 0.2726582 P@1: 0.63000 P@3: 0.51000 P@5: 0.39200 N@3: 0.54743 N@5: 0.52148 early stop: 1\n",
            "\u001b[32m[I 210629 03:50:58 models:110]\u001b[39m 394 1024 train loss: 0.0001333 valid loss: 0.2729283 P@1: 0.63000 P@3: 0.51000 P@5: 0.39200 N@3: 0.54743 N@5: 0.52148 early stop: 2\n",
            "\u001b[32m[I 210629 03:51:00 models:110]\u001b[39m 397 512 train loss: 0.0001654 valid loss: 0.2732126 P@1: 0.63000 P@3: 0.51333 P@5: 0.39200 N@3: 0.54977 N@5: 0.52171 early stop: 0\n",
            "\u001b[32m[I 210629 03:51:02 models:110]\u001b[39m 399 1024 train loss: 0.0001030 valid loss: 0.2735024 P@1: 0.63000 P@3: 0.51333 P@5: 0.39200 N@3: 0.54977 N@5: 0.52186 early stop: 0\n",
            "\u001b[32m[I 210629 03:51:05 models:110]\u001b[39m 402 512 train loss: 0.0001220 valid loss: 0.2737963 P@1: 0.64000 P@3: 0.51667 P@5: 0.39200 N@3: 0.55385 N@5: 0.52335 early stop: 0\n",
            "\u001b[32m[I 210629 03:51:07 models:110]\u001b[39m 404 1024 train loss: 0.0001258 valid loss: 0.2740951 P@1: 0.64000 P@3: 0.51667 P@5: 0.39200 N@3: 0.55447 N@5: 0.52400 early stop: 0\n",
            "\u001b[32m[I 210629 03:51:09 models:110]\u001b[39m 407 512 train loss: 0.0001460 valid loss: 0.2743985 P@1: 0.64000 P@3: 0.51667 P@5: 0.39200 N@3: 0.55447 N@5: 0.52400 early stop: 1\n",
            "\u001b[33m[W 210629 03:51:10 models:137]\u001b[39m Clipping gradients with total norm 0.05401 and max norm 0.00286\n",
            "\u001b[32m[I 210629 03:51:12 models:110]\u001b[39m 409 1024 train loss: 0.0002535 valid loss: 0.2747051 P@1: 0.64000 P@3: 0.51667 P@5: 0.39200 N@3: 0.55508 N@5: 0.52459 early stop: 0\n",
            "\u001b[32m[I 210629 03:51:14 models:110]\u001b[39m 412 512 train loss: 0.0001745 valid loss: 0.2750214 P@1: 0.64000 P@3: 0.51667 P@5: 0.39200 N@3: 0.55508 N@5: 0.52444 early stop: 1\n",
            "\u001b[33m[W 210629 03:51:15 models:137]\u001b[39m Clipping gradients with total norm 0.04881 and max norm 0.00338\n",
            "\u001b[32m[I 210629 03:51:16 models:110]\u001b[39m 414 1024 train loss: 0.0002572 valid loss: 0.2753447 P@1: 0.64000 P@3: 0.51667 P@5: 0.39200 N@3: 0.55447 N@5: 0.52400 early stop: 2\n",
            "\u001b[32m[I 210629 03:51:19 models:110]\u001b[39m 417 512 train loss: 0.0002230 valid loss: 0.2756619 P@1: 0.64000 P@3: 0.51667 P@5: 0.39400 N@3: 0.55447 N@5: 0.52602 early stop: 0\n",
            "\u001b[32m[I 210629 03:51:21 models:110]\u001b[39m 419 1024 train loss: 0.0001641 valid loss: 0.2759770 P@1: 0.64000 P@3: 0.51667 P@5: 0.39400 N@3: 0.55447 N@5: 0.52602 early stop: 1\n",
            "\u001b[32m[I 210629 03:51:24 models:110]\u001b[39m 422 512 train loss: 0.0001217 valid loss: 0.2763024 P@1: 0.64000 P@3: 0.51667 P@5: 0.39400 N@3: 0.55447 N@5: 0.52602 early stop: 2\n",
            "\u001b[32m[I 210629 03:51:27 models:110]\u001b[39m 424 1024 train loss: 0.0001143 valid loss: 0.2766345 P@1: 0.64000 P@3: 0.51667 P@5: 0.39400 N@3: 0.55508 N@5: 0.52646 early stop: 0\n",
            "\u001b[32m[I 210629 03:51:30 models:110]\u001b[39m 427 512 train loss: 0.0001301 valid loss: 0.2769679 P@1: 0.64000 P@3: 0.52000 P@5: 0.39400 N@3: 0.55743 N@5: 0.52679 early stop: 0\n",
            "\u001b[32m[I 210629 03:51:32 models:110]\u001b[39m 429 1024 train loss: 0.0001241 valid loss: 0.2773035 P@1: 0.64000 P@3: 0.52000 P@5: 0.39400 N@3: 0.55804 N@5: 0.52723 early stop: 0\n",
            "\u001b[32m[I 210629 03:51:35 models:110]\u001b[39m 432 512 train loss: 0.0000887 valid loss: 0.2776412 P@1: 0.64000 P@3: 0.52000 P@5: 0.39600 N@3: 0.55804 N@5: 0.52905 early stop: 0\n",
            "\u001b[33m[W 210629 03:51:35 models:137]\u001b[39m Clipping gradients with total norm 0.04149 and max norm 0.00406\n",
            "\u001b[32m[I 210629 03:51:37 models:110]\u001b[39m 434 1024 train loss: 0.0002727 valid loss: 0.2779805 P@1: 0.64000 P@3: 0.52000 P@5: 0.39600 N@3: 0.55804 N@5: 0.52905 early stop: 1\n",
            "\u001b[32m[I 210629 03:51:39 models:110]\u001b[39m 437 512 train loss: 0.0001348 valid loss: 0.2783238 P@1: 0.64000 P@3: 0.52000 P@5: 0.39600 N@3: 0.55866 N@5: 0.52949 early stop: 0\n",
            "\u001b[32m[I 210629 03:51:41 models:110]\u001b[39m 439 1024 train loss: 0.0000984 valid loss: 0.2786715 P@1: 0.64000 P@3: 0.52000 P@5: 0.39600 N@3: 0.55866 N@5: 0.52949 early stop: 1\n",
            "\u001b[32m[I 210629 03:51:44 models:110]\u001b[39m 442 512 train loss: 0.0001291 valid loss: 0.2790284 P@1: 0.64000 P@3: 0.52000 P@5: 0.39800 N@3: 0.55866 N@5: 0.53080 early stop: 0\n",
            "\u001b[32m[I 210629 03:51:46 models:110]\u001b[39m 444 1024 train loss: 0.0001363 valid loss: 0.2793821 P@1: 0.64000 P@3: 0.52000 P@5: 0.39800 N@3: 0.55866 N@5: 0.53101 early stop: 0\n",
            "\u001b[32m[I 210629 03:51:48 models:110]\u001b[39m 447 512 train loss: 0.0001309 valid loss: 0.2797294 P@1: 0.64000 P@3: 0.52000 P@5: 0.39800 N@3: 0.55866 N@5: 0.53101 early stop: 1\n",
            "\u001b[32m[I 210629 03:51:50 models:110]\u001b[39m 449 1024 train loss: 0.0001054 valid loss: 0.2800760 P@1: 0.64000 P@3: 0.52000 P@5: 0.39800 N@3: 0.55866 N@5: 0.53101 early stop: 2\n",
            "\u001b[32m[I 210629 03:51:53 models:110]\u001b[39m 452 512 train loss: 0.0001088 valid loss: 0.2804337 P@1: 0.64000 P@3: 0.52000 P@5: 0.39800 N@3: 0.55866 N@5: 0.53086 early stop: 3\n",
            "\u001b[32m[I 210629 03:51:55 models:110]\u001b[39m 454 1024 train loss: 0.0001201 valid loss: 0.2807921 P@1: 0.64000 P@3: 0.52333 P@5: 0.39800 N@3: 0.56100 N@5: 0.53119 early stop: 0\n",
            "\u001b[32m[I 210629 03:51:58 models:110]\u001b[39m 457 512 train loss: 0.0001396 valid loss: 0.2811559 P@1: 0.64000 P@3: 0.52333 P@5: 0.39800 N@3: 0.56100 N@5: 0.53119 early stop: 1\n",
            "\u001b[32m[I 210629 03:52:00 models:110]\u001b[39m 459 1024 train loss: 0.0000854 valid loss: 0.2815208 P@1: 0.64000 P@3: 0.52333 P@5: 0.39800 N@3: 0.56100 N@5: 0.53119 early stop: 2\n",
            "\u001b[32m[I 210629 03:52:02 models:110]\u001b[39m 462 512 train loss: 0.0001404 valid loss: 0.2818876 P@1: 0.64000 P@3: 0.52333 P@5: 0.39800 N@3: 0.56100 N@5: 0.53119 early stop: 3\n",
            "\u001b[33m[W 210629 03:52:04 models:137]\u001b[39m Clipping gradients with total norm 0.04239 and max norm 0.00687\n",
            "\u001b[32m[I 210629 03:52:04 models:110]\u001b[39m 464 1024 train loss: 0.0002469 valid loss: 0.2822539 P@1: 0.64000 P@3: 0.52333 P@5: 0.39800 N@3: 0.56100 N@5: 0.53119 early stop: 4\n",
            "\u001b[32m[I 210629 03:52:07 models:110]\u001b[39m 467 512 train loss: 0.0001586 valid loss: 0.2826242 P@1: 0.64000 P@3: 0.52333 P@5: 0.39800 N@3: 0.56100 N@5: 0.53119 early stop: 5\n",
            "\u001b[32m[I 210629 03:52:09 models:110]\u001b[39m 469 1024 train loss: 0.0001940 valid loss: 0.2829813 P@1: 0.64000 P@3: 0.52333 P@5: 0.39800 N@3: 0.56100 N@5: 0.53119 early stop: 6\n",
            "\u001b[33m[W 210629 03:52:10 models:137]\u001b[39m Clipping gradients with total norm 0.04958 and max norm 0.00423\n",
            "\u001b[32m[I 210629 03:52:11 models:110]\u001b[39m 472 512 train loss: 0.0003163 valid loss: 0.2833382 P@1: 0.64000 P@3: 0.52333 P@5: 0.39800 N@3: 0.56100 N@5: 0.53119 early stop: 7\n",
            "\u001b[32m[I 210629 03:52:13 models:110]\u001b[39m 474 1024 train loss: 0.0001149 valid loss: 0.2837006 P@1: 0.64000 P@3: 0.52333 P@5: 0.39800 N@3: 0.56100 N@5: 0.53119 early stop: 8\n",
            "\u001b[32m[I 210629 03:52:16 models:110]\u001b[39m 477 512 train loss: 0.0001598 valid loss: 0.2840660 P@1: 0.64000 P@3: 0.52333 P@5: 0.39800 N@3: 0.56100 N@5: 0.53119 early stop: 9\n",
            "\u001b[32m[I 210629 03:52:18 models:110]\u001b[39m 479 1024 train loss: 0.0000854 valid loss: 0.2844326 P@1: 0.64000 P@3: 0.52333 P@5: 0.39600 N@3: 0.56100 N@5: 0.52937 early stop: 10\n",
            "\u001b[33m[W 210629 03:52:19 models:137]\u001b[39m Clipping gradients with total norm 0.04358 and max norm 0.00573\n",
            "\u001b[32m[I 210629 03:52:20 models:110]\u001b[39m 482 512 train loss: 0.0002539 valid loss: 0.2847978 P@1: 0.64000 P@3: 0.52333 P@5: 0.39800 N@3: 0.56100 N@5: 0.53068 early stop: 11\n",
            "\u001b[32m[I 210629 03:52:22 models:110]\u001b[39m 484 1024 train loss: 0.0001803 valid loss: 0.2851610 P@1: 0.64000 P@3: 0.52333 P@5: 0.39800 N@3: 0.56100 N@5: 0.53068 early stop: 12\n",
            "\u001b[32m[I 210629 03:52:25 models:110]\u001b[39m 487 512 train loss: 0.0001541 valid loss: 0.2855185 P@1: 0.64000 P@3: 0.52333 P@5: 0.39800 N@3: 0.56100 N@5: 0.53083 early stop: 13\n",
            "\u001b[32m[I 210629 03:52:27 models:110]\u001b[39m 489 1024 train loss: 0.0000918 valid loss: 0.2858796 P@1: 0.64000 P@3: 0.52333 P@5: 0.39800 N@3: 0.56100 N@5: 0.53083 early stop: 14\n",
            "\u001b[32m[I 210629 03:52:29 models:110]\u001b[39m 492 512 train loss: 0.0001278 valid loss: 0.2862469 P@1: 0.64000 P@3: 0.52333 P@5: 0.39800 N@3: 0.56100 N@5: 0.53098 early stop: 15\n",
            "\u001b[32m[I 210629 03:52:31 models:110]\u001b[39m 494 1024 train loss: 0.0000915 valid loss: 0.2866178 P@1: 0.64000 P@3: 0.52333 P@5: 0.39800 N@3: 0.56100 N@5: 0.53098 early stop: 16\n",
            "\u001b[33m[W 210629 03:52:32 models:137]\u001b[39m Clipping gradients with total norm 0.04899 and max norm 0.00429\n",
            "\u001b[32m[I 210629 03:52:34 models:110]\u001b[39m 497 512 train loss: 0.0002721 valid loss: 0.2869859 P@1: 0.64000 P@3: 0.52333 P@5: 0.39800 N@3: 0.56100 N@5: 0.53098 early stop: 17\n",
            "\u001b[32m[I 210629 03:52:36 models:110]\u001b[39m 499 1024 train loss: 0.0001492 valid loss: 0.2873600 P@1: 0.64000 P@3: 0.52333 P@5: 0.39800 N@3: 0.56039 N@5: 0.53054 early stop: 18\n",
            "\u001b[32m[I 210629 03:52:38 models:110]\u001b[39m 502 512 train loss: 0.0001082 valid loss: 0.2877312 P@1: 0.64000 P@3: 0.52333 P@5: 0.39800 N@3: 0.56039 N@5: 0.53054 early stop: 19\n",
            "\u001b[32m[I 210629 03:52:40 models:110]\u001b[39m 504 1024 train loss: 0.0001281 valid loss: 0.2880963 P@1: 0.64000 P@3: 0.52333 P@5: 0.40000 N@3: 0.56039 N@5: 0.53170 early stop: 0\n",
            "\u001b[33m[W 210629 03:52:42 models:137]\u001b[39m Clipping gradients with total norm 0.02869 and max norm 0.00472\n",
            "\u001b[32m[I 210629 03:52:43 models:110]\u001b[39m 507 512 train loss: 0.0002041 valid loss: 0.2884657 P@1: 0.64000 P@3: 0.52333 P@5: 0.40000 N@3: 0.56039 N@5: 0.53170 early stop: 1\n",
            "\u001b[32m[I 210629 03:52:45 models:110]\u001b[39m 509 1024 train loss: 0.0001693 valid loss: 0.2888394 P@1: 0.64000 P@3: 0.52333 P@5: 0.40000 N@3: 0.56039 N@5: 0.53170 early stop: 2\n",
            "\u001b[32m[I 210629 03:52:48 models:110]\u001b[39m 512 512 train loss: 0.0001480 valid loss: 0.2892140 P@1: 0.65000 P@3: 0.52333 P@5: 0.40000 N@3: 0.56212 N@5: 0.53323 early stop: 0\n",
            "\u001b[32m[I 210629 03:52:50 models:110]\u001b[39m 514 1024 train loss: 0.0001004 valid loss: 0.2895871 P@1: 0.65000 P@3: 0.52333 P@5: 0.40000 N@3: 0.56212 N@5: 0.53323 early stop: 1\n",
            "\u001b[32m[I 210629 03:52:52 models:110]\u001b[39m 517 512 train loss: 0.0001072 valid loss: 0.2899573 P@1: 0.65000 P@3: 0.52333 P@5: 0.40000 N@3: 0.56212 N@5: 0.53323 early stop: 2\n",
            "\u001b[33m[W 210629 03:52:52 models:137]\u001b[39m Clipping gradients with total norm 0.03294 and max norm 0.00402\n",
            "\u001b[32m[I 210629 03:52:54 models:110]\u001b[39m 519 1024 train loss: 0.0002687 valid loss: 0.2903261 P@1: 0.65000 P@3: 0.52333 P@5: 0.39800 N@3: 0.56212 N@5: 0.53141 early stop: 3\n",
            "\u001b[32m[I 210629 03:52:57 models:110]\u001b[39m 522 512 train loss: 0.0001324 valid loss: 0.2906972 P@1: 0.65000 P@3: 0.52333 P@5: 0.39800 N@3: 0.56212 N@5: 0.53141 early stop: 4\n",
            "\u001b[32m[I 210629 03:52:59 models:110]\u001b[39m 524 1024 train loss: 0.0001284 valid loss: 0.2910638 P@1: 0.65000 P@3: 0.52333 P@5: 0.39800 N@3: 0.56212 N@5: 0.53141 early stop: 5\n",
            "\u001b[32m[I 210629 03:53:01 models:110]\u001b[39m 527 512 train loss: 0.0000799 valid loss: 0.2914256 P@1: 0.65000 P@3: 0.52333 P@5: 0.39800 N@3: 0.56212 N@5: 0.53141 early stop: 6\n",
            "\u001b[33m[W 210629 03:53:03 models:137]\u001b[39m Clipping gradients with total norm 0.03408 and max norm 0.00359\n",
            "\u001b[32m[I 210629 03:53:03 models:110]\u001b[39m 529 1024 train loss: 0.0002471 valid loss: 0.2917885 P@1: 0.65000 P@3: 0.52333 P@5: 0.39800 N@3: 0.56212 N@5: 0.53141 early stop: 7\n",
            "\u001b[32m[I 210629 03:53:06 models:110]\u001b[39m 532 512 train loss: 0.0001213 valid loss: 0.2921543 P@1: 0.65000 P@3: 0.52333 P@5: 0.39800 N@3: 0.56212 N@5: 0.53141 early stop: 8\n",
            "\u001b[32m[I 210629 03:53:08 models:110]\u001b[39m 534 1024 train loss: 0.0000956 valid loss: 0.2925147 P@1: 0.65000 P@3: 0.52333 P@5: 0.39800 N@3: 0.56212 N@5: 0.53141 early stop: 9\n",
            "\u001b[32m[I 210629 03:53:10 models:110]\u001b[39m 537 512 train loss: 0.0001397 valid loss: 0.2928702 P@1: 0.65000 P@3: 0.52333 P@5: 0.39800 N@3: 0.56212 N@5: 0.53141 early stop: 10\n",
            "\u001b[32m[I 210629 03:53:12 models:110]\u001b[39m 539 1024 train loss: 0.0000821 valid loss: 0.2932296 P@1: 0.65000 P@3: 0.52333 P@5: 0.39800 N@3: 0.56212 N@5: 0.53141 early stop: 11\n",
            "\u001b[32m[I 210629 03:53:15 models:110]\u001b[39m 542 512 train loss: 0.0001272 valid loss: 0.2935920 P@1: 0.65000 P@3: 0.52333 P@5: 0.39800 N@3: 0.56212 N@5: 0.53141 early stop: 12\n",
            "\u001b[32m[I 210629 03:53:17 models:110]\u001b[39m 544 1024 train loss: 0.0000827 valid loss: 0.2939540 P@1: 0.65000 P@3: 0.52333 P@5: 0.39800 N@3: 0.56212 N@5: 0.53141 early stop: 13\n",
            "\u001b[32m[I 210629 03:53:19 models:110]\u001b[39m 547 512 train loss: 0.0001190 valid loss: 0.2943119 P@1: 0.65000 P@3: 0.52000 P@5: 0.39800 N@3: 0.55977 N@5: 0.53117 early stop: 14\n",
            "\u001b[32m[I 210629 03:53:22 models:110]\u001b[39m 549 1024 train loss: 0.0000897 valid loss: 0.2946704 P@1: 0.65000 P@3: 0.52000 P@5: 0.40000 N@3: 0.55977 N@5: 0.53249 early stop: 15\n",
            "\u001b[32m[I 210629 03:53:24 models:110]\u001b[39m 552 512 train loss: 0.0001118 valid loss: 0.2950297 P@1: 0.65000 P@3: 0.52000 P@5: 0.40200 N@3: 0.55977 N@5: 0.53430 early stop: 0\n",
            "\u001b[32m[I 210629 03:53:26 models:110]\u001b[39m 554 1024 train loss: 0.0001119 valid loss: 0.2953877 P@1: 0.65000 P@3: 0.52000 P@5: 0.40200 N@3: 0.55977 N@5: 0.53430 early stop: 1\n",
            "\u001b[33m[W 210629 03:53:27 models:137]\u001b[39m Clipping gradients with total norm 0.04421 and max norm 0.0044\n",
            "\u001b[32m[I 210629 03:53:29 models:110]\u001b[39m 557 512 train loss: 0.0003240 valid loss: 0.2957488 P@1: 0.65000 P@3: 0.52000 P@5: 0.40200 N@3: 0.55977 N@5: 0.53451 early stop: 0\n",
            "\u001b[32m[I 210629 03:53:31 models:110]\u001b[39m 559 1024 train loss: 0.0001157 valid loss: 0.2961074 P@1: 0.65000 P@3: 0.52000 P@5: 0.40200 N@3: 0.55977 N@5: 0.53451 early stop: 1\n",
            "\u001b[33m[W 210629 03:53:31 models:137]\u001b[39m Clipping gradients with total norm 0.03832 and max norm 0.00239\n",
            "\u001b[32m[I 210629 03:53:33 models:110]\u001b[39m 562 512 train loss: 0.0002816 valid loss: 0.2964584 P@1: 0.65000 P@3: 0.52000 P@5: 0.40200 N@3: 0.55977 N@5: 0.53451 early stop: 2\n",
            "\u001b[32m[I 210629 03:53:35 models:110]\u001b[39m 564 1024 train loss: 0.0000904 valid loss: 0.2968112 P@1: 0.65000 P@3: 0.52333 P@5: 0.40200 N@3: 0.56212 N@5: 0.53474 early stop: 0\n",
            "\u001b[33m[W 210629 03:53:37 models:137]\u001b[39m Clipping gradients with total norm 0.0044 and max norm 0.00079\n",
            "\u001b[32m[I 210629 03:53:38 models:110]\u001b[39m 567 512 train loss: 0.0001216 valid loss: 0.2971751 P@1: 0.65000 P@3: 0.52333 P@5: 0.40200 N@3: 0.56212 N@5: 0.53474 early stop: 1\n",
            "\u001b[32m[I 210629 03:53:40 models:110]\u001b[39m 569 1024 train loss: 0.0000885 valid loss: 0.2975378 P@1: 0.65000 P@3: 0.52333 P@5: 0.40200 N@3: 0.56212 N@5: 0.53474 early stop: 2\n",
            "\u001b[33m[W 210629 03:53:41 models:137]\u001b[39m Clipping gradients with total norm 0.03306 and max norm 0.00503\n",
            "\u001b[32m[I 210629 03:53:42 models:110]\u001b[39m 572 512 train loss: 0.0002387 valid loss: 0.2978973 P@1: 0.65000 P@3: 0.52333 P@5: 0.40200 N@3: 0.56212 N@5: 0.53489 early stop: 0\n",
            "\u001b[32m[I 210629 03:53:44 models:110]\u001b[39m 574 1024 train loss: 0.0001948 valid loss: 0.2982504 P@1: 0.64000 P@3: 0.52333 P@5: 0.40200 N@3: 0.56039 N@5: 0.53364 early stop: 1\n",
            "\u001b[32m[I 210629 03:53:47 models:110]\u001b[39m 577 512 train loss: 0.0001071 valid loss: 0.2986021 P@1: 0.64000 P@3: 0.52333 P@5: 0.40200 N@3: 0.56039 N@5: 0.53364 early stop: 2\n",
            "\u001b[32m[I 210629 03:53:49 models:110]\u001b[39m 579 1024 train loss: 0.0001090 valid loss: 0.2989589 P@1: 0.64000 P@3: 0.52333 P@5: 0.40200 N@3: 0.56039 N@5: 0.53364 early stop: 3\n",
            "\u001b[32m[I 210629 03:53:51 models:110]\u001b[39m 582 512 train loss: 0.0001203 valid loss: 0.2993109 P@1: 0.64000 P@3: 0.52667 P@5: 0.40200 N@3: 0.56273 N@5: 0.53397 early stop: 4\n",
            "\u001b[33m[W 210629 03:53:52 models:137]\u001b[39m Clipping gradients with total norm 0.03662 and max norm 0.00432\n",
            "\u001b[32m[I 210629 03:53:53 models:110]\u001b[39m 584 1024 train loss: 0.0002628 valid loss: 0.2996555 P@1: 0.64000 P@3: 0.52667 P@5: 0.40200 N@3: 0.56273 N@5: 0.53397 early stop: 5\n",
            "\u001b[32m[I 210629 03:53:56 models:110]\u001b[39m 587 512 train loss: 0.0001715 valid loss: 0.2999997 P@1: 0.64000 P@3: 0.52667 P@5: 0.40200 N@3: 0.56273 N@5: 0.53397 early stop: 6\n",
            "\u001b[32m[I 210629 03:53:58 models:110]\u001b[39m 589 1024 train loss: 0.0000967 valid loss: 0.3003476 P@1: 0.64000 P@3: 0.52667 P@5: 0.40200 N@3: 0.56273 N@5: 0.53382 early stop: 7\n",
            "\u001b[32m[I 210629 03:54:01 models:110]\u001b[39m 592 512 train loss: 0.0001052 valid loss: 0.3006942 P@1: 0.64000 P@3: 0.52667 P@5: 0.40200 N@3: 0.56273 N@5: 0.53382 early stop: 8\n",
            "\u001b[32m[I 210629 03:54:03 models:110]\u001b[39m 594 1024 train loss: 0.0001018 valid loss: 0.3010425 P@1: 0.64000 P@3: 0.52667 P@5: 0.40200 N@3: 0.56273 N@5: 0.53382 early stop: 9\n",
            "\u001b[32m[I 210629 03:54:05 models:110]\u001b[39m 597 512 train loss: 0.0001094 valid loss: 0.3013934 P@1: 0.64000 P@3: 0.52667 P@5: 0.40200 N@3: 0.56273 N@5: 0.53382 early stop: 10\n",
            "\u001b[33m[W 210629 03:54:05 models:137]\u001b[39m Clipping gradients with total norm 0.02767 and max norm 0.00383\n",
            "\u001b[32m[I 210629 03:54:07 models:110]\u001b[39m 599 1024 train loss: 0.0002344 valid loss: 0.3017460 P@1: 0.64000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56273 N@5: 0.53513 early stop: 0\n",
            "\u001b[32m[I 210629 03:54:10 models:110]\u001b[39m 602 512 train loss: 0.0001881 valid loss: 0.3020953 P@1: 0.64000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56273 N@5: 0.53513 early stop: 1\n",
            "\u001b[32m[I 210629 03:54:12 models:110]\u001b[39m 604 1024 train loss: 0.0001732 valid loss: 0.3024396 P@1: 0.64000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56273 N@5: 0.53513 early stop: 2\n",
            "\u001b[32m[I 210629 03:54:14 models:110]\u001b[39m 607 512 train loss: 0.0001119 valid loss: 0.3027820 P@1: 0.64000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56273 N@5: 0.53513 early stop: 3\n",
            "\u001b[32m[I 210629 03:54:16 models:110]\u001b[39m 609 1024 train loss: 0.0000942 valid loss: 0.3031240 P@1: 0.64000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56273 N@5: 0.53513 early stop: 4\n",
            "\u001b[32m[I 210629 03:54:19 models:110]\u001b[39m 612 512 train loss: 0.0001245 valid loss: 0.3034665 P@1: 0.64000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56273 N@5: 0.53513 early stop: 5\n",
            "\u001b[32m[I 210629 03:54:21 models:110]\u001b[39m 614 1024 train loss: 0.0000916 valid loss: 0.3038109 P@1: 0.64000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56273 N@5: 0.53563 early stop: 0\n",
            "\u001b[32m[I 210629 03:54:23 models:110]\u001b[39m 617 512 train loss: 0.0001184 valid loss: 0.3041549 P@1: 0.64000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56273 N@5: 0.53563 early stop: 1\n",
            "\u001b[32m[I 210629 03:54:25 models:110]\u001b[39m 619 1024 train loss: 0.0000907 valid loss: 0.3044938 P@1: 0.64000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56273 N@5: 0.53563 early stop: 2\n",
            "\u001b[32m[I 210629 03:54:28 models:110]\u001b[39m 622 512 train loss: 0.0001023 valid loss: 0.3048332 P@1: 0.64000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56273 N@5: 0.53563 early stop: 3\n",
            "\u001b[33m[W 210629 03:54:30 models:137]\u001b[39m Clipping gradients with total norm 0.0294 and max norm 0.00298\n",
            "\u001b[32m[I 210629 03:54:30 models:110]\u001b[39m 624 1024 train loss: 0.0002595 valid loss: 0.3051750 P@1: 0.64000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56273 N@5: 0.53563 early stop: 4\n",
            "\u001b[32m[I 210629 03:54:32 models:110]\u001b[39m 627 512 train loss: 0.0001417 valid loss: 0.3055235 P@1: 0.64000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56273 N@5: 0.53563 early stop: 5\n",
            "\u001b[32m[I 210629 03:54:34 models:110]\u001b[39m 629 1024 train loss: 0.0001830 valid loss: 0.3058671 P@1: 0.64000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56273 N@5: 0.53563 early stop: 6\n",
            "\u001b[32m[I 210629 03:54:37 models:110]\u001b[39m 632 512 train loss: 0.0001146 valid loss: 0.3062078 P@1: 0.64000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56335 N@5: 0.53608 early stop: 0\n",
            "\u001b[32m[I 210629 03:54:39 models:110]\u001b[39m 634 1024 train loss: 0.0001058 valid loss: 0.3065480 P@1: 0.64000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56335 N@5: 0.53608 early stop: 1\n",
            "\u001b[33m[W 210629 03:54:40 models:137]\u001b[39m Clipping gradients with total norm 0.02341 and max norm 0.00401\n",
            "\u001b[32m[I 210629 03:54:42 models:110]\u001b[39m 637 512 train loss: 0.0002629 valid loss: 0.3068778 P@1: 0.64000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56335 N@5: 0.53608 early stop: 2\n",
            "\u001b[33m[W 210629 03:54:43 models:137]\u001b[39m Clipping gradients with total norm 0.04189 and max norm 0.00179\n",
            "\u001b[32m[I 210629 03:54:44 models:110]\u001b[39m 639 1024 train loss: 0.0004449 valid loss: 0.3072100 P@1: 0.64000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56335 N@5: 0.53608 early stop: 3\n",
            "\u001b[32m[I 210629 03:54:46 models:110]\u001b[39m 642 512 train loss: 0.0001164 valid loss: 0.3075496 P@1: 0.64000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56335 N@5: 0.53608 early stop: 4\n",
            "\u001b[33m[W 210629 03:54:46 models:137]\u001b[39m Clipping gradients with total norm 0.0213 and max norm 0.00399\n",
            "\u001b[33m[W 210629 03:54:47 models:137]\u001b[39m Clipping gradients with total norm 0.07412 and max norm 0.00799\n",
            "\u001b[32m[I 210629 03:54:48 models:110]\u001b[39m 644 1024 train loss: 0.0006922 valid loss: 0.3078730 P@1: 0.64000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56335 N@5: 0.53608 early stop: 5\n",
            "\u001b[32m[I 210629 03:54:51 models:110]\u001b[39m 647 512 train loss: 0.0001427 valid loss: 0.3081880 P@1: 0.64000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56335 N@5: 0.53608 early stop: 6\n",
            "\u001b[33m[W 210629 03:54:52 models:137]\u001b[39m Clipping gradients with total norm 0.02683 and max norm 0.00472\n",
            "\u001b[32m[I 210629 03:54:53 models:110]\u001b[39m 649 1024 train loss: 0.0001834 valid loss: 0.3084874 P@1: 0.64000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56335 N@5: 0.53608 early stop: 7\n",
            "\u001b[32m[I 210629 03:54:55 models:110]\u001b[39m 652 512 train loss: 0.0001771 valid loss: 0.3087798 P@1: 0.64000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56335 N@5: 0.53608 early stop: 8\n",
            "\u001b[32m[I 210629 03:54:57 models:110]\u001b[39m 654 1024 train loss: 0.0001372 valid loss: 0.3090865 P@1: 0.64000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56335 N@5: 0.53608 early stop: 9\n",
            "\u001b[32m[I 210629 03:55:00 models:110]\u001b[39m 657 512 train loss: 0.0000965 valid loss: 0.3093999 P@1: 0.64000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56335 N@5: 0.53608 early stop: 10\n",
            "\u001b[32m[I 210629 03:55:02 models:110]\u001b[39m 659 1024 train loss: 0.0001256 valid loss: 0.3097138 P@1: 0.64000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56335 N@5: 0.53608 early stop: 11\n",
            "\u001b[32m[I 210629 03:55:04 models:110]\u001b[39m 662 512 train loss: 0.0000956 valid loss: 0.3100309 P@1: 0.64000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56335 N@5: 0.53608 early stop: 12\n",
            "\u001b[33m[W 210629 03:55:05 models:137]\u001b[39m Clipping gradients with total norm 0.0275 and max norm 0.00369\n",
            "\u001b[32m[I 210629 03:55:06 models:110]\u001b[39m 664 1024 train loss: 0.0002406 valid loss: 0.3103473 P@1: 0.64000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56335 N@5: 0.53608 early stop: 13\n",
            "\u001b[32m[I 210629 03:55:09 models:110]\u001b[39m 667 512 train loss: 0.0001849 valid loss: 0.3106608 P@1: 0.64000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56335 N@5: 0.53608 early stop: 14\n",
            "\u001b[32m[I 210629 03:55:11 models:110]\u001b[39m 669 1024 train loss: 0.0000948 valid loss: 0.3109763 P@1: 0.64000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56335 N@5: 0.53608 early stop: 15\n",
            "\u001b[32m[I 210629 03:55:13 models:110]\u001b[39m 672 512 train loss: 0.0001153 valid loss: 0.3112955 P@1: 0.64000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56335 N@5: 0.53608 early stop: 16\n",
            "\u001b[32m[I 210629 03:55:15 models:110]\u001b[39m 674 1024 train loss: 0.0000821 valid loss: 0.3116143 P@1: 0.63000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56162 N@5: 0.53482 early stop: 17\n",
            "\u001b[32m[I 210629 03:55:18 models:110]\u001b[39m 677 512 train loss: 0.0000981 valid loss: 0.3119323 P@1: 0.63000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56162 N@5: 0.53482 early stop: 18\n",
            "\u001b[32m[I 210629 03:55:20 models:110]\u001b[39m 679 1024 train loss: 0.0000960 valid loss: 0.3122518 P@1: 0.63000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56162 N@5: 0.53482 early stop: 19\n",
            "\u001b[33m[W 210629 03:55:21 models:137]\u001b[39m Clipping gradients with total norm 0.02841 and max norm 0.00291\n",
            "\u001b[32m[I 210629 03:55:22 models:110]\u001b[39m 682 512 train loss: 0.0002540 valid loss: 0.3125686 P@1: 0.63000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56162 N@5: 0.53482 early stop: 20\n",
            "\u001b[32m[I 210629 03:55:25 models:110]\u001b[39m 684 1024 train loss: 0.0001036 valid loss: 0.3128843 P@1: 0.63000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56162 N@5: 0.53482 early stop: 21\n",
            "\u001b[32m[I 210629 03:55:27 models:110]\u001b[39m 687 512 train loss: 0.0001153 valid loss: 0.3132014 P@1: 0.63000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56162 N@5: 0.53482 early stop: 22\n",
            "\u001b[32m[I 210629 03:55:29 models:110]\u001b[39m 689 1024 train loss: 0.0000773 valid loss: 0.3135175 P@1: 0.63000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56162 N@5: 0.53482 early stop: 23\n",
            "\u001b[32m[I 210629 03:55:31 models:110]\u001b[39m 692 512 train loss: 0.0000998 valid loss: 0.3138285 P@1: 0.63000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56162 N@5: 0.53468 early stop: 24\n",
            "\u001b[33m[W 210629 03:55:32 models:137]\u001b[39m Clipping gradients with total norm 0.03402 and max norm 0.00367\n",
            "\u001b[32m[I 210629 03:55:34 models:110]\u001b[39m 694 1024 train loss: 0.0002801 valid loss: 0.3141379 P@1: 0.63000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56162 N@5: 0.53468 early stop: 25\n",
            "\u001b[33m[W 210629 03:55:34 models:137]\u001b[39m Clipping gradients with total norm 0.05169 and max norm 0.00472\n",
            "\u001b[32m[I 210629 03:55:36 models:110]\u001b[39m 697 512 train loss: 0.0005162 valid loss: 0.3144458 P@1: 0.63000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56162 N@5: 0.53468 early stop: 26\n",
            "\u001b[32m[I 210629 03:55:38 models:110]\u001b[39m 699 1024 train loss: 0.0000848 valid loss: 0.3147539 P@1: 0.63000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56162 N@5: 0.53468 early stop: 27\n",
            "\u001b[33m[W 210629 03:55:40 models:137]\u001b[39m Clipping gradients with total norm 0.03212 and max norm 0.00496\n",
            "\u001b[32m[I 210629 03:55:41 models:110]\u001b[39m 702 512 train loss: 0.0002789 valid loss: 0.3150599 P@1: 0.63000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56162 N@5: 0.53468 early stop: 28\n",
            "\u001b[32m[I 210629 03:55:43 models:110]\u001b[39m 704 1024 train loss: 0.0001934 valid loss: 0.3153680 P@1: 0.63000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56162 N@5: 0.53468 early stop: 29\n",
            "\u001b[32m[I 210629 03:55:45 models:110]\u001b[39m 707 512 train loss: 0.0001829 valid loss: 0.3156792 P@1: 0.63000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56162 N@5: 0.53468 early stop: 30\n",
            "\u001b[32m[I 210629 03:55:47 models:110]\u001b[39m 709 1024 train loss: 0.0000814 valid loss: 0.3159896 P@1: 0.63000 P@3: 0.53000 P@5: 0.40400 N@3: 0.56396 N@5: 0.53495 early stop: 31\n",
            "\u001b[32m[I 210629 03:55:50 models:110]\u001b[39m 712 512 train loss: 0.0001181 valid loss: 0.3163012 P@1: 0.63000 P@3: 0.53000 P@5: 0.40400 N@3: 0.56396 N@5: 0.53515 early stop: 32\n",
            "\u001b[32m[I 210629 03:55:52 models:110]\u001b[39m 714 1024 train loss: 0.0000857 valid loss: 0.3166127 P@1: 0.63000 P@3: 0.53000 P@5: 0.40400 N@3: 0.56396 N@5: 0.53515 early stop: 33\n",
            "\u001b[32m[I 210629 03:55:54 models:110]\u001b[39m 717 512 train loss: 0.0001242 valid loss: 0.3169219 P@1: 0.63000 P@3: 0.53000 P@5: 0.40400 N@3: 0.56396 N@5: 0.53515 early stop: 34\n",
            "\u001b[33m[W 210629 03:55:56 models:137]\u001b[39m Clipping gradients with total norm 0.02125 and max norm 0.0027\n",
            "\u001b[32m[I 210629 03:55:56 models:110]\u001b[39m 719 1024 train loss: 0.0001993 valid loss: 0.3172313 P@1: 0.63000 P@3: 0.53000 P@5: 0.40400 N@3: 0.56396 N@5: 0.53515 early stop: 35\n",
            "\u001b[32m[I 210629 03:55:59 models:110]\u001b[39m 722 512 train loss: 0.0001400 valid loss: 0.3175398 P@1: 0.63000 P@3: 0.53000 P@5: 0.40400 N@3: 0.56396 N@5: 0.53515 early stop: 36\n",
            "\u001b[32m[I 210629 03:56:01 models:110]\u001b[39m 724 1024 train loss: 0.0000927 valid loss: 0.3178446 P@1: 0.63000 P@3: 0.53000 P@5: 0.40400 N@3: 0.56396 N@5: 0.53515 early stop: 37\n",
            "\u001b[33m[W 210629 03:56:01 models:137]\u001b[39m Clipping gradients with total norm 0.02757 and max norm 0.00265\n",
            "\u001b[32m[I 210629 03:56:03 models:110]\u001b[39m 727 512 train loss: 0.0002477 valid loss: 0.3181458 P@1: 0.63000 P@3: 0.53333 P@5: 0.40400 N@3: 0.56631 N@5: 0.53539 early stop: 38\n",
            "\u001b[32m[I 210629 03:56:05 models:110]\u001b[39m 729 1024 train loss: 0.0001307 valid loss: 0.3184461 P@1: 0.63000 P@3: 0.53333 P@5: 0.40600 N@3: 0.56631 N@5: 0.53720 early stop: 0\n",
            "\u001b[32m[I 210629 03:56:08 models:110]\u001b[39m 732 512 train loss: 0.0001220 valid loss: 0.3187497 P@1: 0.63000 P@3: 0.53000 P@5: 0.40600 N@3: 0.56396 N@5: 0.53688 early stop: 1\n",
            "\u001b[32m[I 210629 03:56:10 models:110]\u001b[39m 734 1024 train loss: 0.0000836 valid loss: 0.3190568 P@1: 0.63000 P@3: 0.53000 P@5: 0.40600 N@3: 0.56396 N@5: 0.53688 early stop: 2\n",
            "\u001b[32m[I 210629 03:56:12 models:110]\u001b[39m 737 512 train loss: 0.0001155 valid loss: 0.3193629 P@1: 0.63000 P@3: 0.53000 P@5: 0.40600 N@3: 0.56396 N@5: 0.53688 early stop: 3\n",
            "\u001b[32m[I 210629 03:56:14 models:110]\u001b[39m 739 1024 train loss: 0.0000771 valid loss: 0.3196678 P@1: 0.63000 P@3: 0.53000 P@5: 0.40600 N@3: 0.56396 N@5: 0.53688 early stop: 4\n",
            "\u001b[33m[W 210629 03:56:15 models:137]\u001b[39m Clipping gradients with total norm 0.02242 and max norm 0.00293\n",
            "\u001b[32m[I 210629 03:56:17 models:110]\u001b[39m 742 512 train loss: 0.0002078 valid loss: 0.3199663 P@1: 0.63000 P@3: 0.53000 P@5: 0.40600 N@3: 0.56396 N@5: 0.53688 early stop: 5\n",
            "\u001b[33m[W 210629 03:56:18 models:137]\u001b[39m Clipping gradients with total norm 0.04166 and max norm 0.00383\n",
            "\u001b[32m[I 210629 03:56:19 models:110]\u001b[39m 744 1024 train loss: 0.0005308 valid loss: 0.3202698 P@1: 0.63000 P@3: 0.53000 P@5: 0.40600 N@3: 0.56396 N@5: 0.53682 early stop: 6\n",
            "\u001b[32m[I 210629 03:56:22 models:110]\u001b[39m 747 512 train loss: 0.0000892 valid loss: 0.3205755 P@1: 0.63000 P@3: 0.53000 P@5: 0.40600 N@3: 0.56396 N@5: 0.53682 early stop: 7\n",
            "\u001b[33m[W 210629 03:56:22 models:137]\u001b[39m Clipping gradients with total norm 0.02932 and max norm 0.00153\n",
            "\u001b[32m[I 210629 03:56:24 models:110]\u001b[39m 749 1024 train loss: 0.0004257 valid loss: 0.3208777 P@1: 0.63000 P@3: 0.53000 P@5: 0.40600 N@3: 0.56396 N@5: 0.53682 early stop: 8\n",
            "\u001b[32m[I 210629 03:56:26 models:110]\u001b[39m 752 512 train loss: 0.0000803 valid loss: 0.3211738 P@1: 0.63000 P@3: 0.53000 P@5: 0.40600 N@3: 0.56396 N@5: 0.53682 early stop: 9\n",
            "\u001b[32m[I 210629 03:56:28 models:110]\u001b[39m 754 1024 train loss: 0.0001139 valid loss: 0.3214684 P@1: 0.63000 P@3: 0.53000 P@5: 0.40800 N@3: 0.56396 N@5: 0.53813 early stop: 0\n",
            "\u001b[32m[I 210629 03:56:31 models:110]\u001b[39m 757 512 train loss: 0.0000954 valid loss: 0.3217660 P@1: 0.63000 P@3: 0.53000 P@5: 0.40800 N@3: 0.56396 N@5: 0.53813 early stop: 1\n",
            "\u001b[33m[W 210629 03:56:32 models:137]\u001b[39m Clipping gradients with total norm 0.02256 and max norm 0.00324\n",
            "\u001b[32m[I 210629 03:56:33 models:110]\u001b[39m 759 1024 train loss: 0.0005202 valid loss: 0.3220652 P@1: 0.63000 P@3: 0.53000 P@5: 0.40800 N@3: 0.56396 N@5: 0.53813 early stop: 2\n",
            "\u001b[32m[I 210629 03:56:35 models:110]\u001b[39m 762 512 train loss: 0.0001182 valid loss: 0.3223628 P@1: 0.63000 P@3: 0.53000 P@5: 0.40800 N@3: 0.56396 N@5: 0.53834 early stop: 0\n",
            "\u001b[32m[I 210629 03:56:37 models:110]\u001b[39m 764 1024 train loss: 0.0001182 valid loss: 0.3226602 P@1: 0.63000 P@3: 0.53000 P@5: 0.40800 N@3: 0.56396 N@5: 0.53834 early stop: 1\n",
            "\u001b[33m[W 210629 03:56:39 models:137]\u001b[39m Clipping gradients with total norm 0.0367 and max norm 0.00428\n",
            "\u001b[32m[I 210629 03:56:40 models:110]\u001b[39m 767 512 train loss: 0.0003561 valid loss: 0.3229493 P@1: 0.63000 P@3: 0.53333 P@5: 0.40800 N@3: 0.56631 N@5: 0.53857 early stop: 0\n",
            "\u001b[32m[I 210629 03:56:42 models:110]\u001b[39m 769 1024 train loss: 0.0001480 valid loss: 0.3232237 P@1: 0.63000 P@3: 0.53333 P@5: 0.40800 N@3: 0.56631 N@5: 0.53857 early stop: 1\n",
            "\u001b[33m[W 210629 03:56:43 models:137]\u001b[39m Clipping gradients with total norm 0.04359 and max norm 0.00229\n",
            "\u001b[32m[I 210629 03:56:44 models:110]\u001b[39m 772 512 train loss: 0.0004730 valid loss: 0.3235025 P@1: 0.63000 P@3: 0.53333 P@5: 0.41000 N@3: 0.56631 N@5: 0.54008 early stop: 0\n",
            "\u001b[32m[I 210629 03:56:47 models:110]\u001b[39m 774 1024 train loss: 0.0001013 valid loss: 0.3237866 P@1: 0.63000 P@3: 0.53333 P@5: 0.41000 N@3: 0.56631 N@5: 0.54008 early stop: 1\n",
            "\u001b[32m[I 210629 03:56:49 models:110]\u001b[39m 777 512 train loss: 0.0001258 valid loss: 0.3240679 P@1: 0.63000 P@3: 0.53333 P@5: 0.41000 N@3: 0.56631 N@5: 0.54008 early stop: 2\n",
            "\u001b[32m[I 210629 03:56:51 models:110]\u001b[39m 779 1024 train loss: 0.0000816 valid loss: 0.3243493 P@1: 0.63000 P@3: 0.53667 P@5: 0.41000 N@3: 0.56866 N@5: 0.54032 early stop: 0\n",
            "\u001b[32m[I 210629 03:56:54 models:110]\u001b[39m 782 512 train loss: 0.0001242 valid loss: 0.3246351 P@1: 0.63000 P@3: 0.53667 P@5: 0.41000 N@3: 0.56866 N@5: 0.54032 early stop: 1\n",
            "\u001b[32m[I 210629 03:56:56 models:110]\u001b[39m 784 1024 train loss: 0.0000819 valid loss: 0.3249200 P@1: 0.63000 P@3: 0.53667 P@5: 0.41000 N@3: 0.56866 N@5: 0.54032 early stop: 2\n",
            "\u001b[33m[W 210629 03:56:57 models:137]\u001b[39m Clipping gradients with total norm 0.02097 and max norm 0.0029\n",
            "\u001b[32m[I 210629 03:56:58 models:110]\u001b[39m 787 512 train loss: 0.0002673 valid loss: 0.3252015 P@1: 0.63000 P@3: 0.53667 P@5: 0.41000 N@3: 0.56804 N@5: 0.53987 early stop: 3\n",
            "\u001b[32m[I 210629 03:57:00 models:110]\u001b[39m 789 1024 train loss: 0.0001042 valid loss: 0.3254836 P@1: 0.63000 P@3: 0.53667 P@5: 0.41000 N@3: 0.56804 N@5: 0.53987 early stop: 4\n",
            "\u001b[32m[I 210629 03:57:03 models:110]\u001b[39m 792 512 train loss: 0.0001141 valid loss: 0.3257678 P@1: 0.63000 P@3: 0.53667 P@5: 0.41000 N@3: 0.56804 N@5: 0.53987 early stop: 5\n",
            "\u001b[32m[I 210629 03:57:05 models:110]\u001b[39m 794 1024 train loss: 0.0000952 valid loss: 0.3260497 P@1: 0.63000 P@3: 0.53667 P@5: 0.41000 N@3: 0.56804 N@5: 0.53987 early stop: 6\n",
            "\u001b[32m[I 210629 03:57:07 models:110]\u001b[39m 797 512 train loss: 0.0000904 valid loss: 0.3263313 P@1: 0.63000 P@3: 0.53667 P@5: 0.41000 N@3: 0.56804 N@5: 0.53987 early stop: 7\n",
            "\u001b[32m[I 210629 03:57:09 models:110]\u001b[39m 799 1024 train loss: 0.0000980 valid loss: 0.3266129 P@1: 0.63000 P@3: 0.53667 P@5: 0.41000 N@3: 0.56804 N@5: 0.53987 early stop: 8\n",
            "\u001b[32m[I 210629 03:57:12 models:110]\u001b[39m 802 512 train loss: 0.0000909 valid loss: 0.3268962 P@1: 0.63000 P@3: 0.53667 P@5: 0.41000 N@3: 0.56804 N@5: 0.53987 early stop: 9\n",
            "\u001b[32m[I 210629 03:57:14 models:110]\u001b[39m 804 1024 train loss: 0.0000970 valid loss: 0.3271824 P@1: 0.63000 P@3: 0.53667 P@5: 0.41000 N@3: 0.56804 N@5: 0.53987 early stop: 10\n",
            "\u001b[32m[I 210629 03:57:17 models:110]\u001b[39m 807 512 train loss: 0.0001075 valid loss: 0.3274702 P@1: 0.63000 P@3: 0.53667 P@5: 0.41000 N@3: 0.56804 N@5: 0.53987 early stop: 11\n",
            "\u001b[32m[I 210629 03:57:19 models:110]\u001b[39m 809 1024 train loss: 0.0000810 valid loss: 0.3277589 P@1: 0.63000 P@3: 0.53667 P@5: 0.41000 N@3: 0.56804 N@5: 0.53987 early stop: 12\n",
            "\u001b[33m[W 210629 03:57:20 models:137]\u001b[39m Clipping gradients with total norm 0.10506 and max norm 0.00314\n",
            "\u001b[32m[I 210629 03:57:21 models:110]\u001b[39m 812 512 train loss: 0.0003652 valid loss: 0.3280444 P@1: 0.63000 P@3: 0.53667 P@5: 0.41000 N@3: 0.56804 N@5: 0.53987 early stop: 13\n",
            "\u001b[33m[W 210629 03:57:22 models:137]\u001b[39m Clipping gradients with total norm 0.0267 and max norm 0.00228\n",
            "\u001b[32m[I 210629 03:57:23 models:110]\u001b[39m 814 1024 train loss: 0.0002585 valid loss: 0.3283264 P@1: 0.63000 P@3: 0.53667 P@5: 0.41000 N@3: 0.56804 N@5: 0.53987 early stop: 14\n",
            "\u001b[32m[I 210629 03:57:26 models:110]\u001b[39m 817 512 train loss: 0.0000788 valid loss: 0.3286080 P@1: 0.63000 P@3: 0.53667 P@5: 0.41000 N@3: 0.56804 N@5: 0.53987 early stop: 15\n",
            "\u001b[32m[I 210629 03:57:28 models:110]\u001b[39m 819 1024 train loss: 0.0001240 valid loss: 0.3288899 P@1: 0.63000 P@3: 0.53667 P@5: 0.41200 N@3: 0.56804 N@5: 0.54119 early stop: 0\n",
            "\u001b[32m[I 210629 03:57:30 models:110]\u001b[39m 822 512 train loss: 0.0000937 valid loss: 0.3291690 P@1: 0.63000 P@3: 0.53667 P@5: 0.41200 N@3: 0.56804 N@5: 0.54119 early stop: 1\n",
            "\u001b[33m[W 210629 03:57:32 models:137]\u001b[39m Clipping gradients with total norm 0.02349 and max norm 0.0025\n",
            "\u001b[32m[I 210629 03:57:32 models:110]\u001b[39m 824 1024 train loss: 0.0002327 valid loss: 0.3294464 P@1: 0.63000 P@3: 0.53667 P@5: 0.41200 N@3: 0.56804 N@5: 0.54119 early stop: 2\n",
            "\u001b[32m[I 210629 03:57:35 models:110]\u001b[39m 827 512 train loss: 0.0001332 valid loss: 0.3297260 P@1: 0.63000 P@3: 0.53667 P@5: 0.41200 N@3: 0.56804 N@5: 0.54119 early stop: 3\n",
            "\u001b[32m[I 210629 03:57:37 models:110]\u001b[39m 829 1024 train loss: 0.0000861 valid loss: 0.3300047 P@1: 0.63000 P@3: 0.53667 P@5: 0.41200 N@3: 0.56804 N@5: 0.54119 early stop: 4\n",
            "\u001b[32m[I 210629 03:57:39 models:110]\u001b[39m 832 512 train loss: 0.0001139 valid loss: 0.3302839 P@1: 0.63000 P@3: 0.53667 P@5: 0.41200 N@3: 0.56804 N@5: 0.54119 early stop: 5\n",
            "\u001b[33m[W 210629 03:57:41 models:137]\u001b[39m Clipping gradients with total norm 0.02452 and max norm 0.00258\n",
            "\u001b[32m[I 210629 03:57:41 models:110]\u001b[39m 834 1024 train loss: 0.0002368 valid loss: 0.3305653 P@1: 0.63000 P@3: 0.53667 P@5: 0.41200 N@3: 0.56804 N@5: 0.54119 early stop: 6\n",
            "\u001b[32m[I 210629 03:57:44 models:110]\u001b[39m 837 512 train loss: 0.0001349 valid loss: 0.3308445 P@1: 0.63000 P@3: 0.53667 P@5: 0.41200 N@3: 0.56804 N@5: 0.54119 early stop: 7\n",
            "\u001b[32m[I 210629 03:57:46 models:110]\u001b[39m 839 1024 train loss: 0.0000858 valid loss: 0.3311227 P@1: 0.63000 P@3: 0.53667 P@5: 0.41400 N@3: 0.56804 N@5: 0.54250 early stop: 0\n",
            "\u001b[32m[I 210629 03:57:48 models:110]\u001b[39m 842 512 train loss: 0.0001021 valid loss: 0.3314000 P@1: 0.63000 P@3: 0.53667 P@5: 0.41400 N@3: 0.56804 N@5: 0.54250 early stop: 1\n",
            "\u001b[32m[I 210629 03:57:51 models:110]\u001b[39m 844 1024 train loss: 0.0000903 valid loss: 0.3316787 P@1: 0.63000 P@3: 0.53667 P@5: 0.41400 N@3: 0.56804 N@5: 0.54250 early stop: 2\n",
            "\u001b[32m[I 210629 03:57:53 models:110]\u001b[39m 847 512 train loss: 0.0001170 valid loss: 0.3319582 P@1: 0.63000 P@3: 0.53667 P@5: 0.41400 N@3: 0.56804 N@5: 0.54250 early stop: 3\n",
            "\u001b[32m[I 210629 03:57:55 models:110]\u001b[39m 849 1024 train loss: 0.0000765 valid loss: 0.3322383 P@1: 0.63000 P@3: 0.53667 P@5: 0.41400 N@3: 0.56804 N@5: 0.54250 early stop: 4\n",
            "\u001b[32m[I 210629 03:57:58 models:110]\u001b[39m 852 512 train loss: 0.0000936 valid loss: 0.3325184 P@1: 0.63000 P@3: 0.53667 P@5: 0.41400 N@3: 0.56804 N@5: 0.54250 early stop: 5\n",
            "\u001b[32m[I 210629 03:58:00 models:110]\u001b[39m 854 1024 train loss: 0.0000965 valid loss: 0.3327983 P@1: 0.63000 P@3: 0.53667 P@5: 0.41400 N@3: 0.56804 N@5: 0.54250 early stop: 6\n",
            "\u001b[32m[I 210629 03:58:02 models:110]\u001b[39m 857 512 train loss: 0.0001027 valid loss: 0.3330782 P@1: 0.63000 P@3: 0.53667 P@5: 0.41400 N@3: 0.56804 N@5: 0.54250 early stop: 7\n",
            "\u001b[32m[I 210629 03:58:04 models:110]\u001b[39m 859 1024 train loss: 0.0000891 valid loss: 0.3333576 P@1: 0.63000 P@3: 0.53667 P@5: 0.41400 N@3: 0.56804 N@5: 0.54235 early stop: 8\n",
            "\u001b[32m[I 210629 03:58:07 models:110]\u001b[39m 862 512 train loss: 0.0000988 valid loss: 0.3336396 P@1: 0.63000 P@3: 0.53667 P@5: 0.41400 N@3: 0.56804 N@5: 0.54235 early stop: 9\n",
            "\u001b[32m[I 210629 03:58:09 models:110]\u001b[39m 864 1024 train loss: 0.0000911 valid loss: 0.3339217 P@1: 0.63000 P@3: 0.53667 P@5: 0.41400 N@3: 0.56804 N@5: 0.54235 early stop: 10\n",
            "\u001b[33m[W 210629 03:58:11 models:137]\u001b[39m Clipping gradients with total norm 0.01548 and max norm 0.00103\n",
            "\u001b[32m[I 210629 03:58:11 models:110]\u001b[39m 867 512 train loss: 0.0001193 valid loss: 0.3341926 P@1: 0.63000 P@3: 0.53667 P@5: 0.41400 N@3: 0.56804 N@5: 0.54235 early stop: 11\n",
            "\u001b[32m[I 210629 03:58:13 models:110]\u001b[39m 869 1024 train loss: 0.0000740 valid loss: 0.3344651 P@1: 0.63000 P@3: 0.53667 P@5: 0.41400 N@3: 0.56804 N@5: 0.54235 early stop: 12\n",
            "\u001b[32m[I 210629 03:58:16 models:110]\u001b[39m 872 512 train loss: 0.0001098 valid loss: 0.3347396 P@1: 0.63000 P@3: 0.53667 P@5: 0.41400 N@3: 0.56804 N@5: 0.54235 early stop: 13\n",
            "\u001b[32m[I 210629 03:58:18 models:110]\u001b[39m 874 1024 train loss: 0.0000754 valid loss: 0.3350178 P@1: 0.63000 P@3: 0.53667 P@5: 0.41400 N@3: 0.56804 N@5: 0.54235 early stop: 14\n",
            "\u001b[32m[I 210629 03:58:20 models:110]\u001b[39m 877 512 train loss: 0.0000975 valid loss: 0.3352980 P@1: 0.63000 P@3: 0.53667 P@5: 0.41400 N@3: 0.56804 N@5: 0.54235 early stop: 15\n",
            "\u001b[32m[I 210629 03:58:22 models:110]\u001b[39m 879 1024 train loss: 0.0000937 valid loss: 0.3355828 P@1: 0.63000 P@3: 0.53667 P@5: 0.41400 N@3: 0.56804 N@5: 0.54235 early stop: 16\n",
            "\u001b[32m[I 210629 03:58:25 models:110]\u001b[39m 882 512 train loss: 0.0000930 valid loss: 0.3358656 P@1: 0.63000 P@3: 0.53667 P@5: 0.41400 N@3: 0.56804 N@5: 0.54235 early stop: 17\n",
            "\u001b[32m[I 210629 03:58:27 models:110]\u001b[39m 884 1024 train loss: 0.0000949 valid loss: 0.3361461 P@1: 0.63000 P@3: 0.53667 P@5: 0.41400 N@3: 0.56804 N@5: 0.54235 early stop: 18\n",
            "\u001b[32m[I 210629 03:58:29 models:110]\u001b[39m 887 512 train loss: 0.0001135 valid loss: 0.3364269 P@1: 0.63000 P@3: 0.53667 P@5: 0.41400 N@3: 0.56804 N@5: 0.54235 early stop: 19\n",
            "\u001b[32m[I 210629 03:58:31 models:110]\u001b[39m 889 1024 train loss: 0.0000761 valid loss: 0.3367070 P@1: 0.63000 P@3: 0.53667 P@5: 0.41400 N@3: 0.56804 N@5: 0.54235 early stop: 20\n",
            "\u001b[32m[I 210629 03:58:34 models:110]\u001b[39m 892 512 train loss: 0.0000958 valid loss: 0.3369895 P@1: 0.63000 P@3: 0.53667 P@5: 0.41400 N@3: 0.56804 N@5: 0.54235 early stop: 21\n",
            "\u001b[33m[W 210629 03:58:35 models:137]\u001b[39m Clipping gradients with total norm 0.02415 and max norm 0.00287\n",
            "\u001b[32m[I 210629 03:58:36 models:110]\u001b[39m 894 1024 train loss: 0.0002427 valid loss: 0.3372737 P@1: 0.63000 P@3: 0.53667 P@5: 0.41400 N@3: 0.56804 N@5: 0.54235 early stop: 22\n",
            "\u001b[32m[I 210629 03:58:38 models:110]\u001b[39m 897 512 train loss: 0.0001649 valid loss: 0.3375576 P@1: 0.63000 P@3: 0.53667 P@5: 0.41400 N@3: 0.56804 N@5: 0.54235 early stop: 23\n",
            "\u001b[32m[I 210629 03:58:40 models:110]\u001b[39m 899 1024 train loss: 0.0001613 valid loss: 0.3378308 P@1: 0.63000 P@3: 0.53667 P@5: 0.41400 N@3: 0.56804 N@5: 0.54235 early stop: 24\n",
            "\u001b[32m[I 210629 03:58:43 models:110]\u001b[39m 902 512 train loss: 0.0001156 valid loss: 0.3381036 P@1: 0.63000 P@3: 0.53667 P@5: 0.41400 N@3: 0.56804 N@5: 0.54235 early stop: 25\n",
            "\u001b[33m[W 210629 03:58:43 models:137]\u001b[39m Clipping gradients with total norm 0.01937 and max norm 0.00271\n",
            "\u001b[32m[I 210629 03:58:45 models:110]\u001b[39m 904 1024 train loss: 0.0002416 valid loss: 0.3383781 P@1: 0.63000 P@3: 0.53667 P@5: 0.41400 N@3: 0.56804 N@5: 0.54235 early stop: 26\n",
            "\u001b[32m[I 210629 03:58:47 models:110]\u001b[39m 907 512 train loss: 0.0000933 valid loss: 0.3386541 P@1: 0.63000 P@3: 0.53667 P@5: 0.41400 N@3: 0.56804 N@5: 0.54235 early stop: 27\n",
            "\u001b[32m[I 210629 03:58:49 models:110]\u001b[39m 909 1024 train loss: 0.0001142 valid loss: 0.3389298 P@1: 0.63000 P@3: 0.53667 P@5: 0.41400 N@3: 0.56804 N@5: 0.54235 early stop: 28\n",
            "\u001b[32m[I 210629 03:58:52 models:110]\u001b[39m 912 512 train loss: 0.0000969 valid loss: 0.3392060 P@1: 0.63000 P@3: 0.53667 P@5: 0.41400 N@3: 0.56866 N@5: 0.54279 early stop: 0\n",
            "\u001b[32m[I 210629 03:58:54 models:110]\u001b[39m 914 1024 train loss: 0.0000908 valid loss: 0.3394809 P@1: 0.63000 P@3: 0.53667 P@5: 0.41400 N@3: 0.56866 N@5: 0.54279 early stop: 1\n",
            "\u001b[32m[I 210629 03:58:56 models:110]\u001b[39m 917 512 train loss: 0.0000976 valid loss: 0.3397545 P@1: 0.63000 P@3: 0.53667 P@5: 0.41400 N@3: 0.56866 N@5: 0.54279 early stop: 2\n",
            "\u001b[32m[I 210629 03:58:59 models:110]\u001b[39m 919 1024 train loss: 0.0000943 valid loss: 0.3400269 P@1: 0.63000 P@3: 0.53667 P@5: 0.41400 N@3: 0.56866 N@5: 0.54279 early stop: 3\n",
            "\u001b[32m[I 210629 03:59:01 models:110]\u001b[39m 922 512 train loss: 0.0000901 valid loss: 0.3403006 P@1: 0.63000 P@3: 0.53667 P@5: 0.41400 N@3: 0.56866 N@5: 0.54279 early stop: 4\n",
            "\u001b[32m[I 210629 03:59:03 models:110]\u001b[39m 924 1024 train loss: 0.0000897 valid loss: 0.3405771 P@1: 0.63000 P@3: 0.53667 P@5: 0.41400 N@3: 0.56866 N@5: 0.54294 early stop: 0\n",
            "\u001b[32m[I 210629 03:59:06 models:110]\u001b[39m 927 512 train loss: 0.0000958 valid loss: 0.3408536 P@1: 0.63000 P@3: 0.53667 P@5: 0.41400 N@3: 0.56866 N@5: 0.54294 early stop: 1\n",
            "\u001b[32m[I 210629 03:59:08 models:110]\u001b[39m 929 1024 train loss: 0.0000945 valid loss: 0.3411314 P@1: 0.63000 P@3: 0.53667 P@5: 0.41400 N@3: 0.56866 N@5: 0.54294 early stop: 2\n",
            "\u001b[32m[I 210629 03:59:10 models:110]\u001b[39m 932 512 train loss: 0.0000741 valid loss: 0.3414093 P@1: 0.63000 P@3: 0.53667 P@5: 0.41400 N@3: 0.56866 N@5: 0.54294 early stop: 3\n",
            "\u001b[32m[I 210629 03:59:12 models:110]\u001b[39m 934 1024 train loss: 0.0001113 valid loss: 0.3416850 P@1: 0.63000 P@3: 0.53667 P@5: 0.41400 N@3: 0.56866 N@5: 0.54294 early stop: 4\n",
            "\u001b[32m[I 210629 03:59:15 models:110]\u001b[39m 937 512 train loss: 0.0001146 valid loss: 0.3419598 P@1: 0.63000 P@3: 0.53667 P@5: 0.41400 N@3: 0.56866 N@5: 0.54294 early stop: 5\n",
            "\u001b[32m[I 210629 03:59:17 models:110]\u001b[39m 939 1024 train loss: 0.0000758 valid loss: 0.3422349 P@1: 0.63000 P@3: 0.53667 P@5: 0.41400 N@3: 0.56866 N@5: 0.54294 early stop: 6\n",
            "\u001b[32m[I 210629 03:59:19 models:110]\u001b[39m 942 512 train loss: 0.0000884 valid loss: 0.3425103 P@1: 0.63000 P@3: 0.53667 P@5: 0.41400 N@3: 0.56866 N@5: 0.54294 early stop: 7\n",
            "\u001b[32m[I 210629 03:59:21 models:110]\u001b[39m 944 1024 train loss: 0.0000949 valid loss: 0.3427841 P@1: 0.63000 P@3: 0.53667 P@5: 0.41400 N@3: 0.56866 N@5: 0.54294 early stop: 8\n",
            "\u001b[32m[I 210629 03:59:24 models:110]\u001b[39m 947 512 train loss: 0.0000898 valid loss: 0.3430569 P@1: 0.63000 P@3: 0.53667 P@5: 0.41400 N@3: 0.56866 N@5: 0.54294 early stop: 9\n",
            "\u001b[33m[W 210629 03:59:24 models:137]\u001b[39m Clipping gradients with total norm 0.02059 and max norm 0.00242\n",
            "\u001b[32m[I 210629 03:59:26 models:110]\u001b[39m 949 1024 train loss: 0.0002279 valid loss: 0.3433367 P@1: 0.63000 P@3: 0.53667 P@5: 0.41600 N@3: 0.56866 N@5: 0.54425 early stop: 0\n",
            "\u001b[32m[I 210629 03:59:29 models:110]\u001b[39m 952 512 train loss: 0.0001000 valid loss: 0.3436143 P@1: 0.63000 P@3: 0.53333 P@5: 0.41600 N@3: 0.56631 N@5: 0.54402 early stop: 1\n",
            "\u001b[32m[I 210629 03:59:31 models:110]\u001b[39m 954 1024 train loss: 0.0001170 valid loss: 0.3438893 P@1: 0.63000 P@3: 0.53333 P@5: 0.41600 N@3: 0.56631 N@5: 0.54402 early stop: 2\n",
            "\u001b[33m[W 210629 03:59:32 models:137]\u001b[39m Clipping gradients with total norm 0.02547 and max norm 0.00282\n",
            "\u001b[32m[I 210629 03:59:33 models:110]\u001b[39m 957 512 train loss: 0.0002791 valid loss: 0.3441658 P@1: 0.63000 P@3: 0.53333 P@5: 0.41600 N@3: 0.56631 N@5: 0.54402 early stop: 3\n",
            "\u001b[32m[I 210629 03:59:35 models:110]\u001b[39m 959 1024 train loss: 0.0001274 valid loss: 0.3444453 P@1: 0.63000 P@3: 0.53333 P@5: 0.41600 N@3: 0.56631 N@5: 0.54402 early stop: 4\n",
            "\u001b[33m[W 210629 03:59:37 models:137]\u001b[39m Clipping gradients with total norm 0.03315 and max norm 0.00445\n",
            "\u001b[32m[I 210629 03:59:38 models:110]\u001b[39m 962 512 train loss: 0.0004759 valid loss: 0.3447264 P@1: 0.63000 P@3: 0.53333 P@5: 0.41600 N@3: 0.56631 N@5: 0.54402 early stop: 5\n",
            "\u001b[32m[I 210629 03:59:40 models:110]\u001b[39m 964 1024 train loss: 0.0001476 valid loss: 0.3449990 P@1: 0.63000 P@3: 0.53333 P@5: 0.41400 N@3: 0.56631 N@5: 0.54271 early stop: 6\n",
            "\u001b[32m[I 210629 03:59:42 models:110]\u001b[39m 967 512 train loss: 0.0001552 valid loss: 0.3452645 P@1: 0.63000 P@3: 0.53333 P@5: 0.41400 N@3: 0.56631 N@5: 0.54271 early stop: 7\n",
            "\u001b[32m[I 210629 03:59:44 models:110]\u001b[39m 969 1024 train loss: 0.0001102 valid loss: 0.3455258 P@1: 0.63000 P@3: 0.53333 P@5: 0.41600 N@3: 0.56631 N@5: 0.54402 early stop: 8\n",
            "\u001b[32m[I 210629 03:59:47 models:110]\u001b[39m 972 512 train loss: 0.0001481 valid loss: 0.3457909 P@1: 0.63000 P@3: 0.53333 P@5: 0.41600 N@3: 0.56631 N@5: 0.54402 early stop: 9\n",
            "\u001b[32m[I 210629 03:59:49 models:110]\u001b[39m 974 1024 train loss: 0.0000736 valid loss: 0.3460522 P@1: 0.63000 P@3: 0.53333 P@5: 0.41600 N@3: 0.56631 N@5: 0.54402 early stop: 10\n",
            "\u001b[32m[I 210629 03:59:51 models:110]\u001b[39m 977 512 train loss: 0.0000725 valid loss: 0.3463112 P@1: 0.63000 P@3: 0.53333 P@5: 0.41600 N@3: 0.56631 N@5: 0.54402 early stop: 11\n",
            "\u001b[32m[I 210629 03:59:53 models:110]\u001b[39m 979 1024 train loss: 0.0001229 valid loss: 0.3465680 P@1: 0.63000 P@3: 0.53333 P@5: 0.41600 N@3: 0.56631 N@5: 0.54402 early stop: 12\n",
            "\u001b[33m[W 210629 03:59:55 models:137]\u001b[39m Clipping gradients with total norm 0.02615 and max norm 0.00355\n",
            "\u001b[32m[I 210629 03:59:56 models:110]\u001b[39m 982 512 train loss: 0.0002337 valid loss: 0.3468263 P@1: 0.63000 P@3: 0.53333 P@5: 0.41600 N@3: 0.56631 N@5: 0.54402 early stop: 13\n",
            "\u001b[32m[I 210629 03:59:58 models:110]\u001b[39m 984 1024 train loss: 0.0001079 valid loss: 0.3470821 P@1: 0.63000 P@3: 0.53333 P@5: 0.41600 N@3: 0.56631 N@5: 0.54402 early stop: 14\n",
            "\u001b[33m[W 210629 03:59:59 models:137]\u001b[39m Clipping gradients with total norm 0.00402 and max norm 0.00034\n",
            "\u001b[33m[W 210629 03:59:59 models:137]\u001b[39m Clipping gradients with total norm 0.00914 and max norm 0.00068\n",
            "\u001b[32m[I 210629 04:00:00 models:110]\u001b[39m 987 512 train loss: 0.0001740 valid loss: 0.3473357 P@1: 0.63000 P@3: 0.53333 P@5: 0.41600 N@3: 0.56631 N@5: 0.54402 early stop: 15\n",
            "\u001b[33m[W 210629 04:00:02 models:137]\u001b[39m Clipping gradients with total norm 0.01707 and max norm 0.00275\n",
            "\u001b[32m[I 210629 04:00:03 models:110]\u001b[39m 989 1024 train loss: 0.0002056 valid loss: 0.3475892 P@1: 0.63000 P@3: 0.53333 P@5: 0.41400 N@3: 0.56631 N@5: 0.54271 early stop: 16\n",
            "\u001b[32m[I 210629 04:00:05 models:110]\u001b[39m 992 512 train loss: 0.0001416 valid loss: 0.3478440 P@1: 0.63000 P@3: 0.53333 P@5: 0.41400 N@3: 0.56631 N@5: 0.54271 early stop: 17\n",
            "\u001b[32m[I 210629 04:00:07 models:110]\u001b[39m 994 1024 train loss: 0.0001024 valid loss: 0.3481007 P@1: 0.63000 P@3: 0.53333 P@5: 0.41400 N@3: 0.56631 N@5: 0.54271 early stop: 18\n",
            "\u001b[32m[I 210629 04:00:09 models:110]\u001b[39m 997 512 train loss: 0.0001180 valid loss: 0.3483581 P@1: 0.63000 P@3: 0.53333 P@5: 0.41400 N@3: 0.56631 N@5: 0.54271 early stop: 19\n",
            "\u001b[32m[I 210629 04:00:12 models:110]\u001b[39m 999 1024 train loss: 0.0000819 valid loss: 0.3486112 P@1: 0.63000 P@3: 0.53333 P@5: 0.41400 N@3: 0.56631 N@5: 0.54271 early stop: 20\n",
            "\u001b[32m[I 210629 04:00:12 main:76]\u001b[39m Finish Training\n",
            "\u001b[32m[I 210629 04:00:13 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210629 04:00:13 main:79]\u001b[39m Loading Test Set\n",
            "\u001b[32m[I 210629 04:00:13 main:83]\u001b[39m Size of Test Set: 100\n",
            "\u001b[32m[I 210629 04:00:13 main:85]\u001b[39m Predicting\n",
            "\u001b[32m[I 210629 04:00:17 main:91]\u001b[39m Finish Predicting\n",
            "Precision@1,3,5: 0.62 0.49333333333333335 0.394\n",
            "nDCG@1,3,5: 0.62 0.5332755696721645 0.5247262321725323\n",
            "\n",
            "WITH MAG NO MESH, skip=500\n",
            "\n",
            "/content/drive/Shareddrives/NASA/MATCH/PeTaL\n",
            "131\n",
            "/content/drive/Shareddrives/NASA/MATCH\n",
            "    800  260480 4295970 PeTaL/train.json\n",
            "\u001b[32m[I 210629 04:00:20 preprocess:28]\u001b[39m Vocab Size: 26834\n",
            "\u001b[32m[I 210629 04:00:20 preprocess:30]\u001b[39m Getting Dataset: PeTaL/train_texts.txt Max Length: 500\n",
            "\u001b[32m[I 210629 04:00:20 preprocess:32]\u001b[39m Size of Samples: 900\n",
            "\u001b[32m[I 210629 04:00:21 preprocess:28]\u001b[39m Vocab Size: 26834\n",
            "\u001b[32m[I 210629 04:00:21 preprocess:30]\u001b[39m Getting Dataset: PeTaL/test_texts.txt Max Length: 500\n",
            "\u001b[32m[I 210629 04:00:21 preprocess:32]\u001b[39m Size of Samples: 100\n",
            "\u001b[32m[I 210629 04:00:23 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210629 04:00:23 main:35]\u001b[39m Loading Training and Validation Set\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['absorb_and/or_filter_solids', 'chemically_break_down_inorganic_compounds', 'detox/purify', 'manage_environmental_disturbances_in_a_community', 'protect_from_fire', 'protect_from_gases', 'send_vibratory_signals'] will be ignored\n",
            "  .format(sorted(unknown, key=str)))\n",
            "\u001b[32m[I 210629 04:00:23 main:47]\u001b[39m Number of Labels: 124\n",
            "\u001b[32m[I 210629 04:00:23 main:48]\u001b[39m Size of Training Set: 800\n",
            "\u001b[32m[I 210629 04:00:23 main:49]\u001b[39m Size of Validation Set: 100\n",
            "\u001b[32m[I 210629 04:00:23 main:66]\u001b[39m Number of Edges: 101\n",
            "\u001b[32m[I 210629 04:00:23 main:68]\u001b[39m Training\n",
            "\u001b[32m[I 210629 04:00:29 models:110]\u001b[39m 2 512 train loss: 0.2771795 valid loss: 0.1541469 P@1: 0.33000 P@3: 0.19667 P@5: 0.16400 N@3: 0.22386 N@5: 0.22384 early stop: 0\n",
            "\u001b[32m[I 210629 04:00:30 models:142]\u001b[39m SWA Initializing\n",
            "\u001b[32m[I 210629 04:00:31 models:110]\u001b[39m 4 1024 train loss: 0.1447734 valid loss: 0.1382642 P@1: 0.33000 P@3: 0.22000 P@5: 0.18400 N@3: 0.24397 N@5: 0.24337 early stop: 0\n",
            "\u001b[32m[I 210629 04:00:33 models:110]\u001b[39m 7 512 train loss: 0.1442959 valid loss: 0.1372105 P@1: 0.33000 P@3: 0.22000 P@5: 0.18400 N@3: 0.24397 N@5: 0.24346 early stop: 0\n",
            "\u001b[32m[I 210629 04:00:35 models:110]\u001b[39m 9 1024 train loss: 0.1384510 valid loss: 0.1366915 P@1: 0.33000 P@3: 0.22000 P@5: 0.18400 N@3: 0.24274 N@5: 0.24230 early stop: 1\n",
            "\u001b[32m[I 210629 04:00:38 models:110]\u001b[39m 12 512 train loss: 0.1332726 valid loss: 0.1362794 P@1: 0.33000 P@3: 0.25333 P@5: 0.18600 N@3: 0.27358 N@5: 0.25366 early stop: 0\n",
            "\u001b[32m[I 210629 04:00:40 models:110]\u001b[39m 14 1024 train loss: 0.1191194 valid loss: 0.1353962 P@1: 0.33000 P@3: 0.23000 P@5: 0.21000 N@3: 0.25592 N@5: 0.26870 early stop: 0\n",
            "\u001b[32m[I 210629 04:00:42 models:110]\u001b[39m 17 512 train loss: 0.0988421 valid loss: 0.1343497 P@1: 0.32000 P@3: 0.26333 P@5: 0.20400 N@3: 0.27642 N@5: 0.26472 early stop: 1\n",
            "\u001b[32m[I 210629 04:00:45 models:110]\u001b[39m 19 1024 train loss: 0.0835380 valid loss: 0.1336757 P@1: 0.36000 P@3: 0.26667 P@5: 0.21400 N@3: 0.28693 N@5: 0.28126 early stop: 0\n",
            "\u001b[32m[I 210629 04:00:47 models:110]\u001b[39m 22 512 train loss: 0.0694853 valid loss: 0.1331223 P@1: 0.35000 P@3: 0.29000 P@5: 0.22400 N@3: 0.30408 N@5: 0.29466 early stop: 0\n",
            "\u001b[32m[I 210629 04:00:49 models:110]\u001b[39m 24 1024 train loss: 0.0570861 valid loss: 0.1326132 P@1: 0.36000 P@3: 0.30000 P@5: 0.24400 N@3: 0.31683 N@5: 0.31960 early stop: 0\n",
            "\u001b[32m[I 210629 04:00:52 models:110]\u001b[39m 27 512 train loss: 0.0478380 valid loss: 0.1323250 P@1: 0.38000 P@3: 0.31333 P@5: 0.25800 N@3: 0.32978 N@5: 0.33635 early stop: 0\n",
            "\u001b[32m[I 210629 04:00:54 models:110]\u001b[39m 29 1024 train loss: 0.0389898 valid loss: 0.1320691 P@1: 0.39000 P@3: 0.31667 P@5: 0.26200 N@3: 0.33581 N@5: 0.34267 early stop: 0\n",
            "\u001b[32m[I 210629 04:00:56 models:110]\u001b[39m 32 512 train loss: 0.0369129 valid loss: 0.1318203 P@1: 0.40000 P@3: 0.33000 P@5: 0.26800 N@3: 0.35029 N@5: 0.35513 early stop: 0\n",
            "\u001b[32m[I 210629 04:00:58 models:110]\u001b[39m 34 1024 train loss: 0.0339375 valid loss: 0.1317377 P@1: 0.41000 P@3: 0.33667 P@5: 0.27000 N@3: 0.36059 N@5: 0.36494 early stop: 0\n",
            "\u001b[32m[I 210629 04:01:01 models:110]\u001b[39m 37 512 train loss: 0.0293268 valid loss: 0.1315395 P@1: 0.41000 P@3: 0.33667 P@5: 0.27200 N@3: 0.35998 N@5: 0.36619 early stop: 0\n",
            "\u001b[32m[I 210629 04:01:03 models:110]\u001b[39m 39 1024 train loss: 0.0241866 valid loss: 0.1317136 P@1: 0.41000 P@3: 0.34667 P@5: 0.27000 N@3: 0.36651 N@5: 0.36492 early stop: 1\n",
            "\u001b[32m[I 210629 04:01:06 models:110]\u001b[39m 42 512 train loss: 0.0204960 valid loss: 0.1317220 P@1: 0.41000 P@3: 0.34667 P@5: 0.28600 N@3: 0.36712 N@5: 0.37837 early stop: 0\n",
            "\u001b[32m[I 210629 04:01:08 models:110]\u001b[39m 44 1024 train loss: 0.0195786 valid loss: 0.1319282 P@1: 0.41000 P@3: 0.35667 P@5: 0.29200 N@3: 0.37611 N@5: 0.38423 early stop: 0\n",
            "\u001b[32m[I 210629 04:01:10 models:110]\u001b[39m 47 512 train loss: 0.0187607 valid loss: 0.1322976 P@1: 0.41000 P@3: 0.36333 P@5: 0.30000 N@3: 0.38326 N@5: 0.39309 early stop: 0\n",
            "\u001b[32m[I 210629 04:01:12 models:110]\u001b[39m 49 1024 train loss: 0.0180136 valid loss: 0.1325087 P@1: 0.43000 P@3: 0.36333 P@5: 0.30200 N@3: 0.38691 N@5: 0.39797 early stop: 0\n",
            "\u001b[32m[I 210629 04:01:15 models:110]\u001b[39m 52 512 train loss: 0.0143750 valid loss: 0.1332252 P@1: 0.45000 P@3: 0.37000 P@5: 0.30200 N@3: 0.39685 N@5: 0.40373 early stop: 0\n",
            "\u001b[32m[I 210629 04:01:17 models:110]\u001b[39m 54 1024 train loss: 0.0129903 valid loss: 0.1339148 P@1: 0.45000 P@3: 0.37667 P@5: 0.30200 N@3: 0.40277 N@5: 0.40557 early stop: 0\n",
            "\u001b[32m[I 210629 04:01:20 models:110]\u001b[39m 57 512 train loss: 0.0111788 valid loss: 0.1346072 P@1: 0.46000 P@3: 0.39000 P@5: 0.30400 N@3: 0.41621 N@5: 0.41205 early stop: 0\n",
            "\u001b[32m[I 210629 04:01:22 models:110]\u001b[39m 59 1024 train loss: 0.0097688 valid loss: 0.1354286 P@1: 0.48000 P@3: 0.39667 P@5: 0.30400 N@3: 0.42499 N@5: 0.41620 early stop: 0\n",
            "\u001b[32m[I 210629 04:01:24 models:110]\u001b[39m 62 512 train loss: 0.0095993 valid loss: 0.1362481 P@1: 0.48000 P@3: 0.40333 P@5: 0.30600 N@3: 0.42968 N@5: 0.41813 early stop: 0\n",
            "\u001b[32m[I 210629 04:01:26 models:110]\u001b[39m 64 1024 train loss: 0.0090136 valid loss: 0.1370379 P@1: 0.49000 P@3: 0.40667 P@5: 0.31000 N@3: 0.43437 N@5: 0.42342 early stop: 0\n",
            "\u001b[32m[I 210629 04:01:29 models:110]\u001b[39m 67 512 train loss: 0.0071205 valid loss: 0.1381549 P@1: 0.49000 P@3: 0.40333 P@5: 0.31200 N@3: 0.43425 N@5: 0.42684 early stop: 0\n",
            "\u001b[32m[I 210629 04:01:31 models:110]\u001b[39m 69 1024 train loss: 0.0060898 valid loss: 0.1393736 P@1: 0.48000 P@3: 0.40000 P@5: 0.31200 N@3: 0.43078 N@5: 0.42641 early stop: 1\n",
            "\u001b[32m[I 210629 04:01:34 models:110]\u001b[39m 72 512 train loss: 0.0051004 valid loss: 0.1405722 P@1: 0.50000 P@3: 0.40000 P@5: 0.31600 N@3: 0.43478 N@5: 0.43340 early stop: 0\n",
            "\u001b[32m[I 210629 04:01:36 models:110]\u001b[39m 74 1024 train loss: 0.0039598 valid loss: 0.1419868 P@1: 0.51000 P@3: 0.40333 P@5: 0.31600 N@3: 0.43885 N@5: 0.43535 early stop: 0\n",
            "\u001b[32m[I 210629 04:01:38 models:110]\u001b[39m 77 512 train loss: 0.0044179 valid loss: 0.1434920 P@1: 0.52000 P@3: 0.40333 P@5: 0.32200 N@3: 0.44192 N@5: 0.44284 early stop: 0\n",
            "\u001b[32m[I 210629 04:01:40 models:110]\u001b[39m 79 1024 train loss: 0.0034019 valid loss: 0.1448814 P@1: 0.53000 P@3: 0.40667 P@5: 0.32200 N@3: 0.44653 N@5: 0.44561 early stop: 0\n",
            "\u001b[32m[I 210629 04:01:43 models:110]\u001b[39m 82 512 train loss: 0.0039915 valid loss: 0.1461367 P@1: 0.54000 P@3: 0.40667 P@5: 0.32200 N@3: 0.44879 N@5: 0.44808 early stop: 0\n",
            "\u001b[32m[I 210629 04:01:45 models:110]\u001b[39m 84 1024 train loss: 0.0045037 valid loss: 0.1476596 P@1: 0.55000 P@3: 0.41000 P@5: 0.32800 N@3: 0.45226 N@5: 0.45380 early stop: 0\n",
            "\u001b[32m[I 210629 04:01:47 models:110]\u001b[39m 87 512 train loss: 0.0089970 valid loss: 0.1492103 P@1: 0.55000 P@3: 0.40333 P@5: 0.32000 N@3: 0.44818 N@5: 0.44766 early stop: 1\n",
            "\u001b[32m[I 210629 04:01:49 models:110]\u001b[39m 89 1024 train loss: 0.0102714 valid loss: 0.1506459 P@1: 0.54000 P@3: 0.40667 P@5: 0.32200 N@3: 0.44818 N@5: 0.44786 early stop: 2\n",
            "\u001b[32m[I 210629 04:01:52 models:110]\u001b[39m 92 512 train loss: 0.0090854 valid loss: 0.1521361 P@1: 0.53000 P@3: 0.40333 P@5: 0.31400 N@3: 0.44471 N@5: 0.44031 early stop: 3\n",
            "\u001b[32m[I 210629 04:01:54 models:110]\u001b[39m 94 1024 train loss: 0.0109538 valid loss: 0.1534268 P@1: 0.53000 P@3: 0.40333 P@5: 0.32000 N@3: 0.44533 N@5: 0.44496 early stop: 4\n",
            "\u001b[32m[I 210629 04:01:56 models:110]\u001b[39m 97 512 train loss: 0.0079411 valid loss: 0.1543787 P@1: 0.55000 P@3: 0.40000 P@5: 0.32000 N@3: 0.44583 N@5: 0.44713 early stop: 5\n",
            "\u001b[32m[I 210629 04:01:59 models:110]\u001b[39m 99 1024 train loss: 0.0050247 valid loss: 0.1554777 P@1: 0.54000 P@3: 0.41333 P@5: 0.31800 N@3: 0.45349 N@5: 0.44442 early stop: 6\n",
            "\u001b[32m[I 210629 04:02:01 models:110]\u001b[39m 102 512 train loss: 0.0033164 valid loss: 0.1569116 P@1: 0.54000 P@3: 0.41333 P@5: 0.31600 N@3: 0.45410 N@5: 0.44279 early stop: 7\n",
            "\u001b[32m[I 210629 04:02:03 models:110]\u001b[39m 104 1024 train loss: 0.0022634 valid loss: 0.1582062 P@1: 0.55000 P@3: 0.41667 P@5: 0.32600 N@3: 0.45818 N@5: 0.45252 early stop: 8\n",
            "\u001b[32m[I 210629 04:02:05 models:110]\u001b[39m 107 512 train loss: 0.0015739 valid loss: 0.1596420 P@1: 0.55000 P@3: 0.42000 P@5: 0.32400 N@3: 0.46114 N@5: 0.45195 early stop: 9\n",
            "\u001b[32m[I 210629 04:02:08 models:110]\u001b[39m 109 1024 train loss: 0.0011246 valid loss: 0.1611566 P@1: 0.54000 P@3: 0.42000 P@5: 0.32600 N@3: 0.45879 N@5: 0.45172 early stop: 10\n",
            "\u001b[32m[I 210629 04:02:10 models:110]\u001b[39m 112 512 train loss: 0.0006386 valid loss: 0.1627267 P@1: 0.56000 P@3: 0.42000 P@5: 0.33000 N@3: 0.46287 N@5: 0.45918 early stop: 0\n",
            "\u001b[32m[I 210629 04:02:12 models:110]\u001b[39m 114 1024 train loss: 0.0004764 valid loss: 0.1643293 P@1: 0.56000 P@3: 0.42333 P@5: 0.33200 N@3: 0.46583 N@5: 0.46157 early stop: 0\n",
            "\u001b[32m[I 210629 04:02:15 models:110]\u001b[39m 117 512 train loss: 0.0005025 valid loss: 0.1659220 P@1: 0.56000 P@3: 0.42667 P@5: 0.33400 N@3: 0.46879 N@5: 0.46293 early stop: 0\n",
            "\u001b[32m[I 210629 04:02:17 models:110]\u001b[39m 119 1024 train loss: 0.0005813 valid loss: 0.1676190 P@1: 0.56000 P@3: 0.42667 P@5: 0.33400 N@3: 0.46818 N@5: 0.46248 early stop: 1\n",
            "\u001b[32m[I 210629 04:02:19 models:110]\u001b[39m 122 512 train loss: 0.0004803 valid loss: 0.1691976 P@1: 0.57000 P@3: 0.43000 P@5: 0.33000 N@3: 0.47287 N@5: 0.46098 early stop: 2\n",
            "\u001b[32m[I 210629 04:02:21 models:110]\u001b[39m 124 1024 train loss: 0.0002239 valid loss: 0.1708343 P@1: 0.57000 P@3: 0.43000 P@5: 0.32800 N@3: 0.47287 N@5: 0.45916 early stop: 3\n",
            "\u001b[32m[I 210629 04:02:24 models:110]\u001b[39m 127 512 train loss: 0.0002099 valid loss: 0.1725263 P@1: 0.58000 P@3: 0.42667 P@5: 0.32800 N@3: 0.47154 N@5: 0.46018 early stop: 4\n",
            "\u001b[32m[I 210629 04:02:26 models:110]\u001b[39m 129 1024 train loss: 0.0001577 valid loss: 0.1741941 P@1: 0.58000 P@3: 0.42667 P@5: 0.33000 N@3: 0.47154 N@5: 0.46255 early stop: 5\n",
            "\u001b[32m[I 210629 04:02:28 models:110]\u001b[39m 132 512 train loss: 0.0000983 valid loss: 0.1758982 P@1: 0.58000 P@3: 0.42667 P@5: 0.33200 N@3: 0.47215 N@5: 0.46481 early stop: 0\n",
            "\u001b[32m[I 210629 04:02:31 models:110]\u001b[39m 134 1024 train loss: 0.0001022 valid loss: 0.1776073 P@1: 0.58000 P@3: 0.42667 P@5: 0.33400 N@3: 0.47215 N@5: 0.46612 early stop: 0\n",
            "\u001b[32m[I 210629 04:02:33 models:110]\u001b[39m 137 512 train loss: 0.0000836 valid loss: 0.1793232 P@1: 0.58000 P@3: 0.42333 P@5: 0.33200 N@3: 0.46981 N@5: 0.46407 early stop: 1\n",
            "\u001b[32m[I 210629 04:02:35 models:110]\u001b[39m 139 1024 train loss: 0.0000821 valid loss: 0.1810393 P@1: 0.58000 P@3: 0.42667 P@5: 0.33200 N@3: 0.47215 N@5: 0.46468 early stop: 2\n",
            "\u001b[33m[W 210629 04:02:37 models:137]\u001b[39m Clipping gradients with total norm 0.03886 and max norm 0.00397\n",
            "\u001b[32m[I 210629 04:02:38 models:110]\u001b[39m 142 512 train loss: 0.0000729 valid loss: 0.1827480 P@1: 0.58000 P@3: 0.42667 P@5: 0.33200 N@3: 0.47215 N@5: 0.46456 early stop: 3\n",
            "\u001b[32m[I 210629 04:02:40 models:110]\u001b[39m 144 1024 train loss: 0.0000647 valid loss: 0.1844425 P@1: 0.59000 P@3: 0.42667 P@5: 0.33400 N@3: 0.47450 N@5: 0.46849 early stop: 0\n",
            "\u001b[32m[I 210629 04:02:42 models:110]\u001b[39m 147 512 train loss: 0.0000545 valid loss: 0.1861280 P@1: 0.60000 P@3: 0.42667 P@5: 0.33600 N@3: 0.47623 N@5: 0.47090 early stop: 0\n",
            "\u001b[32m[I 210629 04:02:44 models:110]\u001b[39m 149 1024 train loss: 0.0000630 valid loss: 0.1877928 P@1: 0.60000 P@3: 0.42333 P@5: 0.33800 N@3: 0.47388 N@5: 0.47198 early stop: 0\n",
            "\u001b[32m[I 210629 04:02:47 models:110]\u001b[39m 152 512 train loss: 0.0000468 valid loss: 0.1894662 P@1: 0.60000 P@3: 0.42667 P@5: 0.34000 N@3: 0.47684 N@5: 0.47436 early stop: 0\n",
            "\u001b[33m[W 210629 04:02:49 models:137]\u001b[39m Clipping gradients with total norm 0.04294 and max norm 0.00406\n",
            "\u001b[32m[I 210629 04:02:49 models:110]\u001b[39m 154 1024 train loss: 0.0000477 valid loss: 0.1911329 P@1: 0.60000 P@3: 0.42667 P@5: 0.34000 N@3: 0.47684 N@5: 0.47457 early stop: 0\n",
            "\u001b[32m[I 210629 04:02:52 models:110]\u001b[39m 157 512 train loss: 0.0000496 valid loss: 0.1927847 P@1: 0.60000 P@3: 0.42667 P@5: 0.33800 N@3: 0.47746 N@5: 0.47264 early stop: 1\n",
            "\u001b[32m[I 210629 04:02:54 models:110]\u001b[39m 159 1024 train loss: 0.0000426 valid loss: 0.1944532 P@1: 0.60000 P@3: 0.43000 P@5: 0.33600 N@3: 0.47981 N@5: 0.47156 early stop: 2\n",
            "\u001b[32m[I 210629 04:02:56 models:110]\u001b[39m 162 512 train loss: 0.0000416 valid loss: 0.1960789 P@1: 0.60000 P@3: 0.43000 P@5: 0.33600 N@3: 0.47981 N@5: 0.47156 early stop: 3\n",
            "\u001b[33m[W 210629 04:02:58 models:137]\u001b[39m Clipping gradients with total norm 0.07075 and max norm 0.0072\n",
            "\u001b[32m[I 210629 04:02:58 models:110]\u001b[39m 164 1024 train loss: 0.0000471 valid loss: 0.1976843 P@1: 0.60000 P@3: 0.43000 P@5: 0.33800 N@3: 0.47981 N@5: 0.47323 early stop: 4\n",
            "\u001b[32m[I 210629 04:03:01 models:110]\u001b[39m 167 512 train loss: 0.0000341 valid loss: 0.1992928 P@1: 0.60000 P@3: 0.43000 P@5: 0.33600 N@3: 0.47981 N@5: 0.47201 early stop: 5\n",
            "\u001b[32m[I 210629 04:03:03 models:110]\u001b[39m 169 1024 train loss: 0.0000269 valid loss: 0.2008664 P@1: 0.60000 P@3: 0.43333 P@5: 0.33600 N@3: 0.48215 N@5: 0.47225 early stop: 6\n",
            "\u001b[32m[I 210629 04:03:05 models:110]\u001b[39m 172 512 train loss: 0.0000300 valid loss: 0.2024266 P@1: 0.60000 P@3: 0.43333 P@5: 0.33600 N@3: 0.48215 N@5: 0.47210 early stop: 7\n",
            "\u001b[32m[I 210629 04:03:07 models:110]\u001b[39m 174 1024 train loss: 0.0000347 valid loss: 0.2039938 P@1: 0.60000 P@3: 0.43333 P@5: 0.33600 N@3: 0.48215 N@5: 0.47195 early stop: 8\n",
            "\u001b[33m[W 210629 04:03:08 models:137]\u001b[39m Clipping gradients with total norm 0.03442 and max norm 0.00447\n",
            "\u001b[32m[I 210629 04:03:10 models:110]\u001b[39m 177 512 train loss: 0.0000336 valid loss: 0.2055662 P@1: 0.60000 P@3: 0.43333 P@5: 0.33600 N@3: 0.48277 N@5: 0.47246 early stop: 9\n",
            "\u001b[32m[I 210629 04:03:12 models:110]\u001b[39m 179 1024 train loss: 0.0000360 valid loss: 0.2071025 P@1: 0.60000 P@3: 0.43333 P@5: 0.33400 N@3: 0.48277 N@5: 0.47132 early stop: 10\n",
            "\u001b[32m[I 210629 04:03:14 models:110]\u001b[39m 182 512 train loss: 0.0000476 valid loss: 0.2085493 P@1: 0.61000 P@3: 0.43333 P@5: 0.33800 N@3: 0.48450 N@5: 0.47570 early stop: 0\n",
            "\u001b[32m[I 210629 04:03:16 models:110]\u001b[39m 184 1024 train loss: 0.0000357 valid loss: 0.2099957 P@1: 0.61000 P@3: 0.43333 P@5: 0.34000 N@3: 0.48450 N@5: 0.47686 early stop: 0\n",
            "\u001b[32m[I 210629 04:03:19 models:110]\u001b[39m 187 512 train loss: 0.0000317 valid loss: 0.2115048 P@1: 0.62000 P@3: 0.43667 P@5: 0.34000 N@3: 0.48919 N@5: 0.47939 early stop: 0\n",
            "\u001b[32m[I 210629 04:03:21 models:110]\u001b[39m 189 1024 train loss: 0.0000248 valid loss: 0.2130035 P@1: 0.62000 P@3: 0.44000 P@5: 0.33800 N@3: 0.49154 N@5: 0.47840 early stop: 1\n",
            "\u001b[32m[I 210629 04:03:23 models:110]\u001b[39m 192 512 train loss: 0.0000241 valid loss: 0.2144554 P@1: 0.62000 P@3: 0.44000 P@5: 0.33800 N@3: 0.49215 N@5: 0.47901 early stop: 2\n",
            "\u001b[32m[I 210629 04:03:25 models:110]\u001b[39m 194 1024 train loss: 0.0000225 valid loss: 0.2158966 P@1: 0.62000 P@3: 0.44000 P@5: 0.33800 N@3: 0.49215 N@5: 0.47901 early stop: 3\n",
            "\u001b[32m[I 210629 04:03:28 models:110]\u001b[39m 197 512 train loss: 0.0000222 valid loss: 0.2173510 P@1: 0.62000 P@3: 0.44000 P@5: 0.33800 N@3: 0.49215 N@5: 0.47901 early stop: 4\n",
            "\u001b[32m[I 210629 04:03:30 models:110]\u001b[39m 199 1024 train loss: 0.0000210 valid loss: 0.2188040 P@1: 0.62000 P@3: 0.44000 P@5: 0.33800 N@3: 0.49215 N@5: 0.47901 early stop: 5\n",
            "\u001b[32m[I 210629 04:03:32 models:110]\u001b[39m 202 512 train loss: 0.0000222 valid loss: 0.2202252 P@1: 0.62000 P@3: 0.44000 P@5: 0.33800 N@3: 0.49215 N@5: 0.47905 early stop: 6\n",
            "\u001b[32m[I 210629 04:03:35 models:110]\u001b[39m 204 1024 train loss: 0.0000208 valid loss: 0.2216198 P@1: 0.62000 P@3: 0.44000 P@5: 0.34000 N@3: 0.49215 N@5: 0.48086 early stop: 0\n",
            "\u001b[32m[I 210629 04:03:37 models:110]\u001b[39m 207 512 train loss: 0.0000172 valid loss: 0.2230081 P@1: 0.62000 P@3: 0.44000 P@5: 0.34200 N@3: 0.49215 N@5: 0.48232 early stop: 0\n",
            "\u001b[33m[W 210629 04:03:39 models:137]\u001b[39m Clipping gradients with total norm 0.01294 and max norm 0.00226\n",
            "\u001b[32m[I 210629 04:03:39 models:110]\u001b[39m 209 1024 train loss: 0.0000189 valid loss: 0.2244154 P@1: 0.62000 P@3: 0.44667 P@5: 0.34400 N@3: 0.49684 N@5: 0.48434 early stop: 0\n",
            "\u001b[32m[I 210629 04:03:42 models:110]\u001b[39m 212 512 train loss: 0.0000210 valid loss: 0.2258115 P@1: 0.62000 P@3: 0.44667 P@5: 0.34400 N@3: 0.49623 N@5: 0.48373 early stop: 1\n",
            "\u001b[32m[I 210629 04:03:44 models:110]\u001b[39m 214 1024 train loss: 0.0000182 valid loss: 0.2272025 P@1: 0.62000 P@3: 0.45000 P@5: 0.34200 N@3: 0.49858 N@5: 0.48289 early stop: 2\n",
            "\u001b[32m[I 210629 04:03:46 models:110]\u001b[39m 217 512 train loss: 0.0000182 valid loss: 0.2285717 P@1: 0.62000 P@3: 0.45000 P@5: 0.34200 N@3: 0.49858 N@5: 0.48289 early stop: 3\n",
            "\u001b[32m[I 210629 04:03:48 models:110]\u001b[39m 219 1024 train loss: 0.0000157 valid loss: 0.2299210 P@1: 0.62000 P@3: 0.45333 P@5: 0.34200 N@3: 0.50164 N@5: 0.48331 early stop: 4\n",
            "\u001b[32m[I 210629 04:03:51 models:110]\u001b[39m 222 512 train loss: 0.0000141 valid loss: 0.2312554 P@1: 0.62000 P@3: 0.45333 P@5: 0.34000 N@3: 0.50164 N@5: 0.48150 early stop: 5\n",
            "\u001b[32m[I 210629 04:03:53 models:110]\u001b[39m 224 1024 train loss: 0.0000159 valid loss: 0.2325850 P@1: 0.62000 P@3: 0.45333 P@5: 0.34000 N@3: 0.50164 N@5: 0.48150 early stop: 6\n",
            "\u001b[33m[W 210629 04:03:54 models:137]\u001b[39m Clipping gradients with total norm 0.02755 and max norm 0.0011\n",
            "\u001b[32m[I 210629 04:03:55 models:110]\u001b[39m 227 512 train loss: 0.0000166 valid loss: 0.2338998 P@1: 0.62000 P@3: 0.45333 P@5: 0.34000 N@3: 0.50164 N@5: 0.48150 early stop: 7\n",
            "\u001b[33m[W 210629 04:03:56 models:137]\u001b[39m Clipping gradients with total norm 0.02495 and max norm 0.00328\n",
            "\u001b[32m[I 210629 04:03:57 models:110]\u001b[39m 229 1024 train loss: 0.0000231 valid loss: 0.2351982 P@1: 0.61000 P@3: 0.45333 P@5: 0.34000 N@3: 0.49991 N@5: 0.47976 early stop: 8\n",
            "\u001b[32m[I 210629 04:04:00 models:110]\u001b[39m 232 512 train loss: 0.0000267 valid loss: 0.2364727 P@1: 0.61000 P@3: 0.45333 P@5: 0.34200 N@3: 0.49991 N@5: 0.48158 early stop: 9\n",
            "\u001b[32m[I 210629 04:04:02 models:110]\u001b[39m 234 1024 train loss: 0.0000152 valid loss: 0.2377291 P@1: 0.61000 P@3: 0.45333 P@5: 0.34000 N@3: 0.50052 N@5: 0.48088 early stop: 10\n",
            "\u001b[32m[I 210629 04:04:04 models:110]\u001b[39m 237 512 train loss: 0.0000147 valid loss: 0.2389957 P@1: 0.61000 P@3: 0.45333 P@5: 0.34000 N@3: 0.50052 N@5: 0.48088 early stop: 11\n",
            "\u001b[32m[I 210629 04:04:06 models:110]\u001b[39m 239 1024 train loss: 0.0000130 valid loss: 0.2402665 P@1: 0.61000 P@3: 0.45667 P@5: 0.34000 N@3: 0.50287 N@5: 0.48112 early stop: 12\n",
            "\u001b[32m[I 210629 04:04:09 models:110]\u001b[39m 242 512 train loss: 0.0000118 valid loss: 0.2415312 P@1: 0.61000 P@3: 0.45667 P@5: 0.34200 N@3: 0.50287 N@5: 0.48243 early stop: 13\n",
            "\u001b[32m[I 210629 04:04:11 models:110]\u001b[39m 244 1024 train loss: 0.0000120 valid loss: 0.2427857 P@1: 0.61000 P@3: 0.45667 P@5: 0.34200 N@3: 0.50287 N@5: 0.48243 early stop: 14\n",
            "\u001b[32m[I 210629 04:04:13 models:110]\u001b[39m 247 512 train loss: 0.0000122 valid loss: 0.2440247 P@1: 0.61000 P@3: 0.45667 P@5: 0.34200 N@3: 0.50287 N@5: 0.48270 early stop: 15\n",
            "\u001b[32m[I 210629 04:04:16 models:110]\u001b[39m 249 1024 train loss: 0.0000086 valid loss: 0.2452462 P@1: 0.61000 P@3: 0.45333 P@5: 0.34200 N@3: 0.50052 N@5: 0.48254 early stop: 16\n",
            "\u001b[33m[W 210629 04:04:16 models:137]\u001b[39m Clipping gradients with total norm 0.02465 and max norm 0.00194\n",
            "\u001b[32m[I 210629 04:04:18 models:110]\u001b[39m 252 512 train loss: 0.0000179 valid loss: 0.2464447 P@1: 0.61000 P@3: 0.45333 P@5: 0.34400 N@3: 0.50052 N@5: 0.48405 early stop: 17\n",
            "\u001b[32m[I 210629 04:04:20 models:110]\u001b[39m 254 1024 train loss: 0.0000082 valid loss: 0.2476318 P@1: 0.61000 P@3: 0.45667 P@5: 0.34400 N@3: 0.50226 N@5: 0.48373 early stop: 18\n",
            "\u001b[32m[I 210629 04:04:22 models:110]\u001b[39m 257 512 train loss: 0.0000101 valid loss: 0.2488144 P@1: 0.61000 P@3: 0.45667 P@5: 0.34400 N@3: 0.50226 N@5: 0.48373 early stop: 19\n",
            "\u001b[32m[I 210629 04:04:25 models:110]\u001b[39m 259 1024 train loss: 0.0000094 valid loss: 0.2500016 P@1: 0.61000 P@3: 0.45667 P@5: 0.34400 N@3: 0.50226 N@5: 0.48373 early stop: 20\n",
            "\u001b[32m[I 210629 04:04:27 models:110]\u001b[39m 262 512 train loss: 0.0000130 valid loss: 0.2511889 P@1: 0.62000 P@3: 0.45667 P@5: 0.34400 N@3: 0.50399 N@5: 0.48519 early stop: 0\n",
            "\u001b[33m[W 210629 04:04:29 models:137]\u001b[39m Clipping gradients with total norm 0.00806 and max norm 0.00099\n",
            "\u001b[33m[W 210629 04:04:29 models:137]\u001b[39m Clipping gradients with total norm 0.01254 and max norm 0.00197\n",
            "\u001b[32m[I 210629 04:04:29 models:110]\u001b[39m 264 1024 train loss: 0.0000114 valid loss: 0.2523634 P@1: 0.62000 P@3: 0.45667 P@5: 0.34400 N@3: 0.50522 N@5: 0.48642 early stop: 0\n",
            "\u001b[32m[I 210629 04:04:32 models:110]\u001b[39m 267 512 train loss: 0.0000093 valid loss: 0.2535165 P@1: 0.62000 P@3: 0.45667 P@5: 0.34600 N@3: 0.50522 N@5: 0.48746 early stop: 0\n",
            "\u001b[33m[W 210629 04:04:32 models:137]\u001b[39m Clipping gradients with total norm 0.0092 and max norm 0.00155\n",
            "\u001b[32m[I 210629 04:04:34 models:110]\u001b[39m 269 1024 train loss: 0.0000104 valid loss: 0.2546656 P@1: 0.62000 P@3: 0.45667 P@5: 0.34600 N@3: 0.50522 N@5: 0.48746 early stop: 1\n",
            "\u001b[32m[I 210629 04:04:36 models:110]\u001b[39m 272 512 train loss: 0.0000083 valid loss: 0.2558030 P@1: 0.62000 P@3: 0.45667 P@5: 0.34600 N@3: 0.50522 N@5: 0.48746 early stop: 2\n",
            "\u001b[33m[W 210629 04:04:38 models:137]\u001b[39m Clipping gradients with total norm 0.0189 and max norm 0.00131\n",
            "\u001b[32m[I 210629 04:04:38 models:110]\u001b[39m 274 1024 train loss: 0.0000091 valid loss: 0.2569202 P@1: 0.62000 P@3: 0.45667 P@5: 0.34600 N@3: 0.50522 N@5: 0.48746 early stop: 3\n",
            "\u001b[32m[I 210629 04:04:41 models:110]\u001b[39m 277 512 train loss: 0.0000095 valid loss: 0.2580164 P@1: 0.62000 P@3: 0.45667 P@5: 0.34600 N@3: 0.50522 N@5: 0.48746 early stop: 4\n",
            "\u001b[32m[I 210629 04:04:43 models:110]\u001b[39m 279 1024 train loss: 0.0000099 valid loss: 0.2590969 P@1: 0.62000 P@3: 0.45667 P@5: 0.34600 N@3: 0.50522 N@5: 0.48746 early stop: 5\n",
            "\u001b[33m[W 210629 04:04:44 models:137]\u001b[39m Clipping gradients with total norm 0.01136 and max norm 0.00205\n",
            "\u001b[32m[I 210629 04:04:45 models:110]\u001b[39m 282 512 train loss: 0.0000120 valid loss: 0.2601572 P@1: 0.62000 P@3: 0.45667 P@5: 0.34600 N@3: 0.50645 N@5: 0.48835 early stop: 0\n",
            "\u001b[32m[I 210629 04:04:47 models:110]\u001b[39m 284 1024 train loss: 0.0000092 valid loss: 0.2612259 P@1: 0.62000 P@3: 0.45667 P@5: 0.34600 N@3: 0.50706 N@5: 0.48879 early stop: 0\n",
            "\u001b[32m[I 210629 04:04:50 models:110]\u001b[39m 287 512 train loss: 0.0000076 valid loss: 0.2623170 P@1: 0.62000 P@3: 0.45667 P@5: 0.34800 N@3: 0.50706 N@5: 0.49061 early stop: 0\n",
            "\u001b[32m[I 210629 04:04:52 models:110]\u001b[39m 289 1024 train loss: 0.0000075 valid loss: 0.2634086 P@1: 0.62000 P@3: 0.46000 P@5: 0.34800 N@3: 0.50941 N@5: 0.49084 early stop: 0\n",
            "\u001b[32m[I 210629 04:04:55 models:110]\u001b[39m 292 512 train loss: 0.0000086 valid loss: 0.2644826 P@1: 0.62000 P@3: 0.46333 P@5: 0.34800 N@3: 0.51175 N@5: 0.49108 early stop: 0\n",
            "\u001b[32m[I 210629 04:04:57 models:110]\u001b[39m 294 1024 train loss: 0.0000070 valid loss: 0.2655366 P@1: 0.62000 P@3: 0.46333 P@5: 0.34600 N@3: 0.51175 N@5: 0.48926 early stop: 1\n",
            "\u001b[33m[W 210629 04:04:59 models:137]\u001b[39m Clipping gradients with total norm 0.00613 and max norm 0.00113\n",
            "\u001b[32m[I 210629 04:04:59 models:110]\u001b[39m 297 512 train loss: 0.0000081 valid loss: 0.2665842 P@1: 0.62000 P@3: 0.45333 P@5: 0.34600 N@3: 0.50399 N@5: 0.48837 early stop: 2\n",
            "\u001b[32m[I 210629 04:05:01 models:110]\u001b[39m 299 1024 train loss: 0.0000085 valid loss: 0.2676217 P@1: 0.62000 P@3: 0.45333 P@5: 0.34600 N@3: 0.50399 N@5: 0.48837 early stop: 3\n",
            "\u001b[32m[I 210629 04:05:04 models:110]\u001b[39m 302 512 train loss: 0.0000078 valid loss: 0.2686648 P@1: 0.62000 P@3: 0.45667 P@5: 0.34600 N@3: 0.50634 N@5: 0.48869 early stop: 4\n",
            "\u001b[33m[W 210629 04:05:05 models:137]\u001b[39m Clipping gradients with total norm 0.00931 and max norm 0.00139\n",
            "\u001b[33m[W 210629 04:05:05 models:137]\u001b[39m Clipping gradients with total norm 0.01798 and max norm 0.00278\n",
            "\u001b[32m[I 210629 04:05:06 models:110]\u001b[39m 304 1024 train loss: 0.0000143 valid loss: 0.2696866 P@1: 0.62000 P@3: 0.45667 P@5: 0.34600 N@3: 0.50573 N@5: 0.48846 early stop: 5\n",
            "\u001b[32m[I 210629 04:05:08 models:110]\u001b[39m 307 512 train loss: 0.0000283 valid loss: 0.2707042 P@1: 0.62000 P@3: 0.45667 P@5: 0.34400 N@3: 0.50573 N@5: 0.48664 early stop: 6\n",
            "\u001b[33m[W 210629 04:05:08 models:137]\u001b[39m Clipping gradients with total norm 0.20531 and max norm 0.01097\n",
            "\u001b[32m[I 210629 04:05:10 models:110]\u001b[39m 309 1024 train loss: 0.0003137 valid loss: 0.2716123 P@1: 0.62000 P@3: 0.45667 P@5: 0.34400 N@3: 0.50573 N@5: 0.48664 early stop: 7\n",
            "\u001b[32m[I 210629 04:05:13 models:110]\u001b[39m 312 512 train loss: 0.0005511 valid loss: 0.2724625 P@1: 0.62000 P@3: 0.45667 P@5: 0.34400 N@3: 0.50634 N@5: 0.48725 early stop: 8\n",
            "\u001b[32m[I 210629 04:05:15 models:110]\u001b[39m 314 1024 train loss: 0.0135143 valid loss: 0.2726377 P@1: 0.62000 P@3: 0.45667 P@5: 0.34200 N@3: 0.50634 N@5: 0.48639 early stop: 9\n",
            "\u001b[33m[W 210629 04:05:16 models:137]\u001b[39m Clipping gradients with total norm 5.29696 and max norm 1.0\n",
            "\u001b[32m[I 210629 04:05:17 models:110]\u001b[39m 317 512 train loss: 0.1294182 valid loss: 0.2704052 P@1: 0.62000 P@3: 0.45667 P@5: 0.34200 N@3: 0.50634 N@5: 0.48589 early stop: 10\n",
            "\u001b[32m[I 210629 04:05:19 models:110]\u001b[39m 319 1024 train loss: 0.0702553 valid loss: 0.2668837 P@1: 0.62000 P@3: 0.45333 P@5: 0.34000 N@3: 0.50277 N@5: 0.48202 early stop: 11\n",
            "\u001b[32m[I 210629 04:05:22 models:110]\u001b[39m 322 512 train loss: 0.0450905 valid loss: 0.2634449 P@1: 0.62000 P@3: 0.45000 P@5: 0.34000 N@3: 0.50042 N@5: 0.48169 early stop: 12\n",
            "\u001b[32m[I 210629 04:05:24 models:110]\u001b[39m 324 1024 train loss: 0.0293875 valid loss: 0.2604582 P@1: 0.61000 P@3: 0.45333 P@5: 0.33600 N@3: 0.50165 N@5: 0.47885 early stop: 13\n",
            "\u001b[32m[I 210629 04:05:26 models:110]\u001b[39m 327 512 train loss: 0.0179169 valid loss: 0.2579567 P@1: 0.61000 P@3: 0.46000 P@5: 0.34200 N@3: 0.50696 N@5: 0.48568 early stop: 14\n",
            "\u001b[32m[I 210629 04:05:28 models:110]\u001b[39m 329 1024 train loss: 0.0114616 valid loss: 0.2557304 P@1: 0.61000 P@3: 0.46000 P@5: 0.34200 N@3: 0.50696 N@5: 0.48588 early stop: 15\n",
            "\u001b[32m[I 210629 04:05:31 models:110]\u001b[39m 332 512 train loss: 0.0085334 valid loss: 0.2539144 P@1: 0.61000 P@3: 0.45667 P@5: 0.34400 N@3: 0.50461 N@5: 0.48716 early stop: 16\n",
            "\u001b[32m[I 210629 04:05:33 models:110]\u001b[39m 334 1024 train loss: 0.0051439 valid loss: 0.2523101 P@1: 0.61000 P@3: 0.45667 P@5: 0.34400 N@3: 0.50522 N@5: 0.48760 early stop: 17\n",
            "\u001b[32m[I 210629 04:05:35 models:110]\u001b[39m 337 512 train loss: 0.0033241 valid loss: 0.2510292 P@1: 0.61000 P@3: 0.46000 P@5: 0.34400 N@3: 0.50573 N@5: 0.48616 early stop: 18\n",
            "\u001b[32m[I 210629 04:05:37 models:110]\u001b[39m 339 1024 train loss: 0.0019962 valid loss: 0.2499648 P@1: 0.60000 P@3: 0.45667 P@5: 0.34200 N@3: 0.50103 N@5: 0.48313 early stop: 19\n",
            "\u001b[32m[I 210629 04:05:40 models:110]\u001b[39m 342 512 train loss: 0.0015258 valid loss: 0.2489907 P@1: 0.59000 P@3: 0.45667 P@5: 0.34200 N@3: 0.49930 N@5: 0.48139 early stop: 20\n",
            "\u001b[32m[I 210629 04:05:42 models:110]\u001b[39m 344 1024 train loss: 0.0011825 valid loss: 0.2481404 P@1: 0.58000 P@3: 0.46000 P@5: 0.34000 N@3: 0.49939 N@5: 0.47800 early stop: 21\n",
            "\u001b[32m[I 210629 04:05:44 models:110]\u001b[39m 347 512 train loss: 0.0008207 valid loss: 0.2474667 P@1: 0.59000 P@3: 0.45667 P@5: 0.34200 N@3: 0.49877 N@5: 0.48131 early stop: 22\n",
            "\u001b[32m[I 210629 04:05:46 models:110]\u001b[39m 349 1024 train loss: 0.0005695 valid loss: 0.2468675 P@1: 0.59000 P@3: 0.45667 P@5: 0.34200 N@3: 0.49877 N@5: 0.48131 early stop: 23\n",
            "\u001b[32m[I 210629 04:05:49 models:110]\u001b[39m 352 512 train loss: 0.0004656 valid loss: 0.2464076 P@1: 0.59000 P@3: 0.45667 P@5: 0.34200 N@3: 0.49877 N@5: 0.48110 early stop: 24\n",
            "\u001b[32m[I 210629 04:05:51 models:110]\u001b[39m 354 1024 train loss: 0.0003531 valid loss: 0.2459815 P@1: 0.59000 P@3: 0.45667 P@5: 0.34000 N@3: 0.49816 N@5: 0.47918 early stop: 25\n",
            "\u001b[32m[I 210629 04:05:53 models:110]\u001b[39m 357 512 train loss: 0.0003704 valid loss: 0.2456376 P@1: 0.59000 P@3: 0.45667 P@5: 0.33800 N@3: 0.49816 N@5: 0.47767 early stop: 26\n",
            "\u001b[32m[I 210629 04:05:55 models:110]\u001b[39m 359 1024 train loss: 0.0002830 valid loss: 0.2453583 P@1: 0.59000 P@3: 0.46000 P@5: 0.33800 N@3: 0.50050 N@5: 0.47811 early stop: 27\n",
            "\u001b[32m[I 210629 04:05:58 models:110]\u001b[39m 362 512 train loss: 0.0002544 valid loss: 0.2451400 P@1: 0.59000 P@3: 0.46000 P@5: 0.34000 N@3: 0.50050 N@5: 0.47942 early stop: 28\n",
            "\u001b[32m[I 210629 04:06:00 models:110]\u001b[39m 364 1024 train loss: 0.0002181 valid loss: 0.2449733 P@1: 0.60000 P@3: 0.46000 P@5: 0.34000 N@3: 0.50224 N@5: 0.48067 early stop: 29\n",
            "\u001b[32m[I 210629 04:06:02 models:110]\u001b[39m 367 512 train loss: 0.0001823 valid loss: 0.2448533 P@1: 0.60000 P@3: 0.45667 P@5: 0.34200 N@3: 0.49989 N@5: 0.48195 early stop: 30\n",
            "\u001b[32m[I 210629 04:06:04 models:110]\u001b[39m 369 1024 train loss: 0.0001614 valid loss: 0.2447819 P@1: 0.60000 P@3: 0.45667 P@5: 0.34200 N@3: 0.50050 N@5: 0.48240 early stop: 31\n",
            "\u001b[32m[I 210629 04:06:07 models:110]\u001b[39m 372 512 train loss: 0.0001673 valid loss: 0.2447429 P@1: 0.60000 P@3: 0.45667 P@5: 0.34400 N@3: 0.50050 N@5: 0.48373 early stop: 32\n",
            "\u001b[32m[I 210629 04:06:09 models:110]\u001b[39m 374 1024 train loss: 0.0001390 valid loss: 0.2447401 P@1: 0.60000 P@3: 0.45667 P@5: 0.34400 N@3: 0.50050 N@5: 0.48373 early stop: 33\n",
            "\u001b[32m[I 210629 04:06:11 models:110]\u001b[39m 377 512 train loss: 0.0001375 valid loss: 0.2447703 P@1: 0.60000 P@3: 0.45333 P@5: 0.34600 N@3: 0.49816 N@5: 0.48578 early stop: 34\n",
            "\u001b[32m[I 210629 04:06:13 models:110]\u001b[39m 379 1024 train loss: 0.0001187 valid loss: 0.2448339 P@1: 0.60000 P@3: 0.45667 P@5: 0.34600 N@3: 0.50050 N@5: 0.48610 early stop: 35\n",
            "\u001b[32m[I 210629 04:06:16 models:110]\u001b[39m 382 512 train loss: 0.0001083 valid loss: 0.2449139 P@1: 0.59000 P@3: 0.45667 P@5: 0.34600 N@3: 0.49939 N@5: 0.48562 early stop: 36\n",
            "\u001b[32m[I 210629 04:06:18 models:110]\u001b[39m 384 1024 train loss: 0.0001201 valid loss: 0.2450495 P@1: 0.59000 P@3: 0.45667 P@5: 0.34600 N@3: 0.49939 N@5: 0.48562 early stop: 37\n",
            "\u001b[32m[I 210629 04:06:20 models:110]\u001b[39m 387 512 train loss: 0.0001532 valid loss: 0.2451658 P@1: 0.59000 P@3: 0.45667 P@5: 0.34600 N@3: 0.50000 N@5: 0.48623 early stop: 38\n",
            "\u001b[32m[I 210629 04:06:22 models:110]\u001b[39m 389 1024 train loss: 0.0001662 valid loss: 0.2453012 P@1: 0.60000 P@3: 0.46000 P@5: 0.34600 N@3: 0.50408 N@5: 0.48823 early stop: 39\n",
            "\u001b[32m[I 210629 04:06:25 models:110]\u001b[39m 392 512 train loss: 0.0001360 valid loss: 0.2455052 P@1: 0.60000 P@3: 0.45667 P@5: 0.34600 N@3: 0.50031 N@5: 0.48668 early stop: 40\n",
            "\u001b[32m[I 210629 04:06:27 models:110]\u001b[39m 394 1024 train loss: 0.0001152 valid loss: 0.2456948 P@1: 0.60000 P@3: 0.45667 P@5: 0.34600 N@3: 0.50093 N@5: 0.48713 early stop: 41\n",
            "\u001b[32m[I 210629 04:06:30 models:110]\u001b[39m 397 512 train loss: 0.0001136 valid loss: 0.2459102 P@1: 0.60000 P@3: 0.45667 P@5: 0.34600 N@3: 0.50093 N@5: 0.48713 early stop: 42\n",
            "\u001b[32m[I 210629 04:06:32 models:110]\u001b[39m 399 1024 train loss: 0.0001030 valid loss: 0.2461690 P@1: 0.60000 P@3: 0.45667 P@5: 0.34600 N@3: 0.50093 N@5: 0.48713 early stop: 43\n",
            "\u001b[32m[I 210629 04:06:34 models:110]\u001b[39m 402 512 train loss: 0.0000991 valid loss: 0.2464345 P@1: 0.60000 P@3: 0.45667 P@5: 0.34800 N@3: 0.50093 N@5: 0.48844 early stop: 44\n",
            "\u001b[33m[W 210629 04:06:35 models:137]\u001b[39m Clipping gradients with total norm 0.12049 and max norm 0.01303\n",
            "\u001b[32m[I 210629 04:06:36 models:110]\u001b[39m 404 1024 train loss: 0.0001748 valid loss: 0.2467508 P@1: 0.60000 P@3: 0.45667 P@5: 0.34800 N@3: 0.50093 N@5: 0.48844 early stop: 45\n",
            "\u001b[32m[I 210629 04:06:39 models:110]\u001b[39m 407 512 train loss: 0.0005557 valid loss: 0.2470816 P@1: 0.60000 P@3: 0.45667 P@5: 0.34800 N@3: 0.50093 N@5: 0.48844 early stop: 46\n",
            "\u001b[32m[I 210629 04:06:41 models:110]\u001b[39m 409 1024 train loss: 0.0005746 valid loss: 0.2473103 P@1: 0.60000 P@3: 0.45667 P@5: 0.34600 N@3: 0.50093 N@5: 0.48662 early stop: 47\n",
            "\u001b[32m[I 210629 04:06:43 models:110]\u001b[39m 412 512 train loss: 0.0008304 valid loss: 0.2475210 P@1: 0.61000 P@3: 0.45667 P@5: 0.34600 N@3: 0.50266 N@5: 0.48836 early stop: 48\n",
            "\u001b[32m[I 210629 04:06:45 models:110]\u001b[39m 414 1024 train loss: 0.0009117 valid loss: 0.2477993 P@1: 0.61000 P@3: 0.46333 P@5: 0.34600 N@3: 0.50735 N@5: 0.48883 early stop: 49\n",
            "\u001b[32m[I 210629 04:06:48 models:110]\u001b[39m 417 512 train loss: 0.0012130 valid loss: 0.2479654 P@1: 0.61000 P@3: 0.46333 P@5: 0.34600 N@3: 0.50735 N@5: 0.48903 early stop: 50\n",
            "\u001b[32m[I 210629 04:06:50 main:76]\u001b[39m Finish Training\n",
            "\u001b[32m[I 210629 04:06:51 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210629 04:06:51 main:79]\u001b[39m Loading Test Set\n",
            "\u001b[32m[I 210629 04:06:51 main:83]\u001b[39m Size of Test Set: 100\n",
            "\u001b[32m[I 210629 04:06:51 main:85]\u001b[39m Predicting\n",
            "\u001b[32m[I 210629 04:06:55 main:91]\u001b[39m Finish Predicting\n",
            "Precision@1,3,5: 0.54 0.44666666666666666 0.338\n",
            "nDCG@1,3,5: 0.54 0.4745044206312542 0.45532279273530113\n",
            "\n",
            "WITH MAG NO MESH, skip=700\n",
            "\n",
            "/content/drive/Shareddrives/NASA/MATCH/PeTaL\n",
            "129\n",
            "/content/drive/Shareddrives/NASA/MATCH\n",
            "    800  275675 4658257 PeTaL/train.json\n",
            "\u001b[32m[I 210629 04:06:58 preprocess:28]\u001b[39m Vocab Size: 26834\n",
            "\u001b[32m[I 210629 04:06:58 preprocess:30]\u001b[39m Getting Dataset: PeTaL/train_texts.txt Max Length: 500\n",
            "\u001b[32m[I 210629 04:06:58 preprocess:32]\u001b[39m Size of Samples: 900\n",
            "\u001b[32m[I 210629 04:06:59 preprocess:28]\u001b[39m Vocab Size: 26834\n",
            "\u001b[32m[I 210629 04:06:59 preprocess:30]\u001b[39m Getting Dataset: PeTaL/test_texts.txt Max Length: 500\n",
            "\u001b[32m[I 210629 04:06:59 preprocess:32]\u001b[39m Size of Samples: 100\n",
            "\u001b[32m[I 210629 04:07:00 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210629 04:07:00 main:35]\u001b[39m Loading Training and Validation Set\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['absorb_and/or_filter_solids', 'chemically_break_down_inorganic_compounds', 'detox/purify', 'manage_environmental_disturbances_in_a_community', 'protect_from_fire', 'protect_from_gases', 'send_vibratory_signals'] will be ignored\n",
            "  .format(sorted(unknown, key=str)))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['chemically_break_down_inorganic_compounds', 'detox/purify'] will be ignored\n",
            "  .format(sorted(unknown, key=str)))\n",
            "\u001b[32m[I 210629 04:07:00 main:47]\u001b[39m Number of Labels: 124\n",
            "\u001b[32m[I 210629 04:07:00 main:48]\u001b[39m Size of Training Set: 800\n",
            "\u001b[32m[I 210629 04:07:00 main:49]\u001b[39m Size of Validation Set: 100\n",
            "\u001b[32m[I 210629 04:07:00 main:66]\u001b[39m Number of Edges: 101\n",
            "\u001b[32m[I 210629 04:07:00 main:68]\u001b[39m Training\n",
            "\u001b[32m[I 210629 04:07:06 models:110]\u001b[39m 2 512 train loss: 0.2826921 valid loss: 0.1542902 P@1: 0.28000 P@3: 0.17333 P@5: 0.15000 N@3: 0.19506 N@5: 0.19946 early stop: 0\n",
            "\u001b[32m[I 210629 04:07:07 models:142]\u001b[39m SWA Initializing\n",
            "\u001b[32m[I 210629 04:07:08 models:110]\u001b[39m 4 1024 train loss: 0.1455747 valid loss: 0.1449626 P@1: 0.28000 P@3: 0.22333 P@5: 0.19600 N@3: 0.23335 N@5: 0.24283 early stop: 0\n",
            "\u001b[32m[I 210629 04:07:11 models:110]\u001b[39m 7 512 train loss: 0.1412547 valid loss: 0.1433838 P@1: 0.28000 P@3: 0.25333 P@5: 0.19800 N@3: 0.25877 N@5: 0.25129 early stop: 0\n",
            "\u001b[32m[I 210629 04:07:13 models:110]\u001b[39m 9 1024 train loss: 0.1387977 valid loss: 0.1423088 P@1: 0.28000 P@3: 0.22000 P@5: 0.19600 N@3: 0.23531 N@5: 0.24639 early stop: 1\n",
            "\u001b[32m[I 210629 04:07:16 models:110]\u001b[39m 12 512 train loss: 0.1282807 valid loss: 0.1413818 P@1: 0.28000 P@3: 0.25000 P@5: 0.22600 N@3: 0.25642 N@5: 0.27127 early stop: 0\n",
            "\u001b[32m[I 210629 04:07:18 models:110]\u001b[39m 14 1024 train loss: 0.1165706 valid loss: 0.1402886 P@1: 0.29000 P@3: 0.30000 P@5: 0.25000 N@3: 0.29458 N@5: 0.29566 early stop: 0\n",
            "\u001b[32m[I 210629 04:07:20 models:110]\u001b[39m 17 512 train loss: 0.0914464 valid loss: 0.1395420 P@1: 0.35000 P@3: 0.28667 P@5: 0.25000 N@3: 0.29927 N@5: 0.30731 early stop: 0\n",
            "\u001b[32m[I 210629 04:07:22 models:110]\u001b[39m 19 1024 train loss: 0.0783431 valid loss: 0.1384619 P@1: 0.44000 P@3: 0.31000 P@5: 0.26400 N@3: 0.33927 N@5: 0.34357 early stop: 0\n",
            "\u001b[32m[I 210629 04:07:25 models:110]\u001b[39m 22 512 train loss: 0.0669039 valid loss: 0.1377263 P@1: 0.52000 P@3: 0.35333 P@5: 0.27800 N@3: 0.38671 N@5: 0.37284 early stop: 0\n",
            "\u001b[32m[I 210629 04:07:27 models:110]\u001b[39m 24 1024 train loss: 0.0558746 valid loss: 0.1366420 P@1: 0.55000 P@3: 0.38000 P@5: 0.29600 N@3: 0.41652 N@5: 0.39887 early stop: 0\n",
            "\u001b[32m[I 210629 04:07:30 models:110]\u001b[39m 27 512 train loss: 0.0467944 valid loss: 0.1356392 P@1: 0.55000 P@3: 0.38333 P@5: 0.30400 N@3: 0.42439 N@5: 0.41048 early stop: 0\n",
            "\u001b[32m[I 210629 04:07:32 models:110]\u001b[39m 29 1024 train loss: 0.0390843 valid loss: 0.1347200 P@1: 0.51000 P@3: 0.39667 P@5: 0.31600 N@3: 0.42704 N@5: 0.41879 early stop: 0\n",
            "\u001b[32m[I 210629 04:07:34 models:110]\u001b[39m 32 512 train loss: 0.0331576 valid loss: 0.1336517 P@1: 0.52000 P@3: 0.41667 P@5: 0.32000 N@3: 0.44338 N@5: 0.42376 early stop: 0\n",
            "\u001b[32m[I 210629 04:07:36 models:110]\u001b[39m 34 1024 train loss: 0.0307294 valid loss: 0.1325974 P@1: 0.52000 P@3: 0.42667 P@5: 0.32000 N@3: 0.45552 N@5: 0.42938 early stop: 0\n",
            "\u001b[32m[I 210629 04:07:39 models:110]\u001b[39m 37 512 train loss: 0.0289136 valid loss: 0.1317737 P@1: 0.53000 P@3: 0.43000 P@5: 0.33000 N@3: 0.45960 N@5: 0.43845 early stop: 0\n",
            "\u001b[32m[I 210629 04:07:41 models:110]\u001b[39m 39 1024 train loss: 0.0263108 valid loss: 0.1310725 P@1: 0.54000 P@3: 0.43667 P@5: 0.34000 N@3: 0.46603 N@5: 0.44808 early stop: 0\n",
            "\u001b[32m[I 210629 04:07:44 models:110]\u001b[39m 42 512 train loss: 0.0249552 valid loss: 0.1304406 P@1: 0.53000 P@3: 0.44333 P@5: 0.35000 N@3: 0.46960 N@5: 0.45769 early stop: 0\n",
            "\u001b[32m[I 210629 04:07:46 models:110]\u001b[39m 44 1024 train loss: 0.0230079 valid loss: 0.1297128 P@1: 0.55000 P@3: 0.44667 P@5: 0.35000 N@3: 0.47726 N@5: 0.46173 early stop: 0\n",
            "\u001b[32m[I 210629 04:07:48 models:110]\u001b[39m 47 512 train loss: 0.0204339 valid loss: 0.1298358 P@1: 0.57000 P@3: 0.44000 P@5: 0.35000 N@3: 0.47656 N@5: 0.46334 early stop: 0\n",
            "\u001b[32m[I 210629 04:07:50 models:110]\u001b[39m 49 1024 train loss: 0.0169089 valid loss: 0.1295810 P@1: 0.59000 P@3: 0.43667 P@5: 0.35600 N@3: 0.47653 N@5: 0.46828 early stop: 0\n",
            "\u001b[32m[I 210629 04:07:53 models:110]\u001b[39m 52 512 train loss: 0.0136169 valid loss: 0.1300351 P@1: 0.60000 P@3: 0.42333 P@5: 0.35800 N@3: 0.46674 N@5: 0.46897 early stop: 0\n",
            "\u001b[32m[I 210629 04:07:55 models:110]\u001b[39m 54 1024 train loss: 0.0115512 valid loss: 0.1306245 P@1: 0.60000 P@3: 0.43000 P@5: 0.36000 N@3: 0.47143 N@5: 0.47065 early stop: 0\n",
            "\u001b[32m[I 210629 04:07:57 models:110]\u001b[39m 57 512 train loss: 0.0111967 valid loss: 0.1312358 P@1: 0.59000 P@3: 0.43000 P@5: 0.36000 N@3: 0.46970 N@5: 0.46977 early stop: 1\n",
            "\u001b[32m[I 210629 04:08:00 models:110]\u001b[39m 59 1024 train loss: 0.0088768 valid loss: 0.1323194 P@1: 0.61000 P@3: 0.43333 P@5: 0.35600 N@3: 0.47623 N@5: 0.47112 early stop: 0\n",
            "\u001b[32m[I 210629 04:08:02 models:110]\u001b[39m 62 512 train loss: 0.0079968 valid loss: 0.1335158 P@1: 0.62000 P@3: 0.45000 P@5: 0.36200 N@3: 0.49031 N@5: 0.47955 early stop: 0\n",
            "\u001b[32m[I 210629 04:08:04 models:110]\u001b[39m 64 1024 train loss: 0.0074380 valid loss: 0.1345272 P@1: 0.62000 P@3: 0.44667 P@5: 0.36800 N@3: 0.48919 N@5: 0.48432 early stop: 0\n",
            "\u001b[32m[I 210629 04:08:07 models:110]\u001b[39m 67 512 train loss: 0.0090859 valid loss: 0.1357883 P@1: 0.62000 P@3: 0.44333 P@5: 0.37200 N@3: 0.48684 N@5: 0.48700 early stop: 0\n",
            "\u001b[32m[I 210629 04:08:09 models:110]\u001b[39m 69 1024 train loss: 0.0152764 valid loss: 0.1370372 P@1: 0.63000 P@3: 0.44667 P@5: 0.37800 N@3: 0.49031 N@5: 0.49144 early stop: 0\n",
            "\u001b[32m[I 210629 04:08:11 models:110]\u001b[39m 72 512 train loss: 0.0158310 valid loss: 0.1382541 P@1: 0.63000 P@3: 0.45333 P@5: 0.38000 N@3: 0.49500 N@5: 0.49339 early stop: 0\n",
            "\u001b[32m[I 210629 04:08:14 models:110]\u001b[39m 74 1024 train loss: 0.0122943 valid loss: 0.1391425 P@1: 0.63000 P@3: 0.45667 P@5: 0.38000 N@3: 0.49938 N@5: 0.49501 early stop: 0\n",
            "\u001b[32m[I 210629 04:08:16 models:110]\u001b[39m 77 512 train loss: 0.0095295 valid loss: 0.1403228 P@1: 0.63000 P@3: 0.45667 P@5: 0.38000 N@3: 0.49876 N@5: 0.49505 early stop: 0\n",
            "\u001b[32m[I 210629 04:08:18 models:110]\u001b[39m 79 1024 train loss: 0.0076966 valid loss: 0.1416042 P@1: 0.63000 P@3: 0.46000 P@5: 0.37800 N@3: 0.50111 N@5: 0.49401 early stop: 1\n",
            "\u001b[32m[I 210629 04:08:21 models:110]\u001b[39m 82 512 train loss: 0.0063963 valid loss: 0.1430188 P@1: 0.63000 P@3: 0.46333 P@5: 0.37800 N@3: 0.50373 N@5: 0.49530 early stop: 0\n",
            "\u001b[32m[I 210629 04:08:23 models:110]\u001b[39m 84 1024 train loss: 0.0041382 valid loss: 0.1445896 P@1: 0.64000 P@3: 0.47000 P@5: 0.37400 N@3: 0.51077 N@5: 0.49505 early stop: 1\n",
            "\u001b[32m[I 210629 04:08:25 models:110]\u001b[39m 87 512 train loss: 0.0035607 valid loss: 0.1462167 P@1: 0.64000 P@3: 0.47667 P@5: 0.37200 N@3: 0.51546 N@5: 0.49439 early stop: 2\n",
            "\u001b[32m[I 210629 04:08:27 models:110]\u001b[39m 89 1024 train loss: 0.0026853 valid loss: 0.1478929 P@1: 0.64000 P@3: 0.48000 P@5: 0.37000 N@3: 0.51781 N@5: 0.49338 early stop: 3\n",
            "\u001b[32m[I 210629 04:08:30 models:110]\u001b[39m 92 512 train loss: 0.0025875 valid loss: 0.1496742 P@1: 0.65000 P@3: 0.47667 P@5: 0.36800 N@3: 0.51719 N@5: 0.49288 early stop: 4\n",
            "\u001b[32m[I 210629 04:08:32 models:110]\u001b[39m 94 1024 train loss: 0.0031957 valid loss: 0.1514229 P@1: 0.65000 P@3: 0.47333 P@5: 0.36600 N@3: 0.51546 N@5: 0.49065 early stop: 5\n",
            "\u001b[32m[I 210629 04:08:34 models:110]\u001b[39m 97 512 train loss: 0.0027747 valid loss: 0.1534795 P@1: 0.65000 P@3: 0.47333 P@5: 0.36800 N@3: 0.51546 N@5: 0.49261 early stop: 6\n",
            "\u001b[32m[I 210629 04:08:36 models:110]\u001b[39m 99 1024 train loss: 0.0038017 valid loss: 0.1553253 P@1: 0.65000 P@3: 0.47667 P@5: 0.36800 N@3: 0.51781 N@5: 0.49282 early stop: 7\n",
            "\u001b[32m[I 210629 04:08:39 models:110]\u001b[39m 102 512 train loss: 0.0019708 valid loss: 0.1572240 P@1: 0.65000 P@3: 0.48000 P@5: 0.36800 N@3: 0.52149 N@5: 0.49353 early stop: 8\n",
            "\u001b[32m[I 210629 04:08:41 models:110]\u001b[39m 104 1024 train loss: 0.0019126 valid loss: 0.1591744 P@1: 0.66000 P@3: 0.48000 P@5: 0.37000 N@3: 0.52375 N@5: 0.49782 early stop: 0\n",
            "\u001b[32m[I 210629 04:08:44 models:110]\u001b[39m 107 512 train loss: 0.0019996 valid loss: 0.1611080 P@1: 0.66000 P@3: 0.48000 P@5: 0.37600 N@3: 0.52375 N@5: 0.50181 early stop: 0\n",
            "\u001b[32m[I 210629 04:08:46 models:110]\u001b[39m 109 1024 train loss: 0.0021034 valid loss: 0.1631852 P@1: 0.66000 P@3: 0.48333 P@5: 0.37400 N@3: 0.52610 N@5: 0.50058 early stop: 1\n",
            "\u001b[32m[I 210629 04:08:48 models:110]\u001b[39m 112 512 train loss: 0.0017967 valid loss: 0.1654003 P@1: 0.66000 P@3: 0.48667 P@5: 0.37800 N@3: 0.52764 N@5: 0.50267 early stop: 0\n",
            "\u001b[32m[I 210629 04:08:50 models:110]\u001b[39m 114 1024 train loss: 0.0031919 valid loss: 0.1672078 P@1: 0.66000 P@3: 0.48667 P@5: 0.38000 N@3: 0.52764 N@5: 0.50448 early stop: 0\n",
            "\u001b[32m[I 210629 04:08:53 models:110]\u001b[39m 117 512 train loss: 0.0037475 valid loss: 0.1689649 P@1: 0.66000 P@3: 0.48667 P@5: 0.38000 N@3: 0.52764 N@5: 0.50454 early stop: 0\n",
            "\u001b[32m[I 210629 04:08:55 models:110]\u001b[39m 119 1024 train loss: 0.0056850 valid loss: 0.1707090 P@1: 0.66000 P@3: 0.48667 P@5: 0.38000 N@3: 0.52703 N@5: 0.50425 early stop: 1\n",
            "\u001b[32m[I 210629 04:08:57 models:110]\u001b[39m 122 512 train loss: 0.0071899 valid loss: 0.1724139 P@1: 0.66000 P@3: 0.48667 P@5: 0.38000 N@3: 0.52703 N@5: 0.50445 early stop: 2\n",
            "\u001b[32m[I 210629 04:08:59 models:110]\u001b[39m 124 1024 train loss: 0.0076059 valid loss: 0.1739217 P@1: 0.66000 P@3: 0.48667 P@5: 0.38200 N@3: 0.52703 N@5: 0.50576 early stop: 0\n",
            "\u001b[32m[I 210629 04:09:02 models:110]\u001b[39m 127 512 train loss: 0.0121646 valid loss: 0.1752224 P@1: 0.66000 P@3: 0.48667 P@5: 0.38200 N@3: 0.52703 N@5: 0.50576 early stop: 1\n",
            "\u001b[32m[I 210629 04:09:04 models:110]\u001b[39m 129 1024 train loss: 0.0112358 valid loss: 0.1764987 P@1: 0.66000 P@3: 0.48667 P@5: 0.38400 N@3: 0.52703 N@5: 0.50708 early stop: 0\n",
            "\u001b[32m[I 210629 04:09:06 models:110]\u001b[39m 132 512 train loss: 0.0101485 valid loss: 0.1772879 P@1: 0.66000 P@3: 0.48667 P@5: 0.38400 N@3: 0.52703 N@5: 0.50708 early stop: 1\n",
            "\u001b[32m[I 210629 04:09:09 models:110]\u001b[39m 134 1024 train loss: 0.0064843 valid loss: 0.1782449 P@1: 0.65000 P@3: 0.48667 P@5: 0.38400 N@3: 0.52529 N@5: 0.50534 early stop: 2\n",
            "\u001b[32m[I 210629 04:09:11 models:110]\u001b[39m 137 512 train loss: 0.0043751 valid loss: 0.1791401 P@1: 0.65000 P@3: 0.48667 P@5: 0.38400 N@3: 0.52529 N@5: 0.50534 early stop: 3\n",
            "\u001b[32m[I 210629 04:09:13 models:110]\u001b[39m 139 1024 train loss: 0.0030992 valid loss: 0.1803467 P@1: 0.67000 P@3: 0.48333 P@5: 0.38400 N@3: 0.52641 N@5: 0.50729 early stop: 0\n",
            "\u001b[32m[I 210629 04:09:16 models:110]\u001b[39m 142 512 train loss: 0.0016250 valid loss: 0.1815488 P@1: 0.67000 P@3: 0.48667 P@5: 0.38200 N@3: 0.52937 N@5: 0.50660 early stop: 1\n",
            "\u001b[32m[I 210629 04:09:18 models:110]\u001b[39m 144 1024 train loss: 0.0010955 valid loss: 0.1826277 P@1: 0.67000 P@3: 0.49000 P@5: 0.38000 N@3: 0.53172 N@5: 0.50553 early stop: 2\n",
            "\u001b[32m[I 210629 04:09:20 models:110]\u001b[39m 147 512 train loss: 0.0009637 valid loss: 0.1839504 P@1: 0.67000 P@3: 0.49000 P@5: 0.38200 N@3: 0.53172 N@5: 0.50701 early stop: 3\n",
            "\u001b[32m[I 210629 04:09:22 models:110]\u001b[39m 149 1024 train loss: 0.0005285 valid loss: 0.1852652 P@1: 0.67000 P@3: 0.49333 P@5: 0.38200 N@3: 0.53407 N@5: 0.50748 early stop: 0\n",
            "\u001b[32m[I 210629 04:09:25 models:110]\u001b[39m 152 512 train loss: 0.0003683 valid loss: 0.1866779 P@1: 0.67000 P@3: 0.49333 P@5: 0.38200 N@3: 0.53407 N@5: 0.50768 early stop: 0\n",
            "\u001b[32m[I 210629 04:09:27 models:110]\u001b[39m 154 1024 train loss: 0.0002637 valid loss: 0.1879957 P@1: 0.67000 P@3: 0.49000 P@5: 0.38200 N@3: 0.53172 N@5: 0.50765 early stop: 1\n",
            "\u001b[32m[I 210629 04:09:29 models:110]\u001b[39m 157 512 train loss: 0.0001791 valid loss: 0.1893733 P@1: 0.67000 P@3: 0.49000 P@5: 0.38400 N@3: 0.53172 N@5: 0.50947 early stop: 0\n",
            "\u001b[33m[W 210629 04:09:29 models:137]\u001b[39m Clipping gradients with total norm 0.14241 and max norm 0.01657\n",
            "\u001b[32m[I 210629 04:09:31 models:110]\u001b[39m 159 1024 train loss: 0.0001695 valid loss: 0.1907542 P@1: 0.68000 P@3: 0.49000 P@5: 0.38600 N@3: 0.53337 N@5: 0.51260 early stop: 0\n",
            "\u001b[32m[I 210629 04:09:34 models:110]\u001b[39m 162 512 train loss: 0.0001073 valid loss: 0.1921331 P@1: 0.68000 P@3: 0.48667 P@5: 0.38600 N@3: 0.53030 N@5: 0.51217 early stop: 1\n",
            "\u001b[32m[I 210629 04:09:36 models:110]\u001b[39m 164 1024 train loss: 0.0000956 valid loss: 0.1935298 P@1: 0.68000 P@3: 0.48667 P@5: 0.38600 N@3: 0.53030 N@5: 0.51217 early stop: 2\n",
            "\u001b[32m[I 210629 04:09:38 models:110]\u001b[39m 167 512 train loss: 0.0000721 valid loss: 0.1949318 P@1: 0.68000 P@3: 0.48667 P@5: 0.38800 N@3: 0.53030 N@5: 0.51399 early stop: 0\n",
            "\u001b[32m[I 210629 04:09:40 models:110]\u001b[39m 169 1024 train loss: 0.0000642 valid loss: 0.1963155 P@1: 0.68000 P@3: 0.48667 P@5: 0.38600 N@3: 0.53030 N@5: 0.51233 early stop: 1\n",
            "\u001b[32m[I 210629 04:09:43 models:110]\u001b[39m 172 512 train loss: 0.0000640 valid loss: 0.1977022 P@1: 0.68000 P@3: 0.49000 P@5: 0.38600 N@3: 0.53265 N@5: 0.51251 early stop: 2\n",
            "\u001b[32m[I 210629 04:09:45 models:110]\u001b[39m 174 1024 train loss: 0.0000512 valid loss: 0.1990872 P@1: 0.68000 P@3: 0.48667 P@5: 0.38600 N@3: 0.53092 N@5: 0.51293 early stop: 3\n",
            "\u001b[32m[I 210629 04:09:47 models:110]\u001b[39m 177 512 train loss: 0.0000518 valid loss: 0.2004651 P@1: 0.68000 P@3: 0.48333 P@5: 0.38400 N@3: 0.52857 N@5: 0.51109 early stop: 4\n",
            "\u001b[32m[I 210629 04:09:50 models:110]\u001b[39m 179 1024 train loss: 0.0000474 valid loss: 0.2018398 P@1: 0.68000 P@3: 0.48333 P@5: 0.38200 N@3: 0.52796 N@5: 0.50884 early stop: 5\n",
            "\u001b[32m[I 210629 04:09:52 models:110]\u001b[39m 182 512 train loss: 0.0000455 valid loss: 0.2032031 P@1: 0.68000 P@3: 0.48333 P@5: 0.38200 N@3: 0.52796 N@5: 0.50884 early stop: 6\n",
            "\u001b[32m[I 210629 04:09:54 models:110]\u001b[39m 184 1024 train loss: 0.0000406 valid loss: 0.2045584 P@1: 0.68000 P@3: 0.48333 P@5: 0.38200 N@3: 0.52796 N@5: 0.50884 early stop: 7\n",
            "\u001b[32m[I 210629 04:09:57 models:110]\u001b[39m 187 512 train loss: 0.0000476 valid loss: 0.2059158 P@1: 0.68000 P@3: 0.48333 P@5: 0.38200 N@3: 0.52796 N@5: 0.50884 early stop: 8\n",
            "\u001b[32m[I 210629 04:09:59 models:110]\u001b[39m 189 1024 train loss: 0.0000377 valid loss: 0.2072605 P@1: 0.68000 P@3: 0.48333 P@5: 0.38400 N@3: 0.52796 N@5: 0.51086 early stop: 9\n",
            "\u001b[32m[I 210629 04:10:01 models:110]\u001b[39m 192 512 train loss: 0.0000409 valid loss: 0.2085814 P@1: 0.68000 P@3: 0.48333 P@5: 0.38400 N@3: 0.52796 N@5: 0.51106 early stop: 10\n",
            "\u001b[32m[I 210629 04:10:03 models:110]\u001b[39m 194 1024 train loss: 0.0000399 valid loss: 0.2098872 P@1: 0.68000 P@3: 0.48333 P@5: 0.38600 N@3: 0.52796 N@5: 0.51237 early stop: 11\n",
            "\u001b[32m[I 210629 04:10:06 models:110]\u001b[39m 197 512 train loss: 0.0000367 valid loss: 0.2112015 P@1: 0.68000 P@3: 0.48333 P@5: 0.38600 N@3: 0.52796 N@5: 0.51258 early stop: 12\n",
            "\u001b[32m[I 210629 04:10:08 models:110]\u001b[39m 199 1024 train loss: 0.0000337 valid loss: 0.2125072 P@1: 0.68000 P@3: 0.48333 P@5: 0.38800 N@3: 0.52796 N@5: 0.51404 early stop: 0\n",
            "\u001b[33m[W 210629 04:10:09 models:137]\u001b[39m Clipping gradients with total norm 0.02502 and max norm 0.00239\n",
            "\u001b[32m[I 210629 04:10:10 models:110]\u001b[39m 202 512 train loss: 0.0000330 valid loss: 0.2137912 P@1: 0.68000 P@3: 0.48333 P@5: 0.39000 N@3: 0.52796 N@5: 0.51535 early stop: 0\n",
            "\u001b[32m[I 210629 04:10:12 models:110]\u001b[39m 204 1024 train loss: 0.0000308 valid loss: 0.2150666 P@1: 0.67000 P@3: 0.48333 P@5: 0.39000 N@3: 0.52622 N@5: 0.51362 early stop: 1\n",
            "\u001b[32m[I 210629 04:10:15 models:110]\u001b[39m 207 512 train loss: 0.0000278 valid loss: 0.2163271 P@1: 0.67000 P@3: 0.48667 P@5: 0.39000 N@3: 0.52857 N@5: 0.51386 early stop: 2\n",
            "\u001b[33m[W 210629 04:10:17 models:137]\u001b[39m Clipping gradients with total norm 0.01349 and max norm 0.00208\n",
            "\u001b[32m[I 210629 04:10:17 models:110]\u001b[39m 209 1024 train loss: 0.0000273 valid loss: 0.2175671 P@1: 0.67000 P@3: 0.48667 P@5: 0.39000 N@3: 0.52857 N@5: 0.51386 early stop: 3\n",
            "\u001b[32m[I 210629 04:10:19 models:110]\u001b[39m 212 512 train loss: 0.0000292 valid loss: 0.2188013 P@1: 0.68000 P@3: 0.48667 P@5: 0.39000 N@3: 0.53030 N@5: 0.51559 early stop: 0\n",
            "\u001b[32m[I 210629 04:10:22 models:110]\u001b[39m 214 1024 train loss: 0.0000278 valid loss: 0.2200362 P@1: 0.67000 P@3: 0.48667 P@5: 0.39000 N@3: 0.52857 N@5: 0.51400 early stop: 1\n",
            "\u001b[32m[I 210629 04:10:24 models:110]\u001b[39m 217 512 train loss: 0.0000352 valid loss: 0.2212635 P@1: 0.67000 P@3: 0.48667 P@5: 0.39000 N@3: 0.52918 N@5: 0.51445 early stop: 2\n",
            "\u001b[32m[I 210629 04:10:26 models:110]\u001b[39m 219 1024 train loss: 0.0000287 valid loss: 0.2224917 P@1: 0.67000 P@3: 0.48667 P@5: 0.39000 N@3: 0.52918 N@5: 0.51445 early stop: 3\n",
            "\u001b[32m[I 210629 04:10:29 models:110]\u001b[39m 222 512 train loss: 0.0000336 valid loss: 0.2237043 P@1: 0.67000 P@3: 0.48667 P@5: 0.39000 N@3: 0.52918 N@5: 0.51445 early stop: 4\n",
            "\u001b[33m[W 210629 04:10:30 models:137]\u001b[39m Clipping gradients with total norm 0.12346 and max norm 0.00241\n",
            "\u001b[32m[I 210629 04:10:31 models:110]\u001b[39m 224 1024 train loss: 0.0000553 valid loss: 0.2248941 P@1: 0.67000 P@3: 0.48667 P@5: 0.39000 N@3: 0.52918 N@5: 0.51493 early stop: 5\n",
            "\u001b[32m[I 210629 04:10:33 models:110]\u001b[39m 227 512 train loss: 0.0000385 valid loss: 0.2260611 P@1: 0.67000 P@3: 0.48667 P@5: 0.39000 N@3: 0.52918 N@5: 0.51493 early stop: 6\n",
            "\u001b[32m[I 210629 04:10:35 models:110]\u001b[39m 229 1024 train loss: 0.0000212 valid loss: 0.2272183 P@1: 0.67000 P@3: 0.48667 P@5: 0.39000 N@3: 0.52918 N@5: 0.51493 early stop: 7\n",
            "\u001b[32m[I 210629 04:10:38 models:110]\u001b[39m 232 512 train loss: 0.0000217 valid loss: 0.2283778 P@1: 0.67000 P@3: 0.48333 P@5: 0.39000 N@3: 0.52684 N@5: 0.51449 early stop: 8\n",
            "\u001b[32m[I 210629 04:10:40 models:110]\u001b[39m 234 1024 train loss: 0.0000188 valid loss: 0.2295328 P@1: 0.67000 P@3: 0.48333 P@5: 0.39000 N@3: 0.52684 N@5: 0.51449 early stop: 9\n",
            "\u001b[32m[I 210629 04:10:42 models:110]\u001b[39m 237 512 train loss: 0.0000180 valid loss: 0.2306749 P@1: 0.67000 P@3: 0.48333 P@5: 0.39000 N@3: 0.52684 N@5: 0.51449 early stop: 10\n",
            "\u001b[32m[I 210629 04:10:44 models:110]\u001b[39m 239 1024 train loss: 0.0000206 valid loss: 0.2318035 P@1: 0.67000 P@3: 0.48333 P@5: 0.39000 N@3: 0.52745 N@5: 0.51520 early stop: 11\n",
            "\u001b[32m[I 210629 04:10:47 models:110]\u001b[39m 242 512 train loss: 0.0000168 valid loss: 0.2329231 P@1: 0.67000 P@3: 0.48333 P@5: 0.39000 N@3: 0.52745 N@5: 0.51520 early stop: 12\n",
            "\u001b[32m[I 210629 04:10:49 models:110]\u001b[39m 244 1024 train loss: 0.0000206 valid loss: 0.2340293 P@1: 0.67000 P@3: 0.48333 P@5: 0.39000 N@3: 0.52745 N@5: 0.51520 early stop: 13\n",
            "\u001b[32m[I 210629 04:10:51 models:110]\u001b[39m 247 512 train loss: 0.0000235 valid loss: 0.2351259 P@1: 0.67000 P@3: 0.48333 P@5: 0.38800 N@3: 0.52745 N@5: 0.51339 early stop: 14\n",
            "\u001b[32m[I 210629 04:10:53 models:110]\u001b[39m 249 1024 train loss: 0.0000179 valid loss: 0.2362050 P@1: 0.67000 P@3: 0.48333 P@5: 0.38800 N@3: 0.52745 N@5: 0.51339 early stop: 15\n",
            "\u001b[33m[W 210629 04:10:54 models:137]\u001b[39m Clipping gradients with total norm 0.02689 and max norm 0.0025\n",
            "\u001b[32m[I 210629 04:10:56 models:110]\u001b[39m 252 512 train loss: 0.0000275 valid loss: 0.2372900 P@1: 0.67000 P@3: 0.48333 P@5: 0.38800 N@3: 0.52745 N@5: 0.51318 early stop: 16\n",
            "\u001b[32m[I 210629 04:10:58 models:110]\u001b[39m 254 1024 train loss: 0.0000226 valid loss: 0.2383631 P@1: 0.67000 P@3: 0.48333 P@5: 0.38800 N@3: 0.52807 N@5: 0.51380 early stop: 17\n",
            "\u001b[32m[I 210629 04:11:00 models:110]\u001b[39m 257 512 train loss: 0.0000167 valid loss: 0.2394342 P@1: 0.67000 P@3: 0.48333 P@5: 0.39000 N@3: 0.52807 N@5: 0.51561 early stop: 0\n",
            "\u001b[32m[I 210629 04:11:02 models:110]\u001b[39m 259 1024 train loss: 0.0000223 valid loss: 0.2405013 P@1: 0.67000 P@3: 0.48333 P@5: 0.39000 N@3: 0.52807 N@5: 0.51576 early stop: 0\n",
            "\u001b[33m[W 210629 04:11:03 models:137]\u001b[39m Clipping gradients with total norm 0.07602 and max norm 0.0028\n",
            "\u001b[32m[I 210629 04:11:05 models:110]\u001b[39m 262 512 train loss: 0.0000324 valid loss: 0.2415444 P@1: 0.67000 P@3: 0.48667 P@5: 0.38800 N@3: 0.53041 N@5: 0.51478 early stop: 1\n",
            "\u001b[32m[I 210629 04:11:07 models:110]\u001b[39m 264 1024 train loss: 0.0000151 valid loss: 0.2425718 P@1: 0.67000 P@3: 0.49000 P@5: 0.38800 N@3: 0.53276 N@5: 0.51510 early stop: 2\n",
            "\u001b[32m[I 210629 04:11:09 models:110]\u001b[39m 267 512 train loss: 0.0000163 valid loss: 0.2435993 P@1: 0.67000 P@3: 0.49000 P@5: 0.39000 N@3: 0.53276 N@5: 0.51706 early stop: 0\n",
            "\u001b[32m[I 210629 04:11:12 models:110]\u001b[39m 269 1024 train loss: 0.0000156 valid loss: 0.2446234 P@1: 0.67000 P@3: 0.49000 P@5: 0.39000 N@3: 0.53276 N@5: 0.51706 early stop: 1\n",
            "\u001b[33m[W 210629 04:11:13 models:137]\u001b[39m Clipping gradients with total norm 0.00816 and max norm 0.0016\n",
            "\u001b[32m[I 210629 04:11:14 models:110]\u001b[39m 272 512 train loss: 0.0000143 valid loss: 0.2456313 P@1: 0.67000 P@3: 0.49000 P@5: 0.39000 N@3: 0.53276 N@5: 0.51706 early stop: 2\n",
            "\u001b[32m[I 210629 04:11:16 models:110]\u001b[39m 274 1024 train loss: 0.0000219 valid loss: 0.2466215 P@1: 0.67000 P@3: 0.49000 P@5: 0.39000 N@3: 0.53276 N@5: 0.51706 early stop: 3\n",
            "\u001b[32m[I 210629 04:11:19 models:110]\u001b[39m 277 512 train loss: 0.0000145 valid loss: 0.2476072 P@1: 0.67000 P@3: 0.49000 P@5: 0.39000 N@3: 0.53276 N@5: 0.51706 early stop: 4\n",
            "\u001b[33m[W 210629 04:11:19 models:137]\u001b[39m Clipping gradients with total norm 0.01386 and max norm 0.00262\n",
            "\u001b[32m[I 210629 04:11:21 models:110]\u001b[39m 279 1024 train loss: 0.0000210 valid loss: 0.2485843 P@1: 0.67000 P@3: 0.49000 P@5: 0.39000 N@3: 0.53276 N@5: 0.51721 early stop: 0\n",
            "\u001b[32m[I 210629 04:11:23 models:110]\u001b[39m 282 512 train loss: 0.0000153 valid loss: 0.2495587 P@1: 0.67000 P@3: 0.49000 P@5: 0.39000 N@3: 0.53276 N@5: 0.51721 early stop: 1\n",
            "\u001b[32m[I 210629 04:11:25 models:110]\u001b[39m 284 1024 train loss: 0.0000125 valid loss: 0.2505276 P@1: 0.67000 P@3: 0.49333 P@5: 0.39200 N@3: 0.53511 N@5: 0.51885 early stop: 0\n",
            "\u001b[33m[W 210629 04:11:28 models:137]\u001b[39m Clipping gradients with total norm 0.01761 and max norm 0.00179\n",
            "\u001b[32m[I 210629 04:11:28 models:110]\u001b[39m 287 512 train loss: 0.0000210 valid loss: 0.2514884 P@1: 0.67000 P@3: 0.49333 P@5: 0.39200 N@3: 0.53511 N@5: 0.51885 early stop: 1\n",
            "\u001b[32m[I 210629 04:11:30 models:110]\u001b[39m 289 1024 train loss: 0.0000110 valid loss: 0.2524416 P@1: 0.67000 P@3: 0.49333 P@5: 0.39400 N@3: 0.53511 N@5: 0.52016 early stop: 0\n",
            "\u001b[32m[I 210629 04:11:33 models:110]\u001b[39m 292 512 train loss: 0.0000097 valid loss: 0.2533923 P@1: 0.67000 P@3: 0.49333 P@5: 0.39400 N@3: 0.53449 N@5: 0.51955 early stop: 1\n",
            "\u001b[32m[I 210629 04:11:35 models:110]\u001b[39m 294 1024 train loss: 0.0000113 valid loss: 0.2543447 P@1: 0.66000 P@3: 0.49333 P@5: 0.39400 N@3: 0.53276 N@5: 0.51830 early stop: 2\n",
            "\u001b[32m[I 210629 04:11:37 models:110]\u001b[39m 297 512 train loss: 0.0000104 valid loss: 0.2552939 P@1: 0.66000 P@3: 0.49333 P@5: 0.39400 N@3: 0.53276 N@5: 0.51830 early stop: 3\n",
            "\u001b[32m[I 210629 04:11:39 models:110]\u001b[39m 299 1024 train loss: 0.0000086 valid loss: 0.2562334 P@1: 0.66000 P@3: 0.49333 P@5: 0.39400 N@3: 0.53276 N@5: 0.51830 early stop: 4\n",
            "\u001b[32m[I 210629 04:11:42 models:110]\u001b[39m 302 512 train loss: 0.0000094 valid loss: 0.2571611 P@1: 0.66000 P@3: 0.49333 P@5: 0.39600 N@3: 0.53276 N@5: 0.52011 early stop: 5\n",
            "\u001b[32m[I 210629 04:11:44 models:110]\u001b[39m 304 1024 train loss: 0.0000096 valid loss: 0.2580810 P@1: 0.66000 P@3: 0.49333 P@5: 0.39600 N@3: 0.53276 N@5: 0.52011 early stop: 6\n",
            "\u001b[32m[I 210629 04:11:46 models:110]\u001b[39m 307 512 train loss: 0.0000099 valid loss: 0.2589930 P@1: 0.66000 P@3: 0.49333 P@5: 0.39600 N@3: 0.53276 N@5: 0.52011 early stop: 7\n",
            "\u001b[32m[I 210629 04:11:48 models:110]\u001b[39m 309 1024 train loss: 0.0000077 valid loss: 0.2598982 P@1: 0.66000 P@3: 0.49333 P@5: 0.39600 N@3: 0.53276 N@5: 0.52011 early stop: 8\n",
            "\u001b[32m[I 210629 04:11:51 models:110]\u001b[39m 312 512 train loss: 0.0000114 valid loss: 0.2607963 P@1: 0.66000 P@3: 0.49333 P@5: 0.39400 N@3: 0.53276 N@5: 0.51880 early stop: 9\n",
            "\u001b[32m[I 210629 04:11:53 models:110]\u001b[39m 314 1024 train loss: 0.0000086 valid loss: 0.2616848 P@1: 0.66000 P@3: 0.49333 P@5: 0.39400 N@3: 0.53276 N@5: 0.51880 early stop: 10\n",
            "\u001b[32m[I 210629 04:11:55 models:110]\u001b[39m 317 512 train loss: 0.0000126 valid loss: 0.2625711 P@1: 0.66000 P@3: 0.49333 P@5: 0.39400 N@3: 0.53276 N@5: 0.51880 early stop: 11\n",
            "\u001b[32m[I 210629 04:11:57 models:110]\u001b[39m 319 1024 train loss: 0.0000108 valid loss: 0.2634544 P@1: 0.66000 P@3: 0.49333 P@5: 0.39400 N@3: 0.53337 N@5: 0.51941 early stop: 12\n",
            "\u001b[32m[I 210629 04:12:00 models:110]\u001b[39m 322 512 train loss: 0.0000091 valid loss: 0.2643327 P@1: 0.66000 P@3: 0.49667 P@5: 0.39400 N@3: 0.53572 N@5: 0.51965 early stop: 13\n",
            "\u001b[32m[I 210629 04:12:02 models:110]\u001b[39m 324 1024 train loss: 0.0000098 valid loss: 0.2652040 P@1: 0.66000 P@3: 0.49667 P@5: 0.39400 N@3: 0.53572 N@5: 0.51965 early stop: 14\n",
            "\u001b[32m[I 210629 04:12:04 models:110]\u001b[39m 327 512 train loss: 0.0000070 valid loss: 0.2660685 P@1: 0.66000 P@3: 0.49667 P@5: 0.39400 N@3: 0.53572 N@5: 0.51965 early stop: 15\n",
            "\u001b[32m[I 210629 04:12:06 models:110]\u001b[39m 329 1024 train loss: 0.0000082 valid loss: 0.2669285 P@1: 0.66000 P@3: 0.49667 P@5: 0.39400 N@3: 0.53572 N@5: 0.51965 early stop: 16\n",
            "\u001b[32m[I 210629 04:12:09 models:110]\u001b[39m 332 512 train loss: 0.0000074 valid loss: 0.2677827 P@1: 0.66000 P@3: 0.49667 P@5: 0.39400 N@3: 0.53572 N@5: 0.51965 early stop: 17\n",
            "\u001b[32m[I 210629 04:12:11 models:110]\u001b[39m 334 1024 train loss: 0.0000072 valid loss: 0.2686297 P@1: 0.66000 P@3: 0.49667 P@5: 0.39400 N@3: 0.53572 N@5: 0.51985 early stop: 18\n",
            "\u001b[32m[I 210629 04:12:13 models:110]\u001b[39m 337 512 train loss: 0.0000085 valid loss: 0.2694713 P@1: 0.66000 P@3: 0.49667 P@5: 0.39400 N@3: 0.53572 N@5: 0.51985 early stop: 19\n",
            "\u001b[33m[W 210629 04:12:13 models:137]\u001b[39m Clipping gradients with total norm 0.0133 and max norm 0.00046\n",
            "\u001b[32m[I 210629 04:12:15 models:110]\u001b[39m 339 1024 train loss: 0.0000093 valid loss: 0.2703092 P@1: 0.66000 P@3: 0.49667 P@5: 0.39400 N@3: 0.53572 N@5: 0.51959 early stop: 20\n",
            "\u001b[32m[I 210629 04:12:18 models:110]\u001b[39m 342 512 train loss: 0.0000092 valid loss: 0.2711366 P@1: 0.66000 P@3: 0.49667 P@5: 0.39400 N@3: 0.53572 N@5: 0.51959 early stop: 21\n",
            "\u001b[32m[I 210629 04:12:20 models:110]\u001b[39m 344 1024 train loss: 0.0000080 valid loss: 0.2719581 P@1: 0.66000 P@3: 0.49667 P@5: 0.39400 N@3: 0.53572 N@5: 0.51959 early stop: 22\n",
            "\u001b[33m[W 210629 04:12:21 models:137]\u001b[39m Clipping gradients with total norm 0.00501 and max norm 0.00074\n",
            "\u001b[32m[I 210629 04:12:22 models:110]\u001b[39m 347 512 train loss: 0.0000086 valid loss: 0.2727804 P@1: 0.66000 P@3: 0.49667 P@5: 0.39400 N@3: 0.53572 N@5: 0.51959 early stop: 23\n",
            "\u001b[33m[W 210629 04:12:24 models:137]\u001b[39m Clipping gradients with total norm 0.01527 and max norm 0.00183\n",
            "\u001b[32m[I 210629 04:12:24 models:110]\u001b[39m 349 1024 train loss: 0.0000084 valid loss: 0.2736013 P@1: 0.66000 P@3: 0.49667 P@5: 0.39400 N@3: 0.53572 N@5: 0.51959 early stop: 24\n",
            "\u001b[33m[W 210629 04:12:25 models:137]\u001b[39m Clipping gradients with total norm 0.0082 and max norm 0.00084\n",
            "\u001b[32m[I 210629 04:12:27 models:110]\u001b[39m 352 512 train loss: 0.0000085 valid loss: 0.2744185 P@1: 0.66000 P@3: 0.49667 P@5: 0.39400 N@3: 0.53572 N@5: 0.51959 early stop: 25\n",
            "\u001b[32m[I 210629 04:12:29 models:110]\u001b[39m 354 1024 train loss: 0.0000076 valid loss: 0.2752221 P@1: 0.66000 P@3: 0.49667 P@5: 0.39400 N@3: 0.53572 N@5: 0.51959 early stop: 26\n",
            "\u001b[32m[I 210629 04:12:31 models:110]\u001b[39m 357 512 train loss: 0.0000069 valid loss: 0.2760144 P@1: 0.66000 P@3: 0.49667 P@5: 0.39600 N@3: 0.53572 N@5: 0.52090 early stop: 0\n",
            "\u001b[32m[I 210629 04:12:33 models:110]\u001b[39m 359 1024 train loss: 0.0000053 valid loss: 0.2768010 P@1: 0.66000 P@3: 0.49667 P@5: 0.39600 N@3: 0.53572 N@5: 0.52090 early stop: 1\n",
            "\u001b[32m[I 210629 04:12:36 models:110]\u001b[39m 362 512 train loss: 0.0000056 valid loss: 0.2775832 P@1: 0.66000 P@3: 0.49667 P@5: 0.39600 N@3: 0.53572 N@5: 0.52110 early stop: 0\n",
            "\u001b[33m[W 210629 04:12:38 models:137]\u001b[39m Clipping gradients with total norm 0.0054 and max norm 0.00065\n",
            "\u001b[32m[I 210629 04:12:38 models:110]\u001b[39m 364 1024 train loss: 0.0000076 valid loss: 0.2783638 P@1: 0.66000 P@3: 0.49667 P@5: 0.39600 N@3: 0.53572 N@5: 0.52110 early stop: 1\n",
            "\u001b[32m[I 210629 04:12:40 models:110]\u001b[39m 367 512 train loss: 0.0000065 valid loss: 0.2791421 P@1: 0.66000 P@3: 0.49667 P@5: 0.39600 N@3: 0.53572 N@5: 0.52110 early stop: 2\n",
            "\u001b[32m[I 210629 04:12:43 models:110]\u001b[39m 369 1024 train loss: 0.0000075 valid loss: 0.2799115 P@1: 0.66000 P@3: 0.49667 P@5: 0.39600 N@3: 0.53572 N@5: 0.52110 early stop: 3\n",
            "\u001b[32m[I 210629 04:12:45 models:110]\u001b[39m 372 512 train loss: 0.0000064 valid loss: 0.2806749 P@1: 0.66000 P@3: 0.49667 P@5: 0.39600 N@3: 0.53511 N@5: 0.52049 early stop: 4\n",
            "\u001b[32m[I 210629 04:12:47 models:110]\u001b[39m 374 1024 train loss: 0.0000056 valid loss: 0.2814335 P@1: 0.66000 P@3: 0.49667 P@5: 0.39600 N@3: 0.53511 N@5: 0.52049 early stop: 5\n",
            "\u001b[32m[I 210629 04:12:49 models:110]\u001b[39m 377 512 train loss: 0.0000052 valid loss: 0.2821890 P@1: 0.66000 P@3: 0.49667 P@5: 0.39600 N@3: 0.53511 N@5: 0.52049 early stop: 6\n",
            "\u001b[33m[W 210629 04:12:51 models:137]\u001b[39m Clipping gradients with total norm 0.00762 and max norm 0.00102\n",
            "\u001b[32m[I 210629 04:12:51 models:110]\u001b[39m 379 1024 train loss: 0.0000060 valid loss: 0.2829396 P@1: 0.66000 P@3: 0.49667 P@5: 0.39600 N@3: 0.53511 N@5: 0.52049 early stop: 7\n",
            "\u001b[32m[I 210629 04:12:54 models:110]\u001b[39m 382 512 train loss: 0.0000056 valid loss: 0.2836843 P@1: 0.66000 P@3: 0.49667 P@5: 0.39800 N@3: 0.53511 N@5: 0.52180 early stop: 0\n",
            "\u001b[32m[I 210629 04:12:56 models:110]\u001b[39m 384 1024 train loss: 0.0000051 valid loss: 0.2844247 P@1: 0.66000 P@3: 0.49667 P@5: 0.39600 N@3: 0.53511 N@5: 0.52049 early stop: 1\n",
            "\u001b[32m[I 210629 04:12:59 models:110]\u001b[39m 387 512 train loss: 0.0000065 valid loss: 0.2851575 P@1: 0.66000 P@3: 0.49667 P@5: 0.39600 N@3: 0.53511 N@5: 0.52049 early stop: 2\n",
            "\u001b[32m[I 210629 04:13:01 models:110]\u001b[39m 389 1024 train loss: 0.0000043 valid loss: 0.2858850 P@1: 0.66000 P@3: 0.49333 P@5: 0.39600 N@3: 0.53276 N@5: 0.52025 early stop: 3\n",
            "\u001b[32m[I 210629 04:13:03 models:110]\u001b[39m 392 512 train loss: 0.0000050 valid loss: 0.2866095 P@1: 0.66000 P@3: 0.49333 P@5: 0.39600 N@3: 0.53276 N@5: 0.52025 early stop: 4\n",
            "\u001b[32m[I 210629 04:13:05 models:110]\u001b[39m 394 1024 train loss: 0.0000052 valid loss: 0.2873324 P@1: 0.66000 P@3: 0.49333 P@5: 0.39600 N@3: 0.53276 N@5: 0.52025 early stop: 5\n",
            "\u001b[32m[I 210629 04:13:08 models:110]\u001b[39m 397 512 train loss: 0.0000058 valid loss: 0.2880513 P@1: 0.66000 P@3: 0.49333 P@5: 0.39600 N@3: 0.53276 N@5: 0.52025 early stop: 6\n",
            "\u001b[32m[I 210629 04:13:10 models:110]\u001b[39m 399 1024 train loss: 0.0000048 valid loss: 0.2887652 P@1: 0.66000 P@3: 0.49333 P@5: 0.39600 N@3: 0.53276 N@5: 0.52025 early stop: 7\n",
            "\u001b[32m[I 210629 04:13:12 models:110]\u001b[39m 402 512 train loss: 0.0000052 valid loss: 0.2894718 P@1: 0.66000 P@3: 0.49333 P@5: 0.39600 N@3: 0.53276 N@5: 0.52025 early stop: 8\n",
            "\u001b[32m[I 210629 04:13:14 models:110]\u001b[39m 404 1024 train loss: 0.0000050 valid loss: 0.2901730 P@1: 0.66000 P@3: 0.49333 P@5: 0.39600 N@3: 0.53276 N@5: 0.52025 early stop: 9\n",
            "\u001b[33m[W 210629 04:13:16 models:137]\u001b[39m Clipping gradients with total norm 0.00683 and max norm 0.00065\n",
            "\u001b[32m[I 210629 04:13:17 models:110]\u001b[39m 407 512 train loss: 0.0000058 valid loss: 0.2908712 P@1: 0.66000 P@3: 0.49333 P@5: 0.39800 N@3: 0.53276 N@5: 0.52157 early stop: 10\n",
            "\u001b[32m[I 210629 04:13:19 models:110]\u001b[39m 409 1024 train loss: 0.0000046 valid loss: 0.2915653 P@1: 0.66000 P@3: 0.49333 P@5: 0.39800 N@3: 0.53276 N@5: 0.52157 early stop: 11\n",
            "\u001b[32m[I 210629 04:13:21 models:110]\u001b[39m 412 512 train loss: 0.0000047 valid loss: 0.2922541 P@1: 0.66000 P@3: 0.49333 P@5: 0.39800 N@3: 0.53276 N@5: 0.52157 early stop: 12\n",
            "\u001b[32m[I 210629 04:13:23 models:110]\u001b[39m 414 1024 train loss: 0.0000042 valid loss: 0.2929375 P@1: 0.66000 P@3: 0.49333 P@5: 0.39800 N@3: 0.53276 N@5: 0.52157 early stop: 13\n",
            "\u001b[32m[I 210629 04:13:26 models:110]\u001b[39m 417 512 train loss: 0.0000048 valid loss: 0.2936194 P@1: 0.66000 P@3: 0.49333 P@5: 0.39400 N@3: 0.53276 N@5: 0.51824 early stop: 14\n",
            "\u001b[32m[I 210629 04:13:28 models:110]\u001b[39m 419 1024 train loss: 0.0000047 valid loss: 0.2943007 P@1: 0.66000 P@3: 0.49333 P@5: 0.39400 N@3: 0.53276 N@5: 0.51824 early stop: 15\n",
            "\u001b[32m[I 210629 04:13:30 models:110]\u001b[39m 422 512 train loss: 0.0000051 valid loss: 0.2949765 P@1: 0.66000 P@3: 0.49333 P@5: 0.39400 N@3: 0.53276 N@5: 0.51824 early stop: 16\n",
            "\u001b[32m[I 210629 04:13:32 models:110]\u001b[39m 424 1024 train loss: 0.0000047 valid loss: 0.2956480 P@1: 0.66000 P@3: 0.49333 P@5: 0.39400 N@3: 0.53276 N@5: 0.51824 early stop: 17\n",
            "\u001b[32m[I 210629 04:13:35 models:110]\u001b[39m 427 512 train loss: 0.0000040 valid loss: 0.2963113 P@1: 0.66000 P@3: 0.49333 P@5: 0.39400 N@3: 0.53276 N@5: 0.51824 early stop: 18\n",
            "\u001b[32m[I 210629 04:13:37 models:110]\u001b[39m 429 1024 train loss: 0.0000038 valid loss: 0.2969705 P@1: 0.66000 P@3: 0.49333 P@5: 0.39400 N@3: 0.53276 N@5: 0.51824 early stop: 19\n",
            "\u001b[32m[I 210629 04:13:39 models:110]\u001b[39m 432 512 train loss: 0.0000042 valid loss: 0.2976262 P@1: 0.66000 P@3: 0.49333 P@5: 0.39400 N@3: 0.53276 N@5: 0.51824 early stop: 20\n",
            "\u001b[32m[I 210629 04:13:41 models:110]\u001b[39m 434 1024 train loss: 0.0000037 valid loss: 0.2982791 P@1: 0.66000 P@3: 0.49333 P@5: 0.39400 N@3: 0.53215 N@5: 0.51780 early stop: 21\n",
            "\u001b[32m[I 210629 04:13:44 models:110]\u001b[39m 437 512 train loss: 0.0000042 valid loss: 0.2989293 P@1: 0.66000 P@3: 0.49333 P@5: 0.39400 N@3: 0.53215 N@5: 0.51780 early stop: 22\n",
            "\u001b[32m[I 210629 04:13:46 models:110]\u001b[39m 439 1024 train loss: 0.0000034 valid loss: 0.2995772 P@1: 0.66000 P@3: 0.49333 P@5: 0.39400 N@3: 0.53215 N@5: 0.51780 early stop: 23\n",
            "\u001b[32m[I 210629 04:13:48 models:110]\u001b[39m 442 512 train loss: 0.0000037 valid loss: 0.3002222 P@1: 0.66000 P@3: 0.49333 P@5: 0.39600 N@3: 0.53215 N@5: 0.51911 early stop: 24\n",
            "\u001b[32m[I 210629 04:13:51 models:110]\u001b[39m 444 1024 train loss: 0.0000030 valid loss: 0.3008638 P@1: 0.66000 P@3: 0.49333 P@5: 0.39600 N@3: 0.53215 N@5: 0.51911 early stop: 25\n",
            "\u001b[32m[I 210629 04:13:54 models:110]\u001b[39m 447 512 train loss: 0.0000034 valid loss: 0.3015026 P@1: 0.66000 P@3: 0.49333 P@5: 0.39600 N@3: 0.53215 N@5: 0.51911 early stop: 26\n",
            "\u001b[32m[I 210629 04:13:56 models:110]\u001b[39m 449 1024 train loss: 0.0000041 valid loss: 0.3021388 P@1: 0.66000 P@3: 0.49333 P@5: 0.39600 N@3: 0.53276 N@5: 0.51955 early stop: 27\n",
            "\u001b[32m[I 210629 04:13:59 models:110]\u001b[39m 452 512 train loss: 0.0000034 valid loss: 0.3027746 P@1: 0.66000 P@3: 0.49333 P@5: 0.39600 N@3: 0.53276 N@5: 0.51955 early stop: 28\n",
            "\u001b[32m[I 210629 04:14:01 models:110]\u001b[39m 454 1024 train loss: 0.0000031 valid loss: 0.3034057 P@1: 0.66000 P@3: 0.49333 P@5: 0.39600 N@3: 0.53276 N@5: 0.51955 early stop: 29\n",
            "\u001b[32m[I 210629 04:14:04 models:110]\u001b[39m 457 512 train loss: 0.0000034 valid loss: 0.3040330 P@1: 0.66000 P@3: 0.49333 P@5: 0.39800 N@3: 0.53276 N@5: 0.52192 early stop: 0\n",
            "\u001b[32m[I 210629 04:14:06 models:110]\u001b[39m 459 1024 train loss: 0.0000034 valid loss: 0.3046577 P@1: 0.66000 P@3: 0.49333 P@5: 0.39800 N@3: 0.53276 N@5: 0.52192 early stop: 1\n",
            "\u001b[32m[I 210629 04:14:09 models:110]\u001b[39m 462 512 train loss: 0.0000034 valid loss: 0.3052774 P@1: 0.66000 P@3: 0.49333 P@5: 0.39800 N@3: 0.53276 N@5: 0.52192 early stop: 2\n",
            "\u001b[32m[I 210629 04:14:11 models:110]\u001b[39m 464 1024 train loss: 0.0000030 valid loss: 0.3058915 P@1: 0.66000 P@3: 0.49333 P@5: 0.39800 N@3: 0.53276 N@5: 0.52192 early stop: 3\n",
            "\u001b[32m[I 210629 04:14:13 models:110]\u001b[39m 467 512 train loss: 0.0000029 valid loss: 0.3065010 P@1: 0.66000 P@3: 0.49333 P@5: 0.39800 N@3: 0.53276 N@5: 0.52192 early stop: 4\n",
            "\u001b[32m[I 210629 04:14:15 models:110]\u001b[39m 469 1024 train loss: 0.0000031 valid loss: 0.3071083 P@1: 0.66000 P@3: 0.49333 P@5: 0.39800 N@3: 0.53276 N@5: 0.52192 early stop: 5\n",
            "\u001b[32m[I 210629 04:14:18 models:110]\u001b[39m 472 512 train loss: 0.0000027 valid loss: 0.3077146 P@1: 0.66000 P@3: 0.49333 P@5: 0.39800 N@3: 0.53276 N@5: 0.52207 early stop: 0\n",
            "\u001b[32m[I 210629 04:14:20 models:110]\u001b[39m 474 1024 train loss: 0.0000038 valid loss: 0.3083210 P@1: 0.66000 P@3: 0.49333 P@5: 0.39800 N@3: 0.53276 N@5: 0.52207 early stop: 1\n",
            "\u001b[32m[I 210629 04:14:23 models:110]\u001b[39m 477 512 train loss: 0.0000028 valid loss: 0.3089296 P@1: 0.66000 P@3: 0.49333 P@5: 0.39800 N@3: 0.53276 N@5: 0.52207 early stop: 2\n",
            "\u001b[33m[W 210629 04:14:24 models:137]\u001b[39m Clipping gradients with total norm 0.00297 and max norm 0.00056\n",
            "\u001b[32m[I 210629 04:14:25 models:110]\u001b[39m 479 1024 train loss: 0.0000034 valid loss: 0.3095352 P@1: 0.66000 P@3: 0.49333 P@5: 0.39800 N@3: 0.53276 N@5: 0.52207 early stop: 3\n",
            "\u001b[32m[I 210629 04:14:27 models:110]\u001b[39m 482 512 train loss: 0.0000033 valid loss: 0.3101358 P@1: 0.66000 P@3: 0.49333 P@5: 0.39800 N@3: 0.53276 N@5: 0.52207 early stop: 4\n",
            "\u001b[32m[I 210629 04:14:29 models:110]\u001b[39m 484 1024 train loss: 0.0000036 valid loss: 0.3107340 P@1: 0.66000 P@3: 0.49333 P@5: 0.39800 N@3: 0.53276 N@5: 0.52207 early stop: 5\n",
            "\u001b[32m[I 210629 04:14:32 models:110]\u001b[39m 487 512 train loss: 0.0000033 valid loss: 0.3113269 P@1: 0.66000 P@3: 0.49333 P@5: 0.39800 N@3: 0.53276 N@5: 0.52207 early stop: 6\n",
            "\u001b[33m[W 210629 04:14:33 models:137]\u001b[39m Clipping gradients with total norm 0.00334 and max norm 0.00053\n",
            "\u001b[32m[I 210629 04:14:34 models:110]\u001b[39m 489 1024 train loss: 0.0000032 valid loss: 0.3119138 P@1: 0.66000 P@3: 0.49333 P@5: 0.39800 N@3: 0.53276 N@5: 0.52207 early stop: 7\n",
            "\u001b[33m[W 210629 04:14:35 models:137]\u001b[39m Clipping gradients with total norm 0.00239 and max norm 0.00043\n",
            "\u001b[32m[I 210629 04:14:36 models:110]\u001b[39m 492 512 train loss: 0.0000031 valid loss: 0.3124957 P@1: 0.66000 P@3: 0.49333 P@5: 0.39800 N@3: 0.53276 N@5: 0.52207 early stop: 8\n",
            "\u001b[32m[I 210629 04:14:38 models:110]\u001b[39m 494 1024 train loss: 0.0000026 valid loss: 0.3130735 P@1: 0.66000 P@3: 0.49333 P@5: 0.39800 N@3: 0.53276 N@5: 0.52207 early stop: 9\n",
            "\u001b[33m[W 210629 04:14:39 models:137]\u001b[39m Clipping gradients with total norm 0.00196 and max norm 0.00029\n",
            "\u001b[33m[W 210629 04:14:40 models:137]\u001b[39m Clipping gradients with total norm 0.00293 and max norm 0.00039\n",
            "\u001b[32m[I 210629 04:14:41 models:110]\u001b[39m 497 512 train loss: 0.0000031 valid loss: 0.3136470 P@1: 0.66000 P@3: 0.49333 P@5: 0.39800 N@3: 0.53276 N@5: 0.52207 early stop: 10\n",
            "\u001b[32m[I 210629 04:14:43 models:110]\u001b[39m 499 1024 train loss: 0.0000033 valid loss: 0.3142171 P@1: 0.66000 P@3: 0.49333 P@5: 0.39800 N@3: 0.53276 N@5: 0.52207 early stop: 11\n",
            "\u001b[32m[I 210629 04:14:45 models:110]\u001b[39m 502 512 train loss: 0.0000026 valid loss: 0.3147838 P@1: 0.66000 P@3: 0.49667 P@5: 0.39800 N@3: 0.53511 N@5: 0.52231 early stop: 0\n",
            "\u001b[32m[I 210629 04:14:47 models:110]\u001b[39m 504 1024 train loss: 0.0000024 valid loss: 0.3153477 P@1: 0.66000 P@3: 0.49667 P@5: 0.39800 N@3: 0.53511 N@5: 0.52231 early stop: 1\n",
            "\u001b[32m[I 210629 04:14:50 models:110]\u001b[39m 507 512 train loss: 0.0000026 valid loss: 0.3159086 P@1: 0.66000 P@3: 0.49667 P@5: 0.39800 N@3: 0.53511 N@5: 0.52231 early stop: 2\n",
            "\u001b[32m[I 210629 04:14:52 models:110]\u001b[39m 509 1024 train loss: 0.0000028 valid loss: 0.3164679 P@1: 0.66000 P@3: 0.49667 P@5: 0.40000 N@3: 0.53511 N@5: 0.52412 early stop: 0\n",
            "\u001b[32m[I 210629 04:14:54 models:110]\u001b[39m 512 512 train loss: 0.0000026 valid loss: 0.3170266 P@1: 0.66000 P@3: 0.49667 P@5: 0.40000 N@3: 0.53511 N@5: 0.52412 early stop: 1\n",
            "\u001b[32m[I 210629 04:14:57 models:110]\u001b[39m 514 1024 train loss: 0.0000025 valid loss: 0.3175845 P@1: 0.66000 P@3: 0.49667 P@5: 0.40000 N@3: 0.53511 N@5: 0.52412 early stop: 2\n",
            "\u001b[32m[I 210629 04:14:59 models:110]\u001b[39m 517 512 train loss: 0.0000026 valid loss: 0.3181418 P@1: 0.66000 P@3: 0.49667 P@5: 0.40000 N@3: 0.53511 N@5: 0.52412 early stop: 3\n",
            "\u001b[33m[W 210629 04:14:59 models:137]\u001b[39m Clipping gradients with total norm 0.00261 and max norm 0.00047\n",
            "\u001b[32m[I 210629 04:15:01 models:110]\u001b[39m 519 1024 train loss: 0.0000027 valid loss: 0.3186961 P@1: 0.66000 P@3: 0.49667 P@5: 0.39800 N@3: 0.53511 N@5: 0.52231 early stop: 4\n",
            "\u001b[33m[W 210629 04:15:02 models:137]\u001b[39m Clipping gradients with total norm 0.00206 and max norm 0.00027\n",
            "\u001b[32m[I 210629 04:15:04 models:110]\u001b[39m 522 512 train loss: 0.0000028 valid loss: 0.3192482 P@1: 0.66000 P@3: 0.49667 P@5: 0.39800 N@3: 0.53511 N@5: 0.52231 early stop: 5\n",
            "\u001b[32m[I 210629 04:15:06 models:110]\u001b[39m 524 1024 train loss: 0.0000025 valid loss: 0.3197975 P@1: 0.66000 P@3: 0.49667 P@5: 0.39600 N@3: 0.53511 N@5: 0.51994 early stop: 6\n",
            "\u001b[32m[I 210629 04:15:08 models:110]\u001b[39m 527 512 train loss: 0.0000026 valid loss: 0.3203431 P@1: 0.66000 P@3: 0.49667 P@5: 0.39600 N@3: 0.53511 N@5: 0.51994 early stop: 7\n",
            "\u001b[32m[I 210629 04:15:10 models:110]\u001b[39m 529 1024 train loss: 0.0000025 valid loss: 0.3208857 P@1: 0.66000 P@3: 0.49667 P@5: 0.39800 N@3: 0.53511 N@5: 0.52125 early stop: 8\n",
            "\u001b[32m[I 210629 04:15:13 models:110]\u001b[39m 532 512 train loss: 0.0000024 valid loss: 0.3214261 P@1: 0.66000 P@3: 0.49667 P@5: 0.39800 N@3: 0.53511 N@5: 0.52125 early stop: 9\n",
            "\u001b[32m[I 210629 04:15:15 models:110]\u001b[39m 534 1024 train loss: 0.0000025 valid loss: 0.3219648 P@1: 0.66000 P@3: 0.49667 P@5: 0.39800 N@3: 0.53511 N@5: 0.52125 early stop: 10\n",
            "\u001b[32m[I 210629 04:15:17 models:110]\u001b[39m 537 512 train loss: 0.0000023 valid loss: 0.3225026 P@1: 0.66000 P@3: 0.49667 P@5: 0.39800 N@3: 0.53511 N@5: 0.52140 early stop: 11\n",
            "\u001b[33m[W 210629 04:15:19 models:137]\u001b[39m Clipping gradients with total norm 0.00597 and max norm 0.00082\n",
            "\u001b[32m[I 210629 04:15:19 models:110]\u001b[39m 539 1024 train loss: 0.0000030 valid loss: 0.3230394 P@1: 0.66000 P@3: 0.49667 P@5: 0.39800 N@3: 0.53511 N@5: 0.52140 early stop: 12\n",
            "\u001b[32m[I 210629 04:15:22 models:110]\u001b[39m 542 512 train loss: 0.0000023 valid loss: 0.3235735 P@1: 0.66000 P@3: 0.49667 P@5: 0.39800 N@3: 0.53511 N@5: 0.52140 early stop: 13\n",
            "\u001b[32m[I 210629 04:15:24 models:110]\u001b[39m 544 1024 train loss: 0.0000023 valid loss: 0.3241034 P@1: 0.66000 P@3: 0.49667 P@5: 0.39800 N@3: 0.53511 N@5: 0.52155 early stop: 14\n",
            "\u001b[32m[I 210629 04:15:26 models:110]\u001b[39m 547 512 train loss: 0.0000019 valid loss: 0.3246292 P@1: 0.66000 P@3: 0.49667 P@5: 0.39800 N@3: 0.53511 N@5: 0.52155 early stop: 15\n",
            "\u001b[32m[I 210629 04:15:28 models:110]\u001b[39m 549 1024 train loss: 0.0000022 valid loss: 0.3251505 P@1: 0.66000 P@3: 0.49667 P@5: 0.40000 N@3: 0.53511 N@5: 0.52286 early stop: 16\n",
            "\u001b[32m[I 210629 04:15:31 models:110]\u001b[39m 552 512 train loss: 0.0000026 valid loss: 0.3256712 P@1: 0.66000 P@3: 0.49667 P@5: 0.40000 N@3: 0.53511 N@5: 0.52286 early stop: 17\n",
            "\u001b[32m[I 210629 04:15:33 models:110]\u001b[39m 554 1024 train loss: 0.0000023 valid loss: 0.3261900 P@1: 0.66000 P@3: 0.49667 P@5: 0.40000 N@3: 0.53511 N@5: 0.52286 early stop: 18\n",
            "\u001b[32m[I 210629 04:15:35 models:110]\u001b[39m 557 512 train loss: 0.0000029 valid loss: 0.3267042 P@1: 0.66000 P@3: 0.50000 P@5: 0.40000 N@3: 0.53745 N@5: 0.52309 early stop: 19\n",
            "\u001b[32m[I 210629 04:15:37 models:110]\u001b[39m 559 1024 train loss: 0.0000022 valid loss: 0.3272149 P@1: 0.66000 P@3: 0.50000 P@5: 0.40000 N@3: 0.53745 N@5: 0.52309 early stop: 20\n",
            "\u001b[32m[I 210629 04:15:40 models:110]\u001b[39m 562 512 train loss: 0.0000020 valid loss: 0.3277237 P@1: 0.66000 P@3: 0.50000 P@5: 0.40200 N@3: 0.53745 N@5: 0.52441 early stop: 0\n",
            "\u001b[33m[W 210629 04:15:41 models:137]\u001b[39m Clipping gradients with total norm 0.00696 and max norm 0.00068\n",
            "\u001b[32m[I 210629 04:15:42 models:110]\u001b[39m 564 1024 train loss: 0.0000029 valid loss: 0.3282318 P@1: 0.66000 P@3: 0.50000 P@5: 0.40200 N@3: 0.53745 N@5: 0.52441 early stop: 1\n",
            "\u001b[33m[W 210629 04:15:43 models:137]\u001b[39m Clipping gradients with total norm 0.00422 and max norm 0.00058\n",
            "\u001b[32m[I 210629 04:15:44 models:110]\u001b[39m 567 512 train loss: 0.0000025 valid loss: 0.3287394 P@1: 0.66000 P@3: 0.50000 P@5: 0.40400 N@3: 0.53745 N@5: 0.52572 early stop: 0\n",
            "\u001b[33m[W 210629 04:15:45 models:137]\u001b[39m Clipping gradients with total norm 0.00701 and max norm 0.00116\n",
            "\u001b[33m[W 210629 04:15:45 models:137]\u001b[39m Clipping gradients with total norm 0.01361 and max norm 0.00232\n",
            "\u001b[32m[I 210629 04:15:46 models:110]\u001b[39m 569 1024 train loss: 0.0000056 valid loss: 0.3292371 P@1: 0.66000 P@3: 0.50000 P@5: 0.40400 N@3: 0.53745 N@5: 0.52572 early stop: 1\n",
            "\u001b[33m[W 210629 04:15:48 models:137]\u001b[39m Clipping gradients with total norm 0.08469 and max norm 0.00161\n",
            "\u001b[32m[I 210629 04:15:49 models:110]\u001b[39m 572 512 train loss: 0.0000415 valid loss: 0.3297361 P@1: 0.66000 P@3: 0.50000 P@5: 0.40400 N@3: 0.53745 N@5: 0.52572 early stop: 2\n",
            "\u001b[32m[I 210629 04:15:51 models:110]\u001b[39m 574 1024 train loss: 0.0000070 valid loss: 0.3302275 P@1: 0.66000 P@3: 0.50000 P@5: 0.40400 N@3: 0.53745 N@5: 0.52572 early stop: 3\n",
            "\u001b[32m[I 210629 04:15:54 models:110]\u001b[39m 577 512 train loss: 0.0000038 valid loss: 0.3307186 P@1: 0.66000 P@3: 0.50000 P@5: 0.40400 N@3: 0.53745 N@5: 0.52572 early stop: 4\n",
            "\u001b[32m[I 210629 04:15:56 models:110]\u001b[39m 579 1024 train loss: 0.0000030 valid loss: 0.3312124 P@1: 0.66000 P@3: 0.50000 P@5: 0.40400 N@3: 0.53745 N@5: 0.52572 early stop: 5\n",
            "\u001b[33m[W 210629 04:15:56 models:137]\u001b[39m Clipping gradients with total norm 0.0063 and max norm 0.00105\n",
            "\u001b[32m[I 210629 04:15:58 models:110]\u001b[39m 582 512 train loss: 0.0000029 valid loss: 0.3317086 P@1: 0.66000 P@3: 0.50000 P@5: 0.40600 N@3: 0.53745 N@5: 0.52703 early stop: 0\n",
            "\u001b[32m[I 210629 04:16:00 models:110]\u001b[39m 584 1024 train loss: 0.0000021 valid loss: 0.3322045 P@1: 0.66000 P@3: 0.50000 P@5: 0.40600 N@3: 0.53745 N@5: 0.52703 early stop: 1\n",
            "\u001b[33m[W 210629 04:16:01 models:137]\u001b[39m Clipping gradients with total norm 0.00421 and max norm 0.00044\n",
            "\u001b[32m[I 210629 04:16:03 models:110]\u001b[39m 587 512 train loss: 0.0000026 valid loss: 0.3326977 P@1: 0.66000 P@3: 0.50000 P@5: 0.40600 N@3: 0.53745 N@5: 0.52703 early stop: 2\n",
            "\u001b[32m[I 210629 04:16:05 models:110]\u001b[39m 589 1024 train loss: 0.0000021 valid loss: 0.3331866 P@1: 0.66000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53511 N@5: 0.52679 early stop: 3\n",
            "\u001b[32m[I 210629 04:16:07 models:110]\u001b[39m 592 512 train loss: 0.0000020 valid loss: 0.3336716 P@1: 0.66000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53511 N@5: 0.52679 early stop: 4\n",
            "\u001b[32m[I 210629 04:16:09 models:110]\u001b[39m 594 1024 train loss: 0.0000016 valid loss: 0.3341542 P@1: 0.66000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53511 N@5: 0.52679 early stop: 5\n",
            "\u001b[32m[I 210629 04:16:12 models:110]\u001b[39m 597 512 train loss: 0.0000018 valid loss: 0.3346346 P@1: 0.66000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53511 N@5: 0.52679 early stop: 6\n",
            "\u001b[32m[I 210629 04:16:14 models:110]\u001b[39m 599 1024 train loss: 0.0000024 valid loss: 0.3351136 P@1: 0.66000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53511 N@5: 0.52679 early stop: 7\n",
            "\u001b[32m[I 210629 04:16:16 models:110]\u001b[39m 602 512 train loss: 0.0000019 valid loss: 0.3355914 P@1: 0.66000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53511 N@5: 0.52679 early stop: 8\n",
            "\u001b[32m[I 210629 04:16:18 models:110]\u001b[39m 604 1024 train loss: 0.0000021 valid loss: 0.3360693 P@1: 0.66000 P@3: 0.50000 P@5: 0.40600 N@3: 0.53745 N@5: 0.52712 early stop: 0\n",
            "\u001b[32m[I 210629 04:16:21 models:110]\u001b[39m 607 512 train loss: 0.0000019 valid loss: 0.3365465 P@1: 0.66000 P@3: 0.50000 P@5: 0.40600 N@3: 0.53745 N@5: 0.52712 early stop: 1\n",
            "\u001b[32m[I 210629 04:16:23 models:110]\u001b[39m 609 1024 train loss: 0.0000022 valid loss: 0.3370220 P@1: 0.66000 P@3: 0.50000 P@5: 0.40600 N@3: 0.53745 N@5: 0.52712 early stop: 2\n",
            "\u001b[33m[W 210629 04:16:25 models:137]\u001b[39m Clipping gradients with total norm 0.00228 and max norm 0.00042\n",
            "\u001b[32m[I 210629 04:16:25 models:110]\u001b[39m 612 512 train loss: 0.0000022 valid loss: 0.3374937 P@1: 0.66000 P@3: 0.50000 P@5: 0.40600 N@3: 0.53745 N@5: 0.52712 early stop: 3\n",
            "\u001b[32m[I 210629 04:16:27 models:110]\u001b[39m 614 1024 train loss: 0.0000016 valid loss: 0.3379620 P@1: 0.66000 P@3: 0.50000 P@5: 0.40600 N@3: 0.53745 N@5: 0.52712 early stop: 4\n",
            "\u001b[32m[I 210629 04:16:30 models:110]\u001b[39m 617 512 train loss: 0.0000015 valid loss: 0.3384285 P@1: 0.66000 P@3: 0.50000 P@5: 0.40600 N@3: 0.53745 N@5: 0.52712 early stop: 5\n",
            "\u001b[32m[I 210629 04:16:32 models:110]\u001b[39m 619 1024 train loss: 0.0000016 valid loss: 0.3388932 P@1: 0.66000 P@3: 0.50000 P@5: 0.40600 N@3: 0.53745 N@5: 0.52712 early stop: 6\n",
            "\u001b[32m[I 210629 04:16:34 models:110]\u001b[39m 622 512 train loss: 0.0000017 valid loss: 0.3393568 P@1: 0.66000 P@3: 0.50000 P@5: 0.40600 N@3: 0.53745 N@5: 0.52712 early stop: 7\n",
            "\u001b[32m[I 210629 04:16:36 models:110]\u001b[39m 624 1024 train loss: 0.0000016 valid loss: 0.3398193 P@1: 0.66000 P@3: 0.50000 P@5: 0.40600 N@3: 0.53745 N@5: 0.52712 early stop: 8\n",
            "\u001b[33m[W 210629 04:16:37 models:137]\u001b[39m Clipping gradients with total norm 0.00337 and max norm 0.00042\n",
            "\u001b[32m[I 210629 04:16:39 models:110]\u001b[39m 627 512 train loss: 0.0000021 valid loss: 0.3402812 P@1: 0.66000 P@3: 0.50000 P@5: 0.40600 N@3: 0.53745 N@5: 0.52712 early stop: 9\n",
            "\u001b[32m[I 210629 04:16:41 models:110]\u001b[39m 629 1024 train loss: 0.0000015 valid loss: 0.3407432 P@1: 0.66000 P@3: 0.50000 P@5: 0.40600 N@3: 0.53745 N@5: 0.52712 early stop: 10\n",
            "\u001b[32m[I 210629 04:16:43 models:110]\u001b[39m 632 512 train loss: 0.0000016 valid loss: 0.3412056 P@1: 0.66000 P@3: 0.50000 P@5: 0.40600 N@3: 0.53745 N@5: 0.52712 early stop: 11\n",
            "\u001b[32m[I 210629 04:16:45 models:110]\u001b[39m 634 1024 train loss: 0.0000018 valid loss: 0.3416662 P@1: 0.66000 P@3: 0.50000 P@5: 0.40600 N@3: 0.53745 N@5: 0.52712 early stop: 12\n",
            "\u001b[33m[W 210629 04:16:47 models:137]\u001b[39m Clipping gradients with total norm 0.0062 and max norm 0.00024\n",
            "\u001b[32m[I 210629 04:16:48 models:110]\u001b[39m 637 512 train loss: 0.0000021 valid loss: 0.3421265 P@1: 0.66000 P@3: 0.50000 P@5: 0.40600 N@3: 0.53745 N@5: 0.52727 early stop: 0\n",
            "\u001b[33m[W 210629 04:16:49 models:137]\u001b[39m Clipping gradients with total norm 0.00245 and max norm 0.00047\n",
            "\u001b[32m[I 210629 04:16:50 models:110]\u001b[39m 639 1024 train loss: 0.0000021 valid loss: 0.3425827 P@1: 0.66000 P@3: 0.50000 P@5: 0.40600 N@3: 0.53745 N@5: 0.52727 early stop: 1\n",
            "\u001b[33m[W 210629 04:16:52 models:137]\u001b[39m Clipping gradients with total norm 0.00222 and max norm 0.00028\n",
            "\u001b[32m[I 210629 04:16:52 models:110]\u001b[39m 642 512 train loss: 0.0000017 valid loss: 0.3430363 P@1: 0.66000 P@3: 0.50000 P@5: 0.40600 N@3: 0.53745 N@5: 0.52727 early stop: 2\n",
            "\u001b[32m[I 210629 04:16:54 models:110]\u001b[39m 644 1024 train loss: 0.0000016 valid loss: 0.3434880 P@1: 0.66000 P@3: 0.50000 P@5: 0.40600 N@3: 0.53745 N@5: 0.52727 early stop: 3\n",
            "\u001b[32m[I 210629 04:16:57 models:110]\u001b[39m 647 512 train loss: 0.0000023 valid loss: 0.3439371 P@1: 0.66000 P@3: 0.50000 P@5: 0.40600 N@3: 0.53745 N@5: 0.52727 early stop: 4\n",
            "\u001b[33m[W 210629 04:16:58 models:137]\u001b[39m Clipping gradients with total norm 0.00556 and max norm 0.00053\n",
            "\u001b[32m[I 210629 04:16:59 models:110]\u001b[39m 649 1024 train loss: 0.0000027 valid loss: 0.3443815 P@1: 0.66000 P@3: 0.50000 P@5: 0.40600 N@3: 0.53745 N@5: 0.52727 early stop: 5\n",
            "\u001b[32m[I 210629 04:17:01 models:110]\u001b[39m 652 512 train loss: 0.0000014 valid loss: 0.3448222 P@1: 0.66000 P@3: 0.50000 P@5: 0.40400 N@3: 0.53745 N@5: 0.52596 early stop: 6\n",
            "\u001b[32m[I 210629 04:17:03 models:110]\u001b[39m 654 1024 train loss: 0.0000016 valid loss: 0.3452615 P@1: 0.66000 P@3: 0.50000 P@5: 0.40400 N@3: 0.53745 N@5: 0.52596 early stop: 7\n",
            "\u001b[32m[I 210629 04:17:06 models:110]\u001b[39m 657 512 train loss: 0.0000013 valid loss: 0.3457010 P@1: 0.66000 P@3: 0.50000 P@5: 0.40400 N@3: 0.53745 N@5: 0.52596 early stop: 8\n",
            "\u001b[32m[I 210629 04:17:08 models:110]\u001b[39m 659 1024 train loss: 0.0000016 valid loss: 0.3461388 P@1: 0.66000 P@3: 0.50000 P@5: 0.40400 N@3: 0.53745 N@5: 0.52596 early stop: 9\n",
            "\u001b[32m[I 210629 04:17:10 models:110]\u001b[39m 662 512 train loss: 0.0000016 valid loss: 0.3465742 P@1: 0.66000 P@3: 0.50000 P@5: 0.40400 N@3: 0.53745 N@5: 0.52596 early stop: 10\n",
            "\u001b[33m[W 210629 04:17:11 models:137]\u001b[39m Clipping gradients with total norm 0.00701 and max norm 0.00082\n",
            "\u001b[33m[W 210629 04:17:12 models:137]\u001b[39m Clipping gradients with total norm 0.0061 and max norm 0.00039\n",
            "\u001b[32m[I 210629 04:17:12 models:110]\u001b[39m 664 1024 train loss: 0.0000030 valid loss: 0.3470118 P@1: 0.66000 P@3: 0.50000 P@5: 0.40400 N@3: 0.53745 N@5: 0.52596 early stop: 11\n",
            "\u001b[32m[I 210629 04:17:15 models:110]\u001b[39m 667 512 train loss: 0.0000016 valid loss: 0.3474499 P@1: 0.66000 P@3: 0.50000 P@5: 0.40400 N@3: 0.53745 N@5: 0.52596 early stop: 12\n",
            "\u001b[32m[I 210629 04:17:17 models:110]\u001b[39m 669 1024 train loss: 0.0000019 valid loss: 0.3478853 P@1: 0.66000 P@3: 0.50000 P@5: 0.40400 N@3: 0.53745 N@5: 0.52596 early stop: 13\n",
            "\u001b[32m[I 210629 04:17:19 models:110]\u001b[39m 672 512 train loss: 0.0000014 valid loss: 0.3483171 P@1: 0.66000 P@3: 0.50000 P@5: 0.40400 N@3: 0.53745 N@5: 0.52596 early stop: 14\n",
            "\u001b[32m[I 210629 04:17:21 models:110]\u001b[39m 674 1024 train loss: 0.0000021 valid loss: 0.3487481 P@1: 0.66000 P@3: 0.50000 P@5: 0.40400 N@3: 0.53745 N@5: 0.52596 early stop: 15\n",
            "\u001b[32m[I 210629 04:17:24 models:110]\u001b[39m 677 512 train loss: 0.0000014 valid loss: 0.3491803 P@1: 0.66000 P@3: 0.50000 P@5: 0.40400 N@3: 0.53745 N@5: 0.52596 early stop: 16\n",
            "\u001b[32m[I 210629 04:17:26 models:110]\u001b[39m 679 1024 train loss: 0.0000012 valid loss: 0.3496114 P@1: 0.66000 P@3: 0.50333 P@5: 0.40400 N@3: 0.53980 N@5: 0.52619 early stop: 17\n",
            "\u001b[32m[I 210629 04:17:28 models:110]\u001b[39m 682 512 train loss: 0.0000014 valid loss: 0.3500407 P@1: 0.66000 P@3: 0.50333 P@5: 0.40400 N@3: 0.53980 N@5: 0.52619 early stop: 18\n",
            "\u001b[32m[I 210629 04:17:31 models:110]\u001b[39m 684 1024 train loss: 0.0000012 valid loss: 0.3504675 P@1: 0.66000 P@3: 0.50333 P@5: 0.40400 N@3: 0.53980 N@5: 0.52619 early stop: 19\n",
            "\u001b[32m[I 210629 04:17:33 models:110]\u001b[39m 687 512 train loss: 0.0000012 valid loss: 0.3508912 P@1: 0.66000 P@3: 0.50333 P@5: 0.40400 N@3: 0.53980 N@5: 0.52619 early stop: 20\n",
            "\u001b[32m[I 210629 04:17:35 models:110]\u001b[39m 689 1024 train loss: 0.0000012 valid loss: 0.3513132 P@1: 0.66000 P@3: 0.50333 P@5: 0.40400 N@3: 0.53980 N@5: 0.52619 early stop: 21\n",
            "\u001b[33m[W 210629 04:17:36 models:137]\u001b[39m Clipping gradients with total norm 0.00107 and max norm 0.00011\n",
            "\u001b[32m[I 210629 04:17:38 models:110]\u001b[39m 692 512 train loss: 0.0000013 valid loss: 0.3517335 P@1: 0.66000 P@3: 0.50333 P@5: 0.40400 N@3: 0.53980 N@5: 0.52619 early stop: 22\n",
            "\u001b[33m[W 210629 04:17:38 models:137]\u001b[39m Clipping gradients with total norm 0.00086 and max norm 0.00014\n",
            "\u001b[32m[I 210629 04:17:40 models:110]\u001b[39m 694 1024 train loss: 0.0000012 valid loss: 0.3521521 P@1: 0.66000 P@3: 0.50333 P@5: 0.40400 N@3: 0.53980 N@5: 0.52619 early stop: 23\n",
            "\u001b[32m[I 210629 04:17:42 models:110]\u001b[39m 697 512 train loss: 0.0000012 valid loss: 0.3525690 P@1: 0.67000 P@3: 0.50333 P@5: 0.40400 N@3: 0.54153 N@5: 0.52763 early stop: 0\n",
            "\u001b[32m[I 210629 04:17:44 models:110]\u001b[39m 699 1024 train loss: 0.0000013 valid loss: 0.3529871 P@1: 0.67000 P@3: 0.50333 P@5: 0.40400 N@3: 0.54153 N@5: 0.52763 early stop: 1\n",
            "\u001b[33m[W 210629 04:17:46 models:137]\u001b[39m Clipping gradients with total norm 0.0015 and max norm 0.00027\n",
            "\u001b[32m[I 210629 04:17:47 models:110]\u001b[39m 702 512 train loss: 0.0000016 valid loss: 0.3534050 P@1: 0.67000 P@3: 0.50333 P@5: 0.40400 N@3: 0.54153 N@5: 0.52763 early stop: 2\n",
            "\u001b[32m[I 210629 04:17:49 models:110]\u001b[39m 704 1024 train loss: 0.0000011 valid loss: 0.3538232 P@1: 0.67000 P@3: 0.50333 P@5: 0.40400 N@3: 0.54153 N@5: 0.52763 early stop: 3\n",
            "\u001b[32m[I 210629 04:17:51 models:110]\u001b[39m 707 512 train loss: 0.0000013 valid loss: 0.3542370 P@1: 0.67000 P@3: 0.50333 P@5: 0.40400 N@3: 0.54153 N@5: 0.52763 early stop: 4\n",
            "\u001b[32m[I 210629 04:17:53 models:110]\u001b[39m 709 1024 train loss: 0.0000015 valid loss: 0.3546486 P@1: 0.67000 P@3: 0.50333 P@5: 0.40400 N@3: 0.54153 N@5: 0.52763 early stop: 5\n",
            "\u001b[32m[I 210629 04:17:56 models:110]\u001b[39m 712 512 train loss: 0.0000012 valid loss: 0.3550586 P@1: 0.67000 P@3: 0.50333 P@5: 0.40400 N@3: 0.54153 N@5: 0.52763 early stop: 6\n",
            "\u001b[33m[W 210629 04:17:57 models:137]\u001b[39m Clipping gradients with total norm 0.0117 and max norm 0.00056\n",
            "\u001b[32m[I 210629 04:17:58 models:110]\u001b[39m 714 1024 train loss: 0.0000021 valid loss: 0.3554676 P@1: 0.67000 P@3: 0.50333 P@5: 0.40400 N@3: 0.54153 N@5: 0.52763 early stop: 7\n",
            "\u001b[32m[I 210629 04:18:00 models:110]\u001b[39m 717 512 train loss: 0.0000011 valid loss: 0.3558766 P@1: 0.67000 P@3: 0.50333 P@5: 0.40600 N@3: 0.54153 N@5: 0.52914 early stop: 0\n",
            "\u001b[32m[I 210629 04:18:02 models:110]\u001b[39m 719 1024 train loss: 0.0000012 valid loss: 0.3562838 P@1: 0.67000 P@3: 0.50333 P@5: 0.40600 N@3: 0.54153 N@5: 0.52914 early stop: 1\n",
            "\u001b[32m[I 210629 04:18:05 models:110]\u001b[39m 722 512 train loss: 0.0000012 valid loss: 0.3566913 P@1: 0.67000 P@3: 0.50333 P@5: 0.40600 N@3: 0.54153 N@5: 0.52914 early stop: 2\n",
            "\u001b[32m[I 210629 04:18:07 models:110]\u001b[39m 724 1024 train loss: 0.0000012 valid loss: 0.3570990 P@1: 0.67000 P@3: 0.50333 P@5: 0.40600 N@3: 0.54153 N@5: 0.52914 early stop: 3\n",
            "\u001b[32m[I 210629 04:18:09 models:110]\u001b[39m 727 512 train loss: 0.0000012 valid loss: 0.3575058 P@1: 0.67000 P@3: 0.50333 P@5: 0.40800 N@3: 0.54153 N@5: 0.53096 early stop: 0\n",
            "\u001b[32m[I 210629 04:18:11 models:110]\u001b[39m 729 1024 train loss: 0.0000011 valid loss: 0.3579111 P@1: 0.67000 P@3: 0.50333 P@5: 0.40800 N@3: 0.54153 N@5: 0.53096 early stop: 1\n",
            "\u001b[32m[I 210629 04:18:14 models:110]\u001b[39m 732 512 train loss: 0.0000013 valid loss: 0.3583124 P@1: 0.67000 P@3: 0.50667 P@5: 0.40800 N@3: 0.54388 N@5: 0.53119 early stop: 0\n",
            "\u001b[32m[I 210629 04:18:16 models:110]\u001b[39m 734 1024 train loss: 0.0000011 valid loss: 0.3587116 P@1: 0.67000 P@3: 0.50667 P@5: 0.40800 N@3: 0.54388 N@5: 0.53119 early stop: 1\n",
            "\u001b[32m[I 210629 04:18:18 models:110]\u001b[39m 737 512 train loss: 0.0000009 valid loss: 0.3591094 P@1: 0.67000 P@3: 0.50667 P@5: 0.40800 N@3: 0.54388 N@5: 0.53119 early stop: 2\n",
            "\u001b[33m[W 210629 04:18:20 models:137]\u001b[39m Clipping gradients with total norm 0.00478 and max norm 0.00051\n",
            "\u001b[32m[I 210629 04:18:20 models:110]\u001b[39m 739 1024 train loss: 0.0000019 valid loss: 0.3595048 P@1: 0.67000 P@3: 0.50667 P@5: 0.40800 N@3: 0.54388 N@5: 0.53119 early stop: 3\n",
            "\u001b[32m[I 210629 04:18:23 models:110]\u001b[39m 742 512 train loss: 0.0000010 valid loss: 0.3598987 P@1: 0.67000 P@3: 0.50667 P@5: 0.40800 N@3: 0.54388 N@5: 0.53119 early stop: 4\n",
            "\u001b[32m[I 210629 04:18:25 models:110]\u001b[39m 744 1024 train loss: 0.0000012 valid loss: 0.3602923 P@1: 0.67000 P@3: 0.50667 P@5: 0.40800 N@3: 0.54388 N@5: 0.53119 early stop: 5\n",
            "\u001b[33m[W 210629 04:18:25 models:137]\u001b[39m Clipping gradients with total norm 0.00885 and max norm 0.00033\n",
            "\u001b[32m[I 210629 04:18:27 models:110]\u001b[39m 747 512 train loss: 0.0000025 valid loss: 0.3606857 P@1: 0.67000 P@3: 0.50667 P@5: 0.40800 N@3: 0.54388 N@5: 0.53119 early stop: 6\n",
            "\u001b[32m[I 210629 04:18:30 models:110]\u001b[39m 749 1024 train loss: 0.0000009 valid loss: 0.3610788 P@1: 0.67000 P@3: 0.50667 P@5: 0.40800 N@3: 0.54388 N@5: 0.53119 early stop: 7\n",
            "\u001b[32m[I 210629 04:18:32 models:110]\u001b[39m 752 512 train loss: 0.0000009 valid loss: 0.3614710 P@1: 0.67000 P@3: 0.50667 P@5: 0.40800 N@3: 0.54388 N@5: 0.53119 early stop: 8\n",
            "\u001b[33m[W 210629 04:18:34 models:137]\u001b[39m Clipping gradients with total norm 0.005 and max norm 0.00032\n",
            "\u001b[32m[I 210629 04:18:34 models:110]\u001b[39m 754 1024 train loss: 0.0000016 valid loss: 0.3618611 P@1: 0.67000 P@3: 0.50667 P@5: 0.40800 N@3: 0.54388 N@5: 0.53119 early stop: 9\n",
            "\u001b[32m[I 210629 04:18:36 models:110]\u001b[39m 757 512 train loss: 0.0000009 valid loss: 0.3622484 P@1: 0.67000 P@3: 0.50667 P@5: 0.40800 N@3: 0.54388 N@5: 0.53119 early stop: 10\n",
            "\u001b[32m[I 210629 04:18:39 models:110]\u001b[39m 759 1024 train loss: 0.0000012 valid loss: 0.3626350 P@1: 0.67000 P@3: 0.50667 P@5: 0.40800 N@3: 0.54388 N@5: 0.53134 early stop: 0\n",
            "\u001b[32m[I 210629 04:18:41 models:110]\u001b[39m 762 512 train loss: 0.0000010 valid loss: 0.3630210 P@1: 0.67000 P@3: 0.50667 P@5: 0.40800 N@3: 0.54388 N@5: 0.53134 early stop: 1\n",
            "\u001b[32m[I 210629 04:18:43 models:110]\u001b[39m 764 1024 train loss: 0.0000011 valid loss: 0.3634074 P@1: 0.67000 P@3: 0.50667 P@5: 0.40800 N@3: 0.54388 N@5: 0.53134 early stop: 2\n",
            "\u001b[32m[I 210629 04:18:46 models:110]\u001b[39m 767 512 train loss: 0.0000009 valid loss: 0.3637932 P@1: 0.67000 P@3: 0.50667 P@5: 0.40800 N@3: 0.54388 N@5: 0.53134 early stop: 3\n",
            "\u001b[32m[I 210629 04:18:48 models:110]\u001b[39m 769 1024 train loss: 0.0000010 valid loss: 0.3641775 P@1: 0.67000 P@3: 0.50667 P@5: 0.40800 N@3: 0.54388 N@5: 0.53134 early stop: 4\n",
            "\u001b[32m[I 210629 04:18:50 models:110]\u001b[39m 772 512 train loss: 0.0000009 valid loss: 0.3645600 P@1: 0.67000 P@3: 0.50667 P@5: 0.40800 N@3: 0.54388 N@5: 0.53134 early stop: 5\n",
            "\u001b[32m[I 210629 04:18:52 models:110]\u001b[39m 774 1024 train loss: 0.0000008 valid loss: 0.3649412 P@1: 0.67000 P@3: 0.50667 P@5: 0.40800 N@3: 0.54388 N@5: 0.53134 early stop: 6\n",
            "\u001b[32m[I 210629 04:18:55 models:110]\u001b[39m 777 512 train loss: 0.0000011 valid loss: 0.3653222 P@1: 0.67000 P@3: 0.50667 P@5: 0.40800 N@3: 0.54388 N@5: 0.53134 early stop: 7\n",
            "\u001b[32m[I 210629 04:18:57 models:110]\u001b[39m 779 1024 train loss: 0.0000008 valid loss: 0.3657026 P@1: 0.67000 P@3: 0.50667 P@5: 0.40800 N@3: 0.54388 N@5: 0.53134 early stop: 8\n",
            "\u001b[32m[I 210629 04:18:59 models:110]\u001b[39m 782 512 train loss: 0.0000008 valid loss: 0.3660824 P@1: 0.67000 P@3: 0.50667 P@5: 0.40800 N@3: 0.54326 N@5: 0.53090 early stop: 9\n",
            "\u001b[32m[I 210629 04:19:01 models:110]\u001b[39m 784 1024 train loss: 0.0000008 valid loss: 0.3664613 P@1: 0.67000 P@3: 0.50667 P@5: 0.40800 N@3: 0.54326 N@5: 0.53090 early stop: 10\n",
            "\u001b[32m[I 210629 04:19:04 models:110]\u001b[39m 787 512 train loss: 0.0000008 valid loss: 0.3668391 P@1: 0.67000 P@3: 0.50667 P@5: 0.40800 N@3: 0.54326 N@5: 0.53090 early stop: 11\n",
            "\u001b[33m[W 210629 04:19:05 models:137]\u001b[39m Clipping gradients with total norm 0.00231 and max norm 0.00023\n",
            "\u001b[32m[I 210629 04:19:06 models:110]\u001b[39m 789 1024 train loss: 0.0000012 valid loss: 0.3672156 P@1: 0.67000 P@3: 0.50667 P@5: 0.40800 N@3: 0.54326 N@5: 0.53090 early stop: 12\n",
            "\u001b[32m[I 210629 04:19:08 models:110]\u001b[39m 792 512 train loss: 0.0000010 valid loss: 0.3675908 P@1: 0.67000 P@3: 0.50667 P@5: 0.40800 N@3: 0.54326 N@5: 0.53090 early stop: 13\n",
            "\u001b[32m[I 210629 04:19:10 models:110]\u001b[39m 794 1024 train loss: 0.0000010 valid loss: 0.3679636 P@1: 0.67000 P@3: 0.50667 P@5: 0.40800 N@3: 0.54326 N@5: 0.53090 early stop: 14\n",
            "\u001b[32m[I 210629 04:19:13 models:110]\u001b[39m 797 512 train loss: 0.0000009 valid loss: 0.3683340 P@1: 0.67000 P@3: 0.50667 P@5: 0.40800 N@3: 0.54326 N@5: 0.53090 early stop: 15\n",
            "\u001b[32m[I 210629 04:19:15 models:110]\u001b[39m 799 1024 train loss: 0.0000010 valid loss: 0.3687021 P@1: 0.67000 P@3: 0.50667 P@5: 0.40800 N@3: 0.54326 N@5: 0.53090 early stop: 16\n",
            "\u001b[33m[W 210629 04:19:16 models:137]\u001b[39m Clipping gradients with total norm 0.00172 and max norm 0.00031\n",
            "\u001b[32m[I 210629 04:19:17 models:110]\u001b[39m 802 512 train loss: 0.0000011 valid loss: 0.3690699 P@1: 0.67000 P@3: 0.50667 P@5: 0.40800 N@3: 0.54326 N@5: 0.53090 early stop: 17\n",
            "\u001b[32m[I 210629 04:19:19 models:110]\u001b[39m 804 1024 train loss: 0.0000009 valid loss: 0.3694373 P@1: 0.67000 P@3: 0.50667 P@5: 0.40800 N@3: 0.54326 N@5: 0.53090 early stop: 18\n",
            "\u001b[32m[I 210629 04:19:22 models:110]\u001b[39m 807 512 train loss: 0.0000009 valid loss: 0.3698039 P@1: 0.67000 P@3: 0.50667 P@5: 0.40800 N@3: 0.54326 N@5: 0.53090 early stop: 19\n",
            "\u001b[33m[W 210629 04:19:22 models:137]\u001b[39m Clipping gradients with total norm 0.00849 and max norm 0.00025\n",
            "\u001b[32m[I 210629 04:19:24 models:110]\u001b[39m 809 1024 train loss: 0.0000016 valid loss: 0.3701702 P@1: 0.67000 P@3: 0.50667 P@5: 0.40800 N@3: 0.54326 N@5: 0.53090 early stop: 20\n",
            "\u001b[32m[I 210629 04:19:26 models:110]\u001b[39m 812 512 train loss: 0.0000009 valid loss: 0.3705357 P@1: 0.67000 P@3: 0.50667 P@5: 0.40800 N@3: 0.54326 N@5: 0.53090 early stop: 21\n",
            "\u001b[32m[I 210629 04:19:28 models:110]\u001b[39m 814 1024 train loss: 0.0000006 valid loss: 0.3709000 P@1: 0.67000 P@3: 0.50667 P@5: 0.40800 N@3: 0.54326 N@5: 0.53090 early stop: 22\n",
            "\u001b[32m[I 210629 04:19:31 models:110]\u001b[39m 817 512 train loss: 0.0000008 valid loss: 0.3712639 P@1: 0.67000 P@3: 0.50667 P@5: 0.40800 N@3: 0.54326 N@5: 0.53090 early stop: 23\n",
            "\u001b[32m[I 210629 04:19:33 models:110]\u001b[39m 819 1024 train loss: 0.0000007 valid loss: 0.3716273 P@1: 0.67000 P@3: 0.51000 P@5: 0.40800 N@3: 0.54561 N@5: 0.53122 early stop: 24\n",
            "\u001b[33m[W 210629 04:19:33 models:137]\u001b[39m Clipping gradients with total norm 0.0025 and max norm 0.00015\n",
            "\u001b[32m[I 210629 04:19:35 models:110]\u001b[39m 822 512 train loss: 0.0000009 valid loss: 0.3719907 P@1: 0.67000 P@3: 0.51000 P@5: 0.40800 N@3: 0.54561 N@5: 0.53122 early stop: 25\n",
            "\u001b[32m[I 210629 04:19:37 models:110]\u001b[39m 824 1024 train loss: 0.0000006 valid loss: 0.3723535 P@1: 0.67000 P@3: 0.51000 P@5: 0.40800 N@3: 0.54622 N@5: 0.53184 early stop: 0\n",
            "\u001b[32m[I 210629 04:19:40 models:110]\u001b[39m 827 512 train loss: 0.0000008 valid loss: 0.3727144 P@1: 0.67000 P@3: 0.51000 P@5: 0.40800 N@3: 0.54622 N@5: 0.53184 early stop: 1\n",
            "\u001b[32m[I 210629 04:19:42 models:110]\u001b[39m 829 1024 train loss: 0.0000008 valid loss: 0.3730721 P@1: 0.67000 P@3: 0.51000 P@5: 0.40800 N@3: 0.54622 N@5: 0.53184 early stop: 2\n",
            "\u001b[32m[I 210629 04:19:44 models:110]\u001b[39m 832 512 train loss: 0.0000009 valid loss: 0.3734281 P@1: 0.67000 P@3: 0.51000 P@5: 0.40800 N@3: 0.54622 N@5: 0.53184 early stop: 3\n",
            "\u001b[32m[I 210629 04:19:47 models:110]\u001b[39m 834 1024 train loss: 0.0000008 valid loss: 0.3737817 P@1: 0.67000 P@3: 0.51000 P@5: 0.40800 N@3: 0.54622 N@5: 0.53184 early stop: 4\n",
            "\u001b[32m[I 210629 04:19:49 models:110]\u001b[39m 837 512 train loss: 0.0000007 valid loss: 0.3741346 P@1: 0.67000 P@3: 0.51000 P@5: 0.40800 N@3: 0.54622 N@5: 0.53184 early stop: 5\n",
            "\u001b[33m[W 210629 04:19:50 models:137]\u001b[39m Clipping gradients with total norm 0.00185 and max norm 0.00013\n",
            "\u001b[32m[I 210629 04:19:51 models:110]\u001b[39m 839 1024 train loss: 0.0000010 valid loss: 0.3744867 P@1: 0.67000 P@3: 0.51000 P@5: 0.40800 N@3: 0.54622 N@5: 0.53184 early stop: 6\n",
            "\u001b[32m[I 210629 04:19:54 models:110]\u001b[39m 842 512 train loss: 0.0000008 valid loss: 0.3748388 P@1: 0.67000 P@3: 0.51000 P@5: 0.40800 N@3: 0.54622 N@5: 0.53184 early stop: 7\n",
            "\u001b[32m[I 210629 04:19:56 models:110]\u001b[39m 844 1024 train loss: 0.0000008 valid loss: 0.3751893 P@1: 0.67000 P@3: 0.51000 P@5: 0.40800 N@3: 0.54622 N@5: 0.53199 early stop: 0\n",
            "\u001b[32m[I 210629 04:19:58 models:110]\u001b[39m 847 512 train loss: 0.0000008 valid loss: 0.3755391 P@1: 0.67000 P@3: 0.51000 P@5: 0.40800 N@3: 0.54622 N@5: 0.53199 early stop: 1\n",
            "\u001b[32m[I 210629 04:20:00 models:110]\u001b[39m 849 1024 train loss: 0.0000008 valid loss: 0.3758883 P@1: 0.67000 P@3: 0.51000 P@5: 0.40800 N@3: 0.54622 N@5: 0.53199 early stop: 2\n",
            "\u001b[33m[W 210629 04:20:01 models:137]\u001b[39m Clipping gradients with total norm 0.00111 and max norm 0.00021\n",
            "\u001b[32m[I 210629 04:20:03 models:110]\u001b[39m 852 512 train loss: 0.0000007 valid loss: 0.3762371 P@1: 0.67000 P@3: 0.51000 P@5: 0.40800 N@3: 0.54622 N@5: 0.53199 early stop: 3\n",
            "\u001b[33m[W 210629 04:20:03 models:137]\u001b[39m Clipping gradients with total norm 0.00219 and max norm 0.00027\n",
            "\u001b[32m[I 210629 04:20:05 models:110]\u001b[39m 854 1024 train loss: 0.0000009 valid loss: 0.3765852 P@1: 0.67000 P@3: 0.51000 P@5: 0.40800 N@3: 0.54622 N@5: 0.53199 early stop: 4\n",
            "\u001b[32m[I 210629 04:20:07 models:110]\u001b[39m 857 512 train loss: 0.0000007 valid loss: 0.3769318 P@1: 0.67000 P@3: 0.51000 P@5: 0.40800 N@3: 0.54622 N@5: 0.53199 early stop: 5\n",
            "\u001b[33m[W 210629 04:20:09 models:137]\u001b[39m Clipping gradients with total norm 0.00058 and max norm 9e-05\n",
            "\u001b[32m[I 210629 04:20:09 models:110]\u001b[39m 859 1024 train loss: 0.0000007 valid loss: 0.3772768 P@1: 0.66000 P@3: 0.51000 P@5: 0.40800 N@3: 0.54449 N@5: 0.53025 early stop: 6\n",
            "\u001b[32m[I 210629 04:20:12 models:110]\u001b[39m 862 512 train loss: 0.0000006 valid loss: 0.3776203 P@1: 0.66000 P@3: 0.51000 P@5: 0.40800 N@3: 0.54449 N@5: 0.53025 early stop: 7\n",
            "\u001b[33m[W 210629 04:20:13 models:137]\u001b[39m Clipping gradients with total norm 0.00384 and max norm 0.0001\n",
            "\u001b[32m[I 210629 04:20:14 models:110]\u001b[39m 864 1024 train loss: 0.0000010 valid loss: 0.3779627 P@1: 0.66000 P@3: 0.51000 P@5: 0.40800 N@3: 0.54449 N@5: 0.53025 early stop: 8\n",
            "\u001b[33m[W 210629 04:20:14 models:137]\u001b[39m Clipping gradients with total norm 0.00208 and max norm 0.00032\n",
            "\u001b[32m[I 210629 04:20:16 models:110]\u001b[39m 867 512 train loss: 0.0000009 valid loss: 0.3783058 P@1: 0.66000 P@3: 0.51000 P@5: 0.40800 N@3: 0.54449 N@5: 0.53025 early stop: 9\n",
            "\u001b[32m[I 210629 04:20:18 models:110]\u001b[39m 869 1024 train loss: 0.0000006 valid loss: 0.3786500 P@1: 0.66000 P@3: 0.51000 P@5: 0.40800 N@3: 0.54449 N@5: 0.53025 early stop: 10\n",
            "\u001b[32m[I 210629 04:20:21 models:110]\u001b[39m 872 512 train loss: 0.0000007 valid loss: 0.3789939 P@1: 0.66000 P@3: 0.51000 P@5: 0.40800 N@3: 0.54449 N@5: 0.53025 early stop: 11\n",
            "\u001b[33m[W 210629 04:20:22 models:137]\u001b[39m Clipping gradients with total norm 0.00229 and max norm 0.0001\n",
            "\u001b[32m[I 210629 04:20:23 models:110]\u001b[39m 874 1024 train loss: 0.0000008 valid loss: 0.3793372 P@1: 0.66000 P@3: 0.51000 P@5: 0.40800 N@3: 0.54449 N@5: 0.53025 early stop: 12\n",
            "\u001b[32m[I 210629 04:20:25 models:110]\u001b[39m 877 512 train loss: 0.0000007 valid loss: 0.3796794 P@1: 0.66000 P@3: 0.51000 P@5: 0.40800 N@3: 0.54449 N@5: 0.53025 early stop: 13\n",
            "\u001b[33m[W 210629 04:20:26 models:137]\u001b[39m Clipping gradients with total norm 0.00126 and max norm 0.00021\n",
            "\u001b[32m[I 210629 04:20:27 models:110]\u001b[39m 879 1024 train loss: 0.0000008 valid loss: 0.3800217 P@1: 0.66000 P@3: 0.51000 P@5: 0.40800 N@3: 0.54449 N@5: 0.53025 early stop: 14\n",
            "\u001b[32m[I 210629 04:20:30 models:110]\u001b[39m 882 512 train loss: 0.0000009 valid loss: 0.3803641 P@1: 0.66000 P@3: 0.51000 P@5: 0.40800 N@3: 0.54449 N@5: 0.53025 early stop: 15\n",
            "\u001b[32m[I 210629 04:20:32 models:110]\u001b[39m 884 1024 train loss: 0.0000007 valid loss: 0.3807061 P@1: 0.66000 P@3: 0.51000 P@5: 0.40800 N@3: 0.54449 N@5: 0.53025 early stop: 16\n",
            "\u001b[32m[I 210629 04:20:34 models:110]\u001b[39m 887 512 train loss: 0.0000006 valid loss: 0.3810463 P@1: 0.66000 P@3: 0.51000 P@5: 0.40800 N@3: 0.54449 N@5: 0.53025 early stop: 17\n",
            "\u001b[32m[I 210629 04:20:36 models:110]\u001b[39m 889 1024 train loss: 0.0000005 valid loss: 0.3813848 P@1: 0.66000 P@3: 0.51000 P@5: 0.40800 N@3: 0.54449 N@5: 0.53025 early stop: 18\n",
            "\u001b[32m[I 210629 04:20:39 models:110]\u001b[39m 892 512 train loss: 0.0000007 valid loss: 0.3817217 P@1: 0.66000 P@3: 0.51000 P@5: 0.40800 N@3: 0.54449 N@5: 0.53025 early stop: 19\n",
            "\u001b[32m[I 210629 04:20:41 models:110]\u001b[39m 894 1024 train loss: 0.0000009 valid loss: 0.3820591 P@1: 0.66000 P@3: 0.51000 P@5: 0.41000 N@3: 0.54449 N@5: 0.53176 early stop: 20\n",
            "\u001b[33m[W 210629 04:20:42 models:137]\u001b[39m Clipping gradients with total norm 0.00074 and max norm 0.00011\n",
            "\u001b[32m[I 210629 04:20:43 models:110]\u001b[39m 897 512 train loss: 0.0000007 valid loss: 0.3823966 P@1: 0.66000 P@3: 0.51000 P@5: 0.41000 N@3: 0.54449 N@5: 0.53176 early stop: 21\n",
            "\u001b[32m[I 210629 04:20:46 models:110]\u001b[39m 899 1024 train loss: 0.0000006 valid loss: 0.3827326 P@1: 0.66000 P@3: 0.51000 P@5: 0.41000 N@3: 0.54449 N@5: 0.53176 early stop: 22\n",
            "\u001b[33m[W 210629 04:20:46 models:137]\u001b[39m Clipping gradients with total norm 0.00048 and max norm 9e-05\n",
            "\u001b[32m[I 210629 04:20:48 models:110]\u001b[39m 902 512 train loss: 0.0000007 valid loss: 0.3830667 P@1: 0.66000 P@3: 0.51000 P@5: 0.41000 N@3: 0.54449 N@5: 0.53176 early stop: 23\n",
            "\u001b[32m[I 210629 04:20:50 models:110]\u001b[39m 904 1024 train loss: 0.0000007 valid loss: 0.3833989 P@1: 0.66000 P@3: 0.51000 P@5: 0.41000 N@3: 0.54449 N@5: 0.53176 early stop: 24\n",
            "\u001b[33m[W 210629 04:20:51 models:137]\u001b[39m Clipping gradients with total norm 0.00329 and max norm 0.00027\n",
            "\u001b[32m[I 210629 04:20:53 models:110]\u001b[39m 907 512 train loss: 0.0000011 valid loss: 0.3837302 P@1: 0.66000 P@3: 0.51000 P@5: 0.41000 N@3: 0.54449 N@5: 0.53176 early stop: 25\n",
            "\u001b[32m[I 210629 04:20:55 models:110]\u001b[39m 909 1024 train loss: 0.0000007 valid loss: 0.3840608 P@1: 0.66000 P@3: 0.51000 P@5: 0.41000 N@3: 0.54449 N@5: 0.53176 early stop: 26\n",
            "\u001b[33m[W 210629 04:20:57 models:137]\u001b[39m Clipping gradients with total norm 0.00194 and max norm 0.00026\n",
            "\u001b[32m[I 210629 04:20:57 models:110]\u001b[39m 912 512 train loss: 0.0000009 valid loss: 0.3843915 P@1: 0.66000 P@3: 0.51000 P@5: 0.41000 N@3: 0.54449 N@5: 0.53176 early stop: 27\n",
            "\u001b[32m[I 210629 04:20:59 models:110]\u001b[39m 914 1024 train loss: 0.0000009 valid loss: 0.3847255 P@1: 0.66000 P@3: 0.51000 P@5: 0.41000 N@3: 0.54449 N@5: 0.53176 early stop: 28\n",
            "\u001b[32m[I 210629 04:21:02 models:110]\u001b[39m 917 512 train loss: 0.0000006 valid loss: 0.3850597 P@1: 0.66000 P@3: 0.51000 P@5: 0.41000 N@3: 0.54449 N@5: 0.53176 early stop: 29\n",
            "\u001b[32m[I 210629 04:21:04 models:110]\u001b[39m 919 1024 train loss: 0.0000008 valid loss: 0.3853919 P@1: 0.66000 P@3: 0.51000 P@5: 0.41000 N@3: 0.54449 N@5: 0.53176 early stop: 30\n",
            "\u001b[32m[I 210629 04:21:06 models:110]\u001b[39m 922 512 train loss: 0.0000008 valid loss: 0.3857207 P@1: 0.66000 P@3: 0.51000 P@5: 0.41000 N@3: 0.54449 N@5: 0.53176 early stop: 31\n",
            "\u001b[32m[I 210629 04:21:08 models:110]\u001b[39m 924 1024 train loss: 0.0000007 valid loss: 0.3860474 P@1: 0.66000 P@3: 0.51000 P@5: 0.41000 N@3: 0.54449 N@5: 0.53176 early stop: 32\n",
            "\u001b[32m[I 210629 04:21:11 models:110]\u001b[39m 927 512 train loss: 0.0000006 valid loss: 0.3863725 P@1: 0.66000 P@3: 0.51667 P@5: 0.41000 N@3: 0.54918 N@5: 0.53223 early stop: 0\n",
            "\u001b[32m[I 210629 04:21:13 models:110]\u001b[39m 929 1024 train loss: 0.0000005 valid loss: 0.3866967 P@1: 0.66000 P@3: 0.51667 P@5: 0.41000 N@3: 0.54918 N@5: 0.53223 early stop: 1\n",
            "\u001b[33m[W 210629 04:21:14 models:137]\u001b[39m Clipping gradients with total norm 0.00136 and max norm 0.00014\n",
            "\u001b[32m[I 210629 04:21:15 models:110]\u001b[39m 932 512 train loss: 0.0000007 valid loss: 0.3870191 P@1: 0.66000 P@3: 0.51667 P@5: 0.41000 N@3: 0.54918 N@5: 0.53223 early stop: 2\n",
            "\u001b[32m[I 210629 04:21:17 models:110]\u001b[39m 934 1024 train loss: 0.0000005 valid loss: 0.3873396 P@1: 0.66000 P@3: 0.51667 P@5: 0.41000 N@3: 0.54918 N@5: 0.53223 early stop: 3\n",
            "\u001b[32m[I 210629 04:21:20 models:110]\u001b[39m 937 512 train loss: 0.0000006 valid loss: 0.3876596 P@1: 0.66000 P@3: 0.51667 P@5: 0.41000 N@3: 0.54918 N@5: 0.53223 early stop: 4\n",
            "\u001b[33m[W 210629 04:21:21 models:137]\u001b[39m Clipping gradients with total norm 0.00074 and max norm 0.00014\n",
            "\u001b[32m[I 210629 04:21:22 models:110]\u001b[39m 939 1024 train loss: 0.0000007 valid loss: 0.3879798 P@1: 0.66000 P@3: 0.51667 P@5: 0.41000 N@3: 0.54918 N@5: 0.53223 early stop: 5\n",
            "\u001b[33m[W 210629 04:21:24 models:137]\u001b[39m Clipping gradients with total norm 0.02248 and max norm 0.0001\n",
            "\u001b[32m[I 210629 04:21:24 models:110]\u001b[39m 942 512 train loss: 0.0000048 valid loss: 0.3883004 P@1: 0.66000 P@3: 0.51667 P@5: 0.41000 N@3: 0.54918 N@5: 0.53223 early stop: 6\n",
            "\u001b[32m[I 210629 04:21:26 models:110]\u001b[39m 944 1024 train loss: 0.0000007 valid loss: 0.3886215 P@1: 0.66000 P@3: 0.51667 P@5: 0.41000 N@3: 0.54918 N@5: 0.53223 early stop: 7\n",
            "\u001b[32m[I 210629 04:21:29 models:110]\u001b[39m 947 512 train loss: 0.0000005 valid loss: 0.3889415 P@1: 0.66000 P@3: 0.51667 P@5: 0.41000 N@3: 0.54918 N@5: 0.53223 early stop: 8\n",
            "\u001b[32m[I 210629 04:21:31 models:110]\u001b[39m 949 1024 train loss: 0.0000006 valid loss: 0.3892599 P@1: 0.66000 P@3: 0.51667 P@5: 0.41000 N@3: 0.54918 N@5: 0.53223 early stop: 9\n",
            "\u001b[32m[I 210629 04:21:33 models:110]\u001b[39m 952 512 train loss: 0.0000005 valid loss: 0.3895777 P@1: 0.66000 P@3: 0.51667 P@5: 0.41000 N@3: 0.54918 N@5: 0.53223 early stop: 10\n",
            "\u001b[33m[W 210629 04:21:34 models:137]\u001b[39m Clipping gradients with total norm 0.00074 and max norm 0.00014\n",
            "\u001b[32m[I 210629 04:21:35 models:110]\u001b[39m 954 1024 train loss: 0.0000009 valid loss: 0.3898972 P@1: 0.66000 P@3: 0.51667 P@5: 0.41000 N@3: 0.54918 N@5: 0.53223 early stop: 11\n",
            "\u001b[33m[W 210629 04:21:37 models:137]\u001b[39m Clipping gradients with total norm 0.00255 and max norm 0.00011\n",
            "\u001b[32m[I 210629 04:21:38 models:110]\u001b[39m 957 512 train loss: 0.0000009 valid loss: 0.3902186 P@1: 0.66000 P@3: 0.51667 P@5: 0.41000 N@3: 0.54918 N@5: 0.53223 early stop: 12\n",
            "\u001b[32m[I 210629 04:21:40 models:110]\u001b[39m 959 1024 train loss: 0.0000006 valid loss: 0.3905385 P@1: 0.66000 P@3: 0.51667 P@5: 0.41000 N@3: 0.54918 N@5: 0.53223 early stop: 13\n",
            "\u001b[32m[I 210629 04:21:42 models:110]\u001b[39m 962 512 train loss: 0.0000007 valid loss: 0.3908547 P@1: 0.66000 P@3: 0.51667 P@5: 0.41000 N@3: 0.54918 N@5: 0.53223 early stop: 14\n",
            "\u001b[33m[W 210629 04:21:44 models:137]\u001b[39m Clipping gradients with total norm 0.00084 and max norm 9e-05\n",
            "\u001b[32m[I 210629 04:21:44 models:110]\u001b[39m 964 1024 train loss: 0.0000007 valid loss: 0.3911684 P@1: 0.66000 P@3: 0.51667 P@5: 0.41000 N@3: 0.54918 N@5: 0.53223 early stop: 15\n",
            "\u001b[33m[W 210629 04:21:46 models:137]\u001b[39m Clipping gradients with total norm 0.00062 and max norm 6e-05\n",
            "\u001b[32m[I 210629 04:21:47 models:110]\u001b[39m 967 512 train loss: 0.0000005 valid loss: 0.3914808 P@1: 0.66000 P@3: 0.51667 P@5: 0.41000 N@3: 0.54918 N@5: 0.53223 early stop: 16\n",
            "\u001b[33m[W 210629 04:21:48 models:137]\u001b[39m Clipping gradients with total norm 0.00096 and max norm 9e-05\n",
            "\u001b[32m[I 210629 04:21:49 models:110]\u001b[39m 969 1024 train loss: 0.0000006 valid loss: 0.3917937 P@1: 0.66000 P@3: 0.51667 P@5: 0.41000 N@3: 0.54918 N@5: 0.53223 early stop: 17\n",
            "\u001b[32m[I 210629 04:21:51 models:110]\u001b[39m 972 512 train loss: 0.0000009 valid loss: 0.3921064 P@1: 0.66000 P@3: 0.51667 P@5: 0.41000 N@3: 0.54918 N@5: 0.53223 early stop: 18\n",
            "\u001b[32m[I 210629 04:21:54 models:110]\u001b[39m 974 1024 train loss: 0.0000005 valid loss: 0.3924182 P@1: 0.66000 P@3: 0.51667 P@5: 0.41000 N@3: 0.54918 N@5: 0.53223 early stop: 19\n",
            "\u001b[33m[W 210629 04:21:54 models:137]\u001b[39m Clipping gradients with total norm 0.0005 and max norm 0.0001\n",
            "\u001b[32m[I 210629 04:21:56 models:110]\u001b[39m 977 512 train loss: 0.0000007 valid loss: 0.3927285 P@1: 0.66000 P@3: 0.51667 P@5: 0.41000 N@3: 0.54918 N@5: 0.53223 early stop: 20\n",
            "\u001b[32m[I 210629 04:21:58 models:110]\u001b[39m 979 1024 train loss: 0.0000013 valid loss: 0.3930329 P@1: 0.66000 P@3: 0.51667 P@5: 0.41000 N@3: 0.54918 N@5: 0.53223 early stop: 21\n",
            "\u001b[32m[I 210629 04:22:01 models:110]\u001b[39m 982 512 train loss: 0.0000023 valid loss: 0.3933429 P@1: 0.66000 P@3: 0.51667 P@5: 0.41000 N@3: 0.54918 N@5: 0.53223 early stop: 22\n",
            "\u001b[33m[W 210629 04:22:01 models:137]\u001b[39m Clipping gradients with total norm 0.0209 and max norm 0.00177\n",
            "\u001b[33m[W 210629 04:22:02 models:137]\u001b[39m Clipping gradients with total norm 0.03332 and max norm 0.00353\n",
            "\u001b[32m[I 210629 04:22:03 models:110]\u001b[39m 984 1024 train loss: 0.0000151 valid loss: 0.3936527 P@1: 0.66000 P@3: 0.51667 P@5: 0.41000 N@3: 0.54918 N@5: 0.53223 early stop: 23\n",
            "\u001b[33m[W 210629 04:22:03 models:137]\u001b[39m Clipping gradients with total norm 0.09659 and max norm 0.00707\n",
            "\u001b[33m[W 210629 04:22:03 models:137]\u001b[39m Clipping gradients with total norm 0.34716 and max norm 0.01413\n",
            "\u001b[33m[W 210629 04:22:04 models:137]\u001b[39m Clipping gradients with total norm 0.62304 and max norm 0.02826\n",
            "\u001b[33m[W 210629 04:22:04 models:137]\u001b[39m Clipping gradients with total norm 0.70959 and max norm 0.08848\n",
            "\u001b[33m[W 210629 04:22:05 models:137]\u001b[39m Clipping gradients with total norm 3.17138 and max norm 0.17696\n",
            "\u001b[33m[W 210629 04:22:05 models:137]\u001b[39m Clipping gradients with total norm 4.66587 and max norm 0.35391\n",
            "\u001b[32m[I 210629 04:22:05 models:110]\u001b[39m 987 512 train loss: 0.0226820 valid loss: 0.3938366 P@1: 0.66000 P@3: 0.51333 P@5: 0.41200 N@3: 0.54684 N@5: 0.53331 early stop: 0\n",
            "\u001b[33m[W 210629 04:22:05 models:137]\u001b[39m Clipping gradients with total norm 7.80523 and max norm 0.70782\n",
            "\u001b[33m[W 210629 04:22:05 models:137]\u001b[39m Clipping gradients with total norm 7.32846 and max norm 1.0\n",
            "\u001b[33m[W 210629 04:22:06 models:137]\u001b[39m Clipping gradients with total norm 10.29467 and max norm 1.0\n",
            "\u001b[33m[W 210629 04:22:06 models:137]\u001b[39m Clipping gradients with total norm 13.37346 and max norm 1.0\n",
            "\u001b[33m[W 210629 04:22:06 models:137]\u001b[39m Clipping gradients with total norm 8.8929 and max norm 1.0\n",
            "\u001b[33m[W 210629 04:22:06 models:137]\u001b[39m Clipping gradients with total norm 5.63229 and max norm 1.0\n",
            "\u001b[33m[W 210629 04:22:07 models:137]\u001b[39m Clipping gradients with total norm 6.59794 and max norm 1.0\n",
            "\u001b[32m[I 210629 04:22:07 models:110]\u001b[39m 989 1024 train loss: 0.6423014 valid loss: 0.3908738 P@1: 0.67000 P@3: 0.52000 P@5: 0.41000 N@3: 0.55326 N@5: 0.53314 early stop: 1\n",
            "\u001b[32m[I 210629 04:22:10 models:110]\u001b[39m 992 512 train loss: 0.1470261 valid loss: 0.3864597 P@1: 0.67000 P@3: 0.52000 P@5: 0.41200 N@3: 0.55388 N@5: 0.53572 early stop: 0\n",
            "\u001b[32m[I 210629 04:22:12 models:110]\u001b[39m 994 1024 train loss: 0.1000509 valid loss: 0.3818033 P@1: 0.67000 P@3: 0.51667 P@5: 0.41200 N@3: 0.55153 N@5: 0.53554 early stop: 1\n",
            "\u001b[32m[I 210629 04:22:14 models:110]\u001b[39m 997 512 train loss: 0.0728510 valid loss: 0.3772508 P@1: 0.67000 P@3: 0.52000 P@5: 0.40800 N@3: 0.55449 N@5: 0.53339 early stop: 2\n",
            "\u001b[32m[I 210629 04:22:16 models:110]\u001b[39m 999 1024 train loss: 0.0559446 valid loss: 0.3728870 P@1: 0.67000 P@3: 0.51667 P@5: 0.40400 N@3: 0.55233 N@5: 0.53033 early stop: 3\n",
            "\u001b[32m[I 210629 04:22:16 main:76]\u001b[39m Finish Training\n",
            "\u001b[32m[I 210629 04:22:18 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210629 04:22:18 main:79]\u001b[39m Loading Test Set\n",
            "\u001b[32m[I 210629 04:22:18 main:83]\u001b[39m Size of Test Set: 100\n",
            "\u001b[32m[I 210629 04:22:18 main:85]\u001b[39m Predicting\n",
            "\u001b[32m[I 210629 04:22:22 main:91]\u001b[39m Finish Predicting\n",
            "Precision@1,3,5: 0.52 0.43 0.356\n",
            "nDCG@1,3,5: 0.52 0.45706064796970536 0.45781070888528225\n",
            "\n",
            "WITH MAG NO MESH, skip=900\n",
            "\n",
            "/content/drive/Shareddrives/NASA/MATCH/PeTaL\n",
            "130\n",
            "/content/drive/Shareddrives/NASA/MATCH\n",
            "    800  286051 4914820 PeTaL/train.json\n",
            "\u001b[32m[I 210629 04:22:25 preprocess:28]\u001b[39m Vocab Size: 26834\n",
            "\u001b[32m[I 210629 04:22:25 preprocess:30]\u001b[39m Getting Dataset: PeTaL/train_texts.txt Max Length: 500\n",
            "\u001b[32m[I 210629 04:22:25 preprocess:32]\u001b[39m Size of Samples: 900\n",
            "\u001b[32m[I 210629 04:22:26 preprocess:28]\u001b[39m Vocab Size: 26834\n",
            "\u001b[32m[I 210629 04:22:26 preprocess:30]\u001b[39m Getting Dataset: PeTaL/test_texts.txt Max Length: 500\n",
            "\u001b[32m[I 210629 04:22:26 preprocess:32]\u001b[39m Size of Samples: 100\n",
            "\u001b[32m[I 210629 04:22:27 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210629 04:22:27 main:35]\u001b[39m Loading Training and Validation Set\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['absorb_and/or_filter_solids', 'chemically_break_down_inorganic_compounds', 'detox/purify', 'manage_environmental_disturbances_in_a_community', 'protect_from_fire', 'protect_from_gases'] will be ignored\n",
            "  .format(sorted(unknown, key=str)))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['chemically_break_down_inorganic_compounds', 'protect_from_fire'] will be ignored\n",
            "  .format(sorted(unknown, key=str)))\n",
            "\u001b[32m[I 210629 04:22:27 main:47]\u001b[39m Number of Labels: 124\n",
            "\u001b[32m[I 210629 04:22:27 main:48]\u001b[39m Size of Training Set: 800\n",
            "\u001b[32m[I 210629 04:22:27 main:49]\u001b[39m Size of Validation Set: 100\n",
            "\u001b[32m[I 210629 04:22:27 main:66]\u001b[39m Number of Edges: 101\n",
            "\u001b[32m[I 210629 04:22:27 main:68]\u001b[39m Training\n",
            "\u001b[32m[I 210629 04:22:33 models:110]\u001b[39m 2 512 train loss: 0.2741621 valid loss: 0.1554202 P@1: 0.06000 P@3: 0.07000 P@5: 0.07800 N@3: 0.07094 N@5: 0.08418 early stop: 0\n",
            "\u001b[32m[I 210629 04:22:34 models:142]\u001b[39m SWA Initializing\n",
            "\u001b[32m[I 210629 04:22:35 models:110]\u001b[39m 4 1024 train loss: 0.1462430 valid loss: 0.1446651 P@1: 0.29000 P@3: 0.22333 P@5: 0.19800 N@3: 0.23570 N@5: 0.25591 early stop: 0\n",
            "\u001b[32m[I 210629 04:22:38 models:110]\u001b[39m 7 512 train loss: 0.1394760 valid loss: 0.1432830 P@1: 0.29000 P@3: 0.22333 P@5: 0.20600 N@3: 0.23631 N@5: 0.26091 early stop: 0\n",
            "\u001b[32m[I 210629 04:22:40 models:110]\u001b[39m 9 1024 train loss: 0.1373729 valid loss: 0.1429399 P@1: 0.29000 P@3: 0.22333 P@5: 0.18200 N@3: 0.23631 N@5: 0.24297 early stop: 1\n",
            "\u001b[32m[I 210629 04:22:43 models:110]\u001b[39m 12 512 train loss: 0.1314551 valid loss: 0.1425113 P@1: 0.29000 P@3: 0.22333 P@5: 0.18600 N@3: 0.23631 N@5: 0.24624 early stop: 2\n",
            "\u001b[32m[I 210629 04:22:45 models:110]\u001b[39m 14 1024 train loss: 0.1152269 valid loss: 0.1420293 P@1: 0.29000 P@3: 0.22333 P@5: 0.20400 N@3: 0.23570 N@5: 0.25833 early stop: 3\n",
            "\u001b[32m[I 210629 04:22:47 models:110]\u001b[39m 17 512 train loss: 0.1004123 valid loss: 0.1415536 P@1: 0.29000 P@3: 0.22000 P@5: 0.22200 N@3: 0.23704 N@5: 0.27604 early stop: 0\n",
            "\u001b[32m[I 210629 04:22:49 models:110]\u001b[39m 19 1024 train loss: 0.0836444 valid loss: 0.1406109 P@1: 0.29000 P@3: 0.23333 P@5: 0.22800 N@3: 0.24704 N@5: 0.28069 early stop: 0\n",
            "\u001b[32m[I 210629 04:22:52 models:110]\u001b[39m 22 512 train loss: 0.0680692 valid loss: 0.1394630 P@1: 0.31000 P@3: 0.25333 P@5: 0.23400 N@3: 0.26335 N@5: 0.28986 early stop: 0\n",
            "\u001b[32m[I 210629 04:22:54 models:110]\u001b[39m 24 1024 train loss: 0.0615682 valid loss: 0.1380956 P@1: 0.33000 P@3: 0.27000 P@5: 0.23800 N@3: 0.28346 N@5: 0.30360 early stop: 0\n",
            "\u001b[32m[I 210629 04:22:56 models:110]\u001b[39m 27 512 train loss: 0.0481216 valid loss: 0.1367799 P@1: 0.34000 P@3: 0.28000 P@5: 0.25200 N@3: 0.29469 N@5: 0.31893 early stop: 0\n",
            "\u001b[32m[I 210629 04:22:59 models:110]\u001b[39m 29 1024 train loss: 0.0395734 valid loss: 0.1355338 P@1: 0.38000 P@3: 0.30333 P@5: 0.26600 N@3: 0.32112 N@5: 0.34004 early stop: 0\n",
            "\u001b[32m[I 210629 04:23:01 models:110]\u001b[39m 32 512 train loss: 0.0347087 valid loss: 0.1343223 P@1: 0.43000 P@3: 0.31667 P@5: 0.27200 N@3: 0.34173 N@5: 0.35954 early stop: 0\n",
            "\u001b[32m[I 210629 04:23:03 models:110]\u001b[39m 34 1024 train loss: 0.0290998 valid loss: 0.1330222 P@1: 0.46000 P@3: 0.35000 P@5: 0.29600 N@3: 0.37303 N@5: 0.38896 early stop: 0\n",
            "\u001b[32m[I 210629 04:23:06 models:110]\u001b[39m 37 512 train loss: 0.0250295 valid loss: 0.1321203 P@1: 0.46000 P@3: 0.37333 P@5: 0.29600 N@3: 0.39509 N@5: 0.39839 early stop: 0\n",
            "\u001b[32m[I 210629 04:23:08 models:110]\u001b[39m 39 1024 train loss: 0.0211569 valid loss: 0.1310527 P@1: 0.50000 P@3: 0.38667 P@5: 0.30800 N@3: 0.41386 N@5: 0.41608 early stop: 0\n",
            "\u001b[32m[I 210629 04:23:10 models:110]\u001b[39m 42 512 train loss: 0.0202008 valid loss: 0.1309320 P@1: 0.50000 P@3: 0.39333 P@5: 0.31400 N@3: 0.42101 N@5: 0.42462 early stop: 0\n",
            "\u001b[32m[I 210629 04:23:13 models:110]\u001b[39m 44 1024 train loss: 0.0208618 valid loss: 0.1305606 P@1: 0.48000 P@3: 0.39667 P@5: 0.32600 N@3: 0.42174 N@5: 0.43279 early stop: 0\n",
            "\u001b[32m[I 210629 04:23:15 models:110]\u001b[39m 47 512 train loss: 0.0177517 valid loss: 0.1301883 P@1: 0.50000 P@3: 0.40333 P@5: 0.33000 N@3: 0.43112 N@5: 0.43981 early stop: 0\n",
            "\u001b[32m[I 210629 04:23:17 models:110]\u001b[39m 49 1024 train loss: 0.0147486 valid loss: 0.1299330 P@1: 0.51000 P@3: 0.41000 P@5: 0.33400 N@3: 0.43755 N@5: 0.44479 early stop: 0\n",
            "\u001b[32m[I 210629 04:23:20 models:110]\u001b[39m 52 512 train loss: 0.0130747 valid loss: 0.1298083 P@1: 0.52000 P@3: 0.42000 P@5: 0.33800 N@3: 0.44693 N@5: 0.45126 early stop: 0\n",
            "\u001b[32m[I 210629 04:23:22 models:110]\u001b[39m 54 1024 train loss: 0.0127378 valid loss: 0.1298157 P@1: 0.53000 P@3: 0.43667 P@5: 0.33800 N@3: 0.46224 N@5: 0.45628 early stop: 0\n",
            "\u001b[32m[I 210629 04:23:24 models:110]\u001b[39m 57 512 train loss: 0.0117336 valid loss: 0.1304441 P@1: 0.54000 P@3: 0.44667 P@5: 0.34600 N@3: 0.47438 N@5: 0.46883 early stop: 0\n",
            "\u001b[32m[I 210629 04:23:27 models:110]\u001b[39m 59 1024 train loss: 0.0119734 valid loss: 0.1306504 P@1: 0.59000 P@3: 0.45667 P@5: 0.34800 N@3: 0.49175 N@5: 0.47999 early stop: 0\n",
            "\u001b[32m[I 210629 04:23:29 models:110]\u001b[39m 62 512 train loss: 0.0114119 valid loss: 0.1312601 P@1: 0.59000 P@3: 0.45667 P@5: 0.35800 N@3: 0.49317 N@5: 0.48924 early stop: 0\n",
            "\u001b[32m[I 210629 04:23:31 models:110]\u001b[39m 64 1024 train loss: 0.0085209 valid loss: 0.1316390 P@1: 0.60000 P@3: 0.46333 P@5: 0.36600 N@3: 0.50031 N@5: 0.49873 early stop: 0\n",
            "\u001b[32m[I 210629 04:23:34 models:110]\u001b[39m 67 512 train loss: 0.0062906 valid loss: 0.1323184 P@1: 0.60000 P@3: 0.47000 P@5: 0.37800 N@3: 0.50642 N@5: 0.51120 early stop: 0\n",
            "\u001b[32m[I 210629 04:23:36 models:110]\u001b[39m 69 1024 train loss: 0.0043418 valid loss: 0.1332473 P@1: 0.60000 P@3: 0.47333 P@5: 0.38000 N@3: 0.51010 N@5: 0.51411 early stop: 0\n",
            "\u001b[32m[I 210629 04:23:38 models:110]\u001b[39m 72 512 train loss: 0.0038834 valid loss: 0.1343027 P@1: 0.59000 P@3: 0.48000 P@5: 0.38600 N@3: 0.51368 N@5: 0.51850 early stop: 0\n",
            "\u001b[32m[I 210629 04:23:41 models:110]\u001b[39m 74 1024 train loss: 0.0031511 valid loss: 0.1356045 P@1: 0.59000 P@3: 0.48333 P@5: 0.38800 N@3: 0.51603 N@5: 0.52066 early stop: 0\n",
            "\u001b[32m[I 210629 04:23:43 models:110]\u001b[39m 77 512 train loss: 0.0031303 valid loss: 0.1369679 P@1: 0.59000 P@3: 0.48667 P@5: 0.39000 N@3: 0.51909 N@5: 0.52265 early stop: 0\n",
            "\u001b[32m[I 210629 04:23:45 models:110]\u001b[39m 79 1024 train loss: 0.0039545 valid loss: 0.1383108 P@1: 0.59000 P@3: 0.50000 P@5: 0.39200 N@3: 0.52909 N@5: 0.52615 early stop: 0\n",
            "\u001b[32m[I 210629 04:23:48 models:110]\u001b[39m 82 512 train loss: 0.0034146 valid loss: 0.1397633 P@1: 0.59000 P@3: 0.49333 P@5: 0.39400 N@3: 0.52298 N@5: 0.52595 early stop: 1\n",
            "\u001b[32m[I 210629 04:23:50 models:110]\u001b[39m 84 1024 train loss: 0.0033302 valid loss: 0.1414220 P@1: 0.59000 P@3: 0.49000 P@5: 0.39400 N@3: 0.52002 N@5: 0.52602 early stop: 2\n",
            "\u001b[32m[I 210629 04:23:52 models:110]\u001b[39m 87 512 train loss: 0.0032471 valid loss: 0.1428330 P@1: 0.59000 P@3: 0.49000 P@5: 0.39400 N@3: 0.51877 N@5: 0.52522 early stop: 3\n",
            "\u001b[32m[I 210629 04:23:55 models:110]\u001b[39m 89 1024 train loss: 0.0035633 valid loss: 0.1441158 P@1: 0.59000 P@3: 0.50333 P@5: 0.39400 N@3: 0.52896 N@5: 0.52701 early stop: 0\n",
            "\u001b[32m[I 210629 04:23:57 models:110]\u001b[39m 92 512 train loss: 0.0047788 valid loss: 0.1454107 P@1: 0.58000 P@3: 0.50667 P@5: 0.39800 N@3: 0.52896 N@5: 0.52847 early stop: 0\n",
            "\u001b[32m[I 210629 04:23:59 models:110]\u001b[39m 94 1024 train loss: 0.0096548 valid loss: 0.1467941 P@1: 0.58000 P@3: 0.50333 P@5: 0.39600 N@3: 0.52661 N@5: 0.52729 early stop: 1\n",
            "\u001b[32m[I 210629 04:24:02 models:110]\u001b[39m 97 512 train loss: 0.0087642 valid loss: 0.1481014 P@1: 0.58000 P@3: 0.50333 P@5: 0.39800 N@3: 0.52661 N@5: 0.52879 early stop: 0\n",
            "\u001b[32m[I 210629 04:24:04 models:110]\u001b[39m 99 1024 train loss: 0.0075693 valid loss: 0.1489809 P@1: 0.59000 P@3: 0.50333 P@5: 0.39800 N@3: 0.52835 N@5: 0.53029 early stop: 0\n",
            "\u001b[32m[I 210629 04:24:06 models:110]\u001b[39m 102 512 train loss: 0.0077286 valid loss: 0.1498505 P@1: 0.59000 P@3: 0.50000 P@5: 0.39800 N@3: 0.52723 N@5: 0.53088 early stop: 0\n",
            "\u001b[32m[I 210629 04:24:08 models:110]\u001b[39m 104 1024 train loss: 0.0069486 valid loss: 0.1509172 P@1: 0.59000 P@3: 0.50667 P@5: 0.40200 N@3: 0.53192 N@5: 0.53381 early stop: 0\n",
            "\u001b[32m[I 210629 04:24:11 models:110]\u001b[39m 107 512 train loss: 0.0039482 valid loss: 0.1520569 P@1: 0.59000 P@3: 0.50667 P@5: 0.40400 N@3: 0.53112 N@5: 0.53446 early stop: 0\n",
            "\u001b[32m[I 210629 04:24:13 models:110]\u001b[39m 109 1024 train loss: 0.0037100 valid loss: 0.1532902 P@1: 0.59000 P@3: 0.50667 P@5: 0.40600 N@3: 0.53173 N@5: 0.53652 early stop: 0\n",
            "\u001b[32m[I 210629 04:24:16 models:110]\u001b[39m 112 512 train loss: 0.0021208 valid loss: 0.1542957 P@1: 0.59000 P@3: 0.50667 P@5: 0.40800 N@3: 0.53173 N@5: 0.53833 early stop: 0\n",
            "\u001b[32m[I 210629 04:24:18 models:110]\u001b[39m 114 1024 train loss: 0.0011166 valid loss: 0.1555253 P@1: 0.59000 P@3: 0.50667 P@5: 0.40800 N@3: 0.53173 N@5: 0.53798 early stop: 1\n",
            "\u001b[32m[I 210629 04:24:20 models:110]\u001b[39m 117 512 train loss: 0.0007358 valid loss: 0.1568736 P@1: 0.59000 P@3: 0.51000 P@5: 0.40800 N@3: 0.53408 N@5: 0.53787 early stop: 2\n",
            "\u001b[32m[I 210629 04:24:22 models:110]\u001b[39m 119 1024 train loss: 0.0004890 valid loss: 0.1581530 P@1: 0.59000 P@3: 0.51333 P@5: 0.40800 N@3: 0.53642 N@5: 0.53844 early stop: 0\n",
            "\u001b[32m[I 210629 04:24:25 models:110]\u001b[39m 122 512 train loss: 0.0005131 valid loss: 0.1595516 P@1: 0.59000 P@3: 0.51333 P@5: 0.40800 N@3: 0.53581 N@5: 0.53736 early stop: 1\n",
            "\u001b[32m[I 210629 04:24:27 models:110]\u001b[39m 124 1024 train loss: 0.0002322 valid loss: 0.1609300 P@1: 0.60000 P@3: 0.51333 P@5: 0.40800 N@3: 0.53754 N@5: 0.53861 early stop: 0\n",
            "\u001b[32m[I 210629 04:24:29 models:110]\u001b[39m 127 512 train loss: 0.0003820 valid loss: 0.1622754 P@1: 0.60000 P@3: 0.51333 P@5: 0.40800 N@3: 0.53754 N@5: 0.53861 early stop: 1\n",
            "\u001b[33m[W 210629 04:24:31 models:137]\u001b[39m Clipping gradients with total norm 0.24359 and max norm 0.0156\n",
            "\u001b[32m[I 210629 04:24:32 models:110]\u001b[39m 129 1024 train loss: 0.0007751 valid loss: 0.1636659 P@1: 0.60000 P@3: 0.51667 P@5: 0.41000 N@3: 0.53989 N@5: 0.54036 early stop: 0\n",
            "\u001b[32m[I 210629 04:24:34 models:110]\u001b[39m 132 512 train loss: 0.0006185 valid loss: 0.1649915 P@1: 0.60000 P@3: 0.51667 P@5: 0.41400 N@3: 0.54050 N@5: 0.54430 early stop: 0\n",
            "\u001b[32m[I 210629 04:24:36 models:110]\u001b[39m 134 1024 train loss: 0.0005665 valid loss: 0.1663093 P@1: 0.60000 P@3: 0.52000 P@5: 0.41600 N@3: 0.54285 N@5: 0.54570 early stop: 0\n",
            "\u001b[32m[I 210629 04:24:39 models:110]\u001b[39m 137 512 train loss: 0.0010209 valid loss: 0.1677674 P@1: 0.59000 P@3: 0.52667 P@5: 0.41600 N@3: 0.54642 N@5: 0.54519 early stop: 1\n",
            "\u001b[32m[I 210629 04:24:41 models:110]\u001b[39m 139 1024 train loss: 0.0020206 valid loss: 0.1689443 P@1: 0.60000 P@3: 0.52333 P@5: 0.41800 N@3: 0.54581 N@5: 0.54752 early stop: 0\n",
            "\u001b[32m[I 210629 04:24:43 models:110]\u001b[39m 142 512 train loss: 0.0017661 valid loss: 0.1701940 P@1: 0.60000 P@3: 0.52333 P@5: 0.41800 N@3: 0.54704 N@5: 0.54841 early stop: 0\n",
            "\u001b[32m[I 210629 04:24:46 models:110]\u001b[39m 144 1024 train loss: 0.0016935 valid loss: 0.1714839 P@1: 0.60000 P@3: 0.52333 P@5: 0.42000 N@3: 0.54765 N@5: 0.55067 early stop: 0\n",
            "\u001b[32m[I 210629 04:24:48 models:110]\u001b[39m 147 512 train loss: 0.0015117 valid loss: 0.1726242 P@1: 0.60000 P@3: 0.52667 P@5: 0.41600 N@3: 0.55000 N@5: 0.54745 early stop: 1\n",
            "\u001b[32m[I 210629 04:24:50 models:110]\u001b[39m 149 1024 train loss: 0.0011641 valid loss: 0.1737562 P@1: 0.60000 P@3: 0.53333 P@5: 0.41200 N@3: 0.55469 N@5: 0.54438 early stop: 2\n",
            "\u001b[32m[I 210629 04:24:53 models:110]\u001b[39m 152 512 train loss: 0.0006781 valid loss: 0.1749396 P@1: 0.61000 P@3: 0.53333 P@5: 0.41200 N@3: 0.55581 N@5: 0.54521 early stop: 3\n",
            "\u001b[32m[I 210629 04:24:55 models:110]\u001b[39m 154 1024 train loss: 0.0006097 valid loss: 0.1762073 P@1: 0.61000 P@3: 0.53333 P@5: 0.41400 N@3: 0.55581 N@5: 0.54688 early stop: 4\n",
            "\u001b[32m[I 210629 04:24:57 models:110]\u001b[39m 157 512 train loss: 0.0019809 valid loss: 0.1774635 P@1: 0.61000 P@3: 0.53000 P@5: 0.41400 N@3: 0.55285 N@5: 0.54620 early stop: 5\n",
            "\u001b[32m[I 210629 04:24:59 models:110]\u001b[39m 159 1024 train loss: 0.0025277 valid loss: 0.1784311 P@1: 0.61000 P@3: 0.53000 P@5: 0.41200 N@3: 0.55285 N@5: 0.54444 early stop: 6\n",
            "\u001b[32m[I 210629 04:25:02 models:110]\u001b[39m 162 512 train loss: 0.0040527 valid loss: 0.1793637 P@1: 0.61000 P@3: 0.53000 P@5: 0.41800 N@3: 0.55285 N@5: 0.54894 early stop: 7\n",
            "\u001b[32m[I 210629 04:25:04 models:110]\u001b[39m 164 1024 train loss: 0.0040701 valid loss: 0.1802840 P@1: 0.61000 P@3: 0.53000 P@5: 0.41400 N@3: 0.55285 N@5: 0.54631 early stop: 8\n",
            "\u001b[32m[I 210629 04:25:06 models:110]\u001b[39m 167 512 train loss: 0.0046644 valid loss: 0.1812894 P@1: 0.61000 P@3: 0.53000 P@5: 0.41800 N@3: 0.55224 N@5: 0.54868 early stop: 9\n",
            "\u001b[32m[I 210629 04:25:08 models:110]\u001b[39m 169 1024 train loss: 0.0042600 valid loss: 0.1821155 P@1: 0.61000 P@3: 0.53000 P@5: 0.41800 N@3: 0.55224 N@5: 0.54800 early stop: 10\n",
            "\u001b[32m[I 210629 04:25:11 models:110]\u001b[39m 172 512 train loss: 0.0075933 valid loss: 0.1827287 P@1: 0.62000 P@3: 0.52667 P@5: 0.41600 N@3: 0.55162 N@5: 0.54830 early stop: 11\n",
            "\u001b[32m[I 210629 04:25:13 models:110]\u001b[39m 174 1024 train loss: 0.0075904 valid loss: 0.1834357 P@1: 0.62000 P@3: 0.52667 P@5: 0.41000 N@3: 0.55162 N@5: 0.54366 early stop: 12\n",
            "\u001b[32m[I 210629 04:25:15 models:110]\u001b[39m 177 512 train loss: 0.0081076 valid loss: 0.1839687 P@1: 0.62000 P@3: 0.52667 P@5: 0.41200 N@3: 0.55162 N@5: 0.54512 early stop: 13\n",
            "\u001b[32m[I 210629 04:25:17 models:110]\u001b[39m 179 1024 train loss: 0.0066116 valid loss: 0.1847265 P@1: 0.62000 P@3: 0.52667 P@5: 0.41400 N@3: 0.55162 N@5: 0.54656 early stop: 14\n",
            "\u001b[32m[I 210629 04:25:20 models:110]\u001b[39m 182 512 train loss: 0.0055812 valid loss: 0.1852584 P@1: 0.62000 P@3: 0.53000 P@5: 0.42000 N@3: 0.55397 N@5: 0.55204 early stop: 0\n",
            "\u001b[32m[I 210629 04:25:22 models:110]\u001b[39m 184 1024 train loss: 0.0040193 valid loss: 0.1858854 P@1: 0.62000 P@3: 0.53000 P@5: 0.42200 N@3: 0.55397 N@5: 0.55320 early stop: 0\n",
            "\u001b[32m[I 210629 04:25:25 models:110]\u001b[39m 187 512 train loss: 0.0023195 valid loss: 0.1867367 P@1: 0.62000 P@3: 0.53333 P@5: 0.42400 N@3: 0.55631 N@5: 0.55475 early stop: 0\n",
            "\u001b[32m[I 210629 04:25:27 models:110]\u001b[39m 189 1024 train loss: 0.0014774 valid loss: 0.1875133 P@1: 0.62000 P@3: 0.53333 P@5: 0.42400 N@3: 0.55693 N@5: 0.55551 early stop: 0\n",
            "\u001b[32m[I 210629 04:25:29 models:110]\u001b[39m 192 512 train loss: 0.0010611 valid loss: 0.1884244 P@1: 0.62000 P@3: 0.53333 P@5: 0.42400 N@3: 0.55693 N@5: 0.55587 early stop: 0\n",
            "\u001b[32m[I 210629 04:25:31 models:110]\u001b[39m 194 1024 train loss: 0.0011369 valid loss: 0.1893313 P@1: 0.62000 P@3: 0.53333 P@5: 0.42200 N@3: 0.55693 N@5: 0.55470 early stop: 1\n",
            "\u001b[32m[I 210629 04:25:34 models:110]\u001b[39m 197 512 train loss: 0.0008156 valid loss: 0.1902007 P@1: 0.62000 P@3: 0.53333 P@5: 0.42400 N@3: 0.55693 N@5: 0.55622 early stop: 0\n",
            "\u001b[32m[I 210629 04:25:36 models:110]\u001b[39m 199 1024 train loss: 0.0003926 valid loss: 0.1911777 P@1: 0.62000 P@3: 0.53333 P@5: 0.42200 N@3: 0.55693 N@5: 0.55491 early stop: 1\n",
            "\u001b[32m[I 210629 04:25:39 models:110]\u001b[39m 202 512 train loss: 0.0003632 valid loss: 0.1921435 P@1: 0.62000 P@3: 0.53333 P@5: 0.41800 N@3: 0.55693 N@5: 0.55228 early stop: 2\n",
            "\u001b[33m[W 210629 04:25:40 models:137]\u001b[39m Clipping gradients with total norm 0.07014 and max norm 0.01306\n",
            "\u001b[32m[I 210629 04:25:41 models:110]\u001b[39m 204 1024 train loss: 0.0004180 valid loss: 0.1931223 P@1: 0.62000 P@3: 0.53333 P@5: 0.42000 N@3: 0.55693 N@5: 0.55360 early stop: 3\n",
            "\u001b[32m[I 210629 04:25:43 models:110]\u001b[39m 207 512 train loss: 0.0004160 valid loss: 0.1940469 P@1: 0.62000 P@3: 0.53333 P@5: 0.41800 N@3: 0.55631 N@5: 0.55113 early stop: 4\n",
            "\u001b[32m[I 210629 04:25:45 models:110]\u001b[39m 209 1024 train loss: 0.0002722 valid loss: 0.1949764 P@1: 0.62000 P@3: 0.53333 P@5: 0.41600 N@3: 0.55693 N@5: 0.54993 early stop: 5\n",
            "\u001b[32m[I 210629 04:25:48 models:110]\u001b[39m 212 512 train loss: 0.0001763 valid loss: 0.1959378 P@1: 0.62000 P@3: 0.53000 P@5: 0.41600 N@3: 0.55520 N@5: 0.55021 early stop: 6\n",
            "\u001b[32m[I 210629 04:25:50 models:110]\u001b[39m 214 1024 train loss: 0.0002925 valid loss: 0.1968430 P@1: 0.63000 P@3: 0.53000 P@5: 0.41600 N@3: 0.55746 N@5: 0.55247 early stop: 7\n",
            "\u001b[32m[I 210629 04:25:52 models:110]\u001b[39m 217 512 train loss: 0.0003243 valid loss: 0.1977425 P@1: 0.63000 P@3: 0.53000 P@5: 0.41600 N@3: 0.55746 N@5: 0.55247 early stop: 8\n",
            "\u001b[32m[I 210629 04:25:54 models:110]\u001b[39m 219 1024 train loss: 0.0001960 valid loss: 0.1986713 P@1: 0.63000 P@3: 0.53000 P@5: 0.41600 N@3: 0.55746 N@5: 0.55247 early stop: 9\n",
            "\u001b[32m[I 210629 04:25:57 models:110]\u001b[39m 222 512 train loss: 0.0002745 valid loss: 0.1995830 P@1: 0.63000 P@3: 0.53000 P@5: 0.41600 N@3: 0.55746 N@5: 0.55247 early stop: 10\n",
            "\u001b[32m[I 210629 04:25:59 models:110]\u001b[39m 224 1024 train loss: 0.0002617 valid loss: 0.2004885 P@1: 0.63000 P@3: 0.53000 P@5: 0.41600 N@3: 0.55746 N@5: 0.55256 early stop: 11\n",
            "\u001b[32m[I 210629 04:26:01 models:110]\u001b[39m 227 512 train loss: 0.0002877 valid loss: 0.2014220 P@1: 0.63000 P@3: 0.53000 P@5: 0.41600 N@3: 0.55746 N@5: 0.55276 early stop: 12\n",
            "\u001b[32m[I 210629 04:26:03 models:110]\u001b[39m 229 1024 train loss: 0.0001536 valid loss: 0.2023253 P@1: 0.62000 P@3: 0.53000 P@5: 0.41600 N@3: 0.55573 N@5: 0.55149 early stop: 13\n",
            "\u001b[32m[I 210629 04:26:06 models:110]\u001b[39m 232 512 train loss: 0.0001299 valid loss: 0.2032044 P@1: 0.62000 P@3: 0.53000 P@5: 0.41800 N@3: 0.55573 N@5: 0.55331 early stop: 14\n",
            "\u001b[32m[I 210629 04:26:08 models:110]\u001b[39m 234 1024 train loss: 0.0001123 valid loss: 0.2040762 P@1: 0.62000 P@3: 0.53000 P@5: 0.41800 N@3: 0.55573 N@5: 0.55331 early stop: 15\n",
            "\u001b[32m[I 210629 04:26:10 models:110]\u001b[39m 237 512 train loss: 0.0001154 valid loss: 0.2049561 P@1: 0.62000 P@3: 0.53000 P@5: 0.41600 N@3: 0.55573 N@5: 0.55220 early stop: 16\n",
            "\u001b[32m[I 210629 04:26:13 models:110]\u001b[39m 239 1024 train loss: 0.0001291 valid loss: 0.2058163 P@1: 0.62000 P@3: 0.53000 P@5: 0.41800 N@3: 0.55573 N@5: 0.55386 early stop: 17\n",
            "\u001b[32m[I 210629 04:26:15 models:110]\u001b[39m 242 512 train loss: 0.0001386 valid loss: 0.2066766 P@1: 0.62000 P@3: 0.53000 P@5: 0.41800 N@3: 0.55573 N@5: 0.55386 early stop: 18\n",
            "\u001b[32m[I 210629 04:26:17 models:110]\u001b[39m 244 1024 train loss: 0.0001151 valid loss: 0.2075371 P@1: 0.62000 P@3: 0.53333 P@5: 0.42000 N@3: 0.55807 N@5: 0.55600 early stop: 19\n",
            "\u001b[32m[I 210629 04:26:20 models:110]\u001b[39m 247 512 train loss: 0.0001472 valid loss: 0.2083939 P@1: 0.62000 P@3: 0.53333 P@5: 0.42200 N@3: 0.55869 N@5: 0.55791 early stop: 0\n",
            "\u001b[33m[W 210629 04:26:21 models:137]\u001b[39m Clipping gradients with total norm 0.07442 and max norm 0.00695\n",
            "\u001b[32m[I 210629 04:26:22 models:110]\u001b[39m 249 1024 train loss: 0.0002815 valid loss: 0.2092330 P@1: 0.62000 P@3: 0.53333 P@5: 0.42200 N@3: 0.55869 N@5: 0.55791 early stop: 1\n",
            "\u001b[32m[I 210629 04:26:24 models:110]\u001b[39m 252 512 train loss: 0.0001332 valid loss: 0.2100656 P@1: 0.62000 P@3: 0.53333 P@5: 0.42200 N@3: 0.55869 N@5: 0.55811 early stop: 0\n",
            "\u001b[32m[I 210629 04:26:26 models:110]\u001b[39m 254 1024 train loss: 0.0001284 valid loss: 0.2109102 P@1: 0.62000 P@3: 0.53333 P@5: 0.42200 N@3: 0.55930 N@5: 0.55873 early stop: 0\n",
            "\u001b[32m[I 210629 04:26:29 models:110]\u001b[39m 257 512 train loss: 0.0001253 valid loss: 0.2117429 P@1: 0.62000 P@3: 0.53333 P@5: 0.42400 N@3: 0.55930 N@5: 0.56004 early stop: 0\n",
            "\u001b[32m[I 210629 04:26:31 models:110]\u001b[39m 259 1024 train loss: 0.0001281 valid loss: 0.2125600 P@1: 0.62000 P@3: 0.53333 P@5: 0.42400 N@3: 0.55858 N@5: 0.55985 early stop: 1\n",
            "\u001b[32m[I 210629 04:26:34 models:110]\u001b[39m 262 512 train loss: 0.0001235 valid loss: 0.2133736 P@1: 0.62000 P@3: 0.53333 P@5: 0.42600 N@3: 0.55858 N@5: 0.56116 early stop: 0\n",
            "\u001b[32m[I 210629 04:26:36 models:110]\u001b[39m 264 1024 train loss: 0.0001250 valid loss: 0.2141759 P@1: 0.62000 P@3: 0.53333 P@5: 0.42600 N@3: 0.55858 N@5: 0.56116 early stop: 1\n",
            "\u001b[32m[I 210629 04:26:38 models:110]\u001b[39m 267 512 train loss: 0.0001457 valid loss: 0.2149736 P@1: 0.62000 P@3: 0.53667 P@5: 0.42600 N@3: 0.56093 N@5: 0.56149 early stop: 0\n",
            "\u001b[32m[I 210629 04:26:40 models:110]\u001b[39m 269 1024 train loss: 0.0001026 valid loss: 0.2157723 P@1: 0.62000 P@3: 0.53667 P@5: 0.42600 N@3: 0.56093 N@5: 0.56143 early stop: 1\n",
            "\u001b[32m[I 210629 04:26:43 models:110]\u001b[39m 272 512 train loss: 0.0001115 valid loss: 0.2165583 P@1: 0.62000 P@3: 0.53667 P@5: 0.42600 N@3: 0.56093 N@5: 0.56143 early stop: 2\n",
            "\u001b[32m[I 210629 04:26:45 models:110]\u001b[39m 274 1024 train loss: 0.0001081 valid loss: 0.2173349 P@1: 0.62000 P@3: 0.53667 P@5: 0.42600 N@3: 0.56093 N@5: 0.56143 early stop: 3\n",
            "\u001b[32m[I 210629 04:26:47 models:110]\u001b[39m 277 512 train loss: 0.0001181 valid loss: 0.2181122 P@1: 0.62000 P@3: 0.53667 P@5: 0.42600 N@3: 0.56093 N@5: 0.56158 early stop: 0\n",
            "\u001b[32m[I 210629 04:26:50 models:110]\u001b[39m 279 1024 train loss: 0.0001284 valid loss: 0.2188794 P@1: 0.62000 P@3: 0.53667 P@5: 0.42600 N@3: 0.56093 N@5: 0.56158 early stop: 1\n",
            "\u001b[33m[W 210629 04:26:50 models:137]\u001b[39m Clipping gradients with total norm 0.05321 and max norm 0.00794\n",
            "\u001b[32m[I 210629 04:26:52 models:110]\u001b[39m 282 512 train loss: 0.0003298 valid loss: 0.2196362 P@1: 0.62000 P@3: 0.53333 P@5: 0.42600 N@3: 0.55858 N@5: 0.56125 early stop: 2\n",
            "\u001b[32m[I 210629 04:26:54 models:110]\u001b[39m 284 1024 train loss: 0.0001414 valid loss: 0.2203911 P@1: 0.62000 P@3: 0.53333 P@5: 0.42600 N@3: 0.55858 N@5: 0.56125 early stop: 3\n",
            "\u001b[33m[W 210629 04:26:56 models:137]\u001b[39m Clipping gradients with total norm 0.05474 and max norm 0.00414\n",
            "\u001b[32m[I 210629 04:26:57 models:110]\u001b[39m 287 512 train loss: 0.0002979 valid loss: 0.2211495 P@1: 0.63000 P@3: 0.53333 P@5: 0.42800 N@3: 0.56031 N@5: 0.56402 early stop: 0\n",
            "\u001b[32m[I 210629 04:26:59 models:110]\u001b[39m 289 1024 train loss: 0.0001351 valid loss: 0.2219068 P@1: 0.63000 P@3: 0.53333 P@5: 0.42800 N@3: 0.56031 N@5: 0.56402 early stop: 1\n",
            "\u001b[33m[W 210629 04:27:00 models:137]\u001b[39m Clipping gradients with total norm 0.00759 and max norm 0.00085\n",
            "\u001b[32m[I 210629 04:27:01 models:110]\u001b[39m 292 512 train loss: 0.0001724 valid loss: 0.2226449 P@1: 0.63000 P@3: 0.53333 P@5: 0.43000 N@3: 0.56031 N@5: 0.56518 early stop: 0\n",
            "\u001b[32m[I 210629 04:27:03 models:110]\u001b[39m 294 1024 train loss: 0.0000958 valid loss: 0.2233567 P@1: 0.63000 P@3: 0.53333 P@5: 0.43000 N@3: 0.56031 N@5: 0.56553 early stop: 0\n",
            "\u001b[32m[I 210629 04:27:06 models:110]\u001b[39m 297 512 train loss: 0.0000904 valid loss: 0.2240585 P@1: 0.63000 P@3: 0.53333 P@5: 0.43000 N@3: 0.56031 N@5: 0.56553 early stop: 1\n",
            "\u001b[33m[W 210629 04:27:06 models:137]\u001b[39m Clipping gradients with total norm 0.03634 and max norm 0.00618\n",
            "\u001b[32m[I 210629 04:27:08 models:110]\u001b[39m 299 1024 train loss: 0.0002480 valid loss: 0.2247623 P@1: 0.63000 P@3: 0.53333 P@5: 0.43000 N@3: 0.56031 N@5: 0.56538 early stop: 2\n",
            "\u001b[32m[I 210629 04:27:11 models:110]\u001b[39m 302 512 train loss: 0.0001601 valid loss: 0.2254566 P@1: 0.63000 P@3: 0.53333 P@5: 0.43200 N@3: 0.56031 N@5: 0.56720 early stop: 0\n",
            "\u001b[32m[I 210629 04:27:13 models:110]\u001b[39m 304 1024 train loss: 0.0001368 valid loss: 0.2261364 P@1: 0.63000 P@3: 0.53333 P@5: 0.43200 N@3: 0.56031 N@5: 0.56705 early stop: 1\n",
            "\u001b[32m[I 210629 04:27:15 models:110]\u001b[39m 307 512 train loss: 0.0002497 valid loss: 0.2268074 P@1: 0.63000 P@3: 0.53333 P@5: 0.43000 N@3: 0.56031 N@5: 0.56524 early stop: 2\n",
            "\u001b[32m[I 210629 04:27:17 models:110]\u001b[39m 309 1024 train loss: 0.0002021 valid loss: 0.2274678 P@1: 0.63000 P@3: 0.53333 P@5: 0.43000 N@3: 0.56031 N@5: 0.56524 early stop: 3\n",
            "\u001b[32m[I 210629 04:27:20 models:110]\u001b[39m 312 512 train loss: 0.0001709 valid loss: 0.2282273 P@1: 0.63000 P@3: 0.53333 P@5: 0.43000 N@3: 0.56031 N@5: 0.56524 early stop: 4\n",
            "\u001b[32m[I 210629 04:27:22 models:110]\u001b[39m 314 1024 train loss: 0.0001356 valid loss: 0.2289561 P@1: 0.63000 P@3: 0.53333 P@5: 0.43200 N@3: 0.56031 N@5: 0.56675 early stop: 5\n",
            "\u001b[32m[I 210629 04:27:24 models:110]\u001b[39m 317 512 train loss: 0.0001502 valid loss: 0.2296436 P@1: 0.63000 P@3: 0.53333 P@5: 0.43200 N@3: 0.56031 N@5: 0.56660 early stop: 6\n",
            "\u001b[32m[I 210629 04:27:26 models:110]\u001b[39m 319 1024 train loss: 0.0001019 valid loss: 0.2303126 P@1: 0.63000 P@3: 0.53333 P@5: 0.43400 N@3: 0.56031 N@5: 0.56811 early stop: 0\n",
            "\u001b[32m[I 210629 04:27:29 models:110]\u001b[39m 322 512 train loss: 0.0001078 valid loss: 0.2309765 P@1: 0.63000 P@3: 0.53333 P@5: 0.43400 N@3: 0.56093 N@5: 0.56872 early stop: 0\n",
            "\u001b[32m[I 210629 04:27:31 models:110]\u001b[39m 324 1024 train loss: 0.0001339 valid loss: 0.2316366 P@1: 0.63000 P@3: 0.53333 P@5: 0.43400 N@3: 0.56093 N@5: 0.56872 early stop: 1\n",
            "\u001b[32m[I 210629 04:27:34 models:110]\u001b[39m 327 512 train loss: 0.0001191 valid loss: 0.2322786 P@1: 0.63000 P@3: 0.53333 P@5: 0.43400 N@3: 0.56093 N@5: 0.56872 early stop: 2\n",
            "\u001b[32m[I 210629 04:27:36 models:110]\u001b[39m 329 1024 train loss: 0.0001153 valid loss: 0.2329102 P@1: 0.63000 P@3: 0.53333 P@5: 0.43400 N@3: 0.56093 N@5: 0.56872 early stop: 3\n",
            "\u001b[32m[I 210629 04:27:38 models:110]\u001b[39m 332 512 train loss: 0.0001334 valid loss: 0.2335483 P@1: 0.63000 P@3: 0.53333 P@5: 0.43400 N@3: 0.56013 N@5: 0.56792 early stop: 4\n",
            "\u001b[33m[W 210629 04:27:40 models:137]\u001b[39m Clipping gradients with total norm 0.04789 and max norm 0.00625\n",
            "\u001b[32m[I 210629 04:27:40 models:110]\u001b[39m 334 1024 train loss: 0.0002169 valid loss: 0.2341859 P@1: 0.63000 P@3: 0.53333 P@5: 0.43400 N@3: 0.55951 N@5: 0.56741 early stop: 5\n",
            "\u001b[32m[I 210629 04:27:43 models:110]\u001b[39m 337 512 train loss: 0.0001453 valid loss: 0.2348298 P@1: 0.63000 P@3: 0.53333 P@5: 0.43400 N@3: 0.55951 N@5: 0.56726 early stop: 6\n",
            "\u001b[33m[W 210629 04:27:43 models:137]\u001b[39m Clipping gradients with total norm 0.08162 and max norm 0.00381\n",
            "\u001b[32m[I 210629 04:27:45 models:110]\u001b[39m 339 1024 train loss: 0.0005617 valid loss: 0.2354652 P@1: 0.63000 P@3: 0.53333 P@5: 0.43400 N@3: 0.55951 N@5: 0.56726 early stop: 7\n",
            "\u001b[32m[I 210629 04:27:47 models:110]\u001b[39m 342 512 train loss: 0.0001264 valid loss: 0.2360836 P@1: 0.63000 P@3: 0.53667 P@5: 0.43600 N@3: 0.56186 N@5: 0.56881 early stop: 0\n",
            "\u001b[32m[I 210629 04:27:49 models:110]\u001b[39m 344 1024 train loss: 0.0001104 valid loss: 0.2366853 P@1: 0.63000 P@3: 0.53667 P@5: 0.43600 N@3: 0.56186 N@5: 0.56881 early stop: 1\n",
            "\u001b[32m[I 210629 04:27:52 models:110]\u001b[39m 347 512 train loss: 0.0001354 valid loss: 0.2372833 P@1: 0.63000 P@3: 0.53333 P@5: 0.43600 N@3: 0.55951 N@5: 0.56828 early stop: 2\n",
            "\u001b[32m[I 210629 04:27:54 models:110]\u001b[39m 349 1024 train loss: 0.0000964 valid loss: 0.2378782 P@1: 0.63000 P@3: 0.53333 P@5: 0.43600 N@3: 0.55951 N@5: 0.56828 early stop: 3\n",
            "\u001b[33m[W 210629 04:27:56 models:137]\u001b[39m Clipping gradients with total norm 0.0428 and max norm 0.0066\n",
            "\u001b[32m[I 210629 04:27:57 models:110]\u001b[39m 352 512 train loss: 0.0002810 valid loss: 0.2384692 P@1: 0.63000 P@3: 0.53000 P@5: 0.43600 N@3: 0.55717 N@5: 0.56795 early stop: 4\n",
            "\u001b[32m[I 210629 04:27:59 models:110]\u001b[39m 354 1024 train loss: 0.0001853 valid loss: 0.2390612 P@1: 0.63000 P@3: 0.53000 P@5: 0.43600 N@3: 0.55717 N@5: 0.56795 early stop: 5\n",
            "\u001b[32m[I 210629 04:28:01 models:110]\u001b[39m 357 512 train loss: 0.0001781 valid loss: 0.2396506 P@1: 0.63000 P@3: 0.53000 P@5: 0.43600 N@3: 0.55717 N@5: 0.56795 early stop: 6\n",
            "\u001b[32m[I 210629 04:28:03 models:110]\u001b[39m 359 1024 train loss: 0.0000966 valid loss: 0.2402399 P@1: 0.63000 P@3: 0.53000 P@5: 0.43600 N@3: 0.55717 N@5: 0.56795 early stop: 7\n",
            "\u001b[33m[W 210629 04:28:05 models:137]\u001b[39m Clipping gradients with total norm 0.05545 and max norm 0.00597\n",
            "\u001b[32m[I 210629 04:28:06 models:110]\u001b[39m 362 512 train loss: 0.0003287 valid loss: 0.2408204 P@1: 0.63000 P@3: 0.53000 P@5: 0.43600 N@3: 0.55717 N@5: 0.56795 early stop: 8\n",
            "\u001b[32m[I 210629 04:28:08 models:110]\u001b[39m 364 1024 train loss: 0.0001256 valid loss: 0.2413950 P@1: 0.63000 P@3: 0.53000 P@5: 0.43600 N@3: 0.55717 N@5: 0.56795 early stop: 9\n",
            "\u001b[32m[I 210629 04:28:10 models:110]\u001b[39m 367 512 train loss: 0.0001439 valid loss: 0.2419663 P@1: 0.63000 P@3: 0.53000 P@5: 0.43600 N@3: 0.55717 N@5: 0.56795 early stop: 10\n",
            "\u001b[32m[I 210629 04:28:12 models:110]\u001b[39m 369 1024 train loss: 0.0000974 valid loss: 0.2425421 P@1: 0.63000 P@3: 0.53000 P@5: 0.43600 N@3: 0.55717 N@5: 0.56795 early stop: 11\n",
            "\u001b[33m[W 210629 04:28:14 models:137]\u001b[39m Clipping gradients with total norm 0.04709 and max norm 0.00784\n",
            "\u001b[32m[I 210629 04:28:15 models:110]\u001b[39m 372 512 train loss: 0.0002633 valid loss: 0.2431214 P@1: 0.63000 P@3: 0.53000 P@5: 0.43600 N@3: 0.55717 N@5: 0.56795 early stop: 12\n",
            "\u001b[32m[I 210629 04:28:17 models:110]\u001b[39m 374 1024 train loss: 0.0001481 valid loss: 0.2437028 P@1: 0.63000 P@3: 0.53000 P@5: 0.43400 N@3: 0.55717 N@5: 0.56664 early stop: 13\n",
            "\u001b[32m[I 210629 04:28:20 models:110]\u001b[39m 377 512 train loss: 0.0001333 valid loss: 0.2442857 P@1: 0.63000 P@3: 0.53000 P@5: 0.43400 N@3: 0.55717 N@5: 0.56664 early stop: 14\n",
            "\u001b[32m[I 210629 04:28:22 models:110]\u001b[39m 379 1024 train loss: 0.0001213 valid loss: 0.2448445 P@1: 0.63000 P@3: 0.53000 P@5: 0.43400 N@3: 0.55717 N@5: 0.56664 early stop: 15\n",
            "\u001b[32m[I 210629 04:28:24 models:110]\u001b[39m 382 512 train loss: 0.0001156 valid loss: 0.2453893 P@1: 0.63000 P@3: 0.53000 P@5: 0.43400 N@3: 0.55717 N@5: 0.56643 early stop: 16\n",
            "\u001b[32m[I 210629 04:28:26 models:110]\u001b[39m 384 1024 train loss: 0.0000878 valid loss: 0.2459284 P@1: 0.63000 P@3: 0.53000 P@5: 0.43400 N@3: 0.55717 N@5: 0.56643 early stop: 17\n",
            "\u001b[33m[W 210629 04:28:28 models:137]\u001b[39m Clipping gradients with total norm 0.04769 and max norm 0.00661\n",
            "\u001b[32m[I 210629 04:28:29 models:110]\u001b[39m 387 512 train loss: 0.0002503 valid loss: 0.2464799 P@1: 0.63000 P@3: 0.53333 P@5: 0.43400 N@3: 0.55951 N@5: 0.56676 early stop: 18\n",
            "\u001b[32m[I 210629 04:28:31 models:110]\u001b[39m 389 1024 train loss: 0.0001849 valid loss: 0.2470216 P@1: 0.63000 P@3: 0.53333 P@5: 0.43400 N@3: 0.55951 N@5: 0.56676 early stop: 19\n",
            "\u001b[32m[I 210629 04:28:33 models:110]\u001b[39m 392 512 train loss: 0.0001506 valid loss: 0.2475451 P@1: 0.63000 P@3: 0.53333 P@5: 0.43400 N@3: 0.55951 N@5: 0.56655 early stop: 20\n",
            "\u001b[33m[W 210629 04:28:35 models:137]\u001b[39m Clipping gradients with total norm 0.04359 and max norm 0.00738\n",
            "\u001b[32m[I 210629 04:28:35 models:110]\u001b[39m 394 1024 train loss: 0.0001928 valid loss: 0.2480829 P@1: 0.63000 P@3: 0.53333 P@5: 0.43400 N@3: 0.55951 N@5: 0.56655 early stop: 21\n",
            "\u001b[32m[I 210629 04:28:38 models:110]\u001b[39m 397 512 train loss: 0.0002064 valid loss: 0.2486351 P@1: 0.63000 P@3: 0.53333 P@5: 0.43400 N@3: 0.55890 N@5: 0.56611 early stop: 22\n",
            "\u001b[32m[I 210629 04:28:40 models:110]\u001b[39m 399 1024 train loss: 0.0001000 valid loss: 0.2491824 P@1: 0.63000 P@3: 0.53333 P@5: 0.43400 N@3: 0.55890 N@5: 0.56611 early stop: 23\n",
            "\u001b[32m[I 210629 04:28:42 models:110]\u001b[39m 402 512 train loss: 0.0001299 valid loss: 0.2496824 P@1: 0.63000 P@3: 0.53333 P@5: 0.43400 N@3: 0.55890 N@5: 0.56611 early stop: 24\n",
            "\u001b[32m[I 210629 04:28:45 models:110]\u001b[39m 404 1024 train loss: 0.0001113 valid loss: 0.2501792 P@1: 0.63000 P@3: 0.53333 P@5: 0.43400 N@3: 0.55890 N@5: 0.56626 early stop: 25\n",
            "\u001b[32m[I 210629 04:28:47 models:110]\u001b[39m 407 512 train loss: 0.0001599 valid loss: 0.2506824 P@1: 0.63000 P@3: 0.53333 P@5: 0.43400 N@3: 0.55890 N@5: 0.56626 early stop: 26\n",
            "\u001b[33m[W 210629 04:28:48 models:137]\u001b[39m Clipping gradients with total norm 0.04572 and max norm 0.0056\n",
            "\u001b[32m[I 210629 04:28:49 models:110]\u001b[39m 409 1024 train loss: 0.0002466 valid loss: 0.2511716 P@1: 0.63000 P@3: 0.53333 P@5: 0.43400 N@3: 0.55951 N@5: 0.56677 early stop: 27\n",
            "\u001b[33m[W 210629 04:28:50 models:137]\u001b[39m Clipping gradients with total norm 0.07972 and max norm 0.00731\n",
            "\u001b[32m[I 210629 04:28:52 models:110]\u001b[39m 412 512 train loss: 0.0006447 valid loss: 0.2516829 P@1: 0.63000 P@3: 0.53333 P@5: 0.43400 N@3: 0.55951 N@5: 0.56677 early stop: 28\n",
            "\u001b[32m[I 210629 04:28:54 models:110]\u001b[39m 414 1024 train loss: 0.0001207 valid loss: 0.2522047 P@1: 0.63000 P@3: 0.53333 P@5: 0.43400 N@3: 0.55951 N@5: 0.56677 early stop: 29\n",
            "\u001b[32m[I 210629 04:28:57 models:110]\u001b[39m 417 512 train loss: 0.0001568 valid loss: 0.2527090 P@1: 0.63000 P@3: 0.53333 P@5: 0.43400 N@3: 0.55951 N@5: 0.56677 early stop: 30\n",
            "\u001b[32m[I 210629 04:28:59 models:110]\u001b[39m 419 1024 train loss: 0.0000866 valid loss: 0.2532020 P@1: 0.63000 P@3: 0.53333 P@5: 0.43400 N@3: 0.55951 N@5: 0.56677 early stop: 31\n",
            "\u001b[32m[I 210629 04:29:02 models:110]\u001b[39m 422 512 train loss: 0.0001271 valid loss: 0.2536872 P@1: 0.63000 P@3: 0.53333 P@5: 0.43400 N@3: 0.55951 N@5: 0.56677 early stop: 32\n",
            "\u001b[33m[W 210629 04:29:04 models:137]\u001b[39m Clipping gradients with total norm 0.04464 and max norm 0.00611\n",
            "\u001b[32m[I 210629 04:29:04 models:110]\u001b[39m 424 1024 train loss: 0.0002429 valid loss: 0.2541651 P@1: 0.63000 P@3: 0.53333 P@5: 0.43400 N@3: 0.55951 N@5: 0.56677 early stop: 33\n",
            "\u001b[32m[I 210629 04:29:07 models:110]\u001b[39m 427 512 train loss: 0.0001943 valid loss: 0.2546414 P@1: 0.63000 P@3: 0.53333 P@5: 0.43400 N@3: 0.55951 N@5: 0.56677 early stop: 34\n",
            "\u001b[32m[I 210629 04:29:10 models:110]\u001b[39m 429 1024 train loss: 0.0001163 valid loss: 0.2551129 P@1: 0.63000 P@3: 0.53333 P@5: 0.43400 N@3: 0.55951 N@5: 0.56677 early stop: 35\n",
            "\u001b[32m[I 210629 04:29:12 models:110]\u001b[39m 432 512 train loss: 0.0001259 valid loss: 0.2555861 P@1: 0.63000 P@3: 0.53000 P@5: 0.43400 N@3: 0.55717 N@5: 0.56650 early stop: 36\n",
            "\u001b[32m[I 210629 04:29:14 models:110]\u001b[39m 434 1024 train loss: 0.0001112 valid loss: 0.2560595 P@1: 0.63000 P@3: 0.53000 P@5: 0.43400 N@3: 0.55717 N@5: 0.56667 early stop: 37\n",
            "\u001b[33m[W 210629 04:29:15 models:137]\u001b[39m Clipping gradients with total norm 0.03213 and max norm 0.00425\n",
            "\u001b[32m[I 210629 04:29:17 models:110]\u001b[39m 437 512 train loss: 0.0002559 valid loss: 0.2565319 P@1: 0.63000 P@3: 0.53000 P@5: 0.43400 N@3: 0.55717 N@5: 0.56650 early stop: 38\n",
            "\u001b[32m[I 210629 04:29:19 models:110]\u001b[39m 439 1024 train loss: 0.0001183 valid loss: 0.2570000 P@1: 0.63000 P@3: 0.53000 P@5: 0.43400 N@3: 0.55717 N@5: 0.56650 early stop: 39\n",
            "\u001b[32m[I 210629 04:29:21 models:110]\u001b[39m 442 512 train loss: 0.0001045 valid loss: 0.2574600 P@1: 0.63000 P@3: 0.53333 P@5: 0.43400 N@3: 0.55951 N@5: 0.56682 early stop: 40\n",
            "\u001b[32m[I 210629 04:29:24 models:110]\u001b[39m 444 1024 train loss: 0.0001223 valid loss: 0.2579204 P@1: 0.63000 P@3: 0.53333 P@5: 0.43400 N@3: 0.55951 N@5: 0.56699 early stop: 41\n",
            "\u001b[32m[I 210629 04:29:26 models:110]\u001b[39m 447 512 train loss: 0.0001199 valid loss: 0.2583782 P@1: 0.63000 P@3: 0.53333 P@5: 0.43400 N@3: 0.55951 N@5: 0.56699 early stop: 42\n",
            "\u001b[32m[I 210629 04:29:28 models:110]\u001b[39m 449 1024 train loss: 0.0000865 valid loss: 0.2588345 P@1: 0.63000 P@3: 0.53333 P@5: 0.43400 N@3: 0.55951 N@5: 0.56699 early stop: 43\n",
            "\u001b[32m[I 210629 04:29:31 models:110]\u001b[39m 452 512 train loss: 0.0000961 valid loss: 0.2592891 P@1: 0.63000 P@3: 0.53333 P@5: 0.43400 N@3: 0.55951 N@5: 0.56699 early stop: 44\n",
            "\u001b[32m[I 210629 04:29:33 models:110]\u001b[39m 454 1024 train loss: 0.0001017 valid loss: 0.2597470 P@1: 0.64000 P@3: 0.53333 P@5: 0.43400 N@3: 0.56124 N@5: 0.56825 early stop: 45\n",
            "\u001b[32m[I 210629 04:29:35 models:110]\u001b[39m 457 512 train loss: 0.0001126 valid loss: 0.2602060 P@1: 0.64000 P@3: 0.53333 P@5: 0.43400 N@3: 0.56124 N@5: 0.56825 early stop: 46\n",
            "\u001b[32m[I 210629 04:29:37 models:110]\u001b[39m 459 1024 train loss: 0.0000794 valid loss: 0.2606626 P@1: 0.64000 P@3: 0.53333 P@5: 0.43400 N@3: 0.56124 N@5: 0.56807 early stop: 47\n",
            "\u001b[33m[W 210629 04:29:38 models:137]\u001b[39m Clipping gradients with total norm 0.03536 and max norm 0.00538\n",
            "\u001b[32m[I 210629 04:29:40 models:110]\u001b[39m 462 512 train loss: 0.0002929 valid loss: 0.2611130 P@1: 0.64000 P@3: 0.53333 P@5: 0.43400 N@3: 0.56124 N@5: 0.56825 early stop: 48\n",
            "\u001b[32m[I 210629 04:29:42 models:110]\u001b[39m 464 1024 train loss: 0.0001181 valid loss: 0.2615704 P@1: 0.64000 P@3: 0.53333 P@5: 0.43400 N@3: 0.56124 N@5: 0.56825 early stop: 49\n",
            "\u001b[32m[I 210629 04:29:44 models:110]\u001b[39m 467 512 train loss: 0.0001039 valid loss: 0.2620315 P@1: 0.64000 P@3: 0.53333 P@5: 0.43400 N@3: 0.56124 N@5: 0.56825 early stop: 50\n",
            "\u001b[33m[W 210629 04:29:45 models:137]\u001b[39m Clipping gradients with total norm 0.04245 and max norm 0.00553\n",
            "\u001b[32m[I 210629 04:29:46 main:76]\u001b[39m Finish Training\n",
            "\u001b[32m[I 210629 04:29:48 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210629 04:29:48 main:79]\u001b[39m Loading Test Set\n",
            "\u001b[32m[I 210629 04:29:48 main:83]\u001b[39m Size of Test Set: 100\n",
            "\u001b[32m[I 210629 04:29:48 main:85]\u001b[39m Predicting\n",
            "\u001b[32m[I 210629 04:29:52 main:91]\u001b[39m Finish Predicting\n",
            "Precision@1,3,5: 0.62 0.5066666666666667 0.388\n",
            "nDCG@1,3,5: 0.62 0.5393021247456504 0.5315856410945841\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiQNLtsZgDO1",
        "outputId": "8e4e71d2-d538-4597-8394-98bf592803ef"
      },
      "source": [
        "for skip in range(0, 1000, 100):\n",
        "    print(f\"\\nNO MAG WITH MESH, skip={skip}\\n\")\n",
        "    %cd PeTaL/\n",
        "    !python3 Split.py --skip={skip}\n",
        "    %cd ..\n",
        "    !wc PeTaL/train.json\n",
        "\n",
        "    !python3 transform_data_PeTaL.py --dataset $DATASET --no-mag\n",
        "\n",
        "    !python preprocess.py \\\n",
        "    --text-path {DATASET}/train_texts.txt \\\n",
        "    --label-path {DATASET}/train_labels.txt \\\n",
        "    --vocab-path {DATASET}/vocab.npy \\\n",
        "    --emb-path {DATASET}/emb_init.npy \\\n",
        "    --w2v-model {DATASET}/{DATASET}.joint.emb \\\n",
        "\n",
        "    !python preprocess.py \\\n",
        "    --text-path {DATASET}/test_texts.txt \\\n",
        "    --label-path {DATASET}/test_labels.txt \\\n",
        "    --vocab-path {DATASET}/vocab.npy \\\n",
        "\n",
        "    !PYTHONFAULTHANDLER=1 python main.py --data-cnf configure/datasets/{DATASET}.yaml --model-cnf configure/models/{MODEL}-{DATASET}.yaml --mode train --reg 1\n",
        "    !PYTHONFAULTHANDLER=1 python main.py --data-cnf configure/datasets/{DATASET}.yaml --model-cnf configure/models/{MODEL}-{DATASET}.yaml --mode eval\n",
        "\n",
        "    !python evaluation.py \\\n",
        "    --results {DATASET}/results/{MODEL}-{DATASET}-labels.npy \\\n",
        "    --targets {DATASET}/test_labels.npy \\\n",
        "    --train-labels {DATASET}/train_labels.npy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "NO MAG WITH MESH, skip=0\n",
            "\n",
            "/content/drive/.shortcut-targets-by-id/1H34uxYzZnD3lCNKKLXPkhQj27fDosPDC/PeTaL/PeTaL Data/MATCH on PeTaL Data/MATCH/PeTaL\n",
            "131\n",
            "/content/drive/.shortcut-targets-by-id/1H34uxYzZnD3lCNKKLXPkhQj27fDosPDC/PeTaL/PeTaL Data/MATCH on PeTaL Data/MATCH\n",
            "    800  275452 4644161 PeTaL/train.json\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gensim/parsing/preprocessing.py\", line 42, in <module>\n",
            "    from gensim import utils\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gensim/utils.py\", line 45, in <module>\n",
            "    from smart_open import smart_open\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/smart_open/__init__.py\", line 34, in <module>\n",
            "    from .smart_open_lib import open, parse_uri, smart_open, register_compressor  # noqa: E402\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py\", line 35, in <module>\n",
            "    from smart_open import doctools\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/smart_open/doctools.py\", line 21, in <module>\n",
            "    from . import transport\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/smart_open/transport.py\", line 98, in <module>\n",
            "    register_transport('smart_open.gcs')\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/smart_open/transport.py\", line 49, in register_transport\n",
            "    submodule = importlib.import_module(submodule)\n",
            "  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/smart_open/gcs.py\", line 16, in <module>\n",
            "    import google.cloud.storage\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/google/cloud/storage/__init__.py\", line 34, in <module>\n",
            "    from pkg_resources import get_distribution\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\", line 78, in <module>\n",
            "    __import__('pkg_resources.extern.packaging.requirements')\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pkg_resources/_vendor/packaging/requirements.py\", line 9, in <module>\n",
            "    from pkg_resources.extern.pyparsing import stringStart, stringEnd, originalTextFor, ParseException\n",
            "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 670, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 583, in module_from_spec\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pkg_resources/extern/__init__.py\", line 52, in create_module\n",
            "    return self.load_module(spec.name)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pkg_resources/extern/__init__.py\", line 37, in load_module\n",
            "    __import__(extant)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pkg_resources/_vendor/pyparsing.py\", line 3104, in <module>\n",
            "    class GoToColumn(_PositionToken):\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"preprocess.py\", line 8, in <module>\n",
            "    from deepxml.data_utils import build_vocab, convert_to_binary\n",
            "  File \"/content/drive/.shortcut-targets-by-id/1H34uxYzZnD3lCNKKLXPkhQj27fDosPDC/PeTaL/PeTaL Data/MATCH on PeTaL Data/MATCH/deepxml/data_utils.py\", line 7, in <module>\n",
            "    from gensim.models import KeyedVectors\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gensim/__init__.py\", line 5, in <module>\n",
            "    from gensim import parsing, corpora, matutils, interfaces, models, similarities, summarization, utils  # noqa:F401\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gensim/parsing/__init__.py\", line 4, in <module>\n",
            "    from .preprocessing import (remove_stopwords, strip_punctuation, strip_punctuation2,  # noqa:F401\n",
            "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
            "KeyboardInterrupt\n",
            "\u001b[32m[I 210629 06:30:51 preprocess:28]\u001b[39m Vocab Size: 26834\n",
            "\u001b[32m[I 210629 06:30:51 preprocess:30]\u001b[39m Getting Dataset: PeTaL/test_texts.txt Max Length: 500\n",
            "\u001b[32m[I 210629 06:30:51 preprocess:32]\u001b[39m Size of Samples: 100\n",
            "Exception ignored in: <function _get_module_lock.<locals>.cb at 0x7f3ebb879200>\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen importlib._bootstrap>\", line 176, in cb\n",
            "KeyboardInterrupt: \n",
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 12, in <module>\n",
            "    from deepxml.data_utils import get_data, get_mlb, get_word_emb, output_res\n",
            "  File \"/content/drive/.shortcut-targets-by-id/1H34uxYzZnD3lCNKKLXPkhQj27fDosPDC/PeTaL/PeTaL Data/MATCH on PeTaL Data/MATCH/deepxml/data_utils.py\", line 7, in <module>\n",
            "    from gensim.models import KeyedVectors\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gensim/__init__.py\", line 5, in <module>\n",
            "    from gensim import parsing, corpora, matutils, interfaces, models, similarities, summarization, utils  # noqa:F401\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gensim/parsing/__init__.py\", line 4, in <module>\n",
            "    from .preprocessing import (remove_stopwords, strip_punctuation, strip_punctuation2,  # noqa:F401\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gensim/parsing/preprocessing.py\", line 42, in <module>\n",
            "    from gensim import utils\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gensim/utils.py\", line 45, in <module>\n",
            "    from smart_open import smart_open\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/smart_open/__init__.py\", line 34, in <module>\n",
            "    from .smart_open_lib import open, parse_uri, smart_open, register_compressor  # noqa: E402\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py\", line 35, in <module>\n",
            "    from smart_open import doctools\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/smart_open/doctools.py\", line 21, in <module>\n",
            "    from . import transport\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/smart_open/transport.py\", line 98, in <module>\n",
            "    register_transport('smart_open.gcs')\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/smart_open/transport.py\", line 49, in register_transport\n",
            "    submodule = importlib.import_module(submodule)\n",
            "  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/smart_open/gcs.py\", line 16, in <module>\n",
            "    import google.cloud.storage\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/google/cloud/storage/__init__.py\", line 34, in <module>\n",
            "    from pkg_resources import get_distribution\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\", line 3242, in <module>\n",
            "    @_call_aside\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\", line 3226, in _call_aside\n",
            "    f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\", line 3270, in _initialize_master_working_set\n",
            "    for dist in working_set\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\", line 3270, in <genexpr>\n",
            "    for dist in working_set\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\", line 2780, in activate\n",
            "    fixup_namespace_packages(self.location)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\", line 2293, in fixup_namespace_packages\n",
            "    subpath = _handle_ns(package, path_item)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\", line 2194, in _handle_ns\n",
            "    loader = importer.find_spec(packageName).loader\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1364, in find_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 81, in _path_stat\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 7, in <module>\n",
            "    from sklearn.model_selection import train_test_split\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/__init__.py\", line 64, in <module>\n",
            "    from .base import clone\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/base.py\", line 14, in <module>\n",
            "    from .utils.fixes import signature\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py\", line 14, in <module>\n",
            "    from . import _joblib\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/_joblib.py\", line 22, in <module>\n",
            "    from ..externals import joblib\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/externals/joblib/__init__.py\", line 119, in <module>\n",
            "    from .parallel import Parallel\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/externals/joblib/parallel.py\", line 28, in <module>\n",
            "    from ._parallel_backends import (FallbackToBackend, MultiprocessingBackend,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/externals/joblib/_parallel_backends.py\", line 20, in <module>\n",
            "    from .pool import MemmappingPool\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/externals/joblib/pool.py\", line 33, in <module>\n",
            "    from ._memmapping_reducer import get_memmapping_reducers\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/externals/joblib/_memmapping_reducer.py\", line 17, in <module>\n",
            "    from uuid import uuid4\n",
            "  File \"/usr/lib/python3.7/uuid.py\", line 558, in <module>\n",
            "    import _uuid\n",
            "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 963, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 906, in _find_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1280, in find_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1252, in _get_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1364, in find_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 81, in _path_stat\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1219, in _path_importer_cache\n",
            "KeyError: '/content/drive/.shortcut-targets-by-id/1H34uxYzZnD3lCNKKLXPkhQj27fDosPDC/PeTaL/PeTaL Data/MATCH on PeTaL Data/MATCH'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 963, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 906, in _find_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1280, in find_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1249, in _get_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1221, in _path_importer_cache\n",
            "Traceback (most recent call last):\n",
            "  File \"evaluation.py\", line 6, in <module>\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1197, in _path_hooks\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1447, in path_hook_for_FileFinder\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 102, in _path_isdir\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 87, in _path_is_mode_type\n",
            "    from sklearn.preprocessing import MultiLabelBinarizer\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/__init__.py\", line 64, in <module>\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 81, in _path_stat\n",
            "    from .base import clone\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/base.py\", line 14, in <module>\n",
            "    from .utils.fixes import signature\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py\", line 16, in <module>\n",
            "    from .fixes import _Sequence as Sequence\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/fixes.py\", line 85, in <module>\n",
            "    from scipy.special import boxcox  # noqa\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scipy/special/__init__.py\", line 643, in <module>\n",
            "    from .basic import *\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scipy/special/basic.py\", line 19, in <module>\n",
            "    from . import orthogonal\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scipy/special/orthogonal.py\", line 83, in <module>\n",
            "    from scipy import linalg\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scipy/linalg/__init__.py\", line 207, in <module>\n",
            "    from ._decomp_update import *\n",
            "  File \"_decomp_update.pyx\", line 1, in init scipy.linalg._decomp_update\n",
            "KeyboardInterrupt\n",
            "\n",
            "NO MAG WITH MESH, skip=100\n",
            "\n",
            "/content/drive/.shortcut-targets-by-id/1H34uxYzZnD3lCNKKLXPkhQj27fDosPDC/PeTaL/PeTaL Data/MATCH on PeTaL Data/MATCH/PeTaL\n",
            "131\n",
            "/content/drive/.shortcut-targets-by-id/1H34uxYzZnD3lCNKKLXPkhQj27fDosPDC/PeTaL/PeTaL Data/MATCH on PeTaL Data/MATCH\n",
            "    800  274532 4635505 PeTaL/train.json\n",
            "Traceback (most recent call last):\n",
            "  File \"preprocess.py\", line 4, in <module>\n",
            "    import numpy as np\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/numpy/__init__.py\", line 142, in <module>\n",
            "    from . import core\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/numpy/core/__init__.py\", line 40, in <module>\n",
            "    from . import multiarray\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/numpy/core/multiarray.py\", line 12, in <module>\n",
            "    from . import overrides\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/numpy/core/overrides.py\", line 8, in <module>\n",
            "    from numpy.compat._inspect import getargspec\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/numpy/compat/__init__.py\", line 14, in <module>\n",
            "    from . import py3k\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/numpy/compat/py3k.py\", line 15, in <module>\n",
            "    from pathlib import Path, PurePath\n",
            "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 963, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 906, in _find_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1280, in find_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1252, in _get_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1364, in find_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 81, in _path_stat\n",
            "KeyboardInterrupt\n",
            "\u001b[32m[I 210629 06:30:56 preprocess:28]\u001b[39m Vocab Size: 26834\n",
            "\u001b[32m[I 210629 06:30:56 preprocess:30]\u001b[39m Getting Dataset: PeTaL/test_texts.txt Max Length: 500\n",
            "\u001b[32m[I 210629 06:30:56 preprocess:32]\u001b[39m Size of Samples: 100\n",
            "\u001b[32m[I 210629 06:30:58 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210629 06:30:58 main:35]\u001b[39m Loading Training and Validation Set\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['absorb_and/or_filter_solids', 'chemically_break_down_inorganic_compounds', 'detox/purify', 'manage_environmental_disturbances_in_a_community', 'protect_from_fire', 'protect_from_gases', 'send_vibratory_signals'] will be ignored\n",
            "  .format(sorted(unknown, key=str)))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['absorb_and/or_filter_solids', 'detox/purify', 'protect_from_gases'] will be ignored\n",
            "  .format(sorted(unknown, key=str)))\n",
            "\u001b[32m[I 210629 06:30:58 main:47]\u001b[39m Number of Labels: 124\n",
            "\u001b[32m[I 210629 06:30:58 main:48]\u001b[39m Size of Training Set: 800\n",
            "\u001b[32m[I 210629 06:30:58 main:49]\u001b[39m Size of Validation Set: 100\n",
            "\u001b[32m[I 210629 06:30:58 main:66]\u001b[39m Number of Edges: 101\n",
            "\u001b[32m[I 210629 06:30:58 main:68]\u001b[39m Training\n",
            "\n",
            "Aborted!\n",
            "^C\n",
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 9, in <module>\n",
            "    from logzero import logger\n",
            "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 963, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 906, in _find_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1280, in find_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1252, in _get_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1364, in find_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 81, in _path_stat\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"evaluation.py\", line 6, in <module>\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/__init__.py\", line 8, in <module>\n",
            "    from .data import Binarizer\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/data.py\", line 19, in <module>\n",
            "    from scipy import stats\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scipy/stats/__init__.py\", line 367, in <module>\n",
            "    from .stats import *\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scipy/stats/stats.py\", line 173, in <module>\n",
            "    from . import distributions\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scipy/stats/distributions.py\", line 10, in <module>\n",
            "    from ._distn_infrastructure import (entropy, rv_discrete, rv_continuous,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scipy/stats/_distn_infrastructure.py\", line 24, in <module>\n",
            "    from scipy import optimize\n",
            "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 724, in exec_module\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 854, in get_code\n",
            "  File \"<frozen importlib._bootstrap>\", line 224, in _verbose_message\n",
            "KeyboardInterrupt\n",
            "\n",
            "NO MAG WITH MESH, skip=200\n",
            "\n",
            "/content/drive/.shortcut-targets-by-id/1H34uxYzZnD3lCNKKLXPkhQj27fDosPDC/PeTaL/PeTaL Data/MATCH on PeTaL Data/MATCH/PeTaL\n",
            "129\n",
            "/content/drive/.shortcut-targets-by-id/1H34uxYzZnD3lCNKKLXPkhQj27fDosPDC/PeTaL/PeTaL Data/MATCH on PeTaL Data/MATCH\n",
            "    800  284828 4887836 PeTaL/train.json\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen importlib._bootstrap>\", line 1035, in _handle_fromlist\n",
            "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/numpy/core/numerictypes.py\", line 86, in <module>\n",
            "    import numbers\n",
            "  File \"/usr/lib/python3.7/numbers.py\", line 12, in <module>\n",
            "    class Number(metaclass=ABCMeta):\n",
            "  File \"/usr/lib/python3.7/abc.py\", line 126, in __new__\n",
            "    cls = super().__new__(mcls, name, bases, namespace, **kwargs)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"preprocess.py\", line 4, in <module>\n",
            "    import numpy as np\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/numpy/__init__.py\", line 142, in <module>\n",
            "    from . import core\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/numpy/core/__init__.py\", line 93, in <module>\n",
            "    from . import numerictypes as nt\n",
            "  File \"<frozen importlib._bootstrap>\", line 1036, in _handle_fromlist\n",
            "KeyboardInterrupt\n",
            "\u001b[32m[I 210629 06:31:06 preprocess:28]\u001b[39m Vocab Size: 26834\n",
            "\u001b[32m[I 210629 06:31:06 preprocess:30]\u001b[39m Getting Dataset: PeTaL/test_texts.txt Max Length: 500\n",
            "\u001b[32m[I 210629 06:31:06 preprocess:32]\u001b[39m Size of Samples: 100\n",
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 7, in <module>\n",
            "'import warnings' failed; traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/warnings.py\", line 550, in <module>\n",
            "    from sklearn.model_selection import train_test_split\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/__init__.py\", line 64, in <module>\n",
            "    from .base import clone\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/base.py\", line 14, in <module>\n",
            "    from .utils.fixes import signature\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py\", line 14, in <module>\n",
            "    from . import _joblib\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/_joblib.py\", line 22, in <module>\n",
            "    _processoptions(sys.warnoptions)\n",
            "  File \"/usr/lib/python3.7/warnings.py\", line 208, in _processoptions\n",
            "    from ..externals import joblib\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/externals/joblib/__init__.py\", line 119, in <module>\n",
            "    _setoption(arg)\n",
            "  File \"/usr/lib/python3.7/warnings.py\", line 224, in _setoption\n",
            "    from .parallel import Parallel\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/externals/joblib/parallel.py\", line 28, in <module>\n",
            "    import re\n",
            "    from ._parallel_backends import (FallbackToBackend, MultiprocessingBackend,\n",
            "  File \"/usr/lib/python3.7/re.py\", line 124, in <module>\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/externals/joblib/_parallel_backends.py\", line 22, in <module>\n",
            "    from .executor import get_memmapping_executor\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/externals/joblib/executor.py\", line 14, in <module>\n",
            "    from .externals.loky.reusable_executor import get_reusable_executor\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/externals/joblib/externals/loky/__init__.py\", line 12, in <module>\n",
            "    from .backend.reduction import set_loky_pickler\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/externals/joblib/externals/loky/backend/reduction.py\", line 18, in <module>\n",
            "    from cPickle import loads as pickle_loads\n",
            "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 963, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 906, in _find_spec\n",
            "    import enum\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1280, in find_spec\n",
            "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 724, in exec_module\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 857, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 525, in _compile_bytecode\n",
            "KeyboardInterrupt\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1252, in _get_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1364, in find_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 81, in _path_stat\n",
            "KeyboardInterrupt\n",
            "\u001b[32m[I 210629 06:31:08 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210629 06:31:08 main:79]\u001b[39m Loading Test Set\n",
            "\u001b[32m[I 210629 06:31:08 main:83]\u001b[39m Size of Test Set: 100\n",
            "\u001b[32m[I 210629 06:31:08 main:85]\u001b[39m Predicting\n",
            "\u001b[32m[I 210629 06:31:12 main:91]\u001b[39m Finish Predicting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8q5zfFonhoc",
        "outputId": "fb1494f2-2124-434a-af4c-1ff915e73693"
      },
      "source": [
        "for skip in range(0, 1000, 100):\n",
        "    print(f\"\\nNO MAG NO MESH, skip={skip}\\n\")\n",
        "    %cd PeTaL/\n",
        "    !python3 Split.py --skip={skip}\n",
        "    %cd ..\n",
        "    !wc PeTaL/train.json\n",
        "\n",
        "    !python3 transform_data_PeTaL.py --dataset $DATASET --no-mag --no-mesh\n",
        "\n",
        "    !python preprocess.py \\\n",
        "    --text-path {DATASET}/train_texts.txt \\\n",
        "    --label-path {DATASET}/train_labels.txt \\\n",
        "    --vocab-path {DATASET}/vocab.npy \\\n",
        "    --emb-path {DATASET}/emb_init.npy \\\n",
        "    --w2v-model {DATASET}/{DATASET}.joint.emb \\\n",
        "\n",
        "    !python preprocess.py \\\n",
        "    --text-path {DATASET}/test_texts.txt \\\n",
        "    --label-path {DATASET}/test_labels.txt \\\n",
        "    --vocab-path {DATASET}/vocab.npy \\\n",
        "\n",
        "    !PYTHONFAULTHANDLER=1 python main.py --data-cnf configure/datasets/{DATASET}.yaml --model-cnf configure/models/{MODEL}-{DATASET}.yaml --mode train --reg 1\n",
        "    !PYTHONFAULTHANDLER=1 python main.py --data-cnf configure/datasets/{DATASET}.yaml --model-cnf configure/models/{MODEL}-{DATASET}.yaml --mode eval\n",
        "\n",
        "    !python evaluation.py \\\n",
        "    --results {DATASET}/results/{MODEL}-{DATASET}-labels.npy \\\n",
        "    --targets {DATASET}/test_labels.npy \\\n",
        "    --train-labels {DATASET}/train_labels.npy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "NO MAG NO MESH, skip=100\n",
            "\n",
            "/content/drive/Shareddrives/NASA/MATCH/PeTaL\n",
            "131\n",
            "/content/drive/Shareddrives/NASA/MATCH\n",
            "    800  274532 4635505 PeTaL/train.json\n",
            "\u001b[32m[I 210629 05:16:44 preprocess:28]\u001b[39m Vocab Size: 26834\n",
            "\u001b[32m[I 210629 05:16:44 preprocess:30]\u001b[39m Getting Dataset: PeTaL/train_texts.txt Max Length: 500\n",
            "\u001b[32m[I 210629 05:16:44 preprocess:32]\u001b[39m Size of Samples: 900\n",
            "\u001b[32m[I 210629 05:16:46 preprocess:28]\u001b[39m Vocab Size: 26834\n",
            "\u001b[32m[I 210629 05:16:46 preprocess:30]\u001b[39m Getting Dataset: PeTaL/test_texts.txt Max Length: 500\n",
            "\u001b[32m[I 210629 05:16:46 preprocess:32]\u001b[39m Size of Samples: 100\n",
            "\u001b[32m[I 210629 05:16:47 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210629 05:16:47 main:35]\u001b[39m Loading Training and Validation Set\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['absorb_and/or_filter_solids', 'chemically_break_down_inorganic_compounds', 'detox/purify', 'manage_environmental_disturbances_in_a_community', 'protect_from_fire', 'protect_from_gases', 'send_vibratory_signals'] will be ignored\n",
            "  .format(sorted(unknown, key=str)))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['detox/purify', 'protect_from_gases'] will be ignored\n",
            "  .format(sorted(unknown, key=str)))\n",
            "\u001b[32m[I 210629 05:16:47 main:47]\u001b[39m Number of Labels: 124\n",
            "\u001b[32m[I 210629 05:16:47 main:48]\u001b[39m Size of Training Set: 800\n",
            "\u001b[32m[I 210629 05:16:47 main:49]\u001b[39m Size of Validation Set: 100\n",
            "\u001b[32m[I 210629 05:16:47 main:66]\u001b[39m Number of Edges: 101\n",
            "\u001b[32m[I 210629 05:16:47 main:68]\u001b[39m Training\n",
            "\u001b[32m[I 210629 05:16:53 models:110]\u001b[39m 2 512 train loss: 0.2757248 valid loss: 0.1563603 P@1: 0.28000 P@3: 0.20667 P@5: 0.14800 N@3: 0.21978 N@5: 0.20367 early stop: 0\n",
            "\u001b[32m[I 210629 05:16:54 models:142]\u001b[39m SWA Initializing\n",
            "\u001b[32m[I 210629 05:16:55 models:110]\u001b[39m 4 1024 train loss: 0.1462105 valid loss: 0.1466891 P@1: 0.28000 P@3: 0.21333 P@5: 0.17200 N@3: 0.22877 N@5: 0.22520 early stop: 0\n",
            "\u001b[32m[I 210629 05:16:58 models:110]\u001b[39m 7 512 train loss: 0.1397264 valid loss: 0.1452959 P@1: 0.28000 P@3: 0.21000 P@5: 0.18800 N@3: 0.22458 N@5: 0.23730 early stop: 0\n",
            "\u001b[32m[I 210629 05:17:00 models:110]\u001b[39m 9 1024 train loss: 0.1373111 valid loss: 0.1447477 P@1: 0.28000 P@3: 0.22000 P@5: 0.20200 N@3: 0.23162 N@5: 0.25079 early stop: 0\n",
            "\u001b[32m[I 210629 05:17:03 models:110]\u001b[39m 12 512 train loss: 0.1314377 valid loss: 0.1439015 P@1: 0.28000 P@3: 0.22667 P@5: 0.20000 N@3: 0.24000 N@5: 0.25188 early stop: 0\n",
            "\u001b[32m[I 210629 05:17:05 models:110]\u001b[39m 14 1024 train loss: 0.1180654 valid loss: 0.1430544 P@1: 0.28000 P@3: 0.25667 P@5: 0.21200 N@3: 0.26542 N@5: 0.26906 early stop: 0\n",
            "\u001b[32m[I 210629 05:17:07 models:110]\u001b[39m 17 512 train loss: 0.0993827 valid loss: 0.1423684 P@1: 0.32000 P@3: 0.28000 P@5: 0.22000 N@3: 0.28939 N@5: 0.28390 early stop: 0\n",
            "\u001b[32m[I 210629 05:17:09 models:110]\u001b[39m 19 1024 train loss: 0.0834871 valid loss: 0.1419301 P@1: 0.38000 P@3: 0.28667 P@5: 0.23200 N@3: 0.30652 N@5: 0.30387 early stop: 0\n",
            "\u001b[32m[I 210629 05:17:12 models:110]\u001b[39m 22 512 train loss: 0.0731070 valid loss: 0.1419026 P@1: 0.46000 P@3: 0.29333 P@5: 0.23000 N@3: 0.32782 N@5: 0.31970 early stop: 0\n",
            "\u001b[32m[I 210629 05:17:14 models:110]\u001b[39m 24 1024 train loss: 0.0603138 valid loss: 0.1411303 P@1: 0.49000 P@3: 0.31333 P@5: 0.24000 N@3: 0.35335 N@5: 0.33962 early stop: 0\n",
            "\u001b[32m[I 210629 05:17:17 models:110]\u001b[39m 27 512 train loss: 0.0524396 valid loss: 0.1402086 P@1: 0.52000 P@3: 0.32333 P@5: 0.25200 N@3: 0.36956 N@5: 0.35897 early stop: 0\n",
            "\u001b[32m[I 210629 05:17:19 models:110]\u001b[39m 29 1024 train loss: 0.0421083 valid loss: 0.1391954 P@1: 0.52000 P@3: 0.33667 P@5: 0.27000 N@3: 0.38469 N@5: 0.37984 early stop: 0\n",
            "\u001b[32m[I 210629 05:17:21 models:110]\u001b[39m 32 512 train loss: 0.0370426 valid loss: 0.1382463 P@1: 0.54000 P@3: 0.35333 P@5: 0.28000 N@3: 0.40234 N@5: 0.39548 early stop: 0\n",
            "\u001b[32m[I 210629 05:17:23 models:110]\u001b[39m 34 1024 train loss: 0.0307134 valid loss: 0.1373769 P@1: 0.55000 P@3: 0.35667 P@5: 0.29200 N@3: 0.40879 N@5: 0.40952 early stop: 0\n",
            "\u001b[32m[I 210629 05:17:26 models:110]\u001b[39m 37 512 train loss: 0.0270271 valid loss: 0.1368279 P@1: 0.56000 P@3: 0.38000 P@5: 0.30000 N@3: 0.42818 N@5: 0.41999 early stop: 0\n",
            "\u001b[32m[I 210629 05:17:28 models:110]\u001b[39m 39 1024 train loss: 0.0273164 valid loss: 0.1360847 P@1: 0.61000 P@3: 0.38333 P@5: 0.30400 N@3: 0.44183 N@5: 0.43381 early stop: 0\n",
            "\u001b[32m[I 210629 05:17:31 models:110]\u001b[39m 42 512 train loss: 0.0229818 valid loss: 0.1358398 P@1: 0.62000 P@3: 0.40000 P@5: 0.31200 N@3: 0.45591 N@5: 0.44491 early stop: 0\n",
            "\u001b[32m[I 210629 05:17:33 models:110]\u001b[39m 44 1024 train loss: 0.0189626 valid loss: 0.1355312 P@1: 0.62000 P@3: 0.41333 P@5: 0.32000 N@3: 0.46714 N@5: 0.45477 early stop: 0\n",
            "\u001b[32m[I 210629 05:17:35 models:110]\u001b[39m 47 512 train loss: 0.0180488 valid loss: 0.1356316 P@1: 0.62000 P@3: 0.42667 P@5: 0.32400 N@3: 0.47837 N@5: 0.46022 early stop: 0\n",
            "\u001b[32m[I 210629 05:17:38 models:110]\u001b[39m 49 1024 train loss: 0.0174951 valid loss: 0.1355870 P@1: 0.62000 P@3: 0.43333 P@5: 0.33600 N@3: 0.48429 N@5: 0.47092 early stop: 0\n",
            "\u001b[32m[I 210629 05:17:40 models:110]\u001b[39m 52 512 train loss: 0.0187181 valid loss: 0.1356630 P@1: 0.63000 P@3: 0.43333 P@5: 0.34600 N@3: 0.48786 N@5: 0.48294 early stop: 0\n",
            "\u001b[32m[I 210629 05:17:42 models:110]\u001b[39m 54 1024 train loss: 0.0185932 valid loss: 0.1360601 P@1: 0.64000 P@3: 0.44667 P@5: 0.35600 N@3: 0.50021 N@5: 0.49495 early stop: 0\n",
            "\u001b[32m[I 210629 05:17:45 models:110]\u001b[39m 57 512 train loss: 0.0172655 valid loss: 0.1361525 P@1: 0.67000 P@3: 0.46667 P@5: 0.36000 N@3: 0.51887 N@5: 0.50415 early stop: 0\n",
            "\u001b[32m[I 210629 05:17:47 models:110]\u001b[39m 59 1024 train loss: 0.0134890 valid loss: 0.1368678 P@1: 0.67000 P@3: 0.47333 P@5: 0.36000 N@3: 0.52418 N@5: 0.50507 early stop: 0\n",
            "\u001b[32m[I 210629 05:17:50 models:110]\u001b[39m 62 512 train loss: 0.0094312 valid loss: 0.1376975 P@1: 0.67000 P@3: 0.47667 P@5: 0.36600 N@3: 0.52775 N@5: 0.51297 early stop: 0\n",
            "\u001b[32m[I 210629 05:17:52 models:110]\u001b[39m 64 1024 train loss: 0.0076508 valid loss: 0.1383779 P@1: 0.67000 P@3: 0.48000 P@5: 0.36400 N@3: 0.53071 N@5: 0.51275 early stop: 1\n",
            "\u001b[32m[I 210629 05:17:54 models:110]\u001b[39m 67 512 train loss: 0.0056533 valid loss: 0.1393738 P@1: 0.67000 P@3: 0.48667 P@5: 0.36600 N@3: 0.53541 N@5: 0.51480 early stop: 0\n",
            "\u001b[32m[I 210629 05:17:56 models:110]\u001b[39m 69 1024 train loss: 0.0053825 valid loss: 0.1405506 P@1: 0.67000 P@3: 0.48667 P@5: 0.36800 N@3: 0.53541 N@5: 0.51624 early stop: 0\n",
            "\u001b[32m[I 210629 05:17:59 models:110]\u001b[39m 72 512 train loss: 0.0050157 valid loss: 0.1418083 P@1: 0.68000 P@3: 0.49667 P@5: 0.37000 N@3: 0.54541 N@5: 0.52043 early stop: 0\n",
            "\u001b[32m[I 210629 05:18:01 models:110]\u001b[39m 74 1024 train loss: 0.0046145 valid loss: 0.1434034 P@1: 0.68000 P@3: 0.49333 P@5: 0.37200 N@3: 0.54306 N@5: 0.52166 early stop: 0\n",
            "\u001b[32m[I 210629 05:18:04 models:110]\u001b[39m 77 512 train loss: 0.0038060 valid loss: 0.1450835 P@1: 0.68000 P@3: 0.49000 P@5: 0.37400 N@3: 0.54071 N@5: 0.52273 early stop: 0\n",
            "\u001b[32m[I 210629 05:18:06 models:110]\u001b[39m 79 1024 train loss: 0.0039680 valid loss: 0.1465808 P@1: 0.68000 P@3: 0.49333 P@5: 0.37400 N@3: 0.54367 N@5: 0.52382 early stop: 0\n",
            "\u001b[32m[I 210629 05:18:08 models:110]\u001b[39m 82 512 train loss: 0.0050431 valid loss: 0.1481473 P@1: 0.68000 P@3: 0.50000 P@5: 0.37800 N@3: 0.54898 N@5: 0.52755 early stop: 0\n",
            "\u001b[32m[I 210629 05:18:10 models:110]\u001b[39m 84 1024 train loss: 0.0063350 valid loss: 0.1494903 P@1: 0.68000 P@3: 0.51000 P@5: 0.37800 N@3: 0.55602 N@5: 0.52868 early stop: 0\n",
            "\u001b[32m[I 210629 05:18:13 models:110]\u001b[39m 87 512 train loss: 0.0070236 valid loss: 0.1509175 P@1: 0.69000 P@3: 0.50000 P@5: 0.37400 N@3: 0.55071 N@5: 0.52561 early stop: 1\n",
            "\u001b[32m[I 210629 05:18:15 models:110]\u001b[39m 89 1024 train loss: 0.0141922 valid loss: 0.1519539 P@1: 0.69000 P@3: 0.50000 P@5: 0.37600 N@3: 0.55071 N@5: 0.52716 early stop: 2\n",
            "\u001b[32m[I 210629 05:18:17 models:110]\u001b[39m 92 512 train loss: 0.0125166 valid loss: 0.1528771 P@1: 0.69000 P@3: 0.50333 P@5: 0.37600 N@3: 0.55306 N@5: 0.52757 early stop: 3\n",
            "\u001b[32m[I 210629 05:18:20 models:110]\u001b[39m 94 1024 train loss: 0.0099793 valid loss: 0.1539318 P@1: 0.69000 P@3: 0.50333 P@5: 0.37400 N@3: 0.55306 N@5: 0.52656 early stop: 4\n",
            "\u001b[32m[I 210629 05:18:22 models:110]\u001b[39m 97 512 train loss: 0.0077011 valid loss: 0.1547749 P@1: 0.68000 P@3: 0.50333 P@5: 0.37600 N@3: 0.55133 N@5: 0.52682 early stop: 5\n",
            "\u001b[32m[I 210629 05:18:24 models:110]\u001b[39m 99 1024 train loss: 0.0073325 valid loss: 0.1554762 P@1: 0.68000 P@3: 0.50333 P@5: 0.37800 N@3: 0.55133 N@5: 0.52833 early stop: 6\n",
            "\u001b[32m[I 210629 05:18:27 models:110]\u001b[39m 102 512 train loss: 0.0083545 valid loss: 0.1565645 P@1: 0.68000 P@3: 0.50333 P@5: 0.37800 N@3: 0.55071 N@5: 0.52774 early stop: 7\n",
            "\u001b[32m[I 210629 05:18:29 models:110]\u001b[39m 104 1024 train loss: 0.0057289 valid loss: 0.1576691 P@1: 0.68000 P@3: 0.50333 P@5: 0.38200 N@3: 0.55071 N@5: 0.53021 early stop: 0\n",
            "\u001b[32m[I 210629 05:18:31 models:110]\u001b[39m 107 512 train loss: 0.0039410 valid loss: 0.1589974 P@1: 0.68000 P@3: 0.50333 P@5: 0.38400 N@3: 0.55071 N@5: 0.53152 early stop: 0\n",
            "\u001b[32m[I 210629 05:18:33 models:110]\u001b[39m 109 1024 train loss: 0.0022314 valid loss: 0.1604386 P@1: 0.68000 P@3: 0.50333 P@5: 0.38400 N@3: 0.55010 N@5: 0.53043 early stop: 1\n",
            "\u001b[32m[I 210629 05:18:36 models:110]\u001b[39m 112 512 train loss: 0.0013687 valid loss: 0.1617135 P@1: 0.68000 P@3: 0.50333 P@5: 0.38800 N@3: 0.55010 N@5: 0.53408 early stop: 0\n",
            "\u001b[32m[I 210629 05:18:38 models:110]\u001b[39m 114 1024 train loss: 0.0011764 valid loss: 0.1631467 P@1: 0.68000 P@3: 0.50667 P@5: 0.39200 N@3: 0.55367 N@5: 0.53888 early stop: 0\n",
            "\u001b[32m[I 210629 05:18:40 models:110]\u001b[39m 117 512 train loss: 0.0007389 valid loss: 0.1645679 P@1: 0.68000 P@3: 0.51000 P@5: 0.39400 N@3: 0.55541 N@5: 0.54039 early stop: 0\n",
            "\u001b[32m[I 210629 05:18:43 models:110]\u001b[39m 119 1024 train loss: 0.0006153 valid loss: 0.1660810 P@1: 0.69000 P@3: 0.51000 P@5: 0.40200 N@3: 0.55714 N@5: 0.54754 early stop: 0\n",
            "\u001b[32m[I 210629 05:18:45 models:110]\u001b[39m 122 512 train loss: 0.0004200 valid loss: 0.1675999 P@1: 0.69000 P@3: 0.50667 P@5: 0.40000 N@3: 0.55479 N@5: 0.54590 early stop: 1\n",
            "\u001b[32m[I 210629 05:18:47 models:110]\u001b[39m 124 1024 train loss: 0.0003231 valid loss: 0.1690912 P@1: 0.69000 P@3: 0.50667 P@5: 0.39600 N@3: 0.55479 N@5: 0.54313 early stop: 2\n",
            "\u001b[33m[W 210629 05:18:49 models:137]\u001b[39m Clipping gradients with total norm 0.14469 and max norm 0.01933\n",
            "\u001b[32m[I 210629 05:18:50 models:110]\u001b[39m 127 512 train loss: 0.0005916 valid loss: 0.1705630 P@1: 0.69000 P@3: 0.50333 P@5: 0.39800 N@3: 0.55244 N@5: 0.54424 early stop: 3\n",
            "\u001b[32m[I 210629 05:18:52 models:110]\u001b[39m 129 1024 train loss: 0.0003812 valid loss: 0.1720940 P@1: 0.69000 P@3: 0.50000 P@5: 0.39600 N@3: 0.55010 N@5: 0.54241 early stop: 4\n",
            "\u001b[32m[I 210629 05:18:54 models:110]\u001b[39m 132 512 train loss: 0.0008617 valid loss: 0.1736300 P@1: 0.69000 P@3: 0.50000 P@5: 0.39600 N@3: 0.55010 N@5: 0.54306 early stop: 5\n",
            "\u001b[32m[I 210629 05:18:56 models:110]\u001b[39m 134 1024 train loss: 0.0011265 valid loss: 0.1752511 P@1: 0.69000 P@3: 0.50000 P@5: 0.40200 N@3: 0.55071 N@5: 0.54865 early stop: 0\n",
            "\u001b[32m[I 210629 05:18:59 models:110]\u001b[39m 137 512 train loss: 0.0015287 valid loss: 0.1764933 P@1: 0.69000 P@3: 0.50000 P@5: 0.40400 N@3: 0.54948 N@5: 0.54887 early stop: 0\n",
            "\u001b[32m[I 210629 05:19:01 models:110]\u001b[39m 139 1024 train loss: 0.0015429 valid loss: 0.1779059 P@1: 0.68000 P@3: 0.49333 P@5: 0.41000 N@3: 0.54306 N@5: 0.55084 early stop: 0\n",
            "\u001b[32m[I 210629 05:19:04 models:110]\u001b[39m 142 512 train loss: 0.0011854 valid loss: 0.1792507 P@1: 0.68000 P@3: 0.49333 P@5: 0.41200 N@3: 0.54306 N@5: 0.55230 early stop: 0\n",
            "\u001b[32m[I 210629 05:19:06 models:110]\u001b[39m 144 1024 train loss: 0.0017010 valid loss: 0.1805853 P@1: 0.68000 P@3: 0.49667 P@5: 0.41200 N@3: 0.54541 N@5: 0.55263 early stop: 0\n",
            "\u001b[32m[I 210629 05:19:08 models:110]\u001b[39m 147 512 train loss: 0.0012169 valid loss: 0.1819388 P@1: 0.68000 P@3: 0.49667 P@5: 0.41200 N@3: 0.54541 N@5: 0.55307 early stop: 0\n",
            "\u001b[32m[I 210629 05:19:10 models:110]\u001b[39m 149 1024 train loss: 0.0005848 valid loss: 0.1833103 P@1: 0.68000 P@3: 0.49667 P@5: 0.41000 N@3: 0.54479 N@5: 0.55065 early stop: 1\n",
            "\u001b[32m[I 210629 05:19:13 models:110]\u001b[39m 152 512 train loss: 0.0005015 valid loss: 0.1846765 P@1: 0.68000 P@3: 0.49333 P@5: 0.41000 N@3: 0.54306 N@5: 0.55133 early stop: 2\n",
            "\u001b[32m[I 210629 05:19:15 models:110]\u001b[39m 154 1024 train loss: 0.0003167 valid loss: 0.1860998 P@1: 0.68000 P@3: 0.49333 P@5: 0.41000 N@3: 0.54306 N@5: 0.55138 early stop: 3\n",
            "\u001b[32m[I 210629 05:19:17 models:110]\u001b[39m 157 512 train loss: 0.0004951 valid loss: 0.1875163 P@1: 0.68000 P@3: 0.49333 P@5: 0.41000 N@3: 0.54429 N@5: 0.55224 early stop: 4\n",
            "\u001b[32m[I 210629 05:19:19 models:110]\u001b[39m 159 1024 train loss: 0.0003897 valid loss: 0.1889339 P@1: 0.68000 P@3: 0.49333 P@5: 0.41000 N@3: 0.54490 N@5: 0.55268 early stop: 5\n",
            "\u001b[32m[I 210629 05:19:22 models:110]\u001b[39m 162 512 train loss: 0.0002665 valid loss: 0.1902726 P@1: 0.68000 P@3: 0.49333 P@5: 0.40800 N@3: 0.54490 N@5: 0.55086 early stop: 6\n",
            "\u001b[32m[I 210629 05:19:24 models:110]\u001b[39m 164 1024 train loss: 0.0002587 valid loss: 0.1916473 P@1: 0.68000 P@3: 0.50000 P@5: 0.41000 N@3: 0.54960 N@5: 0.55294 early stop: 7\n",
            "\u001b[33m[W 210629 05:19:25 models:137]\u001b[39m Clipping gradients with total norm 0.1056 and max norm 0.01204\n",
            "\u001b[32m[I 210629 05:19:26 models:110]\u001b[39m 167 512 train loss: 0.0004601 valid loss: 0.1930747 P@1: 0.67000 P@3: 0.50333 P@5: 0.41000 N@3: 0.54968 N@5: 0.55092 early stop: 8\n",
            "\u001b[32m[I 210629 05:19:28 models:110]\u001b[39m 169 1024 train loss: 0.0003471 valid loss: 0.1944892 P@1: 0.67000 P@3: 0.51000 P@5: 0.40800 N@3: 0.55437 N@5: 0.54957 early stop: 9\n",
            "\u001b[32m[I 210629 05:19:31 models:110]\u001b[39m 172 512 train loss: 0.0003130 valid loss: 0.1958189 P@1: 0.67000 P@3: 0.51000 P@5: 0.40800 N@3: 0.55499 N@5: 0.55016 early stop: 10\n",
            "\u001b[32m[I 210629 05:19:33 models:110]\u001b[39m 174 1024 train loss: 0.0001572 valid loss: 0.1972379 P@1: 0.67000 P@3: 0.51333 P@5: 0.41000 N@3: 0.55733 N@5: 0.55230 early stop: 11\n",
            "\u001b[32m[I 210629 05:19:36 models:110]\u001b[39m 177 512 train loss: 0.0001856 valid loss: 0.1986495 P@1: 0.68000 P@3: 0.51333 P@5: 0.41000 N@3: 0.55906 N@5: 0.55375 early stop: 0\n",
            "\u001b[33m[W 210629 05:19:37 models:137]\u001b[39m Clipping gradients with total norm 0.06842 and max norm 0.00937\n",
            "\u001b[32m[I 210629 05:19:38 models:110]\u001b[39m 179 1024 train loss: 0.0002534 valid loss: 0.2000372 P@1: 0.68000 P@3: 0.51333 P@5: 0.41000 N@3: 0.55906 N@5: 0.55375 early stop: 1\n",
            "\u001b[33m[W 210629 05:19:39 models:137]\u001b[39m Clipping gradients with total norm 0.23491 and max norm 0.0144\n",
            "\u001b[32m[I 210629 05:19:40 models:110]\u001b[39m 182 512 train loss: 0.0008504 valid loss: 0.2013866 P@1: 0.68000 P@3: 0.51333 P@5: 0.41000 N@3: 0.55906 N@5: 0.55395 early stop: 0\n",
            "\u001b[32m[I 210629 05:19:42 models:110]\u001b[39m 184 1024 train loss: 0.0002088 valid loss: 0.2027218 P@1: 0.68000 P@3: 0.51333 P@5: 0.41000 N@3: 0.55906 N@5: 0.55410 early stop: 0\n",
            "\u001b[32m[I 210629 05:19:45 models:110]\u001b[39m 187 512 train loss: 0.0001901 valid loss: 0.2040800 P@1: 0.68000 P@3: 0.51333 P@5: 0.40800 N@3: 0.55906 N@5: 0.55304 early stop: 1\n",
            "\u001b[32m[I 210629 05:19:47 models:110]\u001b[39m 189 1024 train loss: 0.0005048 valid loss: 0.2054380 P@1: 0.68000 P@3: 0.51333 P@5: 0.40800 N@3: 0.55906 N@5: 0.55289 early stop: 2\n",
            "\u001b[32m[I 210629 05:19:49 models:110]\u001b[39m 192 512 train loss: 0.0003206 valid loss: 0.2067276 P@1: 0.68000 P@3: 0.51667 P@5: 0.40800 N@3: 0.56141 N@5: 0.55313 early stop: 3\n",
            "\u001b[32m[I 210629 05:19:51 models:110]\u001b[39m 194 1024 train loss: 0.0001730 valid loss: 0.2080656 P@1: 0.67000 P@3: 0.51667 P@5: 0.40800 N@3: 0.55968 N@5: 0.55187 early stop: 4\n",
            "\u001b[32m[I 210629 05:19:54 models:110]\u001b[39m 197 512 train loss: 0.0001682 valid loss: 0.2094011 P@1: 0.67000 P@3: 0.51333 P@5: 0.40800 N@3: 0.55733 N@5: 0.55211 early stop: 5\n",
            "\u001b[32m[I 210629 05:19:56 models:110]\u001b[39m 199 1024 train loss: 0.0001223 valid loss: 0.2107124 P@1: 0.67000 P@3: 0.51333 P@5: 0.40800 N@3: 0.55672 N@5: 0.55166 early stop: 6\n",
            "\u001b[33m[W 210629 05:19:57 models:137]\u001b[39m Clipping gradients with total norm 0.06718 and max norm 0.00857\n",
            "\u001b[32m[I 210629 05:19:59 models:110]\u001b[39m 202 512 train loss: 0.0004010 valid loss: 0.2120047 P@1: 0.67000 P@3: 0.51333 P@5: 0.41000 N@3: 0.55610 N@5: 0.55304 early stop: 7\n",
            "\u001b[32m[I 210629 05:20:01 models:110]\u001b[39m 204 1024 train loss: 0.0002227 valid loss: 0.2132869 P@1: 0.67000 P@3: 0.51333 P@5: 0.40800 N@3: 0.55610 N@5: 0.55172 early stop: 8\n",
            "\u001b[32m[I 210629 05:20:03 models:110]\u001b[39m 207 512 train loss: 0.0001618 valid loss: 0.2145501 P@1: 0.68000 P@3: 0.51333 P@5: 0.41000 N@3: 0.55642 N@5: 0.55373 early stop: 9\n",
            "\u001b[32m[I 210629 05:20:05 models:110]\u001b[39m 209 1024 train loss: 0.0001285 valid loss: 0.2157948 P@1: 0.69000 P@3: 0.51333 P@5: 0.41200 N@3: 0.55815 N@5: 0.55591 early stop: 0\n",
            "\u001b[32m[I 210629 05:20:08 models:110]\u001b[39m 212 512 train loss: 0.0001409 valid loss: 0.2170486 P@1: 0.69000 P@3: 0.51333 P@5: 0.41000 N@3: 0.55815 N@5: 0.55440 early stop: 1\n",
            "\u001b[32m[I 210629 05:20:10 models:110]\u001b[39m 214 1024 train loss: 0.0001475 valid loss: 0.2183147 P@1: 0.68000 P@3: 0.51333 P@5: 0.41000 N@3: 0.55642 N@5: 0.55267 early stop: 2\n",
            "\u001b[33m[W 210629 05:20:11 models:137]\u001b[39m Clipping gradients with total norm 0.07602 and max norm 0.00741\n",
            "\u001b[32m[I 210629 05:20:12 models:110]\u001b[39m 217 512 train loss: 0.0003667 valid loss: 0.2195554 P@1: 0.68000 P@3: 0.51667 P@5: 0.41000 N@3: 0.55938 N@5: 0.55361 early stop: 3\n",
            "\u001b[32m[I 210629 05:20:14 models:110]\u001b[39m 219 1024 train loss: 0.0001361 valid loss: 0.2207647 P@1: 0.68000 P@3: 0.51667 P@5: 0.41000 N@3: 0.55938 N@5: 0.55361 early stop: 4\n",
            "\u001b[32m[I 210629 05:20:17 models:110]\u001b[39m 222 512 train loss: 0.0001806 valid loss: 0.2219751 P@1: 0.68000 P@3: 0.51667 P@5: 0.41000 N@3: 0.55938 N@5: 0.55361 early stop: 5\n",
            "\u001b[33m[W 210629 05:20:17 models:137]\u001b[39m Clipping gradients with total norm 0.03725 and max norm 0.00468\n",
            "\u001b[32m[I 210629 05:20:19 models:110]\u001b[39m 224 1024 train loss: 0.0001178 valid loss: 0.2231732 P@1: 0.68000 P@3: 0.51667 P@5: 0.41000 N@3: 0.55938 N@5: 0.55361 early stop: 6\n",
            "\u001b[32m[I 210629 05:20:21 models:110]\u001b[39m 227 512 train loss: 0.0001337 valid loss: 0.2243884 P@1: 0.68000 P@3: 0.51667 P@5: 0.41000 N@3: 0.55938 N@5: 0.55382 early stop: 7\n",
            "\u001b[32m[I 210629 05:20:23 models:110]\u001b[39m 229 1024 train loss: 0.0001394 valid loss: 0.2256082 P@1: 0.68000 P@3: 0.51667 P@5: 0.40800 N@3: 0.55938 N@5: 0.55231 early stop: 8\n",
            "\u001b[32m[I 210629 05:20:26 models:110]\u001b[39m 232 512 train loss: 0.0001705 valid loss: 0.2267993 P@1: 0.68000 P@3: 0.51667 P@5: 0.40800 N@3: 0.55938 N@5: 0.55272 early stop: 9\n",
            "\u001b[32m[I 210629 05:20:28 models:110]\u001b[39m 234 1024 train loss: 0.0001107 valid loss: 0.2279749 P@1: 0.68000 P@3: 0.51667 P@5: 0.40600 N@3: 0.55938 N@5: 0.55090 early stop: 10\n",
            "\u001b[32m[I 210629 05:20:30 models:110]\u001b[39m 237 512 train loss: 0.0000965 valid loss: 0.2291410 P@1: 0.68000 P@3: 0.51667 P@5: 0.40600 N@3: 0.55938 N@5: 0.55090 early stop: 11\n",
            "\u001b[33m[W 210629 05:20:30 models:137]\u001b[39m Clipping gradients with total norm 0.05947 and max norm 0.00856\n",
            "\u001b[32m[I 210629 05:20:32 models:110]\u001b[39m 239 1024 train loss: 0.0003421 valid loss: 0.2302670 P@1: 0.68000 P@3: 0.51667 P@5: 0.40600 N@3: 0.55938 N@5: 0.55090 early stop: 12\n",
            "\u001b[33m[W 210629 05:20:34 models:137]\u001b[39m Clipping gradients with total norm 0.08961 and max norm 0.00687\n",
            "\u001b[32m[I 210629 05:20:35 models:110]\u001b[39m 242 512 train loss: 0.0005721 valid loss: 0.2314270 P@1: 0.68000 P@3: 0.51667 P@5: 0.40600 N@3: 0.55938 N@5: 0.55090 early stop: 13\n",
            "\u001b[32m[I 210629 05:20:37 models:110]\u001b[39m 244 1024 train loss: 0.0001435 valid loss: 0.2325853 P@1: 0.68000 P@3: 0.51333 P@5: 0.40600 N@3: 0.55703 N@5: 0.55052 early stop: 14\n",
            "\u001b[32m[I 210629 05:20:39 models:110]\u001b[39m 247 512 train loss: 0.0001450 valid loss: 0.2336910 P@1: 0.68000 P@3: 0.51333 P@5: 0.40600 N@3: 0.55703 N@5: 0.55052 early stop: 15\n",
            "\u001b[32m[I 210629 05:20:41 models:110]\u001b[39m 249 1024 train loss: 0.0001492 valid loss: 0.2348042 P@1: 0.68000 P@3: 0.51667 P@5: 0.40600 N@3: 0.55938 N@5: 0.55090 early stop: 16\n",
            "\u001b[32m[I 210629 05:20:44 models:110]\u001b[39m 252 512 train loss: 0.0001171 valid loss: 0.2359039 P@1: 0.68000 P@3: 0.51667 P@5: 0.40600 N@3: 0.55938 N@5: 0.55090 early stop: 17\n",
            "\u001b[33m[W 210629 05:20:44 models:137]\u001b[39m Clipping gradients with total norm 0.04571 and max norm 0.00886\n",
            "\u001b[32m[I 210629 05:20:46 models:110]\u001b[39m 254 1024 train loss: 0.0001986 valid loss: 0.2369767 P@1: 0.69000 P@3: 0.51667 P@5: 0.40600 N@3: 0.56111 N@5: 0.55215 early stop: 18\n",
            "\u001b[32m[I 210629 05:20:48 models:110]\u001b[39m 257 512 train loss: 0.0002035 valid loss: 0.2381178 P@1: 0.69000 P@3: 0.51667 P@5: 0.40600 N@3: 0.56111 N@5: 0.55215 early stop: 19\n",
            "\u001b[32m[I 210629 05:20:50 models:110]\u001b[39m 259 1024 train loss: 0.0001384 valid loss: 0.2392056 P@1: 0.69000 P@3: 0.51667 P@5: 0.40600 N@3: 0.56111 N@5: 0.55215 early stop: 20\n",
            "\u001b[32m[I 210629 05:20:53 models:110]\u001b[39m 262 512 train loss: 0.0001352 valid loss: 0.2402565 P@1: 0.69000 P@3: 0.51333 P@5: 0.40600 N@3: 0.55876 N@5: 0.55192 early stop: 21\n",
            "\u001b[32m[I 210629 05:20:55 models:110]\u001b[39m 264 1024 train loss: 0.0003583 valid loss: 0.2413596 P@1: 0.69000 P@3: 0.51333 P@5: 0.40600 N@3: 0.55938 N@5: 0.55243 early stop: 22\n",
            "\u001b[32m[I 210629 05:20:58 models:110]\u001b[39m 267 512 train loss: 0.0003864 valid loss: 0.2424169 P@1: 0.69000 P@3: 0.51667 P@5: 0.40800 N@3: 0.56234 N@5: 0.55484 early stop: 23\n",
            "\u001b[32m[I 210629 05:21:00 models:110]\u001b[39m 269 1024 train loss: 0.0001529 valid loss: 0.2434481 P@1: 0.69000 P@3: 0.51667 P@5: 0.40800 N@3: 0.56234 N@5: 0.55484 early stop: 24\n",
            "\u001b[32m[I 210629 05:21:02 models:110]\u001b[39m 272 512 train loss: 0.0004605 valid loss: 0.2444853 P@1: 0.70000 P@3: 0.51667 P@5: 0.40800 N@3: 0.56407 N@5: 0.55610 early stop: 0\n",
            "\u001b[33m[W 210629 05:21:03 models:137]\u001b[39m Clipping gradients with total norm 0.28757 and max norm 0.0503\n",
            "\u001b[32m[I 210629 05:21:04 models:110]\u001b[39m 274 1024 train loss: 0.0011290 valid loss: 0.2454437 P@1: 0.70000 P@3: 0.51667 P@5: 0.40800 N@3: 0.56407 N@5: 0.55627 early stop: 0\n",
            "\u001b[32m[I 210629 05:21:07 models:110]\u001b[39m 277 512 train loss: 0.0076738 valid loss: 0.2464166 P@1: 0.70000 P@3: 0.52000 P@5: 0.40800 N@3: 0.56642 N@5: 0.55650 early stop: 0\n",
            "\u001b[32m[I 210629 05:21:09 models:110]\u001b[39m 279 1024 train loss: 0.0395310 valid loss: 0.2459908 P@1: 0.68000 P@3: 0.52333 P@5: 0.41000 N@3: 0.56530 N@5: 0.55564 early stop: 1\n",
            "\u001b[32m[I 210629 05:21:11 models:110]\u001b[39m 282 512 train loss: 0.0580271 valid loss: 0.2446608 P@1: 0.69000 P@3: 0.52333 P@5: 0.41000 N@3: 0.56703 N@5: 0.55689 early stop: 0\n",
            "\u001b[32m[I 210629 05:21:13 models:110]\u001b[39m 284 1024 train loss: 0.0357690 valid loss: 0.2428846 P@1: 0.69000 P@3: 0.52667 P@5: 0.41200 N@3: 0.56938 N@5: 0.55909 early stop: 0\n",
            "\u001b[32m[I 210629 05:21:16 models:110]\u001b[39m 287 512 train loss: 0.0222824 valid loss: 0.2413903 P@1: 0.69000 P@3: 0.53000 P@5: 0.41400 N@3: 0.57173 N@5: 0.56078 early stop: 0\n",
            "\u001b[32m[I 210629 05:21:18 models:110]\u001b[39m 289 1024 train loss: 0.0143126 valid loss: 0.2403336 P@1: 0.69000 P@3: 0.53000 P@5: 0.41200 N@3: 0.57173 N@5: 0.55897 early stop: 1\n",
            "\u001b[32m[I 210629 05:21:21 models:110]\u001b[39m 292 512 train loss: 0.0091631 valid loss: 0.2395489 P@1: 0.68000 P@3: 0.52667 P@5: 0.41400 N@3: 0.56703 N@5: 0.55885 early stop: 2\n",
            "\u001b[32m[I 210629 05:21:23 models:110]\u001b[39m 294 1024 train loss: 0.0055303 valid loss: 0.2390295 P@1: 0.68000 P@3: 0.52667 P@5: 0.41400 N@3: 0.56765 N@5: 0.55929 early stop: 3\n",
            "\u001b[32m[I 210629 05:21:25 models:110]\u001b[39m 297 512 train loss: 0.0034664 valid loss: 0.2385556 P@1: 0.68000 P@3: 0.52667 P@5: 0.41400 N@3: 0.56703 N@5: 0.55868 early stop: 4\n",
            "\u001b[32m[I 210629 05:21:27 models:110]\u001b[39m 299 1024 train loss: 0.0021086 valid loss: 0.2384048 P@1: 0.68000 P@3: 0.52667 P@5: 0.41600 N@3: 0.56642 N@5: 0.55940 early stop: 5\n",
            "\u001b[32m[I 210629 05:21:30 models:110]\u001b[39m 302 512 train loss: 0.0012257 valid loss: 0.2382514 P@1: 0.68000 P@3: 0.52667 P@5: 0.41600 N@3: 0.56642 N@5: 0.55961 early stop: 6\n",
            "\u001b[32m[I 210629 05:21:32 models:110]\u001b[39m 304 1024 train loss: 0.0010403 valid loss: 0.2382711 P@1: 0.68000 P@3: 0.52667 P@5: 0.41800 N@3: 0.56642 N@5: 0.56092 early stop: 0\n",
            "\u001b[32m[I 210629 05:21:34 models:110]\u001b[39m 307 512 train loss: 0.0008741 valid loss: 0.2383280 P@1: 0.68000 P@3: 0.52333 P@5: 0.42000 N@3: 0.56407 N@5: 0.56250 early stop: 0\n",
            "\u001b[32m[I 210629 05:21:36 models:110]\u001b[39m 309 1024 train loss: 0.0005388 valid loss: 0.2384403 P@1: 0.68000 P@3: 0.52333 P@5: 0.42000 N@3: 0.56407 N@5: 0.56250 early stop: 1\n",
            "\u001b[32m[I 210629 05:21:39 models:110]\u001b[39m 312 512 train loss: 0.0004039 valid loss: 0.2385657 P@1: 0.68000 P@3: 0.52333 P@5: 0.42000 N@3: 0.56407 N@5: 0.56250 early stop: 2\n",
            "\u001b[32m[I 210629 05:21:41 models:110]\u001b[39m 314 1024 train loss: 0.0005880 valid loss: 0.2387762 P@1: 0.68000 P@3: 0.52333 P@5: 0.42200 N@3: 0.56407 N@5: 0.56401 early stop: 0\n",
            "\u001b[32m[I 210629 05:21:43 models:110]\u001b[39m 317 512 train loss: 0.0006079 valid loss: 0.2389729 P@1: 0.68000 P@3: 0.52333 P@5: 0.42200 N@3: 0.56407 N@5: 0.56401 early stop: 1\n",
            "\u001b[32m[I 210629 05:21:46 models:110]\u001b[39m 319 1024 train loss: 0.0005797 valid loss: 0.2392397 P@1: 0.69000 P@3: 0.52667 P@5: 0.42000 N@3: 0.56876 N@5: 0.56448 early stop: 0\n",
            "\u001b[32m[I 210629 05:21:48 models:110]\u001b[39m 322 512 train loss: 0.0006361 valid loss: 0.2394043 P@1: 0.69000 P@3: 0.52667 P@5: 0.41800 N@3: 0.56815 N@5: 0.56272 early stop: 1\n",
            "\u001b[32m[I 210629 05:21:50 models:110]\u001b[39m 324 1024 train loss: 0.0004981 valid loss: 0.2397178 P@1: 0.69000 P@3: 0.52667 P@5: 0.41800 N@3: 0.56815 N@5: 0.56272 early stop: 2\n",
            "\u001b[32m[I 210629 05:21:53 models:110]\u001b[39m 327 512 train loss: 0.0006562 valid loss: 0.2399778 P@1: 0.70000 P@3: 0.52667 P@5: 0.42000 N@3: 0.57050 N@5: 0.56700 early stop: 0\n",
            "\u001b[32m[I 210629 05:21:55 models:110]\u001b[39m 329 1024 train loss: 0.0004381 valid loss: 0.2403107 P@1: 0.70000 P@3: 0.52667 P@5: 0.42000 N@3: 0.57050 N@5: 0.56685 early stop: 1\n",
            "\u001b[32m[I 210629 05:21:57 models:110]\u001b[39m 332 512 train loss: 0.0004448 valid loss: 0.2406804 P@1: 0.70000 P@3: 0.52667 P@5: 0.42000 N@3: 0.57050 N@5: 0.56685 early stop: 2\n",
            "\u001b[32m[I 210629 05:21:59 models:110]\u001b[39m 334 1024 train loss: 0.0003249 valid loss: 0.2409857 P@1: 0.70000 P@3: 0.52667 P@5: 0.42000 N@3: 0.57050 N@5: 0.56685 early stop: 3\n",
            "\u001b[32m[I 210629 05:22:02 models:110]\u001b[39m 337 512 train loss: 0.0002926 valid loss: 0.2413476 P@1: 0.70000 P@3: 0.52667 P@5: 0.42000 N@3: 0.57050 N@5: 0.56685 early stop: 4\n",
            "\u001b[32m[I 210629 05:22:04 models:110]\u001b[39m 339 1024 train loss: 0.0001778 valid loss: 0.2417272 P@1: 0.70000 P@3: 0.52667 P@5: 0.42000 N@3: 0.57111 N@5: 0.56761 early stop: 0\n",
            "\u001b[32m[I 210629 05:22:07 models:110]\u001b[39m 342 512 train loss: 0.0001796 valid loss: 0.2421098 P@1: 0.70000 P@3: 0.52667 P@5: 0.42200 N@3: 0.57111 N@5: 0.56943 early stop: 0\n",
            "\u001b[32m[I 210629 05:22:09 models:110]\u001b[39m 344 1024 train loss: 0.0001509 valid loss: 0.2424911 P@1: 0.70000 P@3: 0.53000 P@5: 0.42400 N@3: 0.57346 N@5: 0.57157 early stop: 0\n",
            "\u001b[32m[I 210629 05:22:11 models:110]\u001b[39m 347 512 train loss: 0.0002151 valid loss: 0.2428946 P@1: 0.70000 P@3: 0.53000 P@5: 0.42400 N@3: 0.57407 N@5: 0.57208 early stop: 0\n",
            "\u001b[33m[W 210629 05:22:13 models:137]\u001b[39m Clipping gradients with total norm 0.08473 and max norm 0.00919\n",
            "\u001b[32m[I 210629 05:22:13 models:110]\u001b[39m 349 1024 train loss: 0.0004108 valid loss: 0.2433008 P@1: 0.70000 P@3: 0.52667 P@5: 0.42400 N@3: 0.57173 N@5: 0.57181 early stop: 1\n",
            "\u001b[32m[I 210629 05:22:16 models:110]\u001b[39m 352 512 train loss: 0.0002878 valid loss: 0.2436992 P@1: 0.69000 P@3: 0.52667 P@5: 0.42400 N@3: 0.57080 N@5: 0.57088 early stop: 2\n",
            "\u001b[32m[I 210629 05:22:18 models:110]\u001b[39m 354 1024 train loss: 0.0001658 valid loss: 0.2441215 P@1: 0.69000 P@3: 0.52667 P@5: 0.42400 N@3: 0.57080 N@5: 0.57088 early stop: 3\n",
            "\u001b[32m[I 210629 05:22:20 models:110]\u001b[39m 357 512 train loss: 0.0001787 valid loss: 0.2445553 P@1: 0.69000 P@3: 0.52667 P@5: 0.42600 N@3: 0.57080 N@5: 0.57219 early stop: 0\n",
            "\u001b[32m[I 210629 05:22:23 models:110]\u001b[39m 359 1024 train loss: 0.0001138 valid loss: 0.2449795 P@1: 0.68000 P@3: 0.52667 P@5: 0.42800 N@3: 0.56906 N@5: 0.57245 early stop: 0\n",
            "\u001b[32m[I 210629 05:22:25 models:110]\u001b[39m 362 512 train loss: 0.0001708 valid loss: 0.2454076 P@1: 0.68000 P@3: 0.52333 P@5: 0.42800 N@3: 0.56672 N@5: 0.57213 early stop: 1\n",
            "\u001b[32m[I 210629 05:22:27 models:110]\u001b[39m 364 1024 train loss: 0.0001227 valid loss: 0.2458392 P@1: 0.68000 P@3: 0.52333 P@5: 0.42800 N@3: 0.56672 N@5: 0.57213 early stop: 2\n",
            "\u001b[32m[I 210629 05:22:30 models:110]\u001b[39m 367 512 train loss: 0.0001310 valid loss: 0.2462720 P@1: 0.68000 P@3: 0.52333 P@5: 0.43000 N@3: 0.56672 N@5: 0.57394 early stop: 0\n",
            "\u001b[32m[I 210629 05:22:32 models:110]\u001b[39m 369 1024 train loss: 0.0001238 valid loss: 0.2467228 P@1: 0.68000 P@3: 0.52667 P@5: 0.43000 N@3: 0.56968 N@5: 0.57481 early stop: 0\n",
            "\u001b[32m[I 210629 05:22:34 models:110]\u001b[39m 372 512 train loss: 0.0001655 valid loss: 0.2471855 P@1: 0.68000 P@3: 0.52667 P@5: 0.43200 N@3: 0.57029 N@5: 0.57656 early stop: 0\n",
            "\u001b[32m[I 210629 05:22:37 models:110]\u001b[39m 374 1024 train loss: 0.0001100 valid loss: 0.2476419 P@1: 0.68000 P@3: 0.53000 P@5: 0.43200 N@3: 0.57264 N@5: 0.57680 early stop: 0\n",
            "\u001b[32m[I 210629 05:22:39 models:110]\u001b[39m 377 512 train loss: 0.0001428 valid loss: 0.2480904 P@1: 0.68000 P@3: 0.53000 P@5: 0.43200 N@3: 0.57264 N@5: 0.57680 early stop: 1\n",
            "\u001b[32m[I 210629 05:22:41 models:110]\u001b[39m 379 1024 train loss: 0.0001388 valid loss: 0.2485490 P@1: 0.68000 P@3: 0.53000 P@5: 0.43200 N@3: 0.57264 N@5: 0.57695 early stop: 0\n",
            "\u001b[32m[I 210629 05:22:44 models:110]\u001b[39m 382 512 train loss: 0.0001146 valid loss: 0.2490156 P@1: 0.68000 P@3: 0.53000 P@5: 0.43200 N@3: 0.57264 N@5: 0.57695 early stop: 1\n",
            "\u001b[32m[I 210629 05:22:46 models:110]\u001b[39m 384 1024 train loss: 0.0001305 valid loss: 0.2494733 P@1: 0.68000 P@3: 0.53000 P@5: 0.43200 N@3: 0.57264 N@5: 0.57695 early stop: 2\n",
            "\u001b[32m[I 210629 05:22:48 models:110]\u001b[39m 387 512 train loss: 0.0001317 valid loss: 0.2499367 P@1: 0.68000 P@3: 0.53333 P@5: 0.43200 N@3: 0.57499 N@5: 0.57718 early stop: 0\n",
            "\u001b[32m[I 210629 05:22:50 models:110]\u001b[39m 389 1024 train loss: 0.0001090 valid loss: 0.2504083 P@1: 0.68000 P@3: 0.53667 P@5: 0.43200 N@3: 0.57733 N@5: 0.57751 early stop: 0\n",
            "\u001b[33m[W 210629 05:22:51 models:137]\u001b[39m Clipping gradients with total norm 0.07641 and max norm 0.00681\n",
            "\u001b[32m[I 210629 05:22:53 models:110]\u001b[39m 392 512 train loss: 0.0003159 valid loss: 0.2508801 P@1: 0.68000 P@3: 0.53667 P@5: 0.43200 N@3: 0.57733 N@5: 0.57751 early stop: 1\n",
            "\u001b[32m[I 210629 05:22:55 models:110]\u001b[39m 394 1024 train loss: 0.0001666 valid loss: 0.2513615 P@1: 0.68000 P@3: 0.53667 P@5: 0.43200 N@3: 0.57733 N@5: 0.57751 early stop: 2\n",
            "\u001b[33m[W 210629 05:22:57 models:137]\u001b[39m Clipping gradients with total norm 0.0962 and max norm 0.01589\n",
            "\u001b[32m[I 210629 05:22:58 models:110]\u001b[39m 397 512 train loss: 0.0003236 valid loss: 0.2518259 P@1: 0.68000 P@3: 0.53667 P@5: 0.43200 N@3: 0.57733 N@5: 0.57751 early stop: 3\n",
            "\u001b[32m[I 210629 05:23:00 models:110]\u001b[39m 399 1024 train loss: 0.0003576 valid loss: 0.2523088 P@1: 0.68000 P@3: 0.53667 P@5: 0.43200 N@3: 0.57653 N@5: 0.57655 early stop: 4\n",
            "\u001b[32m[I 210629 05:23:02 models:110]\u001b[39m 402 512 train loss: 0.0002166 valid loss: 0.2527964 P@1: 0.68000 P@3: 0.53667 P@5: 0.43200 N@3: 0.57653 N@5: 0.57655 early stop: 5\n",
            "\u001b[32m[I 210629 05:23:04 models:110]\u001b[39m 404 1024 train loss: 0.0001269 valid loss: 0.2532648 P@1: 0.68000 P@3: 0.53667 P@5: 0.43200 N@3: 0.57653 N@5: 0.57655 early stop: 6\n",
            "\u001b[32m[I 210629 05:23:07 models:110]\u001b[39m 407 512 train loss: 0.0001716 valid loss: 0.2537242 P@1: 0.68000 P@3: 0.53667 P@5: 0.43000 N@3: 0.57714 N@5: 0.57586 early stop: 7\n",
            "\u001b[32m[I 210629 05:23:09 models:110]\u001b[39m 409 1024 train loss: 0.0001485 valid loss: 0.2541878 P@1: 0.68000 P@3: 0.53667 P@5: 0.43000 N@3: 0.57714 N@5: 0.57586 early stop: 8\n",
            "\u001b[32m[I 210629 05:23:11 models:110]\u001b[39m 412 512 train loss: 0.0001546 valid loss: 0.2546671 P@1: 0.68000 P@3: 0.53667 P@5: 0.43000 N@3: 0.57714 N@5: 0.57586 early stop: 9\n",
            "\u001b[32m[I 210629 05:23:13 models:110]\u001b[39m 414 1024 train loss: 0.0001071 valid loss: 0.2551476 P@1: 0.68000 P@3: 0.53667 P@5: 0.43000 N@3: 0.57714 N@5: 0.57586 early stop: 10\n",
            "\u001b[32m[I 210629 05:23:16 models:110]\u001b[39m 417 512 train loss: 0.0001103 valid loss: 0.2556243 P@1: 0.68000 P@3: 0.53667 P@5: 0.43000 N@3: 0.57714 N@5: 0.57586 early stop: 11\n",
            "\u001b[33m[W 210629 05:23:17 models:137]\u001b[39m Clipping gradients with total norm 0.05488 and max norm 0.00838\n",
            "\u001b[32m[I 210629 05:23:18 models:110]\u001b[39m 419 1024 train loss: 0.0002804 valid loss: 0.2561105 P@1: 0.68000 P@3: 0.53667 P@5: 0.43000 N@3: 0.57714 N@5: 0.57586 early stop: 12\n",
            "\u001b[33m[W 210629 05:23:19 models:137]\u001b[39m Clipping gradients with total norm 0.07909 and max norm 0.01303\n",
            "\u001b[32m[I 210629 05:23:20 models:110]\u001b[39m 422 512 train loss: 0.0006929 valid loss: 0.2565904 P@1: 0.68000 P@3: 0.53667 P@5: 0.43000 N@3: 0.57714 N@5: 0.57586 early stop: 13\n",
            "\u001b[32m[I 210629 05:23:22 models:110]\u001b[39m 424 1024 train loss: 0.0002502 valid loss: 0.2571001 P@1: 0.68000 P@3: 0.53667 P@5: 0.43000 N@3: 0.57714 N@5: 0.57586 early stop: 14\n",
            "\u001b[32m[I 210629 05:23:25 models:110]\u001b[39m 427 512 train loss: 0.0002635 valid loss: 0.2575851 P@1: 0.68000 P@3: 0.53667 P@5: 0.43000 N@3: 0.57714 N@5: 0.57586 early stop: 15\n",
            "\u001b[32m[I 210629 05:23:27 models:110]\u001b[39m 429 1024 train loss: 0.0001278 valid loss: 0.2580644 P@1: 0.68000 P@3: 0.53667 P@5: 0.43000 N@3: 0.57714 N@5: 0.57586 early stop: 16\n",
            "\u001b[32m[I 210629 05:23:29 models:110]\u001b[39m 432 512 train loss: 0.0001495 valid loss: 0.2585570 P@1: 0.68000 P@3: 0.53667 P@5: 0.43000 N@3: 0.57714 N@5: 0.57586 early stop: 17\n",
            "\u001b[33m[W 210629 05:23:30 models:137]\u001b[39m Clipping gradients with total norm 0.07403 and max norm 0.00835\n",
            "\u001b[32m[I 210629 05:23:32 models:110]\u001b[39m 434 1024 train loss: 0.0003772 valid loss: 0.2590557 P@1: 0.68000 P@3: 0.53667 P@5: 0.43000 N@3: 0.57714 N@5: 0.57586 early stop: 18\n",
            "\u001b[32m[I 210629 05:23:34 models:110]\u001b[39m 437 512 train loss: 0.0001432 valid loss: 0.2595430 P@1: 0.68000 P@3: 0.53667 P@5: 0.43000 N@3: 0.57714 N@5: 0.57586 early stop: 19\n",
            "\u001b[33m[W 210629 05:23:34 models:137]\u001b[39m Clipping gradients with total norm 0.04621 and max norm 0.00923\n",
            "\u001b[32m[I 210629 05:23:36 models:110]\u001b[39m 439 1024 train loss: 0.0003282 valid loss: 0.2600124 P@1: 0.68000 P@3: 0.53333 P@5: 0.43000 N@3: 0.57480 N@5: 0.57562 early stop: 20\n",
            "\u001b[32m[I 210629 05:23:39 models:110]\u001b[39m 442 512 train loss: 0.0002387 valid loss: 0.2605034 P@1: 0.68000 P@3: 0.52667 P@5: 0.43000 N@3: 0.57010 N@5: 0.57497 early stop: 21\n",
            "\u001b[32m[I 210629 05:23:41 models:110]\u001b[39m 444 1024 train loss: 0.0001175 valid loss: 0.2609796 P@1: 0.68000 P@3: 0.52667 P@5: 0.43000 N@3: 0.57010 N@5: 0.57497 early stop: 22\n",
            "\u001b[32m[I 210629 05:23:44 models:110]\u001b[39m 447 512 train loss: 0.0001524 valid loss: 0.2614497 P@1: 0.68000 P@3: 0.52667 P@5: 0.43000 N@3: 0.57010 N@5: 0.57497 early stop: 23\n",
            "\u001b[32m[I 210629 05:23:46 models:110]\u001b[39m 449 1024 train loss: 0.0001009 valid loss: 0.2619207 P@1: 0.68000 P@3: 0.52667 P@5: 0.43000 N@3: 0.57010 N@5: 0.57518 early stop: 24\n",
            "\u001b[32m[I 210629 05:23:49 models:110]\u001b[39m 452 512 train loss: 0.0001331 valid loss: 0.2623963 P@1: 0.68000 P@3: 0.53333 P@5: 0.43000 N@3: 0.57480 N@5: 0.57583 early stop: 25\n",
            "\u001b[32m[I 210629 05:23:52 models:110]\u001b[39m 454 1024 train loss: 0.0000927 valid loss: 0.2628769 P@1: 0.68000 P@3: 0.53333 P@5: 0.43000 N@3: 0.57480 N@5: 0.57603 early stop: 26\n",
            "\u001b[32m[I 210629 05:23:55 models:110]\u001b[39m 457 512 train loss: 0.0000997 valid loss: 0.2633577 P@1: 0.68000 P@3: 0.53333 P@5: 0.43000 N@3: 0.57418 N@5: 0.57542 early stop: 27\n",
            "\u001b[32m[I 210629 05:23:57 models:110]\u001b[39m 459 1024 train loss: 0.0001293 valid loss: 0.2638435 P@1: 0.68000 P@3: 0.53667 P@5: 0.43000 N@3: 0.57653 N@5: 0.57569 early stop: 28\n",
            "\u001b[32m[I 210629 05:23:59 models:110]\u001b[39m 462 512 train loss: 0.0001450 valid loss: 0.2643235 P@1: 0.68000 P@3: 0.53667 P@5: 0.43000 N@3: 0.57653 N@5: 0.57569 early stop: 29\n",
            "\u001b[32m[I 210629 05:24:01 models:110]\u001b[39m 464 1024 train loss: 0.0000931 valid loss: 0.2648014 P@1: 0.68000 P@3: 0.53667 P@5: 0.43000 N@3: 0.57653 N@5: 0.57569 early stop: 30\n",
            "\u001b[33m[W 210629 05:24:03 models:137]\u001b[39m Clipping gradients with total norm 0.03586 and max norm 0.00451\n",
            "\u001b[32m[I 210629 05:24:04 models:110]\u001b[39m 467 512 train loss: 0.0001901 valid loss: 0.2652821 P@1: 0.68000 P@3: 0.53667 P@5: 0.43000 N@3: 0.57653 N@5: 0.57569 early stop: 31\n",
            "\u001b[32m[I 210629 05:24:06 models:110]\u001b[39m 469 1024 train loss: 0.0001277 valid loss: 0.2657667 P@1: 0.68000 P@3: 0.53667 P@5: 0.43000 N@3: 0.57653 N@5: 0.57569 early stop: 32\n",
            "\u001b[32m[I 210629 05:24:08 models:110]\u001b[39m 472 512 train loss: 0.0000944 valid loss: 0.2662531 P@1: 0.68000 P@3: 0.53667 P@5: 0.43000 N@3: 0.57653 N@5: 0.57569 early stop: 33\n",
            "\u001b[32m[I 210629 05:24:10 models:110]\u001b[39m 474 1024 train loss: 0.0001267 valid loss: 0.2667377 P@1: 0.68000 P@3: 0.53667 P@5: 0.43200 N@3: 0.57653 N@5: 0.57700 early stop: 34\n",
            "\u001b[32m[I 210629 05:24:13 models:110]\u001b[39m 477 512 train loss: 0.0001035 valid loss: 0.2672217 P@1: 0.68000 P@3: 0.53667 P@5: 0.43200 N@3: 0.57653 N@5: 0.57700 early stop: 35\n",
            "\u001b[32m[I 210629 05:24:15 models:110]\u001b[39m 479 1024 train loss: 0.0001078 valid loss: 0.2677087 P@1: 0.68000 P@3: 0.53667 P@5: 0.43200 N@3: 0.57653 N@5: 0.57700 early stop: 36\n",
            "\u001b[32m[I 210629 05:24:17 models:110]\u001b[39m 482 512 train loss: 0.0000958 valid loss: 0.2681968 P@1: 0.68000 P@3: 0.53667 P@5: 0.43200 N@3: 0.57714 N@5: 0.57762 early stop: 0\n",
            "\u001b[32m[I 210629 05:24:19 models:110]\u001b[39m 484 1024 train loss: 0.0001084 valid loss: 0.2686865 P@1: 0.68000 P@3: 0.54000 P@5: 0.43000 N@3: 0.57949 N@5: 0.57663 early stop: 1\n",
            "\u001b[32m[I 210629 05:24:22 models:110]\u001b[39m 487 512 train loss: 0.0001022 valid loss: 0.2691747 P@1: 0.68000 P@3: 0.54000 P@5: 0.43000 N@3: 0.57949 N@5: 0.57663 early stop: 2\n",
            "\u001b[33m[W 210629 05:24:22 models:137]\u001b[39m Clipping gradients with total norm 0.04441 and max norm 0.00679\n",
            "\u001b[33m[W 210629 05:24:24 models:137]\u001b[39m Clipping gradients with total norm 0.08826 and max norm 0.00911\n",
            "\u001b[32m[I 210629 05:24:24 models:110]\u001b[39m 489 1024 train loss: 0.0009408 valid loss: 0.2696503 P@1: 0.68000 P@3: 0.54000 P@5: 0.43000 N@3: 0.57949 N@5: 0.57663 early stop: 3\n",
            "\u001b[32m[I 210629 05:24:26 models:110]\u001b[39m 492 512 train loss: 0.0004385 valid loss: 0.2701584 P@1: 0.68000 P@3: 0.54000 P@5: 0.42800 N@3: 0.57949 N@5: 0.57481 early stop: 4\n",
            "\u001b[32m[I 210629 05:24:29 models:110]\u001b[39m 494 1024 train loss: 0.0002693 valid loss: 0.2706506 P@1: 0.68000 P@3: 0.54000 P@5: 0.42600 N@3: 0.57949 N@5: 0.57350 early stop: 5\n",
            "\u001b[32m[I 210629 05:24:31 models:110]\u001b[39m 497 512 train loss: 0.0002596 valid loss: 0.2711403 P@1: 0.68000 P@3: 0.54000 P@5: 0.42600 N@3: 0.57949 N@5: 0.57330 early stop: 6\n",
            "\u001b[32m[I 210629 05:24:33 models:110]\u001b[39m 499 1024 train loss: 0.0001133 valid loss: 0.2716185 P@1: 0.68000 P@3: 0.53667 P@5: 0.43000 N@3: 0.57642 N@5: 0.57650 early stop: 7\n",
            "\u001b[32m[I 210629 05:24:36 models:110]\u001b[39m 502 512 train loss: 0.0001458 valid loss: 0.2720982 P@1: 0.67000 P@3: 0.53667 P@5: 0.43000 N@3: 0.57469 N@5: 0.57546 early stop: 8\n",
            "\u001b[32m[I 210629 05:24:38 models:110]\u001b[39m 504 1024 train loss: 0.0001121 valid loss: 0.2725797 P@1: 0.67000 P@3: 0.53667 P@5: 0.43000 N@3: 0.57469 N@5: 0.57546 early stop: 9\n",
            "\u001b[32m[I 210629 05:24:40 models:110]\u001b[39m 507 512 train loss: 0.0001397 valid loss: 0.2730546 P@1: 0.67000 P@3: 0.53667 P@5: 0.43000 N@3: 0.57469 N@5: 0.57546 early stop: 10\n",
            "\u001b[33m[W 210629 05:24:42 models:137]\u001b[39m Clipping gradients with total norm 0.03498 and max norm 0.00544\n",
            "\u001b[32m[I 210629 05:24:42 models:110]\u001b[39m 509 1024 train loss: 0.0002302 valid loss: 0.2735311 P@1: 0.67000 P@3: 0.53667 P@5: 0.43000 N@3: 0.57469 N@5: 0.57546 early stop: 11\n",
            "\u001b[32m[I 210629 05:24:45 models:110]\u001b[39m 512 512 train loss: 0.0001648 valid loss: 0.2740044 P@1: 0.67000 P@3: 0.53667 P@5: 0.42800 N@3: 0.57469 N@5: 0.57364 early stop: 12\n",
            "\u001b[32m[I 210629 05:24:47 models:110]\u001b[39m 514 1024 train loss: 0.0000968 valid loss: 0.2744688 P@1: 0.67000 P@3: 0.53667 P@5: 0.42800 N@3: 0.57469 N@5: 0.57364 early stop: 13\n",
            "\u001b[32m[I 210629 05:24:49 models:110]\u001b[39m 517 512 train loss: 0.0002330 valid loss: 0.2749346 P@1: 0.67000 P@3: 0.53667 P@5: 0.42800 N@3: 0.57469 N@5: 0.57364 early stop: 14\n",
            "\u001b[32m[I 210629 05:24:51 models:110]\u001b[39m 519 1024 train loss: 0.0001306 valid loss: 0.2754039 P@1: 0.67000 P@3: 0.53667 P@5: 0.42800 N@3: 0.57469 N@5: 0.57364 early stop: 15\n",
            "\u001b[33m[W 210629 05:24:52 models:137]\u001b[39m Clipping gradients with total norm 0.06323 and max norm 0.00766\n",
            "\u001b[32m[I 210629 05:24:54 models:110]\u001b[39m 522 512 train loss: 0.0005658 valid loss: 0.2758841 P@1: 0.67000 P@3: 0.53667 P@5: 0.42800 N@3: 0.57469 N@5: 0.57364 early stop: 16\n",
            "\u001b[32m[I 210629 05:24:56 models:110]\u001b[39m 524 1024 train loss: 0.0001221 valid loss: 0.2763664 P@1: 0.67000 P@3: 0.53667 P@5: 0.42800 N@3: 0.57469 N@5: 0.57364 early stop: 17\n",
            "\u001b[32m[I 210629 05:24:58 models:110]\u001b[39m 527 512 train loss: 0.0001662 valid loss: 0.2768244 P@1: 0.67000 P@3: 0.54000 P@5: 0.42800 N@3: 0.57704 N@5: 0.57397 early stop: 18\n",
            "\u001b[33m[W 210629 05:25:00 models:137]\u001b[39m Clipping gradients with total norm 0.02603 and max norm 0.00454\n",
            "\u001b[32m[I 210629 05:25:00 models:110]\u001b[39m 529 1024 train loss: 0.0001128 valid loss: 0.2772752 P@1: 0.67000 P@3: 0.54000 P@5: 0.42800 N@3: 0.57765 N@5: 0.57441 early stop: 19\n",
            "\u001b[32m[I 210629 05:25:03 models:110]\u001b[39m 532 512 train loss: 0.0001121 valid loss: 0.2777252 P@1: 0.67000 P@3: 0.54000 P@5: 0.42800 N@3: 0.57765 N@5: 0.57441 early stop: 20\n",
            "\u001b[32m[I 210629 05:25:05 models:110]\u001b[39m 534 1024 train loss: 0.0001230 valid loss: 0.2781814 P@1: 0.67000 P@3: 0.54333 P@5: 0.42800 N@3: 0.58000 N@5: 0.57465 early stop: 21\n",
            "\u001b[32m[I 210629 05:25:07 models:110]\u001b[39m 537 512 train loss: 0.0001329 valid loss: 0.2786386 P@1: 0.67000 P@3: 0.54333 P@5: 0.42800 N@3: 0.58000 N@5: 0.57465 early stop: 22\n",
            "\u001b[32m[I 210629 05:25:09 models:110]\u001b[39m 539 1024 train loss: 0.0000855 valid loss: 0.2790965 P@1: 0.67000 P@3: 0.54333 P@5: 0.42800 N@3: 0.58000 N@5: 0.57465 early stop: 23\n",
            "\u001b[32m[I 210629 05:25:12 models:110]\u001b[39m 542 512 train loss: 0.0001152 valid loss: 0.2795518 P@1: 0.67000 P@3: 0.54333 P@5: 0.42600 N@3: 0.58000 N@5: 0.57333 early stop: 24\n",
            "\u001b[32m[I 210629 05:25:14 models:110]\u001b[39m 544 1024 train loss: 0.0002096 valid loss: 0.2800083 P@1: 0.67000 P@3: 0.54333 P@5: 0.42600 N@3: 0.58000 N@5: 0.57333 early stop: 25\n",
            "\u001b[32m[I 210629 05:25:16 models:110]\u001b[39m 547 512 train loss: 0.0001621 valid loss: 0.2804636 P@1: 0.67000 P@3: 0.54333 P@5: 0.42600 N@3: 0.58000 N@5: 0.57333 early stop: 26\n",
            "\u001b[32m[I 210629 05:25:18 models:110]\u001b[39m 549 1024 train loss: 0.0001119 valid loss: 0.2809250 P@1: 0.67000 P@3: 0.54333 P@5: 0.42600 N@3: 0.58000 N@5: 0.57333 early stop: 27\n",
            "\u001b[32m[I 210629 05:25:21 models:110]\u001b[39m 552 512 train loss: 0.0001360 valid loss: 0.2813962 P@1: 0.67000 P@3: 0.54667 P@5: 0.42600 N@3: 0.58235 N@5: 0.57360 early stop: 28\n",
            "\u001b[32m[I 210629 05:25:23 models:110]\u001b[39m 554 1024 train loss: 0.0001058 valid loss: 0.2818613 P@1: 0.67000 P@3: 0.54667 P@5: 0.42600 N@3: 0.58235 N@5: 0.57360 early stop: 29\n",
            "\u001b[33m[W 210629 05:25:24 models:137]\u001b[39m Clipping gradients with total norm 0.02716 and max norm 0.0042\n",
            "\u001b[32m[I 210629 05:25:25 models:110]\u001b[39m 557 512 train loss: 0.0001941 valid loss: 0.2823211 P@1: 0.67000 P@3: 0.54667 P@5: 0.42600 N@3: 0.58235 N@5: 0.57360 early stop: 30\n",
            "\u001b[32m[I 210629 05:25:27 models:110]\u001b[39m 559 1024 train loss: 0.0001350 valid loss: 0.2827816 P@1: 0.67000 P@3: 0.54667 P@5: 0.42600 N@3: 0.58235 N@5: 0.57360 early stop: 31\n",
            "\u001b[32m[I 210629 05:25:30 models:110]\u001b[39m 562 512 train loss: 0.0001279 valid loss: 0.2832430 P@1: 0.67000 P@3: 0.54667 P@5: 0.42600 N@3: 0.58235 N@5: 0.57375 early stop: 32\n",
            "\u001b[32m[I 210629 05:25:32 models:110]\u001b[39m 564 1024 train loss: 0.0000864 valid loss: 0.2837028 P@1: 0.67000 P@3: 0.54667 P@5: 0.42600 N@3: 0.58235 N@5: 0.57375 early stop: 33\n",
            "\u001b[33m[W 210629 05:25:33 models:137]\u001b[39m Clipping gradients with total norm 0.05137 and max norm 0.00511\n",
            "\u001b[32m[I 210629 05:25:34 models:110]\u001b[39m 567 512 train loss: 0.0001549 valid loss: 0.2841594 P@1: 0.67000 P@3: 0.54667 P@5: 0.42600 N@3: 0.58235 N@5: 0.57375 early stop: 34\n",
            "\u001b[32m[I 210629 05:25:37 models:110]\u001b[39m 569 1024 train loss: 0.0002472 valid loss: 0.2845958 P@1: 0.67000 P@3: 0.54667 P@5: 0.42600 N@3: 0.58235 N@5: 0.57375 early stop: 35\n",
            "\u001b[32m[I 210629 05:25:39 models:110]\u001b[39m 572 512 train loss: 0.0001749 valid loss: 0.2850538 P@1: 0.67000 P@3: 0.54667 P@5: 0.42600 N@3: 0.58235 N@5: 0.57375 early stop: 36\n",
            "\u001b[33m[W 210629 05:25:39 models:137]\u001b[39m Clipping gradients with total norm 0.05544 and max norm 0.00537\n",
            "\u001b[32m[I 210629 05:25:41 models:110]\u001b[39m 574 1024 train loss: 0.0008243 valid loss: 0.2855357 P@1: 0.67000 P@3: 0.54667 P@5: 0.42600 N@3: 0.58235 N@5: 0.57375 early stop: 37\n",
            "\u001b[32m[I 210629 05:25:44 models:110]\u001b[39m 577 512 train loss: 0.0002174 valid loss: 0.2859986 P@1: 0.67000 P@3: 0.54667 P@5: 0.42600 N@3: 0.58235 N@5: 0.57375 early stop: 38\n",
            "\u001b[32m[I 210629 05:25:46 models:110]\u001b[39m 579 1024 train loss: 0.0002332 valid loss: 0.2864465 P@1: 0.67000 P@3: 0.54667 P@5: 0.42600 N@3: 0.58235 N@5: 0.57375 early stop: 39\n",
            "\u001b[33m[W 210629 05:25:48 models:137]\u001b[39m Clipping gradients with total norm 0.08245 and max norm 0.01482\n",
            "\u001b[32m[I 210629 05:25:48 models:110]\u001b[39m 582 512 train loss: 0.0002314 valid loss: 0.2869062 P@1: 0.67000 P@3: 0.54667 P@5: 0.42600 N@3: 0.58235 N@5: 0.57375 early stop: 40\n",
            "\u001b[32m[I 210629 05:25:50 models:110]\u001b[39m 584 1024 train loss: 0.0001880 valid loss: 0.2873072 P@1: 0.67000 P@3: 0.54667 P@5: 0.42400 N@3: 0.58235 N@5: 0.57244 early stop: 41\n",
            "\u001b[32m[I 210629 05:25:53 models:110]\u001b[39m 587 512 train loss: 0.0001662 valid loss: 0.2877443 P@1: 0.67000 P@3: 0.54667 P@5: 0.42400 N@3: 0.58235 N@5: 0.57244 early stop: 42\n",
            "\u001b[32m[I 210629 05:25:55 models:110]\u001b[39m 589 1024 train loss: 0.0000984 valid loss: 0.2881788 P@1: 0.67000 P@3: 0.54667 P@5: 0.42400 N@3: 0.58235 N@5: 0.57244 early stop: 43\n",
            "\u001b[32m[I 210629 05:25:57 models:110]\u001b[39m 592 512 train loss: 0.0001187 valid loss: 0.2886065 P@1: 0.67000 P@3: 0.54667 P@5: 0.42400 N@3: 0.58235 N@5: 0.57244 early stop: 44\n",
            "\u001b[33m[W 210629 05:25:57 models:137]\u001b[39m Clipping gradients with total norm 0.05465 and max norm 0.00848\n",
            "\u001b[32m[I 210629 05:25:59 models:110]\u001b[39m 594 1024 train loss: 0.0004182 valid loss: 0.2890362 P@1: 0.67000 P@3: 0.54667 P@5: 0.42400 N@3: 0.58235 N@5: 0.57244 early stop: 45\n",
            "\u001b[32m[I 210629 05:26:02 models:110]\u001b[39m 597 512 train loss: 0.0002067 valid loss: 0.2894639 P@1: 0.67000 P@3: 0.55000 P@5: 0.42400 N@3: 0.58469 N@5: 0.57268 early stop: 46\n",
            "\u001b[32m[I 210629 05:26:04 models:110]\u001b[39m 599 1024 train loss: 0.0001146 valid loss: 0.2899243 P@1: 0.67000 P@3: 0.55000 P@5: 0.42600 N@3: 0.58469 N@5: 0.57399 early stop: 47\n",
            "\u001b[32m[I 210629 05:26:06 models:110]\u001b[39m 602 512 train loss: 0.0001242 valid loss: 0.2903879 P@1: 0.67000 P@3: 0.55000 P@5: 0.42600 N@3: 0.58469 N@5: 0.57399 early stop: 48\n",
            "\u001b[32m[I 210629 05:26:08 models:110]\u001b[39m 604 1024 train loss: 0.0001037 valid loss: 0.2908285 P@1: 0.67000 P@3: 0.55000 P@5: 0.42600 N@3: 0.58469 N@5: 0.57399 early stop: 49\n",
            "\u001b[32m[I 210629 05:26:11 models:110]\u001b[39m 607 512 train loss: 0.0001126 valid loss: 0.2912556 P@1: 0.67000 P@3: 0.55000 P@5: 0.42600 N@3: 0.58469 N@5: 0.57399 early stop: 50\n",
            "\u001b[32m[I 210629 05:26:13 main:76]\u001b[39m Finish Training\n",
            "\u001b[32m[I 210629 05:26:15 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210629 05:26:15 main:79]\u001b[39m Loading Test Set\n",
            "\u001b[32m[I 210629 05:26:15 main:83]\u001b[39m Size of Test Set: 100\n",
            "\u001b[32m[I 210629 05:26:15 main:85]\u001b[39m Predicting\n",
            "\u001b[32m[I 210629 05:26:18 main:91]\u001b[39m Finish Predicting\n",
            "Precision@1,3,5: 0.57 0.4866666666666667 0.366\n",
            "nDCG@1,3,5: 0.57 0.5103147505625059 0.493606439826399\n",
            "\n",
            "NO MAG NO MESH, skip=300\n",
            "\n",
            "/content/drive/Shareddrives/NASA/MATCH/PeTaL\n",
            "130\n",
            "/content/drive/Shareddrives/NASA/MATCH\n",
            "    800  288986 4967896 PeTaL/train.json\n",
            "\u001b[32m[I 210629 05:26:21 preprocess:28]\u001b[39m Vocab Size: 26834\n",
            "\u001b[32m[I 210629 05:26:21 preprocess:30]\u001b[39m Getting Dataset: PeTaL/train_texts.txt Max Length: 500\n",
            "\u001b[32m[I 210629 05:26:21 preprocess:32]\u001b[39m Size of Samples: 900\n",
            "\u001b[32m[I 210629 05:26:22 preprocess:28]\u001b[39m Vocab Size: 26834\n",
            "\u001b[32m[I 210629 05:26:22 preprocess:30]\u001b[39m Getting Dataset: PeTaL/test_texts.txt Max Length: 500\n",
            "\u001b[32m[I 210629 05:26:23 preprocess:32]\u001b[39m Size of Samples: 100\n",
            "\u001b[32m[I 210629 05:26:24 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210629 05:26:24 main:35]\u001b[39m Loading Training and Validation Set\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['absorb_and/or_filter_solids', 'chemically_break_down_inorganic_compounds', 'detox/purify', 'protect_from_fire', 'protect_from_gases', 'send_vibratory_signals'] will be ignored\n",
            "  .format(sorted(unknown, key=str)))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['detox/purify', 'protect_from_gases'] will be ignored\n",
            "  .format(sorted(unknown, key=str)))\n",
            "\u001b[32m[I 210629 05:26:24 main:47]\u001b[39m Number of Labels: 124\n",
            "\u001b[32m[I 210629 05:26:24 main:48]\u001b[39m Size of Training Set: 800\n",
            "\u001b[32m[I 210629 05:26:24 main:49]\u001b[39m Size of Validation Set: 100\n",
            "\u001b[32m[I 210629 05:26:24 main:66]\u001b[39m Number of Edges: 101\n",
            "\u001b[32m[I 210629 05:26:24 main:68]\u001b[39m Training\n",
            "\u001b[32m[I 210629 05:26:30 models:110]\u001b[39m 2 512 train loss: 0.2651234 valid loss: 0.1549987 P@1: 0.37000 P@3: 0.14667 P@5: 0.13200 N@3: 0.19067 N@5: 0.19287 early stop: 0\n",
            "\u001b[32m[I 210629 05:26:31 models:142]\u001b[39m SWA Initializing\n",
            "\u001b[32m[I 210629 05:26:32 models:110]\u001b[39m 4 1024 train loss: 0.1438120 valid loss: 0.1472422 P@1: 0.37000 P@3: 0.26667 P@5: 0.22400 N@3: 0.28927 N@5: 0.28419 early stop: 0\n",
            "\u001b[32m[I 210629 05:26:35 models:110]\u001b[39m 7 512 train loss: 0.1418719 valid loss: 0.1465545 P@1: 0.37000 P@3: 0.26667 P@5: 0.22800 N@3: 0.28927 N@5: 0.29144 early stop: 0\n",
            "\u001b[32m[I 210629 05:26:37 models:110]\u001b[39m 9 1024 train loss: 0.1342046 valid loss: 0.1462865 P@1: 0.37000 P@3: 0.26667 P@5: 0.22400 N@3: 0.28989 N@5: 0.28890 early stop: 1\n",
            "\u001b[32m[I 210629 05:26:39 models:110]\u001b[39m 12 512 train loss: 0.1317135 valid loss: 0.1455954 P@1: 0.37000 P@3: 0.27333 P@5: 0.23200 N@3: 0.29724 N@5: 0.29665 early stop: 0\n",
            "\u001b[32m[I 210629 05:26:41 models:110]\u001b[39m 14 1024 train loss: 0.1216019 valid loss: 0.1452658 P@1: 0.37000 P@3: 0.26333 P@5: 0.25000 N@3: 0.28939 N@5: 0.30914 early stop: 0\n",
            "\u001b[32m[I 210629 05:26:44 models:110]\u001b[39m 17 512 train loss: 0.1041666 valid loss: 0.1452376 P@1: 0.38000 P@3: 0.28667 P@5: 0.26200 N@3: 0.30754 N@5: 0.31714 early stop: 0\n",
            "\u001b[32m[I 210629 05:26:46 models:110]\u001b[39m 19 1024 train loss: 0.0897294 valid loss: 0.1450362 P@1: 0.38000 P@3: 0.29000 P@5: 0.24800 N@3: 0.30866 N@5: 0.30557 early stop: 1\n",
            "\u001b[32m[I 210629 05:26:49 models:110]\u001b[39m 22 512 train loss: 0.0704643 valid loss: 0.1450591 P@1: 0.43000 P@3: 0.30667 P@5: 0.25000 N@3: 0.33274 N@5: 0.31997 early stop: 0\n",
            "\u001b[32m[I 210629 05:26:51 models:110]\u001b[39m 24 1024 train loss: 0.0601265 valid loss: 0.1447686 P@1: 0.45000 P@3: 0.30333 P@5: 0.24600 N@3: 0.33906 N@5: 0.32278 early stop: 0\n",
            "\u001b[32m[I 210629 05:26:53 models:110]\u001b[39m 27 512 train loss: 0.0483143 valid loss: 0.1445751 P@1: 0.50000 P@3: 0.31333 P@5: 0.25600 N@3: 0.35775 N@5: 0.33929 early stop: 0\n",
            "\u001b[32m[I 210629 05:26:55 models:110]\u001b[39m 29 1024 train loss: 0.0375767 valid loss: 0.1446686 P@1: 0.51000 P@3: 0.31667 P@5: 0.28000 N@3: 0.35999 N@5: 0.36360 early stop: 0\n",
            "\u001b[32m[I 210629 05:26:58 models:110]\u001b[39m 32 512 train loss: 0.0315069 valid loss: 0.1449879 P@1: 0.48000 P@3: 0.35333 P@5: 0.28000 N@3: 0.38458 N@5: 0.36581 early stop: 0\n",
            "\u001b[32m[I 210629 05:27:00 models:110]\u001b[39m 34 1024 train loss: 0.0298323 valid loss: 0.1455675 P@1: 0.45000 P@3: 0.35667 P@5: 0.27600 N@3: 0.38419 N@5: 0.36098 early stop: 1\n",
            "\u001b[32m[I 210629 05:27:03 models:110]\u001b[39m 37 512 train loss: 0.0277275 valid loss: 0.1456566 P@1: 0.47000 P@3: 0.36667 P@5: 0.29200 N@3: 0.39531 N@5: 0.38004 early stop: 0\n",
            "\u001b[32m[I 210629 05:27:05 models:110]\u001b[39m 39 1024 train loss: 0.0251981 valid loss: 0.1462219 P@1: 0.47000 P@3: 0.37000 P@5: 0.28800 N@3: 0.39827 N@5: 0.37746 early stop: 1\n",
            "\u001b[32m[I 210629 05:27:07 models:110]\u001b[39m 42 512 train loss: 0.0206543 valid loss: 0.1465765 P@1: 0.49000 P@3: 0.37000 P@5: 0.29600 N@3: 0.40419 N@5: 0.38834 early stop: 0\n",
            "\u001b[32m[I 210629 05:27:09 models:110]\u001b[39m 44 1024 train loss: 0.0173188 valid loss: 0.1472125 P@1: 0.49000 P@3: 0.38333 P@5: 0.30000 N@3: 0.41419 N@5: 0.39365 early stop: 0\n",
            "\u001b[32m[I 210629 05:27:12 models:110]\u001b[39m 47 512 train loss: 0.0156856 valid loss: 0.1477657 P@1: 0.53000 P@3: 0.39667 P@5: 0.30600 N@3: 0.43112 N@5: 0.40554 early stop: 0\n",
            "\u001b[32m[I 210629 05:27:14 models:110]\u001b[39m 49 1024 train loss: 0.0164997 valid loss: 0.1486615 P@1: 0.54000 P@3: 0.40000 P@5: 0.30000 N@3: 0.43642 N@5: 0.40368 early stop: 1\n",
            "\u001b[32m[I 210629 05:27:16 models:110]\u001b[39m 52 512 train loss: 0.0160218 valid loss: 0.1495549 P@1: 0.55000 P@3: 0.39667 P@5: 0.30400 N@3: 0.43765 N@5: 0.41087 early stop: 0\n",
            "\u001b[32m[I 210629 05:27:18 models:110]\u001b[39m 54 1024 train loss: 0.0127020 valid loss: 0.1505051 P@1: 0.56000 P@3: 0.40333 P@5: 0.30800 N@3: 0.44408 N@5: 0.41590 early stop: 0\n",
            "\u001b[32m[I 210629 05:27:21 models:110]\u001b[39m 57 512 train loss: 0.0103710 valid loss: 0.1514600 P@1: 0.56000 P@3: 0.41667 P@5: 0.32200 N@3: 0.45285 N@5: 0.42830 early stop: 0\n",
            "\u001b[32m[I 210629 05:27:23 models:110]\u001b[39m 59 1024 train loss: 0.0084449 valid loss: 0.1528318 P@1: 0.56000 P@3: 0.42000 P@5: 0.33600 N@3: 0.45520 N@5: 0.43950 early stop: 0\n",
            "\u001b[32m[I 210629 05:27:26 models:110]\u001b[39m 62 512 train loss: 0.0064905 valid loss: 0.1539931 P@1: 0.57000 P@3: 0.42333 P@5: 0.34400 N@3: 0.45989 N@5: 0.44998 early stop: 0\n",
            "\u001b[32m[I 210629 05:27:28 models:110]\u001b[39m 64 1024 train loss: 0.0048827 valid loss: 0.1554309 P@1: 0.55000 P@3: 0.42000 P@5: 0.34400 N@3: 0.45469 N@5: 0.44765 early stop: 1\n",
            "\u001b[32m[I 210629 05:27:30 models:110]\u001b[39m 67 512 train loss: 0.0038466 valid loss: 0.1571144 P@1: 0.55000 P@3: 0.42667 P@5: 0.34600 N@3: 0.45877 N@5: 0.44983 early stop: 2\n",
            "\u001b[32m[I 210629 05:27:32 models:110]\u001b[39m 69 1024 train loss: 0.0039515 valid loss: 0.1587607 P@1: 0.56000 P@3: 0.43333 P@5: 0.35400 N@3: 0.46581 N@5: 0.45780 early stop: 0\n",
            "\u001b[32m[I 210629 05:27:35 models:110]\u001b[39m 72 512 train loss: 0.0064180 valid loss: 0.1608589 P@1: 0.56000 P@3: 0.44000 P@5: 0.35800 N@3: 0.47050 N@5: 0.46063 early stop: 0\n",
            "\u001b[32m[I 210629 05:27:37 models:110]\u001b[39m 74 1024 train loss: 0.0072105 valid loss: 0.1625638 P@1: 0.55000 P@3: 0.44000 P@5: 0.35400 N@3: 0.46939 N@5: 0.45653 early stop: 1\n",
            "\u001b[32m[I 210629 05:27:39 models:110]\u001b[39m 77 512 train loss: 0.0065820 valid loss: 0.1643565 P@1: 0.55000 P@3: 0.44333 P@5: 0.35600 N@3: 0.47235 N@5: 0.45828 early stop: 2\n",
            "\u001b[32m[I 210629 05:27:41 models:110]\u001b[39m 79 1024 train loss: 0.0071864 valid loss: 0.1663148 P@1: 0.55000 P@3: 0.44333 P@5: 0.35800 N@3: 0.47296 N@5: 0.46052 early stop: 3\n",
            "\u001b[32m[I 210629 05:27:44 models:110]\u001b[39m 82 512 train loss: 0.0072628 valid loss: 0.1683835 P@1: 0.56000 P@3: 0.44333 P@5: 0.36000 N@3: 0.47531 N@5: 0.46337 early stop: 0\n",
            "\u001b[32m[I 210629 05:27:46 models:110]\u001b[39m 84 1024 train loss: 0.0077626 valid loss: 0.1700651 P@1: 0.55000 P@3: 0.44333 P@5: 0.36400 N@3: 0.47296 N@5: 0.46595 early stop: 0\n",
            "\u001b[32m[I 210629 05:27:49 models:110]\u001b[39m 87 512 train loss: 0.0080448 valid loss: 0.1716991 P@1: 0.56000 P@3: 0.43667 P@5: 0.36400 N@3: 0.47000 N@5: 0.46684 early stop: 0\n",
            "\u001b[32m[I 210629 05:27:51 models:110]\u001b[39m 89 1024 train loss: 0.0062844 valid loss: 0.1735302 P@1: 0.56000 P@3: 0.43000 P@5: 0.36800 N@3: 0.46654 N@5: 0.47032 early stop: 0\n",
            "\u001b[32m[I 210629 05:27:53 models:110]\u001b[39m 92 512 train loss: 0.0094146 valid loss: 0.1742587 P@1: 0.55000 P@3: 0.43333 P@5: 0.37000 N@3: 0.46715 N@5: 0.47061 early stop: 0\n",
            "\u001b[32m[I 210629 05:27:55 models:110]\u001b[39m 94 1024 train loss: 0.0134091 valid loss: 0.1755234 P@1: 0.55000 P@3: 0.44333 P@5: 0.37200 N@3: 0.47419 N@5: 0.47387 early stop: 0\n",
            "\u001b[32m[I 210629 05:27:58 models:110]\u001b[39m 97 512 train loss: 0.0092458 valid loss: 0.1766905 P@1: 0.55000 P@3: 0.45000 P@5: 0.37200 N@3: 0.47827 N@5: 0.47378 early stop: 1\n",
            "\u001b[32m[I 210629 05:28:00 models:110]\u001b[39m 99 1024 train loss: 0.0066213 valid loss: 0.1778421 P@1: 0.55000 P@3: 0.44667 P@5: 0.37200 N@3: 0.47592 N@5: 0.47388 early stop: 0\n",
            "\u001b[32m[I 210629 05:28:03 models:110]\u001b[39m 102 512 train loss: 0.0045326 valid loss: 0.1787633 P@1: 0.56000 P@3: 0.45333 P@5: 0.37400 N@3: 0.48368 N@5: 0.47870 early stop: 0\n",
            "\u001b[32m[I 210629 05:28:05 models:110]\u001b[39m 104 1024 train loss: 0.0046680 valid loss: 0.1802168 P@1: 0.56000 P@3: 0.46333 P@5: 0.37800 N@3: 0.49010 N@5: 0.48233 early stop: 0\n",
            "\u001b[32m[I 210629 05:28:07 models:110]\u001b[39m 107 512 train loss: 0.0037191 valid loss: 0.1817048 P@1: 0.56000 P@3: 0.46000 P@5: 0.37800 N@3: 0.48776 N@5: 0.48180 early stop: 1\n",
            "\u001b[32m[I 210629 05:28:09 models:110]\u001b[39m 109 1024 train loss: 0.0024101 valid loss: 0.1827195 P@1: 0.56000 P@3: 0.47000 P@5: 0.38000 N@3: 0.49541 N@5: 0.48527 early stop: 0\n",
            "\u001b[32m[I 210629 05:28:12 models:110]\u001b[39m 112 512 train loss: 0.0015459 valid loss: 0.1842695 P@1: 0.56000 P@3: 0.47667 P@5: 0.38000 N@3: 0.50072 N@5: 0.48618 early stop: 0\n",
            "\u001b[32m[I 210629 05:28:14 models:110]\u001b[39m 114 1024 train loss: 0.0018854 valid loss: 0.1857166 P@1: 0.56000 P@3: 0.47333 P@5: 0.37600 N@3: 0.49837 N@5: 0.48231 early stop: 1\n",
            "\u001b[32m[I 210629 05:28:16 models:110]\u001b[39m 117 512 train loss: 0.0016229 valid loss: 0.1875298 P@1: 0.56000 P@3: 0.47333 P@5: 0.38000 N@3: 0.49960 N@5: 0.48602 early stop: 2\n",
            "\u001b[32m[I 210629 05:28:18 models:110]\u001b[39m 119 1024 train loss: 0.0012622 valid loss: 0.1891660 P@1: 0.56000 P@3: 0.46667 P@5: 0.38200 N@3: 0.49571 N@5: 0.48799 early stop: 0\n",
            "\u001b[32m[I 210629 05:28:21 models:110]\u001b[39m 122 512 train loss: 0.0013627 valid loss: 0.1907171 P@1: 0.56000 P@3: 0.47000 P@5: 0.38400 N@3: 0.49806 N@5: 0.49028 early stop: 0\n",
            "\u001b[32m[I 210629 05:28:23 models:110]\u001b[39m 124 1024 train loss: 0.0010598 valid loss: 0.1924828 P@1: 0.56000 P@3: 0.46667 P@5: 0.38200 N@3: 0.49571 N@5: 0.48864 early stop: 1\n",
            "\u001b[32m[I 210629 05:28:26 models:110]\u001b[39m 127 512 train loss: 0.0007834 valid loss: 0.1939674 P@1: 0.56000 P@3: 0.47333 P@5: 0.38000 N@3: 0.49979 N@5: 0.48687 early stop: 2\n",
            "\u001b[32m[I 210629 05:28:28 models:110]\u001b[39m 129 1024 train loss: 0.0005231 valid loss: 0.1957099 P@1: 0.56000 P@3: 0.47000 P@5: 0.38200 N@3: 0.49744 N@5: 0.48827 early stop: 3\n",
            "\u001b[32m[I 210629 05:28:30 models:110]\u001b[39m 132 512 train loss: 0.0006698 valid loss: 0.1973625 P@1: 0.56000 P@3: 0.47000 P@5: 0.38200 N@3: 0.49744 N@5: 0.48842 early stop: 4\n",
            "\u001b[32m[I 210629 05:28:32 models:110]\u001b[39m 134 1024 train loss: 0.0003382 valid loss: 0.1991059 P@1: 0.55000 P@3: 0.47667 P@5: 0.38200 N@3: 0.50040 N@5: 0.48734 early stop: 5\n",
            "\u001b[32m[I 210629 05:28:35 models:110]\u001b[39m 137 512 train loss: 0.0003476 valid loss: 0.2006969 P@1: 0.55000 P@3: 0.47667 P@5: 0.38200 N@3: 0.50040 N@5: 0.48734 early stop: 6\n",
            "\u001b[32m[I 210629 05:28:37 models:110]\u001b[39m 139 1024 train loss: 0.0004498 valid loss: 0.2024031 P@1: 0.55000 P@3: 0.48000 P@5: 0.38200 N@3: 0.50275 N@5: 0.48742 early stop: 7\n",
            "\u001b[32m[I 210629 05:28:39 models:110]\u001b[39m 142 512 train loss: 0.0006540 valid loss: 0.2040746 P@1: 0.56000 P@3: 0.48333 P@5: 0.38400 N@3: 0.50683 N@5: 0.49050 early stop: 0\n",
            "\u001b[32m[I 210629 05:28:41 models:110]\u001b[39m 144 1024 train loss: 0.0004962 valid loss: 0.2058579 P@1: 0.56000 P@3: 0.48333 P@5: 0.38400 N@3: 0.50621 N@5: 0.48988 early stop: 1\n",
            "\u001b[33m[W 210629 05:28:42 models:137]\u001b[39m Clipping gradients with total norm 0.16725 and max norm 0.02281\n",
            "\u001b[32m[I 210629 05:28:44 models:110]\u001b[39m 147 512 train loss: 0.0007816 valid loss: 0.2074955 P@1: 0.56000 P@3: 0.48333 P@5: 0.38200 N@3: 0.50621 N@5: 0.48857 early stop: 2\n",
            "\u001b[32m[I 210629 05:28:46 models:110]\u001b[39m 149 1024 train loss: 0.0003037 valid loss: 0.2091878 P@1: 0.56000 P@3: 0.48333 P@5: 0.38400 N@3: 0.50683 N@5: 0.49100 early stop: 0\n",
            "\u001b[32m[I 210629 05:28:48 models:110]\u001b[39m 152 512 train loss: 0.0003267 valid loss: 0.2108222 P@1: 0.57000 P@3: 0.48000 P@5: 0.38400 N@3: 0.50683 N@5: 0.49258 early stop: 0\n",
            "\u001b[32m[I 210629 05:28:51 models:110]\u001b[39m 154 1024 train loss: 0.0001719 valid loss: 0.2124811 P@1: 0.57000 P@3: 0.48000 P@5: 0.38600 N@3: 0.50683 N@5: 0.49510 early stop: 0\n",
            "\u001b[32m[I 210629 05:28:53 models:110]\u001b[39m 157 512 train loss: 0.0001922 valid loss: 0.2141782 P@1: 0.57000 P@3: 0.48000 P@5: 0.38600 N@3: 0.50683 N@5: 0.49525 early stop: 0\n",
            "\u001b[32m[I 210629 05:28:55 models:110]\u001b[39m 159 1024 train loss: 0.0001611 valid loss: 0.2159067 P@1: 0.57000 P@3: 0.48333 P@5: 0.38800 N@3: 0.50979 N@5: 0.49724 early stop: 0\n",
            "\u001b[32m[I 210629 05:28:58 models:110]\u001b[39m 162 512 train loss: 0.0001526 valid loss: 0.2176031 P@1: 0.57000 P@3: 0.48000 P@5: 0.38800 N@3: 0.50744 N@5: 0.49700 early stop: 1\n",
            "\u001b[32m[I 210629 05:29:00 models:110]\u001b[39m 164 1024 train loss: 0.0001068 valid loss: 0.2192783 P@1: 0.57000 P@3: 0.48000 P@5: 0.38800 N@3: 0.50867 N@5: 0.49823 early stop: 0\n",
            "\u001b[32m[I 210629 05:29:02 models:110]\u001b[39m 167 512 train loss: 0.0001635 valid loss: 0.2208810 P@1: 0.57000 P@3: 0.48000 P@5: 0.38800 N@3: 0.50806 N@5: 0.49772 early stop: 1\n",
            "\u001b[33m[W 210629 05:29:04 models:137]\u001b[39m Clipping gradients with total norm 0.09207 and max norm 0.0147\n",
            "\u001b[32m[I 210629 05:29:05 models:110]\u001b[39m 169 1024 train loss: 0.0003535 valid loss: 0.2224532 P@1: 0.58000 P@3: 0.48000 P@5: 0.39000 N@3: 0.51040 N@5: 0.50186 early stop: 0\n",
            "\u001b[32m[I 210629 05:29:07 models:110]\u001b[39m 172 512 train loss: 0.0003807 valid loss: 0.2240796 P@1: 0.58000 P@3: 0.48000 P@5: 0.39000 N@3: 0.50979 N@5: 0.50139 early stop: 1\n",
            "\u001b[32m[I 210629 05:29:09 models:110]\u001b[39m 174 1024 train loss: 0.0002830 valid loss: 0.2257483 P@1: 0.58000 P@3: 0.48333 P@5: 0.39000 N@3: 0.51214 N@5: 0.50157 early stop: 2\n",
            "\u001b[32m[I 210629 05:29:12 models:110]\u001b[39m 177 512 train loss: 0.0001862 valid loss: 0.2273377 P@1: 0.58000 P@3: 0.48333 P@5: 0.39000 N@3: 0.51214 N@5: 0.50172 early stop: 3\n",
            "\u001b[32m[I 210629 05:29:14 models:110]\u001b[39m 179 1024 train loss: 0.0001482 valid loss: 0.2288347 P@1: 0.58000 P@3: 0.48333 P@5: 0.39000 N@3: 0.51214 N@5: 0.50151 early stop: 4\n",
            "\u001b[32m[I 210629 05:29:16 models:110]\u001b[39m 182 512 train loss: 0.0001872 valid loss: 0.2303301 P@1: 0.58000 P@3: 0.48333 P@5: 0.39200 N@3: 0.51214 N@5: 0.50283 early stop: 0\n",
            "\u001b[32m[I 210629 05:29:18 models:110]\u001b[39m 184 1024 train loss: 0.0000989 valid loss: 0.2318565 P@1: 0.58000 P@3: 0.48333 P@5: 0.38800 N@3: 0.51214 N@5: 0.49955 early stop: 1\n",
            "\u001b[32m[I 210629 05:29:21 models:110]\u001b[39m 187 512 train loss: 0.0001217 valid loss: 0.2333837 P@1: 0.58000 P@3: 0.48333 P@5: 0.38800 N@3: 0.51214 N@5: 0.49964 early stop: 2\n",
            "\u001b[32m[I 210629 05:29:23 models:110]\u001b[39m 189 1024 train loss: 0.0001467 valid loss: 0.2349002 P@1: 0.59000 P@3: 0.48333 P@5: 0.38800 N@3: 0.51440 N@5: 0.50190 early stop: 3\n",
            "\u001b[32m[I 210629 05:29:25 models:110]\u001b[39m 192 512 train loss: 0.0001572 valid loss: 0.2363956 P@1: 0.59000 P@3: 0.48333 P@5: 0.39000 N@3: 0.51440 N@5: 0.50322 early stop: 0\n",
            "\u001b[32m[I 210629 05:29:27 models:110]\u001b[39m 194 1024 train loss: 0.0001312 valid loss: 0.2378700 P@1: 0.59000 P@3: 0.48333 P@5: 0.39000 N@3: 0.51440 N@5: 0.50301 early stop: 1\n",
            "\u001b[32m[I 210629 05:29:30 models:110]\u001b[39m 197 512 train loss: 0.0001270 valid loss: 0.2393429 P@1: 0.58000 P@3: 0.48333 P@5: 0.39000 N@3: 0.51267 N@5: 0.50176 early stop: 2\n",
            "\u001b[33m[W 210629 05:29:31 models:137]\u001b[39m Clipping gradients with total norm 0.10041 and max norm 0.00868\n",
            "\u001b[32m[I 210629 05:29:32 models:110]\u001b[39m 199 1024 train loss: 0.0004420 valid loss: 0.2408009 P@1: 0.58000 P@3: 0.48333 P@5: 0.38800 N@3: 0.51267 N@5: 0.50009 early stop: 3\n",
            "\u001b[32m[I 210629 05:29:34 models:110]\u001b[39m 202 512 train loss: 0.0001356 valid loss: 0.2422443 P@1: 0.58000 P@3: 0.48333 P@5: 0.39000 N@3: 0.51267 N@5: 0.50160 early stop: 4\n",
            "\u001b[32m[I 210629 05:29:37 models:110]\u001b[39m 204 1024 train loss: 0.0001495 valid loss: 0.2436785 P@1: 0.58000 P@3: 0.48667 P@5: 0.39000 N@3: 0.51501 N@5: 0.50199 early stop: 5\n",
            "\u001b[32m[I 210629 05:29:39 models:110]\u001b[39m 207 512 train loss: 0.0001265 valid loss: 0.2451057 P@1: 0.58000 P@3: 0.48667 P@5: 0.39000 N@3: 0.51501 N@5: 0.50199 early stop: 6\n",
            "\u001b[32m[I 210629 05:29:41 models:110]\u001b[39m 209 1024 train loss: 0.0001174 valid loss: 0.2465283 P@1: 0.58000 P@3: 0.48667 P@5: 0.38800 N@3: 0.51501 N@5: 0.50088 early stop: 7\n",
            "\u001b[32m[I 210629 05:29:44 models:110]\u001b[39m 212 512 train loss: 0.0001287 valid loss: 0.2479096 P@1: 0.58000 P@3: 0.48333 P@5: 0.38800 N@3: 0.51267 N@5: 0.50079 early stop: 8\n",
            "\u001b[32m[I 210629 05:29:46 models:110]\u001b[39m 214 1024 train loss: 0.0000944 valid loss: 0.2492730 P@1: 0.58000 P@3: 0.48333 P@5: 0.38800 N@3: 0.51267 N@5: 0.50079 early stop: 9\n",
            "\u001b[32m[I 210629 05:29:48 models:110]\u001b[39m 217 512 train loss: 0.0001268 valid loss: 0.2506228 P@1: 0.58000 P@3: 0.48000 P@5: 0.38800 N@3: 0.51032 N@5: 0.50056 early stop: 10\n",
            "\u001b[32m[I 210629 05:29:50 models:110]\u001b[39m 219 1024 train loss: 0.0001511 valid loss: 0.2519653 P@1: 0.58000 P@3: 0.48000 P@5: 0.38600 N@3: 0.51032 N@5: 0.49874 early stop: 11\n",
            "\u001b[32m[I 210629 05:29:53 models:110]\u001b[39m 222 512 train loss: 0.0001397 valid loss: 0.2532917 P@1: 0.58000 P@3: 0.48000 P@5: 0.38600 N@3: 0.51032 N@5: 0.49874 early stop: 12\n",
            "\u001b[32m[I 210629 05:29:55 models:110]\u001b[39m 224 1024 train loss: 0.0001222 valid loss: 0.2546071 P@1: 0.58000 P@3: 0.48000 P@5: 0.38600 N@3: 0.51032 N@5: 0.49874 early stop: 13\n",
            "\u001b[32m[I 210629 05:29:57 models:110]\u001b[39m 227 512 train loss: 0.0001425 valid loss: 0.2559014 P@1: 0.58000 P@3: 0.48000 P@5: 0.38600 N@3: 0.51032 N@5: 0.49874 early stop: 14\n",
            "\u001b[32m[I 210629 05:29:59 models:110]\u001b[39m 229 1024 train loss: 0.0000991 valid loss: 0.2572062 P@1: 0.58000 P@3: 0.48000 P@5: 0.38600 N@3: 0.51032 N@5: 0.49854 early stop: 15\n",
            "\u001b[32m[I 210629 05:30:02 models:110]\u001b[39m 232 512 train loss: 0.0001505 valid loss: 0.2585465 P@1: 0.58000 P@3: 0.48000 P@5: 0.38600 N@3: 0.50971 N@5: 0.49818 early stop: 16\n",
            "\u001b[32m[I 210629 05:30:04 models:110]\u001b[39m 234 1024 train loss: 0.0001188 valid loss: 0.2598259 P@1: 0.59000 P@3: 0.48000 P@5: 0.38600 N@3: 0.51144 N@5: 0.50018 early stop: 17\n",
            "\u001b[32m[I 210629 05:30:06 models:110]\u001b[39m 237 512 train loss: 0.0001211 valid loss: 0.2610672 P@1: 0.59000 P@3: 0.48333 P@5: 0.38800 N@3: 0.51379 N@5: 0.50172 early stop: 18\n",
            "\u001b[33m[W 210629 05:30:07 models:137]\u001b[39m Clipping gradients with total norm 0.0592 and max norm 0.00923\n",
            "\u001b[32m[I 210629 05:30:08 models:110]\u001b[39m 239 1024 train loss: 0.0003290 valid loss: 0.2623329 P@1: 0.59000 P@3: 0.48333 P@5: 0.38800 N@3: 0.51379 N@5: 0.50172 early stop: 19\n",
            "\u001b[32m[I 210629 05:30:11 models:110]\u001b[39m 242 512 train loss: 0.0002215 valid loss: 0.2635396 P@1: 0.59000 P@3: 0.48333 P@5: 0.38800 N@3: 0.51379 N@5: 0.50172 early stop: 20\n",
            "\u001b[32m[I 210629 05:30:13 models:110]\u001b[39m 244 1024 train loss: 0.0001719 valid loss: 0.2646682 P@1: 0.59000 P@3: 0.48667 P@5: 0.39000 N@3: 0.51613 N@5: 0.50377 early stop: 0\n",
            "\u001b[32m[I 210629 05:30:15 models:110]\u001b[39m 247 512 train loss: 0.0001510 valid loss: 0.2657844 P@1: 0.60000 P@3: 0.49000 P@5: 0.39000 N@3: 0.52021 N@5: 0.50543 early stop: 0\n",
            "\u001b[33m[W 210629 05:30:16 models:137]\u001b[39m Clipping gradients with total norm 0.05146 and max norm 0.00974\n",
            "\u001b[32m[I 210629 05:30:18 models:110]\u001b[39m 249 1024 train loss: 0.0002569 valid loss: 0.2669226 P@1: 0.60000 P@3: 0.49000 P@5: 0.39000 N@3: 0.52021 N@5: 0.50543 early stop: 1\n",
            "\u001b[32m[I 210629 05:30:20 models:110]\u001b[39m 252 512 train loss: 0.0002751 valid loss: 0.2681382 P@1: 0.60000 P@3: 0.49000 P@5: 0.39000 N@3: 0.52021 N@5: 0.50543 early stop: 2\n",
            "\u001b[33m[W 210629 05:30:22 models:137]\u001b[39m Clipping gradients with total norm 0.04442 and max norm 0.00732\n",
            "\u001b[32m[I 210629 05:30:22 models:110]\u001b[39m 254 1024 train loss: 0.0002081 valid loss: 0.2693754 P@1: 0.60000 P@3: 0.49000 P@5: 0.39200 N@3: 0.52021 N@5: 0.50674 early stop: 0\n",
            "\u001b[32m[I 210629 05:30:25 models:110]\u001b[39m 257 512 train loss: 0.0002438 valid loss: 0.2705771 P@1: 0.60000 P@3: 0.49000 P@5: 0.39200 N@3: 0.51960 N@5: 0.50630 early stop: 1\n",
            "\u001b[32m[I 210629 05:30:27 models:110]\u001b[39m 259 1024 train loss: 0.0001495 valid loss: 0.2716900 P@1: 0.60000 P@3: 0.49333 P@5: 0.39200 N@3: 0.52194 N@5: 0.50654 early stop: 2\n",
            "\u001b[33m[W 210629 05:30:27 models:137]\u001b[39m Clipping gradients with total norm 0.05631 and max norm 0.00955\n",
            "\u001b[32m[I 210629 05:30:29 models:110]\u001b[39m 262 512 train loss: 0.0002890 valid loss: 0.2727520 P@1: 0.60000 P@3: 0.49333 P@5: 0.39400 N@3: 0.52194 N@5: 0.50835 early stop: 0\n",
            "\u001b[32m[I 210629 05:30:31 models:110]\u001b[39m 264 1024 train loss: 0.0003353 valid loss: 0.2738782 P@1: 0.60000 P@3: 0.49333 P@5: 0.39400 N@3: 0.52194 N@5: 0.50835 early stop: 1\n",
            "\u001b[32m[I 210629 05:30:34 models:110]\u001b[39m 267 512 train loss: 0.0001618 valid loss: 0.2749832 P@1: 0.60000 P@3: 0.49333 P@5: 0.39400 N@3: 0.52194 N@5: 0.50835 early stop: 2\n",
            "\u001b[32m[I 210629 05:30:36 models:110]\u001b[39m 269 1024 train loss: 0.0001258 valid loss: 0.2760723 P@1: 0.60000 P@3: 0.49333 P@5: 0.39400 N@3: 0.52194 N@5: 0.50850 early stop: 0\n",
            "\u001b[32m[I 210629 05:30:39 models:110]\u001b[39m 272 512 train loss: 0.0001345 valid loss: 0.2771553 P@1: 0.60000 P@3: 0.49333 P@5: 0.39400 N@3: 0.52194 N@5: 0.50850 early stop: 1\n",
            "\u001b[32m[I 210629 05:30:41 models:110]\u001b[39m 274 1024 train loss: 0.0000949 valid loss: 0.2782266 P@1: 0.60000 P@3: 0.49333 P@5: 0.39400 N@3: 0.52256 N@5: 0.50911 early stop: 0\n",
            "\u001b[32m[I 210629 05:30:43 models:110]\u001b[39m 277 512 train loss: 0.0001407 valid loss: 0.2792949 P@1: 0.60000 P@3: 0.49333 P@5: 0.39400 N@3: 0.52194 N@5: 0.50865 early stop: 1\n",
            "\u001b[33m[W 210629 05:30:45 models:137]\u001b[39m Clipping gradients with total norm 0.04185 and max norm 0.0056\n",
            "\u001b[32m[I 210629 05:30:45 models:110]\u001b[39m 279 1024 train loss: 0.0002103 valid loss: 0.2803591 P@1: 0.60000 P@3: 0.49333 P@5: 0.39200 N@3: 0.52194 N@5: 0.50683 early stop: 2\n",
            "\u001b[32m[I 210629 05:30:48 models:110]\u001b[39m 282 512 train loss: 0.0001652 valid loss: 0.2814174 P@1: 0.59000 P@3: 0.49333 P@5: 0.39400 N@3: 0.52021 N@5: 0.50670 early stop: 3\n",
            "\u001b[32m[I 210629 05:30:50 models:110]\u001b[39m 284 1024 train loss: 0.0001019 valid loss: 0.2824523 P@1: 0.59000 P@3: 0.49333 P@5: 0.39400 N@3: 0.52021 N@5: 0.50670 early stop: 4\n",
            "\u001b[32m[I 210629 05:30:52 models:110]\u001b[39m 287 512 train loss: 0.0000788 valid loss: 0.2834811 P@1: 0.59000 P@3: 0.49333 P@5: 0.39400 N@3: 0.52021 N@5: 0.50670 early stop: 5\n",
            "\u001b[33m[W 210629 05:30:52 models:137]\u001b[39m Clipping gradients with total norm 0.00917 and max norm 0.00142\n",
            "\u001b[32m[I 210629 05:30:54 models:110]\u001b[39m 289 1024 train loss: 0.0001384 valid loss: 0.2845191 P@1: 0.59000 P@3: 0.49333 P@5: 0.39400 N@3: 0.52021 N@5: 0.50670 early stop: 6\n",
            "\u001b[32m[I 210629 05:30:57 models:110]\u001b[39m 292 512 train loss: 0.0000972 valid loss: 0.2855545 P@1: 0.59000 P@3: 0.49333 P@5: 0.39400 N@3: 0.52021 N@5: 0.50670 early stop: 7\n",
            "\u001b[32m[I 210629 05:30:59 models:110]\u001b[39m 294 1024 train loss: 0.0001063 valid loss: 0.2865778 P@1: 0.59000 P@3: 0.49333 P@5: 0.39400 N@3: 0.52021 N@5: 0.50670 early stop: 8\n",
            "\u001b[32m[I 210629 05:31:01 models:110]\u001b[39m 297 512 train loss: 0.0001297 valid loss: 0.2875909 P@1: 0.59000 P@3: 0.49333 P@5: 0.39400 N@3: 0.52021 N@5: 0.50670 early stop: 9\n",
            "\u001b[32m[I 210629 05:31:03 models:110]\u001b[39m 299 1024 train loss: 0.0000839 valid loss: 0.2885956 P@1: 0.59000 P@3: 0.49000 P@5: 0.39400 N@3: 0.51786 N@5: 0.50638 early stop: 10\n",
            "\u001b[32m[I 210629 05:31:06 models:110]\u001b[39m 302 512 train loss: 0.0002039 valid loss: 0.2895836 P@1: 0.58000 P@3: 0.48667 P@5: 0.39400 N@3: 0.51317 N@5: 0.50388 early stop: 11\n",
            "\u001b[32m[I 210629 05:31:08 models:110]\u001b[39m 304 1024 train loss: 0.0002410 valid loss: 0.2905326 P@1: 0.58000 P@3: 0.48667 P@5: 0.39400 N@3: 0.51317 N@5: 0.50388 early stop: 12\n",
            "\u001b[32m[I 210629 05:31:10 models:110]\u001b[39m 307 512 train loss: 0.0001827 valid loss: 0.2914885 P@1: 0.58000 P@3: 0.48667 P@5: 0.39400 N@3: 0.51317 N@5: 0.50373 early stop: 13\n",
            "\u001b[32m[I 210629 05:31:13 models:110]\u001b[39m 309 1024 train loss: 0.0001392 valid loss: 0.2924371 P@1: 0.58000 P@3: 0.49000 P@5: 0.39400 N@3: 0.51552 N@5: 0.50405 early stop: 14\n",
            "\u001b[32m[I 210629 05:31:15 models:110]\u001b[39m 312 512 train loss: 0.0001123 valid loss: 0.2934216 P@1: 0.58000 P@3: 0.48667 P@5: 0.39400 N@3: 0.51317 N@5: 0.50373 early stop: 15\n",
            "\u001b[32m[I 210629 05:31:17 models:110]\u001b[39m 314 1024 train loss: 0.0000957 valid loss: 0.2943959 P@1: 0.58000 P@3: 0.48667 P@5: 0.39400 N@3: 0.51317 N@5: 0.50373 early stop: 16\n",
            "\u001b[32m[I 210629 05:31:20 models:110]\u001b[39m 317 512 train loss: 0.0000990 valid loss: 0.2953469 P@1: 0.58000 P@3: 0.48667 P@5: 0.39400 N@3: 0.51317 N@5: 0.50373 early stop: 17\n",
            "\u001b[32m[I 210629 05:31:22 models:110]\u001b[39m 319 1024 train loss: 0.0001076 valid loss: 0.2962920 P@1: 0.58000 P@3: 0.48333 P@5: 0.39400 N@3: 0.51082 N@5: 0.50346 early stop: 18\n",
            "\u001b[32m[I 210629 05:31:24 models:110]\u001b[39m 322 512 train loss: 0.0000752 valid loss: 0.2972430 P@1: 0.58000 P@3: 0.48333 P@5: 0.39600 N@3: 0.51082 N@5: 0.50527 early stop: 19\n",
            "\u001b[33m[W 210629 05:31:24 models:137]\u001b[39m Clipping gradients with total norm 0.06921 and max norm 0.0035\n",
            "\u001b[32m[I 210629 05:31:26 models:110]\u001b[39m 324 1024 train loss: 0.0003494 valid loss: 0.2982050 P@1: 0.58000 P@3: 0.48667 P@5: 0.39800 N@3: 0.51317 N@5: 0.50719 early stop: 20\n",
            "\u001b[32m[I 210629 05:31:29 models:110]\u001b[39m 327 512 train loss: 0.0001313 valid loss: 0.2991519 P@1: 0.58000 P@3: 0.48667 P@5: 0.39800 N@3: 0.51317 N@5: 0.50719 early stop: 21\n",
            "\u001b[32m[I 210629 05:31:31 models:110]\u001b[39m 329 1024 train loss: 0.0001125 valid loss: 0.3000722 P@1: 0.58000 P@3: 0.49000 P@5: 0.39600 N@3: 0.51552 N@5: 0.50611 early stop: 22\n",
            "\u001b[32m[I 210629 05:31:33 models:110]\u001b[39m 332 512 train loss: 0.0001077 valid loss: 0.3009833 P@1: 0.58000 P@3: 0.49000 P@5: 0.39600 N@3: 0.51552 N@5: 0.50611 early stop: 23\n",
            "\u001b[33m[W 210629 05:31:35 models:137]\u001b[39m Clipping gradients with total norm 0.04311 and max norm 0.00612\n",
            "\u001b[32m[I 210629 05:31:35 models:110]\u001b[39m 334 1024 train loss: 0.0002306 valid loss: 0.3019017 P@1: 0.58000 P@3: 0.49000 P@5: 0.39600 N@3: 0.51552 N@5: 0.50611 early stop: 24\n",
            "\u001b[32m[I 210629 05:31:38 models:110]\u001b[39m 337 512 train loss: 0.0001914 valid loss: 0.3028078 P@1: 0.58000 P@3: 0.49000 P@5: 0.39600 N@3: 0.51552 N@5: 0.50611 early stop: 25\n",
            "\u001b[32m[I 210629 05:31:40 models:110]\u001b[39m 339 1024 train loss: 0.0001416 valid loss: 0.3037015 P@1: 0.58000 P@3: 0.49000 P@5: 0.39400 N@3: 0.51552 N@5: 0.50430 early stop: 26\n",
            "\u001b[33m[W 210629 05:31:41 models:137]\u001b[39m Clipping gradients with total norm 0.04049 and max norm 0.0063\n",
            "\u001b[32m[I 210629 05:31:42 models:110]\u001b[39m 342 512 train loss: 0.0002783 valid loss: 0.3045632 P@1: 0.58000 P@3: 0.49000 P@5: 0.39600 N@3: 0.51552 N@5: 0.50561 early stop: 27\n",
            "\u001b[32m[I 210629 05:31:44 models:110]\u001b[39m 344 1024 train loss: 0.0002071 valid loss: 0.3054093 P@1: 0.58000 P@3: 0.48667 P@5: 0.39600 N@3: 0.51317 N@5: 0.50528 early stop: 28\n",
            "\u001b[32m[I 210629 05:31:47 models:110]\u001b[39m 347 512 train loss: 0.0002256 valid loss: 0.3062861 P@1: 0.58000 P@3: 0.48667 P@5: 0.39800 N@3: 0.51317 N@5: 0.50710 early stop: 29\n",
            "\u001b[32m[I 210629 05:31:49 models:110]\u001b[39m 349 1024 train loss: 0.0001136 valid loss: 0.3071600 P@1: 0.58000 P@3: 0.48667 P@5: 0.39800 N@3: 0.51317 N@5: 0.50710 early stop: 30\n",
            "\u001b[32m[I 210629 05:31:51 models:110]\u001b[39m 352 512 train loss: 0.0001351 valid loss: 0.3080004 P@1: 0.58000 P@3: 0.48667 P@5: 0.39800 N@3: 0.51317 N@5: 0.50710 early stop: 31\n",
            "\u001b[33m[W 210629 05:31:53 models:137]\u001b[39m Clipping gradients with total norm 0.06588 and max norm 0.00804\n",
            "\u001b[32m[I 210629 05:31:54 models:110]\u001b[39m 354 1024 train loss: 0.0001103 valid loss: 0.3088316 P@1: 0.58000 P@3: 0.48667 P@5: 0.39800 N@3: 0.51317 N@5: 0.50645 early stop: 32\n",
            "\u001b[32m[I 210629 05:31:56 models:110]\u001b[39m 357 512 train loss: 0.0001329 valid loss: 0.3096858 P@1: 0.58000 P@3: 0.48667 P@5: 0.39800 N@3: 0.51317 N@5: 0.50645 early stop: 33\n",
            "\u001b[33m[W 210629 05:31:57 models:137]\u001b[39m Clipping gradients with total norm 0.17977 and max norm 0.01273\n",
            "\u001b[33m[W 210629 05:31:58 models:137]\u001b[39m Clipping gradients with total norm 0.15615 and max norm 0.02546\n",
            "\u001b[32m[I 210629 05:31:58 models:110]\u001b[39m 359 1024 train loss: 0.0004249 valid loss: 0.3104417 P@1: 0.58000 P@3: 0.48667 P@5: 0.40000 N@3: 0.51317 N@5: 0.50776 early stop: 34\n",
            "\u001b[32m[I 210629 05:32:01 models:110]\u001b[39m 362 512 train loss: 0.0025695 valid loss: 0.3110856 P@1: 0.58000 P@3: 0.48667 P@5: 0.39600 N@3: 0.51317 N@5: 0.50494 early stop: 35\n",
            "\u001b[33m[W 210629 05:32:02 models:137]\u001b[39m Clipping gradients with total norm 5.06191 and max norm 1.0\n",
            "\u001b[32m[I 210629 05:32:03 models:110]\u001b[39m 364 1024 train loss: 0.0583434 valid loss: 0.3106562 P@1: 0.59000 P@3: 0.49000 P@5: 0.39800 N@3: 0.51725 N@5: 0.50865 early stop: 36\n",
            "\u001b[33m[W 210629 05:32:04 models:137]\u001b[39m Clipping gradients with total norm 6.34806 and max norm 1.0\n",
            "\u001b[32m[I 210629 05:32:05 models:110]\u001b[39m 367 512 train loss: 0.2759181 valid loss: 0.3056659 P@1: 0.58000 P@3: 0.49333 P@5: 0.39800 N@3: 0.51797 N@5: 0.50621 early stop: 37\n",
            "\u001b[32m[I 210629 05:32:07 models:110]\u001b[39m 369 1024 train loss: 0.1079194 valid loss: 0.2995195 P@1: 0.58000 P@3: 0.50000 P@5: 0.39800 N@3: 0.52389 N@5: 0.50712 early stop: 38\n",
            "\u001b[32m[I 210629 05:32:10 models:110]\u001b[39m 372 512 train loss: 0.0723165 valid loss: 0.2935352 P@1: 0.59000 P@3: 0.49667 P@5: 0.39400 N@3: 0.52328 N@5: 0.50525 early stop: 39\n",
            "\u001b[32m[I 210629 05:32:12 models:110]\u001b[39m 374 1024 train loss: 0.0504017 valid loss: 0.2882683 P@1: 0.58000 P@3: 0.50667 P@5: 0.39000 N@3: 0.52797 N@5: 0.50149 early stop: 40\n",
            "\u001b[32m[I 210629 05:32:15 models:110]\u001b[39m 377 512 train loss: 0.0369903 valid loss: 0.2837932 P@1: 0.57000 P@3: 0.50667 P@5: 0.39000 N@3: 0.52685 N@5: 0.50083 early stop: 41\n",
            "\u001b[32m[I 210629 05:32:17 models:110]\u001b[39m 379 1024 train loss: 0.0277175 valid loss: 0.2798862 P@1: 0.57000 P@3: 0.50333 P@5: 0.38800 N@3: 0.52389 N@5: 0.49831 early stop: 42\n",
            "\u001b[32m[I 210629 05:32:19 models:110]\u001b[39m 382 512 train loss: 0.0228647 valid loss: 0.2764865 P@1: 0.56000 P@3: 0.49667 P@5: 0.38600 N@3: 0.51808 N@5: 0.49510 early stop: 43\n",
            "\u001b[32m[I 210629 05:32:21 models:110]\u001b[39m 384 1024 train loss: 0.0180544 valid loss: 0.2734816 P@1: 0.55000 P@3: 0.49667 P@5: 0.39000 N@3: 0.51573 N@5: 0.49558 early stop: 44\n",
            "\u001b[32m[I 210629 05:32:24 models:110]\u001b[39m 387 512 train loss: 0.0134190 valid loss: 0.2708642 P@1: 0.55000 P@3: 0.48333 P@5: 0.39000 N@3: 0.50635 N@5: 0.49554 early stop: 45\n",
            "\u001b[32m[I 210629 05:32:26 models:110]\u001b[39m 389 1024 train loss: 0.0122628 valid loss: 0.2684431 P@1: 0.56000 P@3: 0.48000 P@5: 0.39000 N@3: 0.50450 N@5: 0.49543 early stop: 46\n",
            "\u001b[32m[I 210629 05:32:28 models:110]\u001b[39m 392 512 train loss: 0.0098608 valid loss: 0.2664442 P@1: 0.56000 P@3: 0.47333 P@5: 0.38600 N@3: 0.49920 N@5: 0.49138 early stop: 47\n",
            "\u001b[32m[I 210629 05:32:30 models:110]\u001b[39m 394 1024 train loss: 0.0080209 valid loss: 0.2646081 P@1: 0.56000 P@3: 0.47333 P@5: 0.38600 N@3: 0.49858 N@5: 0.49104 early stop: 48\n",
            "\u001b[32m[I 210629 05:32:33 models:110]\u001b[39m 397 512 train loss: 0.0051162 valid loss: 0.2629793 P@1: 0.56000 P@3: 0.47333 P@5: 0.38400 N@3: 0.49939 N@5: 0.48986 early stop: 49\n",
            "\u001b[32m[I 210629 05:32:35 models:110]\u001b[39m 399 1024 train loss: 0.0041414 valid loss: 0.2615544 P@1: 0.57000 P@3: 0.46667 P@5: 0.37800 N@3: 0.49642 N@5: 0.48469 early stop: 50\n",
            "\u001b[32m[I 210629 05:32:37 main:76]\u001b[39m Finish Training\n",
            "\u001b[32m[I 210629 05:32:39 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210629 05:32:39 main:79]\u001b[39m Loading Test Set\n",
            "\u001b[32m[I 210629 05:32:39 main:83]\u001b[39m Size of Test Set: 100\n",
            "\u001b[32m[I 210629 05:32:39 main:85]\u001b[39m Predicting\n",
            "\u001b[32m[I 210629 05:32:43 main:91]\u001b[39m Finish Predicting\n",
            "Precision@1,3,5: 0.56 0.45666666666666667 0.368\n",
            "nDCG@1,3,5: 0.56 0.48978377485776564 0.4823503460782453\n",
            "\n",
            "NO MAG NO MESH, skip=500\n",
            "\n",
            "/content/drive/Shareddrives/NASA/MATCH/PeTaL\n",
            "131\n",
            "/content/drive/Shareddrives/NASA/MATCH\n",
            "    800  260480 4295970 PeTaL/train.json\n",
            "\u001b[32m[I 210629 05:32:46 preprocess:28]\u001b[39m Vocab Size: 26834\n",
            "\u001b[32m[I 210629 05:32:46 preprocess:30]\u001b[39m Getting Dataset: PeTaL/train_texts.txt Max Length: 500\n",
            "\u001b[32m[I 210629 05:32:46 preprocess:32]\u001b[39m Size of Samples: 900\n",
            "\u001b[32m[I 210629 05:32:47 preprocess:28]\u001b[39m Vocab Size: 26834\n",
            "\u001b[32m[I 210629 05:32:47 preprocess:30]\u001b[39m Getting Dataset: PeTaL/test_texts.txt Max Length: 500\n",
            "\u001b[32m[I 210629 05:32:47 preprocess:32]\u001b[39m Size of Samples: 100\n",
            "\u001b[32m[I 210629 05:32:48 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210629 05:32:48 main:35]\u001b[39m Loading Training and Validation Set\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['absorb_and/or_filter_solids', 'chemically_break_down_inorganic_compounds', 'detox/purify', 'manage_environmental_disturbances_in_a_community', 'protect_from_fire', 'protect_from_gases', 'send_vibratory_signals'] will be ignored\n",
            "  .format(sorted(unknown, key=str)))\n",
            "\u001b[32m[I 210629 05:32:48 main:47]\u001b[39m Number of Labels: 124\n",
            "\u001b[32m[I 210629 05:32:48 main:48]\u001b[39m Size of Training Set: 800\n",
            "\u001b[32m[I 210629 05:32:48 main:49]\u001b[39m Size of Validation Set: 100\n",
            "\u001b[32m[I 210629 05:32:48 main:66]\u001b[39m Number of Edges: 101\n",
            "\u001b[32m[I 210629 05:32:48 main:68]\u001b[39m Training\n",
            "\u001b[32m[I 210629 05:32:54 models:110]\u001b[39m 2 512 train loss: 0.2673111 valid loss: 0.1505364 P@1: 0.33000 P@3: 0.20667 P@5: 0.17200 N@3: 0.23490 N@5: 0.23650 early stop: 0\n",
            "\u001b[32m[I 210629 05:32:55 models:142]\u001b[39m SWA Initializing\n",
            "\u001b[32m[I 210629 05:32:56 models:110]\u001b[39m 4 1024 train loss: 0.1448982 valid loss: 0.1393378 P@1: 0.33000 P@3: 0.20667 P@5: 0.17800 N@3: 0.23397 N@5: 0.23529 early stop: 1\n",
            "\u001b[32m[I 210629 05:32:59 models:110]\u001b[39m 7 512 train loss: 0.1437393 valid loss: 0.1371908 P@1: 0.33000 P@3: 0.22000 P@5: 0.18600 N@3: 0.24274 N@5: 0.24394 early stop: 0\n",
            "\u001b[32m[I 210629 05:33:01 models:110]\u001b[39m 9 1024 train loss: 0.1407083 valid loss: 0.1367840 P@1: 0.33000 P@3: 0.22000 P@5: 0.18400 N@3: 0.24212 N@5: 0.24195 early stop: 1\n",
            "\u001b[32m[I 210629 05:33:04 models:110]\u001b[39m 12 512 train loss: 0.1348454 valid loss: 0.1363069 P@1: 0.33000 P@3: 0.22000 P@5: 0.18400 N@3: 0.24274 N@5: 0.24198 early stop: 2\n",
            "\u001b[32m[I 210629 05:33:06 models:110]\u001b[39m 14 1024 train loss: 0.1263605 valid loss: 0.1357147 P@1: 0.33000 P@3: 0.22000 P@5: 0.20200 N@3: 0.24274 N@5: 0.25565 early stop: 0\n",
            "\u001b[32m[I 210629 05:33:08 models:110]\u001b[39m 17 512 train loss: 0.1074504 valid loss: 0.1345566 P@1: 0.33000 P@3: 0.23667 P@5: 0.20200 N@3: 0.25631 N@5: 0.25798 early stop: 0\n",
            "\u001b[32m[I 210629 05:33:10 models:110]\u001b[39m 19 1024 train loss: 0.0853759 valid loss: 0.1334055 P@1: 0.33000 P@3: 0.26667 P@5: 0.22200 N@3: 0.27743 N@5: 0.27820 early stop: 0\n",
            "\u001b[32m[I 210629 05:33:13 models:110]\u001b[39m 22 512 train loss: 0.0670680 valid loss: 0.1323720 P@1: 0.31000 P@3: 0.26667 P@5: 0.24600 N@3: 0.27520 N@5: 0.29693 early stop: 0\n",
            "\u001b[32m[I 210629 05:33:15 models:110]\u001b[39m 24 1024 train loss: 0.0562090 valid loss: 0.1314305 P@1: 0.41000 P@3: 0.27333 P@5: 0.23000 N@3: 0.30089 N@5: 0.30504 early stop: 0\n",
            "\u001b[32m[I 210629 05:33:17 models:110]\u001b[39m 27 512 train loss: 0.0456375 valid loss: 0.1305072 P@1: 0.43000 P@3: 0.29667 P@5: 0.23400 N@3: 0.32866 N@5: 0.32390 early stop: 0\n",
            "\u001b[32m[I 210629 05:33:20 models:110]\u001b[39m 29 1024 train loss: 0.0402956 valid loss: 0.1297896 P@1: 0.46000 P@3: 0.30333 P@5: 0.24800 N@3: 0.33918 N@5: 0.34154 early stop: 0\n",
            "\u001b[32m[I 210629 05:33:22 models:110]\u001b[39m 32 512 train loss: 0.0315048 valid loss: 0.1289882 P@1: 0.43000 P@3: 0.29333 P@5: 0.25800 N@3: 0.32855 N@5: 0.34588 early stop: 0\n",
            "\u001b[32m[I 210629 05:33:24 models:110]\u001b[39m 34 1024 train loss: 0.0278876 valid loss: 0.1286155 P@1: 0.42000 P@3: 0.30333 P@5: 0.26800 N@3: 0.33538 N@5: 0.35637 early stop: 0\n",
            "\u001b[32m[I 210629 05:33:27 models:110]\u001b[39m 37 512 train loss: 0.0247028 valid loss: 0.1281594 P@1: 0.42000 P@3: 0.32667 P@5: 0.27200 N@3: 0.35437 N@5: 0.36362 early stop: 0\n",
            "\u001b[32m[I 210629 05:33:29 models:110]\u001b[39m 39 1024 train loss: 0.0239406 valid loss: 0.1271349 P@1: 0.46000 P@3: 0.32667 P@5: 0.28000 N@3: 0.36469 N@5: 0.37895 early stop: 0\n",
            "\u001b[32m[I 210629 05:33:32 models:110]\u001b[39m 42 512 train loss: 0.0172483 valid loss: 0.1267692 P@1: 0.48000 P@3: 0.34000 P@5: 0.28600 N@3: 0.37735 N@5: 0.38780 early stop: 0\n",
            "\u001b[32m[I 210629 05:33:34 models:110]\u001b[39m 44 1024 train loss: 0.0150194 valid loss: 0.1264363 P@1: 0.48000 P@3: 0.35000 P@5: 0.29000 N@3: 0.38572 N@5: 0.39311 early stop: 0\n",
            "\u001b[32m[I 210629 05:33:36 models:110]\u001b[39m 47 512 train loss: 0.0133239 valid loss: 0.1255902 P@1: 0.49000 P@3: 0.36000 P@5: 0.29600 N@3: 0.39775 N@5: 0.40448 early stop: 0\n",
            "\u001b[32m[I 210629 05:33:38 models:110]\u001b[39m 49 1024 train loss: 0.0125988 valid loss: 0.1252230 P@1: 0.51000 P@3: 0.36333 P@5: 0.30800 N@3: 0.40479 N@5: 0.41861 early stop: 0\n",
            "\u001b[32m[I 210629 05:33:41 models:110]\u001b[39m 52 512 train loss: 0.0111306 valid loss: 0.1250763 P@1: 0.51000 P@3: 0.37667 P@5: 0.30600 N@3: 0.41541 N@5: 0.41945 early stop: 0\n",
            "\u001b[32m[I 210629 05:33:43 models:110]\u001b[39m 54 1024 train loss: 0.0094540 valid loss: 0.1252873 P@1: 0.53000 P@3: 0.38333 P@5: 0.30800 N@3: 0.42754 N@5: 0.42922 early stop: 0\n",
            "\u001b[32m[I 210629 05:33:46 models:110]\u001b[39m 57 512 train loss: 0.0098588 valid loss: 0.1253901 P@1: 0.54000 P@3: 0.39333 P@5: 0.31000 N@3: 0.43754 N@5: 0.43538 early stop: 0\n",
            "\u001b[32m[I 210629 05:33:48 models:110]\u001b[39m 59 1024 train loss: 0.0093899 valid loss: 0.1258123 P@1: 0.54000 P@3: 0.39333 P@5: 0.31400 N@3: 0.43754 N@5: 0.43960 early stop: 0\n",
            "\u001b[32m[I 210629 05:33:50 models:110]\u001b[39m 62 512 train loss: 0.0076312 valid loss: 0.1259289 P@1: 0.55000 P@3: 0.40000 P@5: 0.31800 N@3: 0.44592 N@5: 0.44614 early stop: 0\n",
            "\u001b[32m[I 210629 05:33:52 models:110]\u001b[39m 64 1024 train loss: 0.0069364 valid loss: 0.1264250 P@1: 0.56000 P@3: 0.41000 P@5: 0.32200 N@3: 0.45469 N@5: 0.45122 early stop: 0\n",
            "\u001b[32m[I 210629 05:33:55 models:110]\u001b[39m 67 512 train loss: 0.0064279 valid loss: 0.1268507 P@1: 0.57000 P@3: 0.41667 P@5: 0.33000 N@3: 0.46111 N@5: 0.45966 early stop: 0\n",
            "\u001b[32m[I 210629 05:33:57 models:110]\u001b[39m 69 1024 train loss: 0.0056479 valid loss: 0.1274485 P@1: 0.57000 P@3: 0.41667 P@5: 0.34200 N@3: 0.46295 N@5: 0.47043 early stop: 0\n",
            "\u001b[32m[I 210629 05:34:00 models:110]\u001b[39m 72 512 train loss: 0.0052031 valid loss: 0.1284667 P@1: 0.57000 P@3: 0.42333 P@5: 0.34800 N@3: 0.46703 N@5: 0.47561 early stop: 0\n",
            "\u001b[32m[I 210629 05:34:02 models:110]\u001b[39m 74 1024 train loss: 0.0042927 valid loss: 0.1293194 P@1: 0.58000 P@3: 0.43333 P@5: 0.34800 N@3: 0.47703 N@5: 0.47839 early stop: 0\n",
            "\u001b[32m[I 210629 05:34:04 models:110]\u001b[39m 77 512 train loss: 0.0030381 valid loss: 0.1300571 P@1: 0.59000 P@3: 0.43000 P@5: 0.34800 N@3: 0.47703 N@5: 0.48079 early stop: 0\n",
            "\u001b[32m[I 210629 05:34:06 models:110]\u001b[39m 79 1024 train loss: 0.0034180 valid loss: 0.1308077 P@1: 0.59000 P@3: 0.43333 P@5: 0.35400 N@3: 0.47938 N@5: 0.48560 early stop: 0\n",
            "\u001b[32m[I 210629 05:34:09 models:110]\u001b[39m 82 512 train loss: 0.0049402 valid loss: 0.1317796 P@1: 0.60000 P@3: 0.44333 P@5: 0.35000 N@3: 0.48815 N@5: 0.48436 early stop: 1\n",
            "\u001b[32m[I 210629 05:34:11 models:110]\u001b[39m 84 1024 train loss: 0.0047713 valid loss: 0.1328051 P@1: 0.60000 P@3: 0.45333 P@5: 0.35200 N@3: 0.49580 N@5: 0.48721 early stop: 0\n",
            "\u001b[32m[I 210629 05:34:14 models:110]\u001b[39m 87 512 train loss: 0.0047973 valid loss: 0.1337698 P@1: 0.61000 P@3: 0.45667 P@5: 0.35400 N@3: 0.49988 N@5: 0.49019 early stop: 0\n",
            "\u001b[32m[I 210629 05:34:16 models:110]\u001b[39m 89 1024 train loss: 0.0041546 valid loss: 0.1349923 P@1: 0.61000 P@3: 0.46333 P@5: 0.35400 N@3: 0.50519 N@5: 0.49115 early stop: 0\n",
            "\u001b[32m[I 210629 05:34:18 models:110]\u001b[39m 92 512 train loss: 0.0079683 valid loss: 0.1360416 P@1: 0.61000 P@3: 0.46000 P@5: 0.35800 N@3: 0.50346 N@5: 0.49478 early stop: 0\n",
            "\u001b[32m[I 210629 05:34:20 models:110]\u001b[39m 94 1024 train loss: 0.0082266 valid loss: 0.1366953 P@1: 0.62000 P@3: 0.46333 P@5: 0.35800 N@3: 0.50876 N@5: 0.49794 early stop: 0\n",
            "\u001b[32m[I 210629 05:34:23 models:110]\u001b[39m 97 512 train loss: 0.0074123 valid loss: 0.1376715 P@1: 0.61000 P@3: 0.46000 P@5: 0.36000 N@3: 0.50469 N@5: 0.49793 early stop: 1\n",
            "\u001b[32m[I 210629 05:34:25 models:110]\u001b[39m 99 1024 train loss: 0.0095990 valid loss: 0.1384722 P@1: 0.60000 P@3: 0.46333 P@5: 0.36000 N@3: 0.50530 N@5: 0.49688 early stop: 2\n",
            "\u001b[32m[I 210629 05:34:28 models:110]\u001b[39m 102 512 train loss: 0.0093764 valid loss: 0.1392693 P@1: 0.60000 P@3: 0.46333 P@5: 0.36400 N@3: 0.50592 N@5: 0.50014 early stop: 0\n",
            "\u001b[32m[I 210629 05:34:30 models:110]\u001b[39m 104 1024 train loss: 0.0072220 valid loss: 0.1400631 P@1: 0.61000 P@3: 0.46333 P@5: 0.36400 N@3: 0.50765 N@5: 0.50187 early stop: 0\n",
            "\u001b[32m[I 210629 05:34:32 models:110]\u001b[39m 107 512 train loss: 0.0053121 valid loss: 0.1409475 P@1: 0.61000 P@3: 0.46667 P@5: 0.36000 N@3: 0.51122 N@5: 0.49994 early stop: 1\n",
            "\u001b[32m[I 210629 05:34:34 models:110]\u001b[39m 109 1024 train loss: 0.0048006 valid loss: 0.1419271 P@1: 0.61000 P@3: 0.46333 P@5: 0.36600 N@3: 0.50877 N@5: 0.50527 early stop: 0\n",
            "\u001b[32m[I 210629 05:34:37 models:110]\u001b[39m 112 512 train loss: 0.0033226 valid loss: 0.1428277 P@1: 0.61000 P@3: 0.46667 P@5: 0.36600 N@3: 0.51050 N@5: 0.50592 early stop: 0\n",
            "\u001b[32m[I 210629 05:34:39 models:110]\u001b[39m 114 1024 train loss: 0.0036483 valid loss: 0.1437233 P@1: 0.61000 P@3: 0.46667 P@5: 0.37000 N@3: 0.51050 N@5: 0.50890 early stop: 0\n",
            "\u001b[32m[I 210629 05:34:42 models:110]\u001b[39m 117 512 train loss: 0.0059206 valid loss: 0.1443729 P@1: 0.62000 P@3: 0.46333 P@5: 0.37400 N@3: 0.51050 N@5: 0.51355 early stop: 0\n",
            "\u001b[32m[I 210629 05:34:44 models:110]\u001b[39m 119 1024 train loss: 0.0036223 valid loss: 0.1452871 P@1: 0.63000 P@3: 0.46333 P@5: 0.37800 N@3: 0.51224 N@5: 0.51841 early stop: 0\n",
            "\u001b[32m[I 210629 05:34:46 models:110]\u001b[39m 122 512 train loss: 0.0026244 valid loss: 0.1462852 P@1: 0.63000 P@3: 0.46333 P@5: 0.37600 N@3: 0.51224 N@5: 0.51709 early stop: 1\n",
            "\u001b[32m[I 210629 05:34:48 models:110]\u001b[39m 124 1024 train loss: 0.0017214 valid loss: 0.1474458 P@1: 0.62000 P@3: 0.46333 P@5: 0.38200 N@3: 0.51050 N@5: 0.51999 early stop: 0\n",
            "\u001b[32m[I 210629 05:34:51 models:110]\u001b[39m 127 512 train loss: 0.0013477 valid loss: 0.1484495 P@1: 0.62000 P@3: 0.46000 P@5: 0.38200 N@3: 0.50816 N@5: 0.51976 early stop: 1\n",
            "\u001b[32m[I 210629 05:34:53 models:110]\u001b[39m 129 1024 train loss: 0.0010619 valid loss: 0.1494702 P@1: 0.61000 P@3: 0.46667 P@5: 0.38000 N@3: 0.51173 N@5: 0.51764 early stop: 2\n",
            "\u001b[32m[I 210629 05:34:55 models:110]\u001b[39m 132 512 train loss: 0.0011856 valid loss: 0.1506338 P@1: 0.62000 P@3: 0.47000 P@5: 0.38000 N@3: 0.51592 N@5: 0.51859 early stop: 3\n",
            "\u001b[32m[I 210629 05:34:58 models:110]\u001b[39m 134 1024 train loss: 0.0008372 valid loss: 0.1517905 P@1: 0.62000 P@3: 0.47333 P@5: 0.38200 N@3: 0.51765 N@5: 0.51976 early stop: 4\n",
            "\u001b[32m[I 210629 05:35:00 models:110]\u001b[39m 137 512 train loss: 0.0006366 valid loss: 0.1528185 P@1: 0.63000 P@3: 0.46667 P@5: 0.38400 N@3: 0.51407 N@5: 0.52175 early stop: 0\n",
            "\u001b[32m[I 210629 05:35:02 models:110]\u001b[39m 139 1024 train loss: 0.0003130 valid loss: 0.1541222 P@1: 0.64000 P@3: 0.47000 P@5: 0.38800 N@3: 0.51876 N@5: 0.52757 early stop: 0\n",
            "\u001b[32m[I 210629 05:35:05 models:110]\u001b[39m 142 512 train loss: 0.0003273 valid loss: 0.1554224 P@1: 0.64000 P@3: 0.47000 P@5: 0.38800 N@3: 0.51876 N@5: 0.52772 early stop: 0\n",
            "\u001b[32m[I 210629 05:35:07 models:110]\u001b[39m 144 1024 train loss: 0.0003454 valid loss: 0.1567779 P@1: 0.64000 P@3: 0.47000 P@5: 0.38800 N@3: 0.51876 N@5: 0.52757 early stop: 1\n",
            "\u001b[32m[I 210629 05:35:09 models:110]\u001b[39m 147 512 train loss: 0.0001518 valid loss: 0.1581064 P@1: 0.64000 P@3: 0.47000 P@5: 0.38800 N@3: 0.51876 N@5: 0.52789 early stop: 0\n",
            "\u001b[33m[W 210629 05:35:10 models:137]\u001b[39m Clipping gradients with total norm 0.123 and max norm 0.01509\n",
            "\u001b[32m[I 210629 05:35:12 models:110]\u001b[39m 149 1024 train loss: 0.0001539 valid loss: 0.1593849 P@1: 0.65000 P@3: 0.47333 P@5: 0.38800 N@3: 0.52284 N@5: 0.53006 early stop: 0\n",
            "\u001b[32m[I 210629 05:35:14 models:110]\u001b[39m 152 512 train loss: 0.0000995 valid loss: 0.1607013 P@1: 0.65000 P@3: 0.47667 P@5: 0.39000 N@3: 0.52580 N@5: 0.53263 early stop: 0\n",
            "\u001b[32m[I 210629 05:35:16 models:110]\u001b[39m 154 1024 train loss: 0.0000686 valid loss: 0.1620512 P@1: 0.65000 P@3: 0.47333 P@5: 0.38800 N@3: 0.52346 N@5: 0.53108 early stop: 1\n",
            "\u001b[32m[I 210629 05:35:19 models:110]\u001b[39m 157 512 train loss: 0.0000538 valid loss: 0.1634135 P@1: 0.65000 P@3: 0.47333 P@5: 0.38800 N@3: 0.52407 N@5: 0.53152 early stop: 2\n",
            "\u001b[32m[I 210629 05:35:21 models:110]\u001b[39m 159 1024 train loss: 0.0000520 valid loss: 0.1647449 P@1: 0.65000 P@3: 0.47000 P@5: 0.39200 N@3: 0.52173 N@5: 0.53456 early stop: 0\n",
            "\u001b[32m[I 210629 05:35:23 models:110]\u001b[39m 162 512 train loss: 0.0000436 valid loss: 0.1660655 P@1: 0.64000 P@3: 0.47000 P@5: 0.39200 N@3: 0.51999 N@5: 0.53274 early stop: 1\n",
            "\u001b[32m[I 210629 05:35:25 models:110]\u001b[39m 164 1024 train loss: 0.0000361 valid loss: 0.1673767 P@1: 0.64000 P@3: 0.47000 P@5: 0.39200 N@3: 0.51938 N@5: 0.53230 early stop: 2\n",
            "\u001b[32m[I 210629 05:35:28 models:110]\u001b[39m 167 512 train loss: 0.0000308 valid loss: 0.1686920 P@1: 0.64000 P@3: 0.47000 P@5: 0.39000 N@3: 0.51876 N@5: 0.53028 early stop: 3\n",
            "\u001b[32m[I 210629 05:35:30 models:110]\u001b[39m 169 1024 train loss: 0.0000370 valid loss: 0.1699950 P@1: 0.64000 P@3: 0.47000 P@5: 0.39200 N@3: 0.51876 N@5: 0.53159 early stop: 4\n",
            "\u001b[32m[I 210629 05:35:32 models:110]\u001b[39m 172 512 train loss: 0.0000278 valid loss: 0.1712884 P@1: 0.64000 P@3: 0.47000 P@5: 0.39000 N@3: 0.51876 N@5: 0.53048 early stop: 5\n",
            "\u001b[33m[W 210629 05:35:33 models:137]\u001b[39m Clipping gradients with total norm 0.10106 and max norm 0.00143\n",
            "\u001b[32m[I 210629 05:35:35 models:110]\u001b[39m 174 1024 train loss: 0.0000636 valid loss: 0.1725728 P@1: 0.64000 P@3: 0.47000 P@5: 0.39000 N@3: 0.51876 N@5: 0.53048 early stop: 6\n",
            "\u001b[32m[I 210629 05:35:37 models:110]\u001b[39m 177 512 train loss: 0.0000271 valid loss: 0.1738549 P@1: 0.64000 P@3: 0.47000 P@5: 0.39200 N@3: 0.51938 N@5: 0.53226 early stop: 7\n",
            "\u001b[32m[I 210629 05:35:39 models:110]\u001b[39m 179 1024 train loss: 0.0000250 valid loss: 0.1751229 P@1: 0.64000 P@3: 0.47000 P@5: 0.39200 N@3: 0.51938 N@5: 0.53297 early stop: 8\n",
            "\u001b[32m[I 210629 05:35:42 models:110]\u001b[39m 182 512 train loss: 0.0000220 valid loss: 0.1763734 P@1: 0.64000 P@3: 0.47000 P@5: 0.39200 N@3: 0.51938 N@5: 0.53297 early stop: 9\n",
            "\u001b[32m[I 210629 05:35:44 models:110]\u001b[39m 184 1024 train loss: 0.0000240 valid loss: 0.1776137 P@1: 0.64000 P@3: 0.47000 P@5: 0.39200 N@3: 0.51938 N@5: 0.53297 early stop: 10\n",
            "\u001b[32m[I 210629 05:35:46 models:110]\u001b[39m 187 512 train loss: 0.0000201 valid loss: 0.1788453 P@1: 0.64000 P@3: 0.47000 P@5: 0.39200 N@3: 0.51938 N@5: 0.53301 early stop: 11\n",
            "\u001b[32m[I 210629 05:35:48 models:110]\u001b[39m 189 1024 train loss: 0.0000224 valid loss: 0.1800764 P@1: 0.64000 P@3: 0.47000 P@5: 0.39200 N@3: 0.51938 N@5: 0.53301 early stop: 12\n",
            "\u001b[32m[I 210629 05:35:51 models:110]\u001b[39m 192 512 train loss: 0.0000181 valid loss: 0.1813055 P@1: 0.64000 P@3: 0.47000 P@5: 0.39200 N@3: 0.51938 N@5: 0.53284 early stop: 13\n",
            "\u001b[32m[I 210629 05:35:53 models:110]\u001b[39m 194 1024 train loss: 0.0000206 valid loss: 0.1825227 P@1: 0.64000 P@3: 0.47000 P@5: 0.39200 N@3: 0.51938 N@5: 0.53284 early stop: 14\n",
            "\u001b[32m[I 210629 05:35:55 models:110]\u001b[39m 197 512 train loss: 0.0000183 valid loss: 0.1837281 P@1: 0.64000 P@3: 0.47000 P@5: 0.39200 N@3: 0.51938 N@5: 0.53284 early stop: 15\n",
            "\u001b[32m[I 210629 05:35:57 models:110]\u001b[39m 199 1024 train loss: 0.0000189 valid loss: 0.1849240 P@1: 0.64000 P@3: 0.47000 P@5: 0.38800 N@3: 0.51938 N@5: 0.53021 early stop: 16\n",
            "\u001b[32m[I 210629 05:36:00 models:110]\u001b[39m 202 512 train loss: 0.0000166 valid loss: 0.1861127 P@1: 0.64000 P@3: 0.46667 P@5: 0.38800 N@3: 0.51765 N@5: 0.53048 early stop: 17\n",
            "\u001b[32m[I 210629 05:36:02 models:110]\u001b[39m 204 1024 train loss: 0.0000159 valid loss: 0.1872957 P@1: 0.64000 P@3: 0.46667 P@5: 0.39000 N@3: 0.51765 N@5: 0.53179 early stop: 18\n",
            "\u001b[32m[I 210629 05:36:04 models:110]\u001b[39m 207 512 train loss: 0.0000155 valid loss: 0.1884626 P@1: 0.64000 P@3: 0.47000 P@5: 0.39400 N@3: 0.51999 N@5: 0.53566 early stop: 0\n",
            "\u001b[32m[I 210629 05:36:06 models:110]\u001b[39m 209 1024 train loss: 0.0000135 valid loss: 0.1896128 P@1: 0.64000 P@3: 0.47000 P@5: 0.39200 N@3: 0.51999 N@5: 0.53415 early stop: 1\n",
            "\u001b[32m[I 210629 05:36:09 models:110]\u001b[39m 212 512 train loss: 0.0000153 valid loss: 0.1907552 P@1: 0.64000 P@3: 0.47000 P@5: 0.39200 N@3: 0.51999 N@5: 0.53398 early stop: 2\n",
            "\u001b[32m[I 210629 05:36:11 models:110]\u001b[39m 214 1024 train loss: 0.0000140 valid loss: 0.1918916 P@1: 0.64000 P@3: 0.47000 P@5: 0.39000 N@3: 0.51938 N@5: 0.53131 early stop: 3\n",
            "\u001b[32m[I 210629 05:36:13 models:110]\u001b[39m 217 512 train loss: 0.0000135 valid loss: 0.1930246 P@1: 0.64000 P@3: 0.47000 P@5: 0.39000 N@3: 0.51938 N@5: 0.53131 early stop: 4\n",
            "\u001b[32m[I 210629 05:36:15 models:110]\u001b[39m 219 1024 train loss: 0.0000113 valid loss: 0.1941475 P@1: 0.64000 P@3: 0.47000 P@5: 0.39200 N@3: 0.51999 N@5: 0.53357 early stop: 5\n",
            "\u001b[33m[W 210629 05:36:16 models:137]\u001b[39m Clipping gradients with total norm 0.00604 and max norm 0.00112\n",
            "\u001b[32m[I 210629 05:36:18 models:110]\u001b[39m 222 512 train loss: 0.0000141 valid loss: 0.1952528 P@1: 0.63000 P@3: 0.46667 P@5: 0.39200 N@3: 0.51653 N@5: 0.53261 early stop: 6\n",
            "\u001b[32m[I 210629 05:36:20 models:110]\u001b[39m 224 1024 train loss: 0.0000108 valid loss: 0.1963445 P@1: 0.64000 P@3: 0.46667 P@5: 0.39200 N@3: 0.51826 N@5: 0.53390 early stop: 7\n",
            "\u001b[32m[I 210629 05:36:22 models:110]\u001b[39m 227 512 train loss: 0.0000114 valid loss: 0.1974302 P@1: 0.64000 P@3: 0.46333 P@5: 0.39200 N@3: 0.51592 N@5: 0.53366 early stop: 8\n",
            "\u001b[32m[I 210629 05:36:25 models:110]\u001b[39m 229 1024 train loss: 0.0000115 valid loss: 0.1985123 P@1: 0.64000 P@3: 0.46333 P@5: 0.39200 N@3: 0.51592 N@5: 0.53402 early stop: 9\n",
            "\u001b[32m[I 210629 05:36:27 models:110]\u001b[39m 232 512 train loss: 0.0000117 valid loss: 0.1995878 P@1: 0.64000 P@3: 0.46667 P@5: 0.39200 N@3: 0.51888 N@5: 0.53490 early stop: 10\n",
            "\u001b[32m[I 210629 05:36:29 models:110]\u001b[39m 234 1024 train loss: 0.0000120 valid loss: 0.2006437 P@1: 0.64000 P@3: 0.46667 P@5: 0.39200 N@3: 0.51888 N@5: 0.53490 early stop: 11\n",
            "\u001b[33m[W 210629 05:36:30 models:137]\u001b[39m Clipping gradients with total norm 0.01151 and max norm 0.00175\n",
            "\u001b[32m[I 210629 05:36:31 models:110]\u001b[39m 237 512 train loss: 0.0000144 valid loss: 0.2016756 P@1: 0.64000 P@3: 0.46667 P@5: 0.39200 N@3: 0.51888 N@5: 0.53507 early stop: 12\n",
            "\u001b[33m[W 210629 05:36:33 models:137]\u001b[39m Clipping gradients with total norm 0.01598 and max norm 0.00236\n",
            "\u001b[32m[I 210629 05:36:34 models:110]\u001b[39m 239 1024 train loss: 0.0000128 valid loss: 0.2026940 P@1: 0.65000 P@3: 0.46667 P@5: 0.39200 N@3: 0.52061 N@5: 0.53633 early stop: 0\n",
            "\u001b[32m[I 210629 05:36:36 models:110]\u001b[39m 242 512 train loss: 0.0000112 valid loss: 0.2037130 P@1: 0.65000 P@3: 0.46667 P@5: 0.39200 N@3: 0.51999 N@5: 0.53581 early stop: 1\n",
            "\u001b[33m[W 210629 05:36:36 models:137]\u001b[39m Clipping gradients with total norm 0.00716 and max norm 0.00121\n",
            "\u001b[32m[I 210629 05:36:38 models:110]\u001b[39m 244 1024 train loss: 0.0000128 valid loss: 0.2047159 P@1: 0.65000 P@3: 0.47000 P@5: 0.39200 N@3: 0.52234 N@5: 0.53605 early stop: 2\n",
            "\u001b[33m[W 210629 05:36:40 models:137]\u001b[39m Clipping gradients with total norm 0.00406 and max norm 0.00065\n",
            "\u001b[32m[I 210629 05:36:41 models:110]\u001b[39m 247 512 train loss: 0.0000090 valid loss: 0.2057144 P@1: 0.65000 P@3: 0.47000 P@5: 0.39200 N@3: 0.52234 N@5: 0.53605 early stop: 3\n",
            "\u001b[32m[I 210629 05:36:43 models:110]\u001b[39m 249 1024 train loss: 0.0000096 valid loss: 0.2067092 P@1: 0.65000 P@3: 0.47333 P@5: 0.39400 N@3: 0.52469 N@5: 0.53819 early stop: 0\n",
            "\u001b[32m[I 210629 05:36:45 models:110]\u001b[39m 252 512 train loss: 0.0000091 valid loss: 0.2077029 P@1: 0.65000 P@3: 0.47333 P@5: 0.39400 N@3: 0.52407 N@5: 0.53775 early stop: 1\n",
            "\u001b[32m[I 210629 05:36:47 models:110]\u001b[39m 254 1024 train loss: 0.0000088 valid loss: 0.2086879 P@1: 0.65000 P@3: 0.47667 P@5: 0.39400 N@3: 0.52642 N@5: 0.53807 early stop: 2\n",
            "\u001b[32m[I 210629 05:36:50 models:110]\u001b[39m 257 512 train loss: 0.0000074 valid loss: 0.2096617 P@1: 0.64000 P@3: 0.47667 P@5: 0.39600 N@3: 0.52469 N@5: 0.53854 early stop: 0\n",
            "\u001b[32m[I 210629 05:36:52 models:110]\u001b[39m 259 1024 train loss: 0.0000081 valid loss: 0.2106264 P@1: 0.64000 P@3: 0.47667 P@5: 0.39600 N@3: 0.52530 N@5: 0.53915 early stop: 0\n",
            "\u001b[32m[I 210629 05:36:55 models:110]\u001b[39m 262 512 train loss: 0.0000073 valid loss: 0.2115861 P@1: 0.64000 P@3: 0.47667 P@5: 0.39600 N@3: 0.52530 N@5: 0.53936 early stop: 0\n",
            "\u001b[32m[I 210629 05:36:57 models:110]\u001b[39m 264 1024 train loss: 0.0000081 valid loss: 0.2125356 P@1: 0.64000 P@3: 0.47667 P@5: 0.39600 N@3: 0.52530 N@5: 0.53936 early stop: 1\n",
            "\u001b[32m[I 210629 05:36:59 models:110]\u001b[39m 267 512 train loss: 0.0000079 valid loss: 0.2134717 P@1: 0.64000 P@3: 0.48333 P@5: 0.39600 N@3: 0.53061 N@5: 0.54053 early stop: 0\n",
            "\u001b[32m[I 210629 05:37:01 models:110]\u001b[39m 269 1024 train loss: 0.0000076 valid loss: 0.2144004 P@1: 0.64000 P@3: 0.48333 P@5: 0.39800 N@3: 0.53061 N@5: 0.54235 early stop: 0\n",
            "\u001b[32m[I 210629 05:37:04 models:110]\u001b[39m 272 512 train loss: 0.0000078 valid loss: 0.2153219 P@1: 0.64000 P@3: 0.48333 P@5: 0.39800 N@3: 0.53122 N@5: 0.54296 early stop: 0\n",
            "\u001b[32m[I 210629 05:37:06 models:110]\u001b[39m 274 1024 train loss: 0.0000074 valid loss: 0.2162338 P@1: 0.64000 P@3: 0.48667 P@5: 0.39800 N@3: 0.53357 N@5: 0.54329 early stop: 0\n",
            "\u001b[32m[I 210629 05:37:08 models:110]\u001b[39m 277 512 train loss: 0.0000070 valid loss: 0.2171370 P@1: 0.63000 P@3: 0.48667 P@5: 0.39800 N@3: 0.53184 N@5: 0.54221 early stop: 1\n",
            "\u001b[32m[I 210629 05:37:11 models:110]\u001b[39m 279 1024 train loss: 0.0000069 valid loss: 0.2180367 P@1: 0.63000 P@3: 0.48667 P@5: 0.40000 N@3: 0.53184 N@5: 0.54372 early stop: 0\n",
            "\u001b[32m[I 210629 05:37:13 models:110]\u001b[39m 282 512 train loss: 0.0000061 valid loss: 0.2189292 P@1: 0.63000 P@3: 0.48667 P@5: 0.40000 N@3: 0.53184 N@5: 0.54372 early stop: 1\n",
            "\u001b[32m[I 210629 05:37:15 models:110]\u001b[39m 284 1024 train loss: 0.0000066 valid loss: 0.2198122 P@1: 0.63000 P@3: 0.48667 P@5: 0.40000 N@3: 0.53184 N@5: 0.54372 early stop: 2\n",
            "\u001b[32m[I 210629 05:37:18 models:110]\u001b[39m 287 512 train loss: 0.0000061 valid loss: 0.2206905 P@1: 0.64000 P@3: 0.48667 P@5: 0.40000 N@3: 0.53357 N@5: 0.54516 early stop: 0\n",
            "\u001b[32m[I 210629 05:37:20 models:110]\u001b[39m 289 1024 train loss: 0.0000062 valid loss: 0.2215633 P@1: 0.64000 P@3: 0.48667 P@5: 0.40000 N@3: 0.53357 N@5: 0.54516 early stop: 1\n",
            "\u001b[32m[I 210629 05:37:22 models:110]\u001b[39m 292 512 train loss: 0.0000060 valid loss: 0.2224278 P@1: 0.64000 P@3: 0.48667 P@5: 0.40000 N@3: 0.53357 N@5: 0.54516 early stop: 2\n",
            "\u001b[32m[I 210629 05:37:25 models:110]\u001b[39m 294 1024 train loss: 0.0000066 valid loss: 0.2232862 P@1: 0.64000 P@3: 0.48667 P@5: 0.40000 N@3: 0.53357 N@5: 0.54536 early stop: 0\n",
            "\u001b[32m[I 210629 05:37:27 models:110]\u001b[39m 297 512 train loss: 0.0000053 valid loss: 0.2241406 P@1: 0.64000 P@3: 0.49000 P@5: 0.40000 N@3: 0.53592 N@5: 0.54563 early stop: 0\n",
            "\u001b[33m[W 210629 05:37:29 models:137]\u001b[39m Clipping gradients with total norm 0.00646 and max norm 0.00095\n",
            "\u001b[32m[I 210629 05:37:29 models:110]\u001b[39m 299 1024 train loss: 0.0000076 valid loss: 0.2249878 P@1: 0.64000 P@3: 0.49000 P@5: 0.39800 N@3: 0.53592 N@5: 0.54432 early stop: 1\n",
            "\u001b[32m[I 210629 05:37:32 models:110]\u001b[39m 302 512 train loss: 0.0000058 valid loss: 0.2258268 P@1: 0.64000 P@3: 0.49000 P@5: 0.39800 N@3: 0.53592 N@5: 0.54432 early stop: 2\n",
            "\u001b[32m[I 210629 05:37:34 models:110]\u001b[39m 304 1024 train loss: 0.0000061 valid loss: 0.2266619 P@1: 0.64000 P@3: 0.49000 P@5: 0.39800 N@3: 0.53592 N@5: 0.54432 early stop: 3\n",
            "\u001b[33m[W 210629 05:37:34 models:137]\u001b[39m Clipping gradients with total norm 0.00726 and max norm 0.00125\n",
            "\u001b[32m[I 210629 05:37:36 models:110]\u001b[39m 307 512 train loss: 0.0000064 valid loss: 0.2274839 P@1: 0.64000 P@3: 0.49000 P@5: 0.40000 N@3: 0.53592 N@5: 0.54583 early stop: 0\n",
            "\u001b[33m[W 210629 05:37:37 models:137]\u001b[39m Clipping gradients with total norm 0.01217 and max norm 0.00144\n",
            "\u001b[32m[I 210629 05:37:38 models:110]\u001b[39m 309 1024 train loss: 0.0000086 valid loss: 0.2282973 P@1: 0.64000 P@3: 0.49000 P@5: 0.40000 N@3: 0.53592 N@5: 0.54583 early stop: 1\n",
            "\u001b[32m[I 210629 05:37:41 models:110]\u001b[39m 312 512 train loss: 0.0000051 valid loss: 0.2291057 P@1: 0.64000 P@3: 0.49000 P@5: 0.40000 N@3: 0.53592 N@5: 0.54583 early stop: 2\n",
            "\u001b[33m[W 210629 05:37:42 models:137]\u001b[39m Clipping gradients with total norm 0.00668 and max norm 0.00055\n",
            "\u001b[32m[I 210629 05:37:43 models:110]\u001b[39m 314 1024 train loss: 0.0000067 valid loss: 0.2299041 P@1: 0.64000 P@3: 0.49000 P@5: 0.40000 N@3: 0.53592 N@5: 0.54562 early stop: 3\n",
            "\u001b[33m[W 210629 05:37:44 models:137]\u001b[39m Clipping gradients with total norm 0.00847 and max norm 0.00114\n",
            "\u001b[32m[I 210629 05:37:46 models:110]\u001b[39m 317 512 train loss: 0.0000059 valid loss: 0.2307023 P@1: 0.64000 P@3: 0.49000 P@5: 0.40000 N@3: 0.53592 N@5: 0.54562 early stop: 4\n",
            "\u001b[33m[W 210629 05:37:46 models:137]\u001b[39m Clipping gradients with total norm 0.00708 and max norm 0.00041\n",
            "\u001b[32m[I 210629 05:37:48 models:110]\u001b[39m 319 1024 train loss: 0.0000071 valid loss: 0.2314960 P@1: 0.64000 P@3: 0.49000 P@5: 0.39800 N@3: 0.53592 N@5: 0.54381 early stop: 5\n",
            "\u001b[32m[I 210629 05:37:50 models:110]\u001b[39m 322 512 train loss: 0.0000058 valid loss: 0.2322742 P@1: 0.64000 P@3: 0.49000 P@5: 0.39800 N@3: 0.53592 N@5: 0.54381 early stop: 6\n",
            "\u001b[33m[W 210629 05:37:51 models:137]\u001b[39m Clipping gradients with total norm 0.00456 and max norm 0.00078\n",
            "\u001b[33m[W 210629 05:37:52 models:137]\u001b[39m Clipping gradients with total norm 0.01094 and max norm 0.00156\n",
            "\u001b[32m[I 210629 05:37:52 models:110]\u001b[39m 324 1024 train loss: 0.0000070 valid loss: 0.2330459 P@1: 0.64000 P@3: 0.49000 P@5: 0.39800 N@3: 0.53592 N@5: 0.54381 early stop: 7\n",
            "\u001b[32m[I 210629 05:37:55 models:110]\u001b[39m 327 512 train loss: 0.0000044 valid loss: 0.2338158 P@1: 0.64000 P@3: 0.49000 P@5: 0.39800 N@3: 0.53592 N@5: 0.54381 early stop: 8\n",
            "\u001b[32m[I 210629 05:37:57 models:110]\u001b[39m 329 1024 train loss: 0.0000045 valid loss: 0.2345793 P@1: 0.64000 P@3: 0.49000 P@5: 0.39800 N@3: 0.53592 N@5: 0.54381 early stop: 9\n",
            "\u001b[32m[I 210629 05:37:59 models:110]\u001b[39m 332 512 train loss: 0.0000055 valid loss: 0.2353373 P@1: 0.64000 P@3: 0.49000 P@5: 0.39800 N@3: 0.53592 N@5: 0.54381 early stop: 10\n",
            "\u001b[33m[W 210629 05:38:01 models:137]\u001b[39m Clipping gradients with total norm 0.00302 and max norm 0.00035\n",
            "\u001b[32m[I 210629 05:38:01 models:110]\u001b[39m 334 1024 train loss: 0.0000051 valid loss: 0.2360898 P@1: 0.64000 P@3: 0.49000 P@5: 0.39800 N@3: 0.53592 N@5: 0.54381 early stop: 11\n",
            "\u001b[32m[I 210629 05:38:04 models:110]\u001b[39m 337 512 train loss: 0.0000050 valid loss: 0.2368382 P@1: 0.64000 P@3: 0.49000 P@5: 0.39800 N@3: 0.53592 N@5: 0.54402 early stop: 12\n",
            "\u001b[32m[I 210629 05:38:06 models:110]\u001b[39m 339 1024 train loss: 0.0000040 valid loss: 0.2375838 P@1: 0.64000 P@3: 0.49000 P@5: 0.39800 N@3: 0.53592 N@5: 0.54422 early stop: 13\n",
            "\u001b[32m[I 210629 05:38:08 models:110]\u001b[39m 342 512 train loss: 0.0000052 valid loss: 0.2383347 P@1: 0.64000 P@3: 0.49000 P@5: 0.39800 N@3: 0.53592 N@5: 0.54422 early stop: 14\n",
            "\u001b[32m[I 210629 05:38:11 models:110]\u001b[39m 344 1024 train loss: 0.0000035 valid loss: 0.2390807 P@1: 0.64000 P@3: 0.49000 P@5: 0.39800 N@3: 0.53592 N@5: 0.54422 early stop: 15\n",
            "\u001b[32m[I 210629 05:38:13 models:110]\u001b[39m 347 512 train loss: 0.0000044 valid loss: 0.2398200 P@1: 0.64000 P@3: 0.49000 P@5: 0.39800 N@3: 0.53592 N@5: 0.54419 early stop: 16\n",
            "\u001b[32m[I 210629 05:38:15 models:110]\u001b[39m 349 1024 train loss: 0.0000050 valid loss: 0.2405473 P@1: 0.64000 P@3: 0.49000 P@5: 0.39600 N@3: 0.53592 N@5: 0.54268 early stop: 17\n",
            "\u001b[32m[I 210629 05:38:18 models:110]\u001b[39m 352 512 train loss: 0.0000040 valid loss: 0.2412649 P@1: 0.64000 P@3: 0.49000 P@5: 0.39600 N@3: 0.53592 N@5: 0.54268 early stop: 18\n",
            "\u001b[32m[I 210629 05:38:20 models:110]\u001b[39m 354 1024 train loss: 0.0000038 valid loss: 0.2419745 P@1: 0.63000 P@3: 0.49000 P@5: 0.39600 N@3: 0.53357 N@5: 0.54066 early stop: 19\n",
            "\u001b[32m[I 210629 05:38:22 models:110]\u001b[39m 357 512 train loss: 0.0000038 valid loss: 0.2426776 P@1: 0.63000 P@3: 0.49000 P@5: 0.39600 N@3: 0.53357 N@5: 0.54066 early stop: 20\n",
            "\u001b[32m[I 210629 05:38:24 models:110]\u001b[39m 359 1024 train loss: 0.0000046 valid loss: 0.2433804 P@1: 0.63000 P@3: 0.49000 P@5: 0.39600 N@3: 0.53357 N@5: 0.54066 early stop: 21\n",
            "\u001b[32m[I 210629 05:38:27 models:110]\u001b[39m 362 512 train loss: 0.0000041 valid loss: 0.2440786 P@1: 0.63000 P@3: 0.49000 P@5: 0.39600 N@3: 0.53357 N@5: 0.54066 early stop: 22\n",
            "\u001b[32m[I 210629 05:38:29 models:110]\u001b[39m 364 1024 train loss: 0.0000036 valid loss: 0.2447734 P@1: 0.63000 P@3: 0.49000 P@5: 0.39600 N@3: 0.53357 N@5: 0.54066 early stop: 23\n",
            "\u001b[32m[I 210629 05:38:31 models:110]\u001b[39m 367 512 train loss: 0.0000035 valid loss: 0.2454619 P@1: 0.63000 P@3: 0.49000 P@5: 0.39600 N@3: 0.53357 N@5: 0.54066 early stop: 24\n",
            "\u001b[32m[I 210629 05:38:33 models:110]\u001b[39m 369 1024 train loss: 0.0000033 valid loss: 0.2461429 P@1: 0.63000 P@3: 0.49333 P@5: 0.39600 N@3: 0.53592 N@5: 0.54113 early stop: 25\n",
            "\u001b[32m[I 210629 05:38:36 models:110]\u001b[39m 372 512 train loss: 0.0000034 valid loss: 0.2468176 P@1: 0.63000 P@3: 0.49333 P@5: 0.39600 N@3: 0.53653 N@5: 0.54174 early stop: 26\n",
            "\u001b[32m[I 210629 05:38:38 models:110]\u001b[39m 374 1024 train loss: 0.0000031 valid loss: 0.2474919 P@1: 0.63000 P@3: 0.49333 P@5: 0.39600 N@3: 0.53653 N@5: 0.54174 early stop: 27\n",
            "\u001b[32m[I 210629 05:38:40 models:110]\u001b[39m 377 512 train loss: 0.0000033 valid loss: 0.2481646 P@1: 0.63000 P@3: 0.49000 P@5: 0.39600 N@3: 0.53418 N@5: 0.54151 early stop: 28\n",
            "\u001b[32m[I 210629 05:38:42 models:110]\u001b[39m 379 1024 train loss: 0.0000041 valid loss: 0.2488347 P@1: 0.63000 P@3: 0.49000 P@5: 0.39600 N@3: 0.53418 N@5: 0.54151 early stop: 29\n",
            "\u001b[32m[I 210629 05:38:45 models:110]\u001b[39m 382 512 train loss: 0.0000031 valid loss: 0.2495008 P@1: 0.63000 P@3: 0.49000 P@5: 0.39600 N@3: 0.53418 N@5: 0.54151 early stop: 30\n",
            "\u001b[32m[I 210629 05:38:47 models:110]\u001b[39m 384 1024 train loss: 0.0000030 valid loss: 0.2501620 P@1: 0.63000 P@3: 0.49000 P@5: 0.39600 N@3: 0.53418 N@5: 0.54151 early stop: 31\n",
            "\u001b[32m[I 210629 05:38:49 models:110]\u001b[39m 387 512 train loss: 0.0000030 valid loss: 0.2508179 P@1: 0.63000 P@3: 0.49000 P@5: 0.39600 N@3: 0.53418 N@5: 0.54151 early stop: 32\n",
            "\u001b[32m[I 210629 05:38:52 models:110]\u001b[39m 389 1024 train loss: 0.0000037 valid loss: 0.2514711 P@1: 0.63000 P@3: 0.49000 P@5: 0.39800 N@3: 0.53418 N@5: 0.54282 early stop: 33\n",
            "\u001b[32m[I 210629 05:38:54 models:110]\u001b[39m 392 512 train loss: 0.0000030 valid loss: 0.2521212 P@1: 0.63000 P@3: 0.49000 P@5: 0.39800 N@3: 0.53418 N@5: 0.54282 early stop: 34\n",
            "\u001b[32m[I 210629 05:38:56 models:110]\u001b[39m 394 1024 train loss: 0.0000035 valid loss: 0.2527679 P@1: 0.63000 P@3: 0.49000 P@5: 0.39800 N@3: 0.53418 N@5: 0.54282 early stop: 35\n",
            "\u001b[32m[I 210629 05:38:59 models:110]\u001b[39m 397 512 train loss: 0.0000026 valid loss: 0.2534113 P@1: 0.63000 P@3: 0.49000 P@5: 0.39800 N@3: 0.53418 N@5: 0.54282 early stop: 36\n",
            "\u001b[32m[I 210629 05:39:01 models:110]\u001b[39m 399 1024 train loss: 0.0000029 valid loss: 0.2540514 P@1: 0.63000 P@3: 0.49000 P@5: 0.39800 N@3: 0.53418 N@5: 0.54282 early stop: 37\n",
            "\u001b[32m[I 210629 05:39:03 models:110]\u001b[39m 402 512 train loss: 0.0000027 valid loss: 0.2546866 P@1: 0.63000 P@3: 0.49000 P@5: 0.40200 N@3: 0.53418 N@5: 0.54595 early stop: 0\n",
            "\u001b[32m[I 210629 05:39:05 models:110]\u001b[39m 404 1024 train loss: 0.0000027 valid loss: 0.2553154 P@1: 0.63000 P@3: 0.49000 P@5: 0.40200 N@3: 0.53418 N@5: 0.54580 early stop: 1\n",
            "\u001b[32m[I 210629 05:39:08 models:110]\u001b[39m 407 512 train loss: 0.0000025 valid loss: 0.2559397 P@1: 0.63000 P@3: 0.49000 P@5: 0.40200 N@3: 0.53418 N@5: 0.54580 early stop: 2\n",
            "\u001b[32m[I 210629 05:39:10 models:110]\u001b[39m 409 1024 train loss: 0.0000029 valid loss: 0.2565628 P@1: 0.63000 P@3: 0.49000 P@5: 0.40200 N@3: 0.53418 N@5: 0.54580 early stop: 3\n",
            "\u001b[32m[I 210629 05:39:12 models:110]\u001b[39m 412 512 train loss: 0.0000024 valid loss: 0.2571831 P@1: 0.63000 P@3: 0.49333 P@5: 0.40200 N@3: 0.53653 N@5: 0.54613 early stop: 0\n",
            "\u001b[32m[I 210629 05:39:15 models:110]\u001b[39m 414 1024 train loss: 0.0000029 valid loss: 0.2577946 P@1: 0.63000 P@3: 0.49333 P@5: 0.40200 N@3: 0.53653 N@5: 0.54613 early stop: 1\n",
            "\u001b[32m[I 210629 05:39:18 models:110]\u001b[39m 417 512 train loss: 0.0000028 valid loss: 0.2584023 P@1: 0.63000 P@3: 0.49333 P@5: 0.40200 N@3: 0.53653 N@5: 0.54613 early stop: 2\n",
            "\u001b[33m[W 210629 05:39:18 models:137]\u001b[39m Clipping gradients with total norm 0.00134 and max norm 0.00027\n",
            "\u001b[33m[W 210629 05:39:20 models:137]\u001b[39m Clipping gradients with total norm 0.01708 and max norm 0.00027\n",
            "\u001b[32m[I 210629 05:39:20 models:110]\u001b[39m 419 1024 train loss: 0.0000054 valid loss: 0.2590069 P@1: 0.63000 P@3: 0.49333 P@5: 0.40200 N@3: 0.53653 N@5: 0.54627 early stop: 0\n",
            "\u001b[32m[I 210629 05:39:23 models:110]\u001b[39m 422 512 train loss: 0.0000025 valid loss: 0.2596078 P@1: 0.63000 P@3: 0.49667 P@5: 0.40200 N@3: 0.53888 N@5: 0.54639 early stop: 0\n",
            "\u001b[32m[I 210629 05:39:25 models:110]\u001b[39m 424 1024 train loss: 0.0000022 valid loss: 0.2602067 P@1: 0.63000 P@3: 0.49667 P@5: 0.40200 N@3: 0.53888 N@5: 0.54639 early stop: 1\n",
            "\u001b[32m[I 210629 05:39:27 models:110]\u001b[39m 427 512 train loss: 0.0000023 valid loss: 0.2608032 P@1: 0.63000 P@3: 0.49667 P@5: 0.40200 N@3: 0.53888 N@5: 0.54639 early stop: 2\n",
            "\u001b[32m[I 210629 05:39:29 models:110]\u001b[39m 429 1024 train loss: 0.0000024 valid loss: 0.2613961 P@1: 0.63000 P@3: 0.49667 P@5: 0.40200 N@3: 0.53888 N@5: 0.54639 early stop: 3\n",
            "\u001b[32m[I 210629 05:39:32 models:110]\u001b[39m 432 512 train loss: 0.0000028 valid loss: 0.2619857 P@1: 0.63000 P@3: 0.49667 P@5: 0.40400 N@3: 0.53888 N@5: 0.54821 early stop: 0\n",
            "\u001b[32m[I 210629 05:39:34 models:110]\u001b[39m 434 1024 train loss: 0.0000022 valid loss: 0.2625695 P@1: 0.63000 P@3: 0.49667 P@5: 0.40400 N@3: 0.53888 N@5: 0.54821 early stop: 1\n",
            "\u001b[32m[I 210629 05:39:37 models:110]\u001b[39m 437 512 train loss: 0.0000033 valid loss: 0.2631502 P@1: 0.63000 P@3: 0.49667 P@5: 0.40400 N@3: 0.53888 N@5: 0.54821 early stop: 2\n",
            "\u001b[33m[W 210629 05:39:38 models:137]\u001b[39m Clipping gradients with total norm 0.00515 and max norm 0.00032\n",
            "\u001b[32m[I 210629 05:39:39 models:110]\u001b[39m 439 1024 train loss: 0.0000028 valid loss: 0.2637309 P@1: 0.63000 P@3: 0.49667 P@5: 0.40400 N@3: 0.53888 N@5: 0.54821 early stop: 3\n",
            "\u001b[32m[I 210629 05:39:41 models:110]\u001b[39m 442 512 train loss: 0.0000025 valid loss: 0.2643087 P@1: 0.63000 P@3: 0.49667 P@5: 0.40400 N@3: 0.53888 N@5: 0.54821 early stop: 4\n",
            "\u001b[33m[W 210629 05:39:43 models:137]\u001b[39m Clipping gradients with total norm 0.01955 and max norm 0.00015\n",
            "\u001b[32m[I 210629 05:39:43 models:110]\u001b[39m 444 1024 train loss: 0.0000053 valid loss: 0.2648799 P@1: 0.63000 P@3: 0.49667 P@5: 0.40400 N@3: 0.53888 N@5: 0.54821 early stop: 5\n",
            "\u001b[32m[I 210629 05:39:46 models:110]\u001b[39m 447 512 train loss: 0.0000027 valid loss: 0.2654429 P@1: 0.63000 P@3: 0.49667 P@5: 0.40400 N@3: 0.53888 N@5: 0.54821 early stop: 6\n",
            "\u001b[32m[I 210629 05:39:48 models:110]\u001b[39m 449 1024 train loss: 0.0000021 valid loss: 0.2660019 P@1: 0.63000 P@3: 0.49667 P@5: 0.40400 N@3: 0.53888 N@5: 0.54821 early stop: 7\n",
            "\u001b[32m[I 210629 05:39:50 models:110]\u001b[39m 452 512 train loss: 0.0000026 valid loss: 0.2665585 P@1: 0.63000 P@3: 0.49667 P@5: 0.40400 N@3: 0.53888 N@5: 0.54821 early stop: 8\n",
            "\u001b[32m[I 210629 05:39:52 models:110]\u001b[39m 454 1024 train loss: 0.0000025 valid loss: 0.2671138 P@1: 0.63000 P@3: 0.49667 P@5: 0.40400 N@3: 0.53888 N@5: 0.54821 early stop: 9\n",
            "\u001b[32m[I 210629 05:39:55 models:110]\u001b[39m 457 512 train loss: 0.0000021 valid loss: 0.2676681 P@1: 0.63000 P@3: 0.49667 P@5: 0.40400 N@3: 0.53888 N@5: 0.54821 early stop: 10\n",
            "\u001b[33m[W 210629 05:39:55 models:137]\u001b[39m Clipping gradients with total norm 0.00161 and max norm 0.00032\n",
            "\u001b[32m[I 210629 05:39:57 models:110]\u001b[39m 459 1024 train loss: 0.0000026 valid loss: 0.2682170 P@1: 0.63000 P@3: 0.49667 P@5: 0.40400 N@3: 0.53888 N@5: 0.54821 early stop: 11\n",
            "\u001b[32m[I 210629 05:39:59 models:110]\u001b[39m 462 512 train loss: 0.0000026 valid loss: 0.2687602 P@1: 0.63000 P@3: 0.49667 P@5: 0.40400 N@3: 0.53888 N@5: 0.54821 early stop: 12\n",
            "\u001b[32m[I 210629 05:40:01 models:110]\u001b[39m 464 1024 train loss: 0.0000024 valid loss: 0.2692967 P@1: 0.63000 P@3: 0.49667 P@5: 0.40400 N@3: 0.53888 N@5: 0.54821 early stop: 13\n",
            "\u001b[32m[I 210629 05:40:04 models:110]\u001b[39m 467 512 train loss: 0.0000025 valid loss: 0.2698271 P@1: 0.62000 P@3: 0.49667 P@5: 0.40400 N@3: 0.53714 N@5: 0.54696 early stop: 14\n",
            "\u001b[32m[I 210629 05:40:06 models:110]\u001b[39m 469 1024 train loss: 0.0000024 valid loss: 0.2703578 P@1: 0.62000 P@3: 0.49667 P@5: 0.40400 N@3: 0.53714 N@5: 0.54696 early stop: 15\n",
            "\u001b[32m[I 210629 05:40:08 models:110]\u001b[39m 472 512 train loss: 0.0000024 valid loss: 0.2708879 P@1: 0.62000 P@3: 0.49667 P@5: 0.40400 N@3: 0.53714 N@5: 0.54696 early stop: 16\n",
            "\u001b[32m[I 210629 05:40:10 models:110]\u001b[39m 474 1024 train loss: 0.0000020 valid loss: 0.2714165 P@1: 0.62000 P@3: 0.49667 P@5: 0.40400 N@3: 0.53714 N@5: 0.54696 early stop: 17\n",
            "\u001b[32m[I 210629 05:40:13 models:110]\u001b[39m 477 512 train loss: 0.0000022 valid loss: 0.2719435 P@1: 0.62000 P@3: 0.49667 P@5: 0.40400 N@3: 0.53714 N@5: 0.54696 early stop: 18\n",
            "\u001b[32m[I 210629 05:40:15 models:110]\u001b[39m 479 1024 train loss: 0.0000018 valid loss: 0.2724680 P@1: 0.62000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53714 N@5: 0.54827 early stop: 0\n",
            "\u001b[32m[I 210629 05:40:17 models:110]\u001b[39m 482 512 train loss: 0.0000021 valid loss: 0.2729890 P@1: 0.62000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53714 N@5: 0.54827 early stop: 1\n",
            "\u001b[32m[I 210629 05:40:20 models:110]\u001b[39m 484 1024 train loss: 0.0000024 valid loss: 0.2735116 P@1: 0.62000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53714 N@5: 0.54827 early stop: 2\n",
            "\u001b[32m[I 210629 05:40:22 models:110]\u001b[39m 487 512 train loss: 0.0000020 valid loss: 0.2740330 P@1: 0.62000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53714 N@5: 0.54827 early stop: 3\n",
            "\u001b[32m[I 210629 05:40:24 models:110]\u001b[39m 489 1024 train loss: 0.0000019 valid loss: 0.2745523 P@1: 0.62000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53714 N@5: 0.54827 early stop: 4\n",
            "\u001b[32m[I 210629 05:40:27 models:110]\u001b[39m 492 512 train loss: 0.0000021 valid loss: 0.2750675 P@1: 0.62000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53714 N@5: 0.54827 early stop: 5\n",
            "\u001b[32m[I 210629 05:40:29 models:110]\u001b[39m 494 1024 train loss: 0.0000020 valid loss: 0.2755799 P@1: 0.62000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53714 N@5: 0.54827 early stop: 6\n",
            "\u001b[32m[I 210629 05:40:31 models:110]\u001b[39m 497 512 train loss: 0.0000021 valid loss: 0.2760895 P@1: 0.62000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53714 N@5: 0.54827 early stop: 7\n",
            "\u001b[32m[I 210629 05:40:33 models:110]\u001b[39m 499 1024 train loss: 0.0000019 valid loss: 0.2766022 P@1: 0.62000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53714 N@5: 0.54812 early stop: 8\n",
            "\u001b[32m[I 210629 05:40:36 models:110]\u001b[39m 502 512 train loss: 0.0000016 valid loss: 0.2771147 P@1: 0.62000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53714 N@5: 0.54812 early stop: 9\n",
            "\u001b[33m[W 210629 05:40:37 models:137]\u001b[39m Clipping gradients with total norm 0.00281 and max norm 0.00038\n",
            "\u001b[32m[I 210629 05:40:38 models:110]\u001b[39m 504 1024 train loss: 0.0000019 valid loss: 0.2776261 P@1: 0.62000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53714 N@5: 0.54812 early stop: 10\n",
            "\u001b[32m[I 210629 05:40:40 models:110]\u001b[39m 507 512 train loss: 0.0000016 valid loss: 0.2781354 P@1: 0.62000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53714 N@5: 0.54812 early stop: 11\n",
            "\u001b[32m[I 210629 05:40:42 models:110]\u001b[39m 509 1024 train loss: 0.0000018 valid loss: 0.2786424 P@1: 0.62000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53714 N@5: 0.54812 early stop: 12\n",
            "\u001b[33m[W 210629 05:40:45 models:137]\u001b[39m Clipping gradients with total norm 0.00231 and max norm 0.00028\n",
            "\u001b[32m[I 210629 05:40:45 models:110]\u001b[39m 512 512 train loss: 0.0000018 valid loss: 0.2791475 P@1: 0.62000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53714 N@5: 0.54812 early stop: 13\n",
            "\u001b[32m[I 210629 05:40:47 models:110]\u001b[39m 514 1024 train loss: 0.0000015 valid loss: 0.2796454 P@1: 0.62000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53714 N@5: 0.54812 early stop: 14\n",
            "\u001b[32m[I 210629 05:40:50 models:110]\u001b[39m 517 512 train loss: 0.0000025 valid loss: 0.2801435 P@1: 0.62000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53714 N@5: 0.54812 early stop: 15\n",
            "\u001b[32m[I 210629 05:40:52 models:110]\u001b[39m 519 1024 train loss: 0.0000016 valid loss: 0.2806420 P@1: 0.62000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53714 N@5: 0.54812 early stop: 16\n",
            "\u001b[32m[I 210629 05:40:54 models:110]\u001b[39m 522 512 train loss: 0.0000020 valid loss: 0.2811356 P@1: 0.62000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53714 N@5: 0.54812 early stop: 17\n",
            "\u001b[33m[W 210629 05:40:55 models:137]\u001b[39m Clipping gradients with total norm 0.01897 and max norm 0.00046\n",
            "\u001b[32m[I 210629 05:40:56 models:110]\u001b[39m 524 1024 train loss: 0.0000095 valid loss: 0.2816263 P@1: 0.62000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53714 N@5: 0.54812 early stop: 18\n",
            "\u001b[32m[I 210629 05:40:59 models:110]\u001b[39m 527 512 train loss: 0.0000019 valid loss: 0.2821158 P@1: 0.62000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53714 N@5: 0.54812 early stop: 19\n",
            "\u001b[33m[W 210629 05:40:59 models:137]\u001b[39m Clipping gradients with total norm 0.01654 and max norm 0.00021\n",
            "\u001b[32m[I 210629 05:41:01 models:110]\u001b[39m 529 1024 train loss: 0.0000044 valid loss: 0.2826032 P@1: 0.62000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53714 N@5: 0.54812 early stop: 20\n",
            "\u001b[32m[I 210629 05:41:03 models:110]\u001b[39m 532 512 train loss: 0.0000018 valid loss: 0.2830851 P@1: 0.62000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53714 N@5: 0.54812 early stop: 21\n",
            "\u001b[32m[I 210629 05:41:05 models:110]\u001b[39m 534 1024 train loss: 0.0000019 valid loss: 0.2835612 P@1: 0.62000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53714 N@5: 0.54812 early stop: 22\n",
            "\u001b[32m[I 210629 05:41:08 models:110]\u001b[39m 537 512 train loss: 0.0000018 valid loss: 0.2840326 P@1: 0.62000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53714 N@5: 0.54812 early stop: 23\n",
            "\u001b[32m[I 210629 05:41:10 models:110]\u001b[39m 539 1024 train loss: 0.0000019 valid loss: 0.2845089 P@1: 0.62000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53653 N@5: 0.54768 early stop: 24\n",
            "\u001b[32m[I 210629 05:41:12 models:110]\u001b[39m 542 512 train loss: 0.0000019 valid loss: 0.2849858 P@1: 0.62000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53653 N@5: 0.54768 early stop: 25\n",
            "\u001b[32m[I 210629 05:41:14 models:110]\u001b[39m 544 1024 train loss: 0.0000013 valid loss: 0.2854609 P@1: 0.62000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53653 N@5: 0.54768 early stop: 26\n",
            "\u001b[32m[I 210629 05:41:17 models:110]\u001b[39m 547 512 train loss: 0.0000016 valid loss: 0.2859322 P@1: 0.62000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53714 N@5: 0.54819 early stop: 27\n",
            "\u001b[33m[W 210629 05:41:19 models:137]\u001b[39m Clipping gradients with total norm 0.00173 and max norm 0.00019\n",
            "\u001b[32m[I 210629 05:41:19 models:110]\u001b[39m 549 1024 train loss: 0.0000015 valid loss: 0.2864017 P@1: 0.62000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53714 N@5: 0.54819 early stop: 28\n",
            "\u001b[32m[I 210629 05:41:21 models:110]\u001b[39m 552 512 train loss: 0.0000014 valid loss: 0.2868698 P@1: 0.62000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53714 N@5: 0.54819 early stop: 29\n",
            "\u001b[32m[I 210629 05:41:24 models:110]\u001b[39m 554 1024 train loss: 0.0000015 valid loss: 0.2873348 P@1: 0.61000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53541 N@5: 0.54646 early stop: 30\n",
            "\u001b[32m[I 210629 05:41:26 models:110]\u001b[39m 557 512 train loss: 0.0000013 valid loss: 0.2877969 P@1: 0.61000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53541 N@5: 0.54646 early stop: 31\n",
            "\u001b[32m[I 210629 05:41:28 models:110]\u001b[39m 559 1024 train loss: 0.0000012 valid loss: 0.2882580 P@1: 0.61000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53541 N@5: 0.54646 early stop: 32\n",
            "\u001b[32m[I 210629 05:41:30 models:110]\u001b[39m 562 512 train loss: 0.0000016 valid loss: 0.2887199 P@1: 0.61000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53541 N@5: 0.54646 early stop: 33\n",
            "\u001b[33m[W 210629 05:41:31 models:137]\u001b[39m Clipping gradients with total norm 0.00166 and max norm 0.00013\n",
            "\u001b[32m[I 210629 05:41:33 models:110]\u001b[39m 564 1024 train loss: 0.0000015 valid loss: 0.2891825 P@1: 0.61000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53541 N@5: 0.54646 early stop: 34\n",
            "\u001b[33m[W 210629 05:41:33 models:137]\u001b[39m Clipping gradients with total norm 0.00118 and max norm 0.00016\n",
            "\u001b[32m[I 210629 05:41:35 models:110]\u001b[39m 567 512 train loss: 0.0000016 valid loss: 0.2896429 P@1: 0.61000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53541 N@5: 0.54646 early stop: 35\n",
            "\u001b[33m[W 210629 05:41:35 models:137]\u001b[39m Clipping gradients with total norm 0.00356 and max norm 0.00019\n",
            "\u001b[32m[I 210629 05:41:37 models:110]\u001b[39m 569 1024 train loss: 0.0000016 valid loss: 0.2901046 P@1: 0.61000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53541 N@5: 0.54646 early stop: 36\n",
            "\u001b[32m[I 210629 05:41:40 models:110]\u001b[39m 572 512 train loss: 0.0000013 valid loss: 0.2905629 P@1: 0.61000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53541 N@5: 0.54646 early stop: 37\n",
            "\u001b[32m[I 210629 05:41:42 models:110]\u001b[39m 574 1024 train loss: 0.0000018 valid loss: 0.2910175 P@1: 0.61000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53541 N@5: 0.54646 early stop: 38\n",
            "\u001b[32m[I 210629 05:41:44 models:110]\u001b[39m 577 512 train loss: 0.0000014 valid loss: 0.2914675 P@1: 0.61000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53541 N@5: 0.54646 early stop: 39\n",
            "\u001b[32m[I 210629 05:41:46 models:110]\u001b[39m 579 1024 train loss: 0.0000013 valid loss: 0.2919146 P@1: 0.61000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53541 N@5: 0.54646 early stop: 40\n",
            "\u001b[32m[I 210629 05:41:49 models:110]\u001b[39m 582 512 train loss: 0.0000012 valid loss: 0.2923571 P@1: 0.61000 P@3: 0.49667 P@5: 0.40600 N@3: 0.53541 N@5: 0.54646 early stop: 41\n",
            "\u001b[32m[I 210629 05:41:51 models:110]\u001b[39m 584 1024 train loss: 0.0000011 valid loss: 0.2927962 P@1: 0.61000 P@3: 0.50000 P@5: 0.40600 N@3: 0.53776 N@5: 0.54669 early stop: 42\n",
            "\u001b[33m[W 210629 05:41:52 models:137]\u001b[39m Clipping gradients with total norm 0.00491 and max norm 0.00024\n",
            "\u001b[32m[I 210629 05:41:53 models:110]\u001b[39m 587 512 train loss: 0.0000024 valid loss: 0.2932318 P@1: 0.61000 P@3: 0.50000 P@5: 0.40600 N@3: 0.53776 N@5: 0.54669 early stop: 43\n",
            "\u001b[32m[I 210629 05:41:55 models:110]\u001b[39m 589 1024 train loss: 0.0000014 valid loss: 0.2936621 P@1: 0.61000 P@3: 0.50000 P@5: 0.40600 N@3: 0.53776 N@5: 0.54669 early stop: 44\n",
            "\u001b[33m[W 210629 05:41:56 models:137]\u001b[39m Clipping gradients with total norm 0.00669 and max norm 0.00028\n",
            "\u001b[32m[I 210629 05:41:58 models:110]\u001b[39m 592 512 train loss: 0.0000026 valid loss: 0.2940904 P@1: 0.61000 P@3: 0.50000 P@5: 0.40600 N@3: 0.53776 N@5: 0.54669 early stop: 45\n",
            "\u001b[32m[I 210629 05:42:00 models:110]\u001b[39m 594 1024 train loss: 0.0000011 valid loss: 0.2945167 P@1: 0.61000 P@3: 0.50000 P@5: 0.40600 N@3: 0.53776 N@5: 0.54654 early stop: 46\n",
            "\u001b[32m[I 210629 05:42:02 models:110]\u001b[39m 597 512 train loss: 0.0000012 valid loss: 0.2949421 P@1: 0.61000 P@3: 0.50000 P@5: 0.40600 N@3: 0.53776 N@5: 0.54654 early stop: 47\n",
            "\u001b[32m[I 210629 05:42:04 models:110]\u001b[39m 599 1024 train loss: 0.0000013 valid loss: 0.2953669 P@1: 0.61000 P@3: 0.50000 P@5: 0.40600 N@3: 0.53776 N@5: 0.54654 early stop: 48\n",
            "\u001b[32m[I 210629 05:42:07 models:110]\u001b[39m 602 512 train loss: 0.0000015 valid loss: 0.2957934 P@1: 0.61000 P@3: 0.50000 P@5: 0.40600 N@3: 0.53776 N@5: 0.54654 early stop: 49\n",
            "\u001b[32m[I 210629 05:42:09 models:110]\u001b[39m 604 1024 train loss: 0.0000014 valid loss: 0.2962168 P@1: 0.61000 P@3: 0.50000 P@5: 0.40600 N@3: 0.53776 N@5: 0.54654 early stop: 50\n",
            "\u001b[32m[I 210629 05:42:11 main:76]\u001b[39m Finish Training\n",
            "\u001b[32m[I 210629 05:42:13 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210629 05:42:13 main:79]\u001b[39m Loading Test Set\n",
            "\u001b[32m[I 210629 05:42:13 main:83]\u001b[39m Size of Test Set: 100\n",
            "\u001b[32m[I 210629 05:42:13 main:85]\u001b[39m Predicting\n",
            "\u001b[32m[I 210629 05:42:17 main:91]\u001b[39m Finish Predicting\n",
            "Precision@1,3,5: 0.61 0.5166666666666667 0.414\n",
            "nDCG@1,3,5: 0.61 0.5470391808903413 0.5414032137757637\n",
            "\n",
            "NO MAG NO MESH, skip=700\n",
            "\n",
            "/content/drive/Shareddrives/NASA/MATCH/PeTaL\n",
            "129\n",
            "/content/drive/Shareddrives/NASA/MATCH\n",
            "    800  275675 4658257 PeTaL/train.json\n",
            "\u001b[32m[I 210629 05:42:20 preprocess:28]\u001b[39m Vocab Size: 26834\n",
            "\u001b[32m[I 210629 05:42:20 preprocess:30]\u001b[39m Getting Dataset: PeTaL/train_texts.txt Max Length: 500\n",
            "\u001b[32m[I 210629 05:42:20 preprocess:32]\u001b[39m Size of Samples: 900\n",
            "\u001b[32m[I 210629 05:42:21 preprocess:28]\u001b[39m Vocab Size: 26834\n",
            "\u001b[32m[I 210629 05:42:21 preprocess:30]\u001b[39m Getting Dataset: PeTaL/test_texts.txt Max Length: 500\n",
            "\u001b[32m[I 210629 05:42:21 preprocess:32]\u001b[39m Size of Samples: 100\n",
            "\u001b[32m[I 210629 05:42:23 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210629 05:42:23 main:35]\u001b[39m Loading Training and Validation Set\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['absorb_and/or_filter_solids', 'chemically_break_down_inorganic_compounds', 'detox/purify', 'manage_environmental_disturbances_in_a_community', 'protect_from_fire', 'protect_from_gases', 'send_vibratory_signals'] will be ignored\n",
            "  .format(sorted(unknown, key=str)))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['chemically_break_down_inorganic_compounds', 'detox/purify'] will be ignored\n",
            "  .format(sorted(unknown, key=str)))\n",
            "\u001b[32m[I 210629 05:42:23 main:47]\u001b[39m Number of Labels: 124\n",
            "\u001b[32m[I 210629 05:42:23 main:48]\u001b[39m Size of Training Set: 800\n",
            "\u001b[32m[I 210629 05:42:23 main:49]\u001b[39m Size of Validation Set: 100\n",
            "\u001b[32m[I 210629 05:42:23 main:66]\u001b[39m Number of Edges: 101\n",
            "\u001b[32m[I 210629 05:42:23 main:68]\u001b[39m Training\n",
            "\u001b[32m[I 210629 05:42:29 models:110]\u001b[39m 2 512 train loss: 0.2893008 valid loss: 0.1546339 P@1: 0.28000 P@3: 0.17667 P@5: 0.16000 N@3: 0.19129 N@5: 0.20267 early stop: 0\n",
            "\u001b[32m[I 210629 05:42:30 models:142]\u001b[39m SWA Initializing\n",
            "\u001b[32m[I 210629 05:42:31 models:110]\u001b[39m 4 1024 train loss: 0.1486437 valid loss: 0.1461605 P@1: 0.28000 P@3: 0.18667 P@5: 0.17800 N@3: 0.20508 N@5: 0.21970 early stop: 0\n",
            "\u001b[32m[I 210629 05:42:33 models:110]\u001b[39m 7 512 train loss: 0.1429371 valid loss: 0.1437863 P@1: 0.28000 P@3: 0.18667 P@5: 0.19000 N@3: 0.20631 N@5: 0.23433 early stop: 0\n",
            "\u001b[32m[I 210629 05:42:35 models:110]\u001b[39m 9 1024 train loss: 0.1399485 valid loss: 0.1430019 P@1: 0.28000 P@3: 0.18667 P@5: 0.17600 N@3: 0.20631 N@5: 0.22075 early stop: 1\n",
            "\u001b[32m[I 210629 05:42:38 models:110]\u001b[39m 12 512 train loss: 0.1322352 valid loss: 0.1423696 P@1: 0.28000 P@3: 0.18667 P@5: 0.20600 N@3: 0.20631 N@5: 0.24231 early stop: 0\n",
            "\u001b[32m[I 210629 05:42:40 models:110]\u001b[39m 14 1024 train loss: 0.1247840 valid loss: 0.1414780 P@1: 0.28000 P@3: 0.23667 P@5: 0.19600 N@3: 0.24212 N@5: 0.24096 early stop: 1\n",
            "\u001b[32m[I 210629 05:42:43 models:110]\u001b[39m 17 512 train loss: 0.1006431 valid loss: 0.1401022 P@1: 0.28000 P@3: 0.27333 P@5: 0.21800 N@3: 0.27715 N@5: 0.26964 early stop: 0\n",
            "\u001b[32m[I 210629 05:42:45 models:110]\u001b[39m 19 1024 train loss: 0.0813109 valid loss: 0.1384361 P@1: 0.29000 P@3: 0.28333 P@5: 0.25000 N@3: 0.28715 N@5: 0.29969 early stop: 0\n",
            "\u001b[32m[I 210629 05:42:47 models:110]\u001b[39m 22 512 train loss: 0.0664966 valid loss: 0.1368853 P@1: 0.38000 P@3: 0.35333 P@5: 0.27800 N@3: 0.35888 N@5: 0.34828 early stop: 0\n",
            "\u001b[32m[I 210629 05:42:49 models:110]\u001b[39m 24 1024 train loss: 0.0544811 valid loss: 0.1349203 P@1: 0.46000 P@3: 0.37667 P@5: 0.30600 N@3: 0.39428 N@5: 0.39021 early stop: 0\n",
            "\u001b[32m[I 210629 05:42:52 models:110]\u001b[39m 27 512 train loss: 0.0461260 valid loss: 0.1328653 P@1: 0.57000 P@3: 0.38333 P@5: 0.32400 N@3: 0.42457 N@5: 0.43020 early stop: 0\n",
            "\u001b[32m[I 210629 05:42:54 models:110]\u001b[39m 29 1024 train loss: 0.0388562 valid loss: 0.1308465 P@1: 0.59000 P@3: 0.39667 P@5: 0.33400 N@3: 0.44324 N@5: 0.44605 early stop: 0\n",
            "\u001b[32m[I 210629 05:42:57 models:110]\u001b[39m 32 512 train loss: 0.0345015 valid loss: 0.1288353 P@1: 0.61000 P@3: 0.41667 P@5: 0.34800 N@3: 0.46265 N@5: 0.46249 early stop: 0\n",
            "\u001b[32m[I 210629 05:42:59 models:110]\u001b[39m 34 1024 train loss: 0.0293488 valid loss: 0.1270896 P@1: 0.64000 P@3: 0.42667 P@5: 0.35800 N@3: 0.47956 N@5: 0.47806 early stop: 0\n",
            "\u001b[32m[I 210629 05:43:01 models:110]\u001b[39m 37 512 train loss: 0.0249786 valid loss: 0.1254496 P@1: 0.65000 P@3: 0.44000 P@5: 0.36200 N@3: 0.49252 N@5: 0.48701 early stop: 0\n",
            "\u001b[32m[I 210629 05:43:03 models:110]\u001b[39m 39 1024 train loss: 0.0245633 valid loss: 0.1241192 P@1: 0.65000 P@3: 0.45667 P@5: 0.36600 N@3: 0.50548 N@5: 0.49321 early stop: 0\n",
            "\u001b[32m[I 210629 05:43:06 models:110]\u001b[39m 42 512 train loss: 0.0205874 valid loss: 0.1227506 P@1: 0.64000 P@3: 0.45333 P@5: 0.36800 N@3: 0.50130 N@5: 0.49319 early stop: 1\n",
            "\u001b[32m[I 210629 05:43:08 models:110]\u001b[39m 44 1024 train loss: 0.0207505 valid loss: 0.1215736 P@1: 0.66000 P@3: 0.46333 P@5: 0.37000 N@3: 0.51252 N@5: 0.49957 early stop: 0\n",
            "\u001b[32m[I 210629 05:43:11 models:110]\u001b[39m 47 512 train loss: 0.0218457 valid loss: 0.1209292 P@1: 0.66000 P@3: 0.47333 P@5: 0.37200 N@3: 0.52255 N@5: 0.50475 early stop: 0\n",
            "\u001b[32m[I 210629 05:43:13 models:110]\u001b[39m 49 1024 train loss: 0.0201902 valid loss: 0.1206700 P@1: 0.64000 P@3: 0.48667 P@5: 0.37600 N@3: 0.53061 N@5: 0.50777 early stop: 0\n",
            "\u001b[32m[I 210629 05:43:15 models:110]\u001b[39m 52 512 train loss: 0.0202058 valid loss: 0.1201848 P@1: 0.65000 P@3: 0.49000 P@5: 0.38000 N@3: 0.53530 N@5: 0.51365 early stop: 0\n",
            "\u001b[32m[I 210629 05:43:17 models:110]\u001b[39m 54 1024 train loss: 0.0130948 valid loss: 0.1201296 P@1: 0.67000 P@3: 0.49333 P@5: 0.38200 N@3: 0.54295 N@5: 0.52002 early stop: 0\n",
            "\u001b[32m[I 210629 05:43:20 models:110]\u001b[39m 57 512 train loss: 0.0097950 valid loss: 0.1200759 P@1: 0.69000 P@3: 0.51000 P@5: 0.38600 N@3: 0.55796 N@5: 0.52874 early stop: 0\n",
            "\u001b[32m[I 210629 05:43:22 models:110]\u001b[39m 59 1024 train loss: 0.0090637 valid loss: 0.1203167 P@1: 0.69000 P@3: 0.52000 P@5: 0.39000 N@3: 0.56562 N@5: 0.53327 early stop: 0\n",
            "\u001b[32m[I 210629 05:43:25 models:110]\u001b[39m 62 512 train loss: 0.0066370 valid loss: 0.1208820 P@1: 0.69000 P@3: 0.51667 P@5: 0.38800 N@3: 0.56439 N@5: 0.53329 early stop: 0\n",
            "\u001b[32m[I 210629 05:43:27 models:110]\u001b[39m 64 1024 train loss: 0.0054583 valid loss: 0.1213724 P@1: 0.69000 P@3: 0.51667 P@5: 0.39200 N@3: 0.56501 N@5: 0.53612 early stop: 0\n",
            "\u001b[32m[I 210629 05:43:29 models:110]\u001b[39m 67 512 train loss: 0.0054488 valid loss: 0.1221102 P@1: 0.68000 P@3: 0.52000 P@5: 0.39400 N@3: 0.56490 N@5: 0.53592 early stop: 1\n",
            "\u001b[32m[I 210629 05:43:31 models:110]\u001b[39m 69 1024 train loss: 0.0055683 valid loss: 0.1231240 P@1: 0.68000 P@3: 0.52000 P@5: 0.39800 N@3: 0.56429 N@5: 0.53749 early stop: 0\n",
            "\u001b[32m[I 210629 05:43:34 models:110]\u001b[39m 72 512 train loss: 0.0037601 valid loss: 0.1243220 P@1: 0.67000 P@3: 0.51667 P@5: 0.40000 N@3: 0.56021 N@5: 0.53749 early stop: 0\n",
            "\u001b[32m[I 210629 05:43:36 models:110]\u001b[39m 74 1024 train loss: 0.0028004 valid loss: 0.1256524 P@1: 0.67000 P@3: 0.51667 P@5: 0.40200 N@3: 0.56021 N@5: 0.53935 early stop: 0\n",
            "\u001b[32m[I 210629 05:43:39 models:110]\u001b[39m 77 512 train loss: 0.0024046 valid loss: 0.1272500 P@1: 0.67000 P@3: 0.51667 P@5: 0.40400 N@3: 0.55888 N@5: 0.53951 early stop: 0\n",
            "\u001b[32m[I 210629 05:43:41 models:110]\u001b[39m 79 1024 train loss: 0.0026709 valid loss: 0.1287547 P@1: 0.67000 P@3: 0.51667 P@5: 0.40200 N@3: 0.55949 N@5: 0.53844 early stop: 1\n",
            "\u001b[32m[I 210629 05:43:43 models:110]\u001b[39m 82 512 train loss: 0.0029727 valid loss: 0.1303905 P@1: 0.66000 P@3: 0.52000 P@5: 0.40200 N@3: 0.56010 N@5: 0.53695 early stop: 2\n",
            "\u001b[32m[I 210629 05:43:45 models:110]\u001b[39m 84 1024 train loss: 0.0048059 valid loss: 0.1320660 P@1: 0.66000 P@3: 0.52333 P@5: 0.40200 N@3: 0.56307 N@5: 0.53721 early stop: 3\n",
            "\u001b[32m[I 210629 05:43:48 models:110]\u001b[39m 87 512 train loss: 0.0061642 valid loss: 0.1336916 P@1: 0.67000 P@3: 0.51667 P@5: 0.40400 N@3: 0.56072 N@5: 0.53953 early stop: 0\n",
            "\u001b[32m[I 210629 05:43:50 models:110]\u001b[39m 89 1024 train loss: 0.0079606 valid loss: 0.1350983 P@1: 0.66000 P@3: 0.52333 P@5: 0.40600 N@3: 0.56429 N@5: 0.54074 early stop: 0\n",
            "\u001b[32m[I 210629 05:43:53 models:110]\u001b[39m 92 512 train loss: 0.0087207 valid loss: 0.1363530 P@1: 0.66000 P@3: 0.52000 P@5: 0.40400 N@3: 0.56195 N@5: 0.53893 early stop: 1\n",
            "\u001b[32m[I 210629 05:43:55 models:110]\u001b[39m 94 1024 train loss: 0.0065578 valid loss: 0.1377279 P@1: 0.66000 P@3: 0.52000 P@5: 0.40800 N@3: 0.56104 N@5: 0.54161 early stop: 0\n",
            "\u001b[32m[I 210629 05:43:57 models:110]\u001b[39m 97 512 train loss: 0.0062101 valid loss: 0.1393820 P@1: 0.67000 P@3: 0.51667 P@5: 0.40600 N@3: 0.56032 N@5: 0.54202 early stop: 0\n",
            "\u001b[32m[I 210629 05:43:59 models:110]\u001b[39m 99 1024 train loss: 0.0057231 valid loss: 0.1409200 P@1: 0.67000 P@3: 0.51667 P@5: 0.40800 N@3: 0.55848 N@5: 0.54158 early stop: 1\n",
            "\u001b[32m[I 210629 05:44:02 models:110]\u001b[39m 102 512 train loss: 0.0042675 valid loss: 0.1426326 P@1: 0.67000 P@3: 0.51667 P@5: 0.40800 N@3: 0.55848 N@5: 0.54158 early stop: 2\n",
            "\u001b[32m[I 210629 05:44:04 models:110]\u001b[39m 104 1024 train loss: 0.0044461 valid loss: 0.1440556 P@1: 0.67000 P@3: 0.52000 P@5: 0.40800 N@3: 0.56082 N@5: 0.54181 early stop: 3\n",
            "\u001b[32m[I 210629 05:44:06 models:110]\u001b[39m 107 512 train loss: 0.0035257 valid loss: 0.1455067 P@1: 0.67000 P@3: 0.52000 P@5: 0.40800 N@3: 0.56144 N@5: 0.54243 early stop: 0\n",
            "\u001b[32m[I 210629 05:44:08 models:110]\u001b[39m 109 1024 train loss: 0.0056461 valid loss: 0.1469229 P@1: 0.67000 P@3: 0.52000 P@5: 0.41000 N@3: 0.56144 N@5: 0.54268 early stop: 0\n",
            "\u001b[32m[I 210629 05:44:11 models:110]\u001b[39m 112 512 train loss: 0.0042258 valid loss: 0.1483613 P@1: 0.66000 P@3: 0.52000 P@5: 0.41400 N@3: 0.55829 N@5: 0.54280 early stop: 0\n",
            "\u001b[32m[I 210629 05:44:13 models:110]\u001b[39m 114 1024 train loss: 0.0098810 valid loss: 0.1499050 P@1: 0.67000 P@3: 0.52333 P@5: 0.41600 N@3: 0.56298 N@5: 0.54594 early stop: 0\n",
            "\u001b[32m[I 210629 05:44:16 models:110]\u001b[39m 117 512 train loss: 0.0153544 valid loss: 0.1508593 P@1: 0.66000 P@3: 0.52333 P@5: 0.41400 N@3: 0.56125 N@5: 0.54270 early stop: 1\n",
            "\u001b[32m[I 210629 05:44:18 models:110]\u001b[39m 119 1024 train loss: 0.0121260 valid loss: 0.1518077 P@1: 0.65000 P@3: 0.52333 P@5: 0.41800 N@3: 0.55952 N@5: 0.54427 early stop: 2\n",
            "\u001b[32m[I 210629 05:44:20 models:110]\u001b[39m 122 512 train loss: 0.0080054 valid loss: 0.1528470 P@1: 0.65000 P@3: 0.52333 P@5: 0.41600 N@3: 0.55952 N@5: 0.54273 early stop: 3\n",
            "\u001b[32m[I 210629 05:44:22 models:110]\u001b[39m 124 1024 train loss: 0.0040926 valid loss: 0.1538126 P@1: 0.65000 P@3: 0.52000 P@5: 0.41800 N@3: 0.55779 N@5: 0.54504 early stop: 4\n",
            "\u001b[32m[I 210629 05:44:25 models:110]\u001b[39m 127 512 train loss: 0.0023760 valid loss: 0.1551537 P@1: 0.65000 P@3: 0.52333 P@5: 0.42000 N@3: 0.56013 N@5: 0.54709 early stop: 0\n",
            "\u001b[32m[I 210629 05:44:27 models:110]\u001b[39m 129 1024 train loss: 0.0014883 valid loss: 0.1563248 P@1: 0.65000 P@3: 0.52333 P@5: 0.42200 N@3: 0.56013 N@5: 0.54891 early stop: 0\n",
            "\u001b[32m[I 210629 05:44:29 models:110]\u001b[39m 132 512 train loss: 0.0007128 valid loss: 0.1576068 P@1: 0.65000 P@3: 0.52333 P@5: 0.42200 N@3: 0.56013 N@5: 0.54896 early stop: 0\n",
            "\u001b[32m[I 210629 05:44:32 models:110]\u001b[39m 134 1024 train loss: 0.0005170 valid loss: 0.1589480 P@1: 0.65000 P@3: 0.52333 P@5: 0.42200 N@3: 0.56075 N@5: 0.54905 early stop: 0\n",
            "\u001b[32m[I 210629 05:44:34 models:110]\u001b[39m 137 512 train loss: 0.0006892 valid loss: 0.1603143 P@1: 0.65000 P@3: 0.52667 P@5: 0.42200 N@3: 0.56248 N@5: 0.54868 early stop: 1\n",
            "\u001b[32m[I 210629 05:44:36 models:110]\u001b[39m 139 1024 train loss: 0.0004045 valid loss: 0.1617395 P@1: 0.66000 P@3: 0.52333 P@5: 0.42400 N@3: 0.56186 N@5: 0.55151 early stop: 0\n",
            "\u001b[32m[I 210629 05:44:39 models:110]\u001b[39m 142 512 train loss: 0.0006238 valid loss: 0.1631473 P@1: 0.66000 P@3: 0.52667 P@5: 0.42600 N@3: 0.56421 N@5: 0.55306 early stop: 0\n",
            "\u001b[32m[I 210629 05:44:41 models:110]\u001b[39m 144 1024 train loss: 0.0004309 valid loss: 0.1645357 P@1: 0.66000 P@3: 0.52667 P@5: 0.42600 N@3: 0.56421 N@5: 0.55306 early stop: 1\n",
            "\u001b[32m[I 210629 05:44:43 models:110]\u001b[39m 147 512 train loss: 0.0003314 valid loss: 0.1660063 P@1: 0.66000 P@3: 0.53000 P@5: 0.42600 N@3: 0.56656 N@5: 0.55332 early stop: 0\n",
            "\u001b[32m[I 210629 05:44:46 models:110]\u001b[39m 149 1024 train loss: 0.0001695 valid loss: 0.1674405 P@1: 0.66000 P@3: 0.53333 P@5: 0.42200 N@3: 0.56829 N@5: 0.54971 early stop: 1\n",
            "\u001b[32m[I 210629 05:44:48 models:110]\u001b[39m 152 512 train loss: 0.0001191 valid loss: 0.1688922 P@1: 0.66000 P@3: 0.53333 P@5: 0.42000 N@3: 0.56829 N@5: 0.54817 early stop: 2\n",
            "\u001b[32m[I 210629 05:44:50 models:110]\u001b[39m 154 1024 train loss: 0.0000998 valid loss: 0.1703531 P@1: 0.65000 P@3: 0.53333 P@5: 0.42000 N@3: 0.56717 N@5: 0.54727 early stop: 3\n",
            "\u001b[32m[I 210629 05:44:53 models:110]\u001b[39m 157 512 train loss: 0.0000813 valid loss: 0.1717852 P@1: 0.65000 P@3: 0.52667 P@5: 0.42000 N@3: 0.56248 N@5: 0.54661 early stop: 4\n",
            "\u001b[32m[I 210629 05:44:55 models:110]\u001b[39m 159 1024 train loss: 0.0000763 valid loss: 0.1732224 P@1: 0.65000 P@3: 0.52667 P@5: 0.41800 N@3: 0.56248 N@5: 0.54530 early stop: 5\n",
            "\u001b[33m[W 210629 05:44:55 models:137]\u001b[39m Clipping gradients with total norm 0.06164 and max norm 0.011\n",
            "\u001b[32m[I 210629 05:44:57 models:110]\u001b[39m 162 512 train loss: 0.0000915 valid loss: 0.1747001 P@1: 0.65000 P@3: 0.52667 P@5: 0.42000 N@3: 0.56248 N@5: 0.54661 early stop: 6\n",
            "\u001b[32m[I 210629 05:44:59 models:110]\u001b[39m 164 1024 train loss: 0.0001119 valid loss: 0.1761123 P@1: 0.65000 P@3: 0.52667 P@5: 0.41800 N@3: 0.56248 N@5: 0.54525 early stop: 7\n",
            "\u001b[32m[I 210629 05:45:02 models:110]\u001b[39m 167 512 train loss: 0.0000802 valid loss: 0.1775248 P@1: 0.65000 P@3: 0.52667 P@5: 0.41800 N@3: 0.56248 N@5: 0.54525 early stop: 8\n",
            "\u001b[32m[I 210629 05:45:04 models:110]\u001b[39m 169 1024 train loss: 0.0000517 valid loss: 0.1789567 P@1: 0.65000 P@3: 0.52667 P@5: 0.41800 N@3: 0.56186 N@5: 0.54464 early stop: 9\n",
            "\u001b[32m[I 210629 05:45:06 models:110]\u001b[39m 172 512 train loss: 0.0000489 valid loss: 0.1803616 P@1: 0.65000 P@3: 0.52333 P@5: 0.41600 N@3: 0.55952 N@5: 0.54300 early stop: 10\n",
            "\u001b[32m[I 210629 05:45:08 models:110]\u001b[39m 174 1024 train loss: 0.0000455 valid loss: 0.1817641 P@1: 0.65000 P@3: 0.52000 P@5: 0.41400 N@3: 0.55656 N@5: 0.54065 early stop: 11\n",
            "\u001b[32m[I 210629 05:45:11 models:110]\u001b[39m 177 512 train loss: 0.0000440 valid loss: 0.1831789 P@1: 0.65000 P@3: 0.52000 P@5: 0.41400 N@3: 0.55656 N@5: 0.54039 early stop: 12\n",
            "\u001b[32m[I 210629 05:45:13 models:110]\u001b[39m 179 1024 train loss: 0.0000459 valid loss: 0.1845851 P@1: 0.65000 P@3: 0.52000 P@5: 0.41800 N@3: 0.55656 N@5: 0.54387 early stop: 13\n",
            "\u001b[32m[I 210629 05:45:15 models:110]\u001b[39m 182 512 train loss: 0.0000435 valid loss: 0.1859605 P@1: 0.65000 P@3: 0.52000 P@5: 0.41800 N@3: 0.55656 N@5: 0.54387 early stop: 14\n",
            "\u001b[32m[I 210629 05:45:17 models:110]\u001b[39m 184 1024 train loss: 0.0000344 valid loss: 0.1873367 P@1: 0.66000 P@3: 0.52000 P@5: 0.42000 N@3: 0.55829 N@5: 0.54724 early stop: 15\n",
            "\u001b[32m[I 210629 05:45:20 models:110]\u001b[39m 187 512 train loss: 0.0000351 valid loss: 0.1887152 P@1: 0.67000 P@3: 0.52000 P@5: 0.42000 N@3: 0.56002 N@5: 0.54898 early stop: 16\n",
            "\u001b[32m[I 210629 05:45:22 models:110]\u001b[39m 189 1024 train loss: 0.0000338 valid loss: 0.1900919 P@1: 0.67000 P@3: 0.52000 P@5: 0.42000 N@3: 0.56002 N@5: 0.54898 early stop: 17\n",
            "\u001b[32m[I 210629 05:45:24 models:110]\u001b[39m 192 512 train loss: 0.0000313 valid loss: 0.1914417 P@1: 0.67000 P@3: 0.52000 P@5: 0.42000 N@3: 0.56002 N@5: 0.54883 early stop: 18\n",
            "\u001b[32m[I 210629 05:45:26 models:110]\u001b[39m 194 1024 train loss: 0.0000297 valid loss: 0.1927730 P@1: 0.67000 P@3: 0.52000 P@5: 0.42000 N@3: 0.56002 N@5: 0.54883 early stop: 19\n",
            "\u001b[32m[I 210629 05:45:29 models:110]\u001b[39m 197 512 train loss: 0.0000329 valid loss: 0.1940930 P@1: 0.67000 P@3: 0.52000 P@5: 0.42200 N@3: 0.56002 N@5: 0.55064 early stop: 20\n",
            "\u001b[32m[I 210629 05:45:31 models:110]\u001b[39m 199 1024 train loss: 0.0000358 valid loss: 0.1954129 P@1: 0.66000 P@3: 0.52000 P@5: 0.42200 N@3: 0.55768 N@5: 0.54830 early stop: 21\n",
            "\u001b[32m[I 210629 05:45:33 models:110]\u001b[39m 202 512 train loss: 0.0000299 valid loss: 0.1967202 P@1: 0.66000 P@3: 0.52000 P@5: 0.42200 N@3: 0.55706 N@5: 0.54785 early stop: 22\n",
            "\u001b[32m[I 210629 05:45:36 models:110]\u001b[39m 204 1024 train loss: 0.0000232 valid loss: 0.1980136 P@1: 0.66000 P@3: 0.52000 P@5: 0.42200 N@3: 0.55706 N@5: 0.54821 early stop: 23\n",
            "\u001b[32m[I 210629 05:45:38 models:110]\u001b[39m 207 512 train loss: 0.0000238 valid loss: 0.1993017 P@1: 0.66000 P@3: 0.52333 P@5: 0.42000 N@3: 0.55941 N@5: 0.54723 early stop: 24\n",
            "\u001b[32m[I 210629 05:45:40 models:110]\u001b[39m 209 1024 train loss: 0.0000240 valid loss: 0.2005899 P@1: 0.66000 P@3: 0.52333 P@5: 0.42000 N@3: 0.55941 N@5: 0.54723 early stop: 25\n",
            "\u001b[32m[I 210629 05:45:43 models:110]\u001b[39m 212 512 train loss: 0.0000236 valid loss: 0.2018556 P@1: 0.66000 P@3: 0.52667 P@5: 0.42000 N@3: 0.56175 N@5: 0.54761 early stop: 26\n",
            "\u001b[32m[I 210629 05:45:45 models:110]\u001b[39m 214 1024 train loss: 0.0000231 valid loss: 0.2030880 P@1: 0.66000 P@3: 0.52667 P@5: 0.42000 N@3: 0.56175 N@5: 0.54746 early stop: 27\n",
            "\u001b[32m[I 210629 05:45:47 models:110]\u001b[39m 217 512 train loss: 0.0000265 valid loss: 0.2043092 P@1: 0.66000 P@3: 0.52667 P@5: 0.42000 N@3: 0.56175 N@5: 0.54746 early stop: 28\n",
            "\u001b[32m[I 210629 05:45:49 models:110]\u001b[39m 219 1024 train loss: 0.0000196 valid loss: 0.2055291 P@1: 0.66000 P@3: 0.52667 P@5: 0.42000 N@3: 0.56175 N@5: 0.54761 early stop: 29\n",
            "\u001b[32m[I 210629 05:45:52 models:110]\u001b[39m 222 512 train loss: 0.0000194 valid loss: 0.2067512 P@1: 0.66000 P@3: 0.52667 P@5: 0.42000 N@3: 0.56175 N@5: 0.54761 early stop: 30\n",
            "\u001b[32m[I 210629 05:45:54 models:110]\u001b[39m 224 1024 train loss: 0.0000180 valid loss: 0.2079691 P@1: 0.66000 P@3: 0.52333 P@5: 0.42200 N@3: 0.55879 N@5: 0.54866 early stop: 31\n",
            "\u001b[32m[I 210629 05:45:56 models:110]\u001b[39m 227 512 train loss: 0.0000178 valid loss: 0.2091805 P@1: 0.67000 P@3: 0.52333 P@5: 0.42200 N@3: 0.56052 N@5: 0.55039 early stop: 32\n",
            "\u001b[32m[I 210629 05:45:58 models:110]\u001b[39m 229 1024 train loss: 0.0000190 valid loss: 0.2103777 P@1: 0.67000 P@3: 0.52333 P@5: 0.42200 N@3: 0.56052 N@5: 0.55039 early stop: 33\n",
            "\u001b[32m[I 210629 05:46:01 models:110]\u001b[39m 232 512 train loss: 0.0000164 valid loss: 0.2115580 P@1: 0.67000 P@3: 0.52333 P@5: 0.42200 N@3: 0.56052 N@5: 0.55039 early stop: 34\n",
            "\u001b[32m[I 210629 05:46:03 models:110]\u001b[39m 234 1024 train loss: 0.0000161 valid loss: 0.2127259 P@1: 0.67000 P@3: 0.52333 P@5: 0.42200 N@3: 0.56052 N@5: 0.55039 early stop: 35\n",
            "\u001b[32m[I 210629 05:46:05 models:110]\u001b[39m 237 512 train loss: 0.0000168 valid loss: 0.2138857 P@1: 0.67000 P@3: 0.52333 P@5: 0.42200 N@3: 0.56052 N@5: 0.55039 early stop: 36\n",
            "\u001b[32m[I 210629 05:46:07 models:110]\u001b[39m 239 1024 train loss: 0.0000147 valid loss: 0.2150345 P@1: 0.67000 P@3: 0.52333 P@5: 0.42200 N@3: 0.56114 N@5: 0.55102 early stop: 37\n",
            "\u001b[32m[I 210629 05:46:10 models:110]\u001b[39m 242 512 train loss: 0.0000141 valid loss: 0.2161750 P@1: 0.67000 P@3: 0.52333 P@5: 0.42000 N@3: 0.56114 N@5: 0.54920 early stop: 38\n",
            "\u001b[32m[I 210629 05:46:12 models:110]\u001b[39m 244 1024 train loss: 0.0000125 valid loss: 0.2173116 P@1: 0.67000 P@3: 0.52333 P@5: 0.42200 N@3: 0.56114 N@5: 0.55072 early stop: 39\n",
            "\u001b[32m[I 210629 05:46:14 models:110]\u001b[39m 247 512 train loss: 0.0000154 valid loss: 0.2184239 P@1: 0.67000 P@3: 0.52667 P@5: 0.42200 N@3: 0.56349 N@5: 0.55105 early stop: 40\n",
            "\u001b[32m[I 210629 05:46:16 models:110]\u001b[39m 249 1024 train loss: 0.0000138 valid loss: 0.2195233 P@1: 0.67000 P@3: 0.52333 P@5: 0.42000 N@3: 0.56042 N@5: 0.54825 early stop: 41\n",
            "\u001b[32m[I 210629 05:46:19 models:110]\u001b[39m 252 512 train loss: 0.0000156 valid loss: 0.2206273 P@1: 0.67000 P@3: 0.52333 P@5: 0.42000 N@3: 0.56042 N@5: 0.54825 early stop: 42\n",
            "\u001b[32m[I 210629 05:46:21 models:110]\u001b[39m 254 1024 train loss: 0.0000160 valid loss: 0.2217360 P@1: 0.67000 P@3: 0.52333 P@5: 0.42000 N@3: 0.56042 N@5: 0.54825 early stop: 43\n",
            "\u001b[32m[I 210629 05:46:23 models:110]\u001b[39m 257 512 train loss: 0.0000142 valid loss: 0.2228323 P@1: 0.67000 P@3: 0.52333 P@5: 0.42000 N@3: 0.56042 N@5: 0.54825 early stop: 44\n",
            "\u001b[32m[I 210629 05:46:25 models:110]\u001b[39m 259 1024 train loss: 0.0000115 valid loss: 0.2239210 P@1: 0.67000 P@3: 0.53000 P@5: 0.42000 N@3: 0.56511 N@5: 0.54881 early stop: 45\n",
            "\u001b[32m[I 210629 05:46:28 models:110]\u001b[39m 262 512 train loss: 0.0000127 valid loss: 0.2249977 P@1: 0.67000 P@3: 0.53000 P@5: 0.42000 N@3: 0.56511 N@5: 0.54881 early stop: 46\n",
            "\u001b[32m[I 210629 05:46:30 models:110]\u001b[39m 264 1024 train loss: 0.0000120 valid loss: 0.2260644 P@1: 0.66000 P@3: 0.53000 P@5: 0.41800 N@3: 0.56338 N@5: 0.54625 early stop: 47\n",
            "\u001b[32m[I 210629 05:46:32 models:110]\u001b[39m 267 512 train loss: 0.0000135 valid loss: 0.2271135 P@1: 0.66000 P@3: 0.53000 P@5: 0.42200 N@3: 0.56338 N@5: 0.54907 early stop: 48\n",
            "\u001b[32m[I 210629 05:46:35 models:110]\u001b[39m 269 1024 train loss: 0.0000113 valid loss: 0.2281467 P@1: 0.66000 P@3: 0.53000 P@5: 0.42400 N@3: 0.56338 N@5: 0.55088 early stop: 49\n",
            "\u001b[32m[I 210629 05:46:37 models:110]\u001b[39m 272 512 train loss: 0.0000102 valid loss: 0.2291714 P@1: 0.67000 P@3: 0.53000 P@5: 0.42400 N@3: 0.56511 N@5: 0.55234 early stop: 50\n",
            "\u001b[33m[W 210629 05:46:38 models:137]\u001b[39m Clipping gradients with total norm 0.00881 and max norm 0.00165\n",
            "\u001b[32m[I 210629 05:46:39 main:76]\u001b[39m Finish Training\n",
            "\u001b[32m[I 210629 05:46:41 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210629 05:46:41 main:79]\u001b[39m Loading Test Set\n",
            "\u001b[32m[I 210629 05:46:41 main:83]\u001b[39m Size of Test Set: 100\n",
            "\u001b[32m[I 210629 05:46:41 main:85]\u001b[39m Predicting\n",
            "\u001b[32m[I 210629 05:46:45 main:91]\u001b[39m Finish Predicting\n",
            "Precision@1,3,5: 0.57 0.48 0.378\n",
            "nDCG@1,3,5: 0.57 0.5152237629649679 0.5042756289836591\n",
            "\n",
            "NO MAG NO MESH, skip=900\n",
            "\n",
            "/content/drive/Shareddrives/NASA/MATCH/PeTaL\n",
            "130\n",
            "/content/drive/Shareddrives/NASA/MATCH\n",
            "    800  286051 4914820 PeTaL/train.json\n",
            "\u001b[32m[I 210629 05:46:48 preprocess:28]\u001b[39m Vocab Size: 26834\n",
            "\u001b[32m[I 210629 05:46:48 preprocess:30]\u001b[39m Getting Dataset: PeTaL/train_texts.txt Max Length: 500\n",
            "\u001b[32m[I 210629 05:46:48 preprocess:32]\u001b[39m Size of Samples: 900\n",
            "\u001b[32m[I 210629 05:46:49 preprocess:28]\u001b[39m Vocab Size: 26834\n",
            "\u001b[32m[I 210629 05:46:49 preprocess:30]\u001b[39m Getting Dataset: PeTaL/test_texts.txt Max Length: 500\n",
            "\u001b[32m[I 210629 05:46:49 preprocess:32]\u001b[39m Size of Samples: 100\n",
            "\u001b[32m[I 210629 05:46:50 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210629 05:46:50 main:35]\u001b[39m Loading Training and Validation Set\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['absorb_and/or_filter_solids', 'chemically_break_down_inorganic_compounds', 'detox/purify', 'manage_environmental_disturbances_in_a_community', 'protect_from_fire', 'protect_from_gases'] will be ignored\n",
            "  .format(sorted(unknown, key=str)))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['chemically_break_down_inorganic_compounds', 'protect_from_fire'] will be ignored\n",
            "  .format(sorted(unknown, key=str)))\n",
            "\u001b[32m[I 210629 05:46:50 main:47]\u001b[39m Number of Labels: 124\n",
            "\u001b[32m[I 210629 05:46:50 main:48]\u001b[39m Size of Training Set: 800\n",
            "\u001b[32m[I 210629 05:46:50 main:49]\u001b[39m Size of Validation Set: 100\n",
            "\u001b[32m[I 210629 05:46:50 main:66]\u001b[39m Number of Edges: 101\n",
            "\u001b[32m[I 210629 05:46:50 main:68]\u001b[39m Training\n",
            "\u001b[32m[I 210629 05:46:56 models:110]\u001b[39m 2 512 train loss: 0.2794774 valid loss: 0.1528514 P@1: 0.29000 P@3: 0.15667 P@5: 0.13400 N@3: 0.18078 N@5: 0.18782 early stop: 0\n",
            "\u001b[32m[I 210629 05:46:57 models:142]\u001b[39m SWA Initializing\n",
            "\u001b[32m[I 210629 05:46:59 models:110]\u001b[39m 4 1024 train loss: 0.1448262 valid loss: 0.1447223 P@1: 0.29000 P@3: 0.22333 P@5: 0.19600 N@3: 0.23877 N@5: 0.25483 early stop: 0\n",
            "\u001b[32m[I 210629 05:47:01 models:110]\u001b[39m 7 512 train loss: 0.1398866 valid loss: 0.1437563 P@1: 0.29000 P@3: 0.22333 P@5: 0.19800 N@3: 0.23570 N@5: 0.25445 early stop: 1\n",
            "\u001b[32m[I 210629 05:47:03 models:110]\u001b[39m 9 1024 train loss: 0.1384577 valid loss: 0.1434568 P@1: 0.29000 P@3: 0.22333 P@5: 0.19800 N@3: 0.23631 N@5: 0.25542 early stop: 0\n",
            "\u001b[32m[I 210629 05:47:06 models:110]\u001b[39m 12 512 train loss: 0.1309059 valid loss: 0.1431730 P@1: 0.29000 P@3: 0.24000 P@5: 0.19600 N@3: 0.24743 N@5: 0.25458 early stop: 1\n",
            "\u001b[32m[I 210629 05:47:08 models:110]\u001b[39m 14 1024 train loss: 0.1169517 valid loss: 0.1425753 P@1: 0.29000 P@3: 0.23333 P@5: 0.20800 N@3: 0.24765 N@5: 0.26799 early stop: 0\n",
            "\u001b[32m[I 210629 05:47:10 models:110]\u001b[39m 17 512 train loss: 0.1026052 valid loss: 0.1418068 P@1: 0.29000 P@3: 0.24667 P@5: 0.22000 N@3: 0.25888 N@5: 0.28014 early stop: 0\n",
            "\u001b[32m[I 210629 05:47:13 models:110]\u001b[39m 19 1024 train loss: 0.0830465 valid loss: 0.1408167 P@1: 0.34000 P@3: 0.26000 P@5: 0.24000 N@3: 0.27693 N@5: 0.30549 early stop: 0\n",
            "\u001b[32m[I 210629 05:47:15 models:110]\u001b[39m 22 512 train loss: 0.0709805 valid loss: 0.1395955 P@1: 0.38000 P@3: 0.29000 P@5: 0.25400 N@3: 0.31235 N@5: 0.33229 early stop: 0\n",
            "\u001b[32m[I 210629 05:47:17 models:110]\u001b[39m 24 1024 train loss: 0.0577034 valid loss: 0.1384334 P@1: 0.41000 P@3: 0.32667 P@5: 0.24800 N@3: 0.34765 N@5: 0.34289 early stop: 0\n",
            "\u001b[32m[I 210629 05:47:20 models:110]\u001b[39m 27 512 train loss: 0.0493116 valid loss: 0.1370725 P@1: 0.48000 P@3: 0.34000 P@5: 0.25400 N@3: 0.37224 N@5: 0.36151 early stop: 0\n",
            "\u001b[32m[I 210629 05:47:22 models:110]\u001b[39m 29 1024 train loss: 0.0401335 valid loss: 0.1357369 P@1: 0.49000 P@3: 0.33667 P@5: 0.27200 N@3: 0.37285 N@5: 0.37858 early stop: 0\n",
            "\u001b[32m[I 210629 05:47:25 models:110]\u001b[39m 32 512 train loss: 0.0348075 valid loss: 0.1347454 P@1: 0.50000 P@3: 0.33667 P@5: 0.27600 N@3: 0.37458 N@5: 0.38641 early stop: 0\n",
            "\u001b[32m[I 210629 05:47:27 models:110]\u001b[39m 34 1024 train loss: 0.0301678 valid loss: 0.1340549 P@1: 0.49000 P@3: 0.35000 P@5: 0.29000 N@3: 0.38162 N@5: 0.39699 early stop: 0\n",
            "\u001b[32m[I 210629 05:47:29 models:110]\u001b[39m 37 512 train loss: 0.0264829 valid loss: 0.1334017 P@1: 0.52000 P@3: 0.36667 P@5: 0.29000 N@3: 0.40039 N@5: 0.40531 early stop: 0\n",
            "\u001b[32m[I 210629 05:47:32 models:110]\u001b[39m 39 1024 train loss: 0.0223865 valid loss: 0.1326058 P@1: 0.54000 P@3: 0.37000 P@5: 0.30200 N@3: 0.40989 N@5: 0.42179 early stop: 0\n",
            "\u001b[32m[I 210629 05:47:34 models:110]\u001b[39m 42 512 train loss: 0.0183572 valid loss: 0.1323542 P@1: 0.55000 P@3: 0.38333 P@5: 0.31000 N@3: 0.42418 N@5: 0.43199 early stop: 0\n",
            "\u001b[32m[I 210629 05:47:36 models:110]\u001b[39m 44 1024 train loss: 0.0165083 valid loss: 0.1321200 P@1: 0.57000 P@3: 0.39667 P@5: 0.31600 N@3: 0.43898 N@5: 0.44151 early stop: 0\n",
            "\u001b[32m[I 210629 05:47:39 models:110]\u001b[39m 47 512 train loss: 0.0165500 valid loss: 0.1323385 P@1: 0.57000 P@3: 0.39667 P@5: 0.31400 N@3: 0.43960 N@5: 0.43984 early stop: 1\n",
            "\u001b[32m[I 210629 05:47:41 models:110]\u001b[39m 49 1024 train loss: 0.0161121 valid loss: 0.1327628 P@1: 0.58000 P@3: 0.40000 P@5: 0.31600 N@3: 0.44490 N@5: 0.44386 early stop: 0\n",
            "\u001b[32m[I 210629 05:47:43 models:110]\u001b[39m 52 512 train loss: 0.0154364 valid loss: 0.1331751 P@1: 0.58000 P@3: 0.41000 P@5: 0.32400 N@3: 0.45213 N@5: 0.45049 early stop: 0\n",
            "\u001b[32m[I 210629 05:47:46 models:110]\u001b[39m 54 1024 train loss: 0.0112003 valid loss: 0.1338360 P@1: 0.60000 P@3: 0.42333 P@5: 0.32400 N@3: 0.46602 N@5: 0.45645 early stop: 0\n",
            "\u001b[32m[I 210629 05:47:48 models:110]\u001b[39m 57 512 train loss: 0.0091956 valid loss: 0.1342710 P@1: 0.61000 P@3: 0.43000 P@5: 0.33000 N@3: 0.47244 N@5: 0.46445 early stop: 0\n",
            "\u001b[32m[I 210629 05:47:51 models:110]\u001b[39m 59 1024 train loss: 0.0062859 valid loss: 0.1349741 P@1: 0.60000 P@3: 0.43667 P@5: 0.34000 N@3: 0.47725 N@5: 0.47281 early stop: 0\n",
            "\u001b[32m[I 210629 05:47:53 models:110]\u001b[39m 62 512 train loss: 0.0056581 valid loss: 0.1358589 P@1: 0.60000 P@3: 0.44000 P@5: 0.34600 N@3: 0.47837 N@5: 0.47680 early stop: 0\n",
            "\u001b[32m[I 210629 05:47:55 models:110]\u001b[39m 64 1024 train loss: 0.0059172 valid loss: 0.1367467 P@1: 0.61000 P@3: 0.44333 P@5: 0.34400 N@3: 0.48244 N@5: 0.47671 early stop: 1\n",
            "\u001b[32m[I 210629 05:47:58 models:110]\u001b[39m 67 512 train loss: 0.0067200 valid loss: 0.1379480 P@1: 0.61000 P@3: 0.45333 P@5: 0.34000 N@3: 0.49194 N@5: 0.47738 early stop: 0\n",
            "\u001b[32m[I 210629 05:48:00 models:110]\u001b[39m 69 1024 train loss: 0.0087392 valid loss: 0.1388905 P@1: 0.61000 P@3: 0.46000 P@5: 0.34200 N@3: 0.49725 N@5: 0.47953 early stop: 0\n",
            "\u001b[32m[I 210629 05:48:03 models:110]\u001b[39m 72 512 train loss: 0.0102513 valid loss: 0.1401590 P@1: 0.62000 P@3: 0.46333 P@5: 0.34200 N@3: 0.50194 N@5: 0.48164 early stop: 0\n",
            "\u001b[32m[I 210629 05:48:05 models:110]\u001b[39m 74 1024 train loss: 0.0129116 valid loss: 0.1410896 P@1: 0.63000 P@3: 0.46667 P@5: 0.35000 N@3: 0.50356 N@5: 0.48802 early stop: 0\n",
            "\u001b[32m[I 210629 05:48:07 models:110]\u001b[39m 77 512 train loss: 0.0189615 valid loss: 0.1421753 P@1: 0.63000 P@3: 0.46333 P@5: 0.35000 N@3: 0.50183 N@5: 0.48822 early stop: 0\n",
            "\u001b[32m[I 210629 05:48:09 models:110]\u001b[39m 79 1024 train loss: 0.0142776 valid loss: 0.1428096 P@1: 0.62000 P@3: 0.47333 P@5: 0.36000 N@3: 0.50775 N@5: 0.49472 early stop: 0\n",
            "\u001b[32m[I 210629 05:48:12 models:110]\u001b[39m 82 512 train loss: 0.0114263 valid loss: 0.1436900 P@1: 0.62000 P@3: 0.47667 P@5: 0.36000 N@3: 0.51010 N@5: 0.49535 early stop: 0\n",
            "\u001b[32m[I 210629 05:48:14 models:110]\u001b[39m 84 1024 train loss: 0.0073794 valid loss: 0.1446726 P@1: 0.62000 P@3: 0.47333 P@5: 0.36600 N@3: 0.50714 N@5: 0.49977 early stop: 0\n",
            "\u001b[32m[I 210629 05:48:17 models:110]\u001b[39m 87 512 train loss: 0.0048679 valid loss: 0.1456192 P@1: 0.62000 P@3: 0.47333 P@5: 0.36400 N@3: 0.50652 N@5: 0.49765 early stop: 1\n",
            "\u001b[32m[I 210629 05:48:19 models:110]\u001b[39m 89 1024 train loss: 0.0027913 valid loss: 0.1467326 P@1: 0.62000 P@3: 0.47000 P@5: 0.37000 N@3: 0.50479 N@5: 0.50308 early stop: 0\n",
            "\u001b[32m[I 210629 05:48:21 models:110]\u001b[39m 92 512 train loss: 0.0022997 valid loss: 0.1479163 P@1: 0.62000 P@3: 0.46667 P@5: 0.36800 N@3: 0.50244 N@5: 0.50094 early stop: 1\n",
            "\u001b[32m[I 210629 05:48:23 models:110]\u001b[39m 94 1024 train loss: 0.0021345 valid loss: 0.1492235 P@1: 0.62000 P@3: 0.46667 P@5: 0.37000 N@3: 0.50244 N@5: 0.50231 early stop: 2\n",
            "\u001b[32m[I 210629 05:48:26 models:110]\u001b[39m 97 512 train loss: 0.0013823 valid loss: 0.1507271 P@1: 0.62000 P@3: 0.47000 P@5: 0.37200 N@3: 0.50479 N@5: 0.50462 early stop: 0\n",
            "\u001b[32m[I 210629 05:48:28 models:110]\u001b[39m 99 1024 train loss: 0.0008324 valid loss: 0.1522282 P@1: 0.62000 P@3: 0.47667 P@5: 0.37400 N@3: 0.50948 N@5: 0.50627 early stop: 0\n",
            "\u001b[32m[I 210629 05:48:31 models:110]\u001b[39m 102 512 train loss: 0.0007092 valid loss: 0.1536295 P@1: 0.62000 P@3: 0.48000 P@5: 0.37800 N@3: 0.51183 N@5: 0.50875 early stop: 0\n",
            "\u001b[32m[I 210629 05:48:33 models:110]\u001b[39m 104 1024 train loss: 0.0005298 valid loss: 0.1551490 P@1: 0.62000 P@3: 0.48333 P@5: 0.37600 N@3: 0.51418 N@5: 0.50717 early stop: 1\n",
            "\u001b[32m[I 210629 05:48:35 models:110]\u001b[39m 107 512 train loss: 0.0004988 valid loss: 0.1567642 P@1: 0.62000 P@3: 0.48667 P@5: 0.37800 N@3: 0.51652 N@5: 0.51022 early stop: 0\n",
            "\u001b[32m[I 210629 05:48:38 models:110]\u001b[39m 109 1024 train loss: 0.0017626 valid loss: 0.1583285 P@1: 0.61000 P@3: 0.49333 P@5: 0.38000 N@3: 0.51948 N@5: 0.51095 early stop: 0\n",
            "\u001b[32m[I 210629 05:48:40 models:110]\u001b[39m 112 512 train loss: 0.0048265 valid loss: 0.1596329 P@1: 0.61000 P@3: 0.49667 P@5: 0.38200 N@3: 0.52244 N@5: 0.51369 early stop: 0\n",
            "\u001b[32m[I 210629 05:48:42 models:110]\u001b[39m 114 1024 train loss: 0.0069397 valid loss: 0.1612198 P@1: 0.61000 P@3: 0.49333 P@5: 0.38200 N@3: 0.52071 N@5: 0.51421 early stop: 0\n",
            "\u001b[32m[I 210629 05:48:45 models:110]\u001b[39m 117 512 train loss: 0.0111604 valid loss: 0.1619306 P@1: 0.61000 P@3: 0.49667 P@5: 0.38200 N@3: 0.52306 N@5: 0.51439 early stop: 0\n",
            "\u001b[32m[I 210629 05:48:47 models:110]\u001b[39m 119 1024 train loss: 0.0090461 valid loss: 0.1629845 P@1: 0.62000 P@3: 0.50333 P@5: 0.38000 N@3: 0.53071 N@5: 0.51544 early stop: 0\n",
            "\u001b[32m[I 210629 05:48:50 models:110]\u001b[39m 122 512 train loss: 0.0075016 valid loss: 0.1638296 P@1: 0.62000 P@3: 0.50333 P@5: 0.37800 N@3: 0.53133 N@5: 0.51453 early stop: 1\n",
            "\u001b[32m[I 210629 05:48:52 models:110]\u001b[39m 124 1024 train loss: 0.0050237 valid loss: 0.1647543 P@1: 0.62000 P@3: 0.50333 P@5: 0.37800 N@3: 0.53071 N@5: 0.51391 early stop: 2\n",
            "\u001b[32m[I 210629 05:48:54 models:110]\u001b[39m 127 512 train loss: 0.0024075 valid loss: 0.1658018 P@1: 0.62000 P@3: 0.49667 P@5: 0.38000 N@3: 0.52602 N@5: 0.51487 early stop: 3\n",
            "\u001b[32m[I 210629 05:48:56 models:110]\u001b[39m 129 1024 train loss: 0.0019265 valid loss: 0.1669498 P@1: 0.62000 P@3: 0.49667 P@5: 0.38000 N@3: 0.52602 N@5: 0.51487 early stop: 4\n",
            "\u001b[32m[I 210629 05:48:59 models:110]\u001b[39m 132 512 train loss: 0.0012514 valid loss: 0.1680814 P@1: 0.62000 P@3: 0.49333 P@5: 0.38200 N@3: 0.52306 N@5: 0.51606 early stop: 0\n",
            "\u001b[32m[I 210629 05:49:01 models:110]\u001b[39m 134 1024 train loss: 0.0007592 valid loss: 0.1693946 P@1: 0.62000 P@3: 0.49333 P@5: 0.38200 N@3: 0.52367 N@5: 0.51668 early stop: 0\n",
            "\u001b[32m[I 210629 05:49:04 models:110]\u001b[39m 137 512 train loss: 0.0005199 valid loss: 0.1705891 P@1: 0.62000 P@3: 0.49333 P@5: 0.38000 N@3: 0.52367 N@5: 0.51477 early stop: 1\n",
            "\u001b[32m[I 210629 05:49:06 models:110]\u001b[39m 139 1024 train loss: 0.0003816 valid loss: 0.1719062 P@1: 0.62000 P@3: 0.49333 P@5: 0.38000 N@3: 0.52367 N@5: 0.51498 early stop: 2\n",
            "\u001b[32m[I 210629 05:49:08 models:110]\u001b[39m 142 512 train loss: 0.0003139 valid loss: 0.1731462 P@1: 0.62000 P@3: 0.49333 P@5: 0.38000 N@3: 0.52367 N@5: 0.51498 early stop: 3\n",
            "\u001b[32m[I 210629 05:49:10 models:110]\u001b[39m 144 1024 train loss: 0.0002487 valid loss: 0.1744431 P@1: 0.62000 P@3: 0.49333 P@5: 0.38200 N@3: 0.52367 N@5: 0.51629 early stop: 4\n",
            "\u001b[32m[I 210629 05:49:13 models:110]\u001b[39m 147 512 train loss: 0.0002702 valid loss: 0.1757495 P@1: 0.62000 P@3: 0.49333 P@5: 0.38400 N@3: 0.52367 N@5: 0.51810 early stop: 0\n",
            "\u001b[32m[I 210629 05:49:15 models:110]\u001b[39m 149 1024 train loss: 0.0002452 valid loss: 0.1770037 P@1: 0.62000 P@3: 0.49333 P@5: 0.38600 N@3: 0.52367 N@5: 0.51968 early stop: 0\n",
            "\u001b[32m[I 210629 05:49:18 models:110]\u001b[39m 152 512 train loss: 0.0002289 valid loss: 0.1783019 P@1: 0.62000 P@3: 0.49333 P@5: 0.38600 N@3: 0.52367 N@5: 0.51951 early stop: 1\n",
            "\u001b[32m[I 210629 05:49:20 models:110]\u001b[39m 154 1024 train loss: 0.0002553 valid loss: 0.1795891 P@1: 0.62000 P@3: 0.49333 P@5: 0.38800 N@3: 0.52429 N@5: 0.52146 early stop: 0\n",
            "\u001b[32m[I 210629 05:49:22 models:110]\u001b[39m 157 512 train loss: 0.0002222 valid loss: 0.1809043 P@1: 0.62000 P@3: 0.50000 P@5: 0.39000 N@3: 0.52898 N@5: 0.52346 early stop: 0\n",
            "\u001b[32m[I 210629 05:49:24 models:110]\u001b[39m 159 1024 train loss: 0.0001717 valid loss: 0.1821868 P@1: 0.61000 P@3: 0.50333 P@5: 0.39000 N@3: 0.52960 N@5: 0.52235 early stop: 1\n",
            "\u001b[33m[W 210629 05:49:25 models:137]\u001b[39m Clipping gradients with total norm 0.13245 and max norm 0.01776\n",
            "\u001b[32m[I 210629 05:49:27 models:110]\u001b[39m 162 512 train loss: 0.0004704 valid loss: 0.1834559 P@1: 0.61000 P@3: 0.50333 P@5: 0.39000 N@3: 0.52960 N@5: 0.52255 early stop: 2\n",
            "\u001b[32m[I 210629 05:49:29 models:110]\u001b[39m 164 1024 train loss: 0.0002981 valid loss: 0.1847160 P@1: 0.61000 P@3: 0.49667 P@5: 0.39200 N@3: 0.52552 N@5: 0.52375 early stop: 0\n",
            "\u001b[32m[I 210629 05:49:32 models:110]\u001b[39m 167 512 train loss: 0.0001484 valid loss: 0.1859483 P@1: 0.61000 P@3: 0.49667 P@5: 0.39000 N@3: 0.52552 N@5: 0.52244 early stop: 1\n",
            "\u001b[32m[I 210629 05:49:34 models:110]\u001b[39m 169 1024 train loss: 0.0002737 valid loss: 0.1872363 P@1: 0.61000 P@3: 0.49667 P@5: 0.39000 N@3: 0.52552 N@5: 0.52224 early stop: 2\n",
            "\u001b[32m[I 210629 05:49:36 models:110]\u001b[39m 172 512 train loss: 0.0001669 valid loss: 0.1884907 P@1: 0.62000 P@3: 0.49667 P@5: 0.39000 N@3: 0.52786 N@5: 0.52441 early stop: 0\n",
            "\u001b[32m[I 210629 05:49:39 models:110]\u001b[39m 174 1024 train loss: 0.0001711 valid loss: 0.1897477 P@1: 0.62000 P@3: 0.49667 P@5: 0.38800 N@3: 0.52786 N@5: 0.52310 early stop: 1\n",
            "\u001b[32m[I 210629 05:49:41 models:110]\u001b[39m 177 512 train loss: 0.0002569 valid loss: 0.1909724 P@1: 0.62000 P@3: 0.49667 P@5: 0.38800 N@3: 0.52786 N@5: 0.52310 early stop: 2\n",
            "\u001b[32m[I 210629 05:49:43 models:110]\u001b[39m 179 1024 train loss: 0.0001747 valid loss: 0.1921703 P@1: 0.63000 P@3: 0.49667 P@5: 0.39000 N@3: 0.52960 N@5: 0.52567 early stop: 0\n",
            "\u001b[32m[I 210629 05:49:46 models:110]\u001b[39m 182 512 train loss: 0.0002189 valid loss: 0.1933694 P@1: 0.63000 P@3: 0.50000 P@5: 0.39200 N@3: 0.53194 N@5: 0.52747 early stop: 0\n",
            "\u001b[33m[W 210629 05:49:48 models:137]\u001b[39m Clipping gradients with total norm 0.08628 and max norm 0.01463\n",
            "\u001b[32m[I 210629 05:49:48 models:110]\u001b[39m 184 1024 train loss: 0.0003208 valid loss: 0.1945625 P@1: 0.63000 P@3: 0.49667 P@5: 0.39400 N@3: 0.52960 N@5: 0.52855 early stop: 0\n",
            "\u001b[32m[I 210629 05:49:50 models:110]\u001b[39m 187 512 train loss: 0.0003272 valid loss: 0.1957837 P@1: 0.63000 P@3: 0.49667 P@5: 0.39200 N@3: 0.52960 N@5: 0.52724 early stop: 1\n",
            "\u001b[32m[I 210629 05:49:53 models:110]\u001b[39m 189 1024 train loss: 0.0009143 valid loss: 0.1969007 P@1: 0.63000 P@3: 0.49667 P@5: 0.39400 N@3: 0.52960 N@5: 0.52861 early stop: 0\n",
            "\u001b[32m[I 210629 05:49:55 models:110]\u001b[39m 192 512 train loss: 0.0004684 valid loss: 0.1979786 P@1: 0.63000 P@3: 0.50000 P@5: 0.39400 N@3: 0.53194 N@5: 0.52843 early stop: 1\n",
            "\u001b[32m[I 210629 05:49:57 models:110]\u001b[39m 194 1024 train loss: 0.0002325 valid loss: 0.1989879 P@1: 0.63000 P@3: 0.50333 P@5: 0.39600 N@3: 0.53429 N@5: 0.52998 early stop: 0\n",
            "\u001b[32m[I 210629 05:50:00 models:110]\u001b[39m 197 512 train loss: 0.0002353 valid loss: 0.2001428 P@1: 0.63000 P@3: 0.50333 P@5: 0.39600 N@3: 0.53429 N@5: 0.52998 early stop: 1\n",
            "\u001b[32m[I 210629 05:50:02 models:110]\u001b[39m 199 1024 train loss: 0.0002599 valid loss: 0.2011927 P@1: 0.63000 P@3: 0.50333 P@5: 0.39800 N@3: 0.53429 N@5: 0.53194 early stop: 0\n",
            "\u001b[32m[I 210629 05:50:05 models:110]\u001b[39m 202 512 train loss: 0.0005732 valid loss: 0.2022769 P@1: 0.63000 P@3: 0.50333 P@5: 0.39800 N@3: 0.53490 N@5: 0.53255 early stop: 0\n",
            "\u001b[32m[I 210629 05:50:07 models:110]\u001b[39m 204 1024 train loss: 0.0004721 valid loss: 0.2032992 P@1: 0.63000 P@3: 0.50333 P@5: 0.40000 N@3: 0.53490 N@5: 0.53487 early stop: 0\n",
            "\u001b[32m[I 210629 05:50:09 models:110]\u001b[39m 207 512 train loss: 0.0031718 valid loss: 0.2044083 P@1: 0.63000 P@3: 0.50333 P@5: 0.40000 N@3: 0.53429 N@5: 0.53467 early stop: 1\n",
            "\u001b[32m[I 210629 05:50:11 models:110]\u001b[39m 209 1024 train loss: 0.0086203 valid loss: 0.2048228 P@1: 0.63000 P@3: 0.50667 P@5: 0.40000 N@3: 0.53663 N@5: 0.53509 early stop: 0\n",
            "\u001b[32m[I 210629 05:50:14 models:110]\u001b[39m 212 512 train loss: 0.0142824 valid loss: 0.2051060 P@1: 0.64000 P@3: 0.50000 P@5: 0.40000 N@3: 0.53367 N@5: 0.53622 early stop: 0\n",
            "\u001b[32m[I 210629 05:50:16 models:110]\u001b[39m 214 1024 train loss: 0.0158696 valid loss: 0.2053026 P@1: 0.64000 P@3: 0.50000 P@5: 0.39600 N@3: 0.53367 N@5: 0.53213 early stop: 1\n",
            "\u001b[32m[I 210629 05:50:19 models:110]\u001b[39m 217 512 train loss: 0.0205758 valid loss: 0.2049853 P@1: 0.64000 P@3: 0.49667 P@5: 0.39600 N@3: 0.53133 N@5: 0.53145 early stop: 2\n",
            "\u001b[32m[I 210629 05:50:21 models:110]\u001b[39m 219 1024 train loss: 0.0125248 valid loss: 0.2046575 P@1: 0.64000 P@3: 0.49667 P@5: 0.39400 N@3: 0.53133 N@5: 0.52978 early stop: 3\n",
            "\u001b[32m[I 210629 05:50:23 models:110]\u001b[39m 222 512 train loss: 0.0077723 valid loss: 0.2043319 P@1: 0.64000 P@3: 0.49667 P@5: 0.39600 N@3: 0.53133 N@5: 0.53139 early stop: 4\n",
            "\u001b[32m[I 210629 05:50:26 models:110]\u001b[39m 224 1024 train loss: 0.0058282 valid loss: 0.2042160 P@1: 0.63000 P@3: 0.49333 P@5: 0.39600 N@3: 0.52725 N@5: 0.52942 early stop: 5\n",
            "\u001b[32m[I 210629 05:50:28 models:110]\u001b[39m 227 512 train loss: 0.0036563 valid loss: 0.2042166 P@1: 0.63000 P@3: 0.49333 P@5: 0.39600 N@3: 0.52725 N@5: 0.52942 early stop: 6\n",
            "\u001b[32m[I 210629 05:50:30 models:110]\u001b[39m 229 1024 train loss: 0.0020887 valid loss: 0.2044845 P@1: 0.63000 P@3: 0.48667 P@5: 0.39600 N@3: 0.52256 N@5: 0.52892 early stop: 7\n",
            "\u001b[32m[I 210629 05:50:33 models:110]\u001b[39m 232 512 train loss: 0.0018186 valid loss: 0.2047200 P@1: 0.63000 P@3: 0.48667 P@5: 0.39600 N@3: 0.52194 N@5: 0.52848 early stop: 8\n",
            "\u001b[32m[I 210629 05:50:35 models:110]\u001b[39m 234 1024 train loss: 0.0010671 valid loss: 0.2050087 P@1: 0.63000 P@3: 0.49000 P@5: 0.39400 N@3: 0.52429 N@5: 0.52725 early stop: 9\n",
            "\u001b[32m[I 210629 05:50:37 models:110]\u001b[39m 237 512 train loss: 0.0006210 valid loss: 0.2053606 P@1: 0.63000 P@3: 0.49000 P@5: 0.39600 N@3: 0.52429 N@5: 0.52876 early stop: 10\n",
            "\u001b[32m[I 210629 05:50:39 models:110]\u001b[39m 239 1024 train loss: 0.0005835 valid loss: 0.2057830 P@1: 0.63000 P@3: 0.49000 P@5: 0.39400 N@3: 0.52429 N@5: 0.52710 early stop: 11\n",
            "\u001b[32m[I 210629 05:50:42 models:110]\u001b[39m 242 512 train loss: 0.0002902 valid loss: 0.2061874 P@1: 0.63000 P@3: 0.49000 P@5: 0.39400 N@3: 0.52429 N@5: 0.52710 early stop: 12\n",
            "\u001b[32m[I 210629 05:50:44 models:110]\u001b[39m 244 1024 train loss: 0.0002434 valid loss: 0.2066506 P@1: 0.63000 P@3: 0.49333 P@5: 0.39600 N@3: 0.52663 N@5: 0.52924 early stop: 13\n",
            "\u001b[32m[I 210629 05:50:47 models:110]\u001b[39m 247 512 train loss: 0.0002492 valid loss: 0.2071442 P@1: 0.63000 P@3: 0.49667 P@5: 0.39600 N@3: 0.52960 N@5: 0.52995 early stop: 14\n",
            "\u001b[32m[I 210629 05:50:49 models:110]\u001b[39m 249 1024 train loss: 0.0001759 valid loss: 0.2076146 P@1: 0.63000 P@3: 0.49667 P@5: 0.39800 N@3: 0.52960 N@5: 0.53177 early stop: 15\n",
            "\u001b[32m[I 210629 05:50:51 models:110]\u001b[39m 252 512 train loss: 0.0002291 valid loss: 0.2081051 P@1: 0.63000 P@3: 0.49667 P@5: 0.40000 N@3: 0.52960 N@5: 0.53358 early stop: 16\n",
            "\u001b[32m[I 210629 05:50:53 models:110]\u001b[39m 254 1024 train loss: 0.0001723 valid loss: 0.2086164 P@1: 0.63000 P@3: 0.49667 P@5: 0.40000 N@3: 0.52960 N@5: 0.53358 early stop: 17\n",
            "\u001b[32m[I 210629 05:50:56 models:110]\u001b[39m 257 512 train loss: 0.0002297 valid loss: 0.2091284 P@1: 0.63000 P@3: 0.50000 P@5: 0.40000 N@3: 0.53317 N@5: 0.53488 early stop: 18\n",
            "\u001b[32m[I 210629 05:50:58 models:110]\u001b[39m 259 1024 train loss: 0.0001699 valid loss: 0.2096595 P@1: 0.63000 P@3: 0.50000 P@5: 0.40000 N@3: 0.53317 N@5: 0.53488 early stop: 19\n",
            "\u001b[33m[W 210629 05:50:59 models:137]\u001b[39m Clipping gradients with total norm 0.0965 and max norm 0.01248\n",
            "\u001b[32m[I 210629 05:51:00 models:110]\u001b[39m 262 512 train loss: 0.0003617 valid loss: 0.2101959 P@1: 0.63000 P@3: 0.50000 P@5: 0.40000 N@3: 0.53317 N@5: 0.53488 early stop: 20\n",
            "\u001b[32m[I 210629 05:51:03 models:110]\u001b[39m 264 1024 train loss: 0.0002499 valid loss: 0.2107351 P@1: 0.63000 P@3: 0.50000 P@5: 0.40000 N@3: 0.53317 N@5: 0.53488 early stop: 21\n",
            "\u001b[32m[I 210629 05:51:05 models:110]\u001b[39m 267 512 train loss: 0.0001659 valid loss: 0.2112913 P@1: 0.63000 P@3: 0.50000 P@5: 0.40000 N@3: 0.53317 N@5: 0.53488 early stop: 22\n",
            "\u001b[32m[I 210629 05:51:07 models:110]\u001b[39m 269 1024 train loss: 0.0001906 valid loss: 0.2118422 P@1: 0.63000 P@3: 0.50000 P@5: 0.39800 N@3: 0.53317 N@5: 0.53306 early stop: 23\n",
            "\u001b[32m[I 210629 05:51:10 models:110]\u001b[39m 272 512 train loss: 0.0002189 valid loss: 0.2123914 P@1: 0.63000 P@3: 0.50000 P@5: 0.40000 N@3: 0.53317 N@5: 0.53488 early stop: 24\n",
            "\u001b[32m[I 210629 05:51:12 models:110]\u001b[39m 274 1024 train loss: 0.0001362 valid loss: 0.2129438 P@1: 0.63000 P@3: 0.50000 P@5: 0.40000 N@3: 0.53256 N@5: 0.53458 early stop: 25\n",
            "\u001b[32m[I 210629 05:51:14 models:110]\u001b[39m 277 512 train loss: 0.0001753 valid loss: 0.2134846 P@1: 0.63000 P@3: 0.50000 P@5: 0.40200 N@3: 0.53256 N@5: 0.53645 early stop: 0\n",
            "\u001b[33m[W 210629 05:51:16 models:137]\u001b[39m Clipping gradients with total norm 0.0666 and max norm 0.01315\n",
            "\u001b[32m[I 210629 05:51:17 models:110]\u001b[39m 279 1024 train loss: 0.0002916 valid loss: 0.2140327 P@1: 0.63000 P@3: 0.50000 P@5: 0.40200 N@3: 0.53256 N@5: 0.53645 early stop: 1\n",
            "\u001b[32m[I 210629 05:51:19 models:110]\u001b[39m 282 512 train loss: 0.0003475 valid loss: 0.2145795 P@1: 0.63000 P@3: 0.50000 P@5: 0.40200 N@3: 0.53194 N@5: 0.53601 early stop: 2\n",
            "\u001b[32m[I 210629 05:51:21 models:110]\u001b[39m 284 1024 train loss: 0.0001932 valid loss: 0.2151224 P@1: 0.63000 P@3: 0.50000 P@5: 0.40400 N@3: 0.53194 N@5: 0.53732 early stop: 0\n",
            "\u001b[32m[I 210629 05:51:24 models:110]\u001b[39m 287 512 train loss: 0.0001879 valid loss: 0.2156682 P@1: 0.63000 P@3: 0.50000 P@5: 0.40400 N@3: 0.53194 N@5: 0.53732 early stop: 1\n",
            "\u001b[32m[I 210629 05:51:26 models:110]\u001b[39m 289 1024 train loss: 0.0002823 valid loss: 0.2162076 P@1: 0.63000 P@3: 0.50000 P@5: 0.40400 N@3: 0.53194 N@5: 0.53732 early stop: 2\n",
            "\u001b[32m[I 210629 05:51:28 models:110]\u001b[39m 292 512 train loss: 0.0003149 valid loss: 0.2167287 P@1: 0.63000 P@3: 0.50000 P@5: 0.40400 N@3: 0.53194 N@5: 0.53747 early stop: 0\n",
            "\u001b[33m[W 210629 05:51:29 models:137]\u001b[39m Clipping gradients with total norm 0.08104 and max norm 0.00862\n",
            "\u001b[32m[I 210629 05:51:31 models:110]\u001b[39m 294 1024 train loss: 0.0004670 valid loss: 0.2172619 P@1: 0.64000 P@3: 0.50000 P@5: 0.40400 N@3: 0.53367 N@5: 0.53872 early stop: 0\n",
            "\u001b[32m[I 210629 05:51:33 models:110]\u001b[39m 297 512 train loss: 0.0002604 valid loss: 0.2178231 P@1: 0.64000 P@3: 0.50000 P@5: 0.40400 N@3: 0.53367 N@5: 0.53872 early stop: 1\n",
            "\u001b[33m[W 210629 05:51:35 models:137]\u001b[39m Clipping gradients with total norm 0.04014 and max norm 0.00755\n",
            "\u001b[32m[I 210629 05:51:35 models:110]\u001b[39m 299 1024 train loss: 0.0002398 valid loss: 0.2184006 P@1: 0.64000 P@3: 0.50333 P@5: 0.40400 N@3: 0.53602 N@5: 0.53896 early stop: 0\n",
            "\u001b[32m[I 210629 05:51:38 models:110]\u001b[39m 302 512 train loss: 0.0002156 valid loss: 0.2189847 P@1: 0.64000 P@3: 0.50333 P@5: 0.40400 N@3: 0.53602 N@5: 0.53896 early stop: 1\n",
            "\u001b[32m[I 210629 05:51:40 models:110]\u001b[39m 304 1024 train loss: 0.0002081 valid loss: 0.2195173 P@1: 0.64000 P@3: 0.50333 P@5: 0.40400 N@3: 0.53663 N@5: 0.53940 early stop: 0\n",
            "\u001b[32m[I 210629 05:51:43 models:110]\u001b[39m 307 512 train loss: 0.0002357 valid loss: 0.2200324 P@1: 0.64000 P@3: 0.50333 P@5: 0.40400 N@3: 0.53663 N@5: 0.53940 early stop: 1\n",
            "\u001b[32m[I 210629 05:51:45 models:110]\u001b[39m 309 1024 train loss: 0.0001388 valid loss: 0.2205563 P@1: 0.64000 P@3: 0.50333 P@5: 0.40400 N@3: 0.53663 N@5: 0.53940 early stop: 2\n",
            "\u001b[32m[I 210629 05:51:47 models:110]\u001b[39m 312 512 train loss: 0.0001562 valid loss: 0.2210879 P@1: 0.64000 P@3: 0.50333 P@5: 0.40400 N@3: 0.53663 N@5: 0.53940 early stop: 3\n",
            "\u001b[32m[I 210629 05:51:49 models:110]\u001b[39m 314 1024 train loss: 0.0001345 valid loss: 0.2216290 P@1: 0.64000 P@3: 0.50333 P@5: 0.40400 N@3: 0.53663 N@5: 0.53940 early stop: 4\n",
            "\u001b[32m[I 210629 05:51:52 models:110]\u001b[39m 317 512 train loss: 0.0001847 valid loss: 0.2221742 P@1: 0.64000 P@3: 0.50333 P@5: 0.40400 N@3: 0.53663 N@5: 0.53955 early stop: 0\n",
            "\u001b[32m[I 210629 05:51:54 models:110]\u001b[39m 319 1024 train loss: 0.0001160 valid loss: 0.2227192 P@1: 0.64000 P@3: 0.50667 P@5: 0.40400 N@3: 0.53898 N@5: 0.54008 early stop: 0\n",
            "\u001b[32m[I 210629 05:51:57 models:110]\u001b[39m 322 512 train loss: 0.0001350 valid loss: 0.2232568 P@1: 0.64000 P@3: 0.50667 P@5: 0.40200 N@3: 0.53898 N@5: 0.53897 early stop: 1\n",
            "\u001b[32m[I 210629 05:51:59 models:110]\u001b[39m 324 1024 train loss: 0.0001189 valid loss: 0.2237909 P@1: 0.64000 P@3: 0.50667 P@5: 0.40200 N@3: 0.53898 N@5: 0.53897 early stop: 2\n",
            "\u001b[32m[I 210629 05:52:01 models:110]\u001b[39m 327 512 train loss: 0.0001125 valid loss: 0.2243338 P@1: 0.64000 P@3: 0.50667 P@5: 0.40200 N@3: 0.53837 N@5: 0.53836 early stop: 3\n",
            "\u001b[32m[I 210629 05:52:03 models:110]\u001b[39m 329 1024 train loss: 0.0001500 valid loss: 0.2248731 P@1: 0.64000 P@3: 0.50667 P@5: 0.40400 N@3: 0.53837 N@5: 0.53967 early stop: 4\n",
            "\u001b[32m[I 210629 05:52:06 models:110]\u001b[39m 332 512 train loss: 0.0001534 valid loss: 0.2254045 P@1: 0.64000 P@3: 0.50667 P@5: 0.40400 N@3: 0.53837 N@5: 0.53967 early stop: 5\n",
            "\u001b[32m[I 210629 05:52:08 models:110]\u001b[39m 334 1024 train loss: 0.0000882 valid loss: 0.2259402 P@1: 0.64000 P@3: 0.50667 P@5: 0.40400 N@3: 0.53837 N@5: 0.53967 early stop: 6\n",
            "\u001b[32m[I 210629 05:52:11 models:110]\u001b[39m 337 512 train loss: 0.0001146 valid loss: 0.2264868 P@1: 0.64000 P@3: 0.50667 P@5: 0.40600 N@3: 0.53837 N@5: 0.54118 early stop: 0\n",
            "\u001b[33m[W 210629 05:52:11 models:137]\u001b[39m Clipping gradients with total norm 0.08788 and max norm 0.00813\n",
            "\u001b[32m[I 210629 05:52:13 models:110]\u001b[39m 339 1024 train loss: 0.0005174 valid loss: 0.2270069 P@1: 0.64000 P@3: 0.50667 P@5: 0.40600 N@3: 0.53837 N@5: 0.54118 early stop: 1\n",
            "\u001b[32m[I 210629 05:52:15 models:110]\u001b[39m 342 512 train loss: 0.0001247 valid loss: 0.2275164 P@1: 0.64000 P@3: 0.50667 P@5: 0.40600 N@3: 0.53837 N@5: 0.54118 early stop: 2\n",
            "\u001b[33m[W 210629 05:52:17 models:137]\u001b[39m Clipping gradients with total norm 0.10313 and max norm 0.00637\n",
            "\u001b[32m[I 210629 05:52:17 models:110]\u001b[39m 344 1024 train loss: 0.0005046 valid loss: 0.2280364 P@1: 0.64000 P@3: 0.50667 P@5: 0.40600 N@3: 0.53837 N@5: 0.54118 early stop: 3\n",
            "\u001b[32m[I 210629 05:52:20 models:110]\u001b[39m 347 512 train loss: 0.0001130 valid loss: 0.2285626 P@1: 0.64000 P@3: 0.51000 P@5: 0.40600 N@3: 0.54071 N@5: 0.54145 early stop: 0\n",
            "\u001b[32m[I 210629 05:52:22 models:110]\u001b[39m 349 1024 train loss: 0.0001317 valid loss: 0.2290943 P@1: 0.64000 P@3: 0.50667 P@5: 0.40600 N@3: 0.53837 N@5: 0.54122 early stop: 1\n",
            "\u001b[32m[I 210629 05:52:25 models:110]\u001b[39m 352 512 train loss: 0.0001459 valid loss: 0.2296222 P@1: 0.64000 P@3: 0.50667 P@5: 0.40600 N@3: 0.53837 N@5: 0.54122 early stop: 2\n",
            "\u001b[32m[I 210629 05:52:27 models:110]\u001b[39m 354 1024 train loss: 0.0000897 valid loss: 0.2301406 P@1: 0.64000 P@3: 0.50667 P@5: 0.40600 N@3: 0.53837 N@5: 0.54122 early stop: 3\n",
            "\u001b[32m[I 210629 05:52:29 models:110]\u001b[39m 357 512 train loss: 0.0001571 valid loss: 0.2306519 P@1: 0.64000 P@3: 0.50667 P@5: 0.40600 N@3: 0.53837 N@5: 0.54122 early stop: 4\n",
            "\u001b[32m[I 210629 05:52:31 models:110]\u001b[39m 359 1024 train loss: 0.0000856 valid loss: 0.2311598 P@1: 0.64000 P@3: 0.50667 P@5: 0.40600 N@3: 0.53898 N@5: 0.54204 early stop: 0\n",
            "\u001b[32m[I 210629 05:52:34 models:110]\u001b[39m 362 512 train loss: 0.0001154 valid loss: 0.2316684 P@1: 0.64000 P@3: 0.50667 P@5: 0.40600 N@3: 0.53898 N@5: 0.54204 early stop: 1\n",
            "\u001b[32m[I 210629 05:52:36 models:110]\u001b[39m 364 1024 train loss: 0.0001437 valid loss: 0.2321800 P@1: 0.63000 P@3: 0.50667 P@5: 0.41000 N@3: 0.53725 N@5: 0.54293 early stop: 0\n",
            "\u001b[32m[I 210629 05:52:38 models:110]\u001b[39m 367 512 train loss: 0.0002761 valid loss: 0.2326939 P@1: 0.63000 P@3: 0.50667 P@5: 0.41000 N@3: 0.53663 N@5: 0.54246 early stop: 1\n",
            "\u001b[32m[I 210629 05:52:41 models:110]\u001b[39m 369 1024 train loss: 0.0001157 valid loss: 0.2331992 P@1: 0.64000 P@3: 0.51000 P@5: 0.41200 N@3: 0.54071 N@5: 0.54625 early stop: 0\n",
            "\u001b[32m[I 210629 05:52:43 models:110]\u001b[39m 372 512 train loss: 0.0001580 valid loss: 0.2336989 P@1: 0.64000 P@3: 0.51000 P@5: 0.41200 N@3: 0.54071 N@5: 0.54625 early stop: 1\n",
            "\u001b[32m[I 210629 05:52:45 models:110]\u001b[39m 374 1024 train loss: 0.0001197 valid loss: 0.2341977 P@1: 0.64000 P@3: 0.51000 P@5: 0.41200 N@3: 0.54071 N@5: 0.54625 early stop: 2\n",
            "\u001b[32m[I 210629 05:52:48 models:110]\u001b[39m 377 512 train loss: 0.0001208 valid loss: 0.2346883 P@1: 0.64000 P@3: 0.51000 P@5: 0.41200 N@3: 0.54071 N@5: 0.54645 early stop: 0\n",
            "\u001b[32m[I 210629 05:52:50 models:110]\u001b[39m 379 1024 train loss: 0.0001123 valid loss: 0.2351742 P@1: 0.64000 P@3: 0.51000 P@5: 0.41200 N@3: 0.54071 N@5: 0.54645 early stop: 1\n",
            "\u001b[32m[I 210629 05:52:53 models:110]\u001b[39m 382 512 train loss: 0.0001035 valid loss: 0.2356592 P@1: 0.64000 P@3: 0.51000 P@5: 0.41200 N@3: 0.54071 N@5: 0.54645 early stop: 2\n",
            "\u001b[32m[I 210629 05:52:55 models:110]\u001b[39m 384 1024 train loss: 0.0001146 valid loss: 0.2361422 P@1: 0.63000 P@3: 0.51000 P@5: 0.41200 N@3: 0.53898 N@5: 0.54472 early stop: 3\n",
            "\u001b[32m[I 210629 05:52:57 models:110]\u001b[39m 387 512 train loss: 0.0001114 valid loss: 0.2366228 P@1: 0.63000 P@3: 0.51000 P@5: 0.41200 N@3: 0.53898 N@5: 0.54472 early stop: 4\n",
            "\u001b[33m[W 210629 05:52:58 models:137]\u001b[39m Clipping gradients with total norm 0.04196 and max norm 0.0076\n",
            "\u001b[32m[I 210629 05:52:59 models:110]\u001b[39m 389 1024 train loss: 0.0002334 valid loss: 0.2370956 P@1: 0.63000 P@3: 0.51000 P@5: 0.41200 N@3: 0.53898 N@5: 0.54472 early stop: 5\n",
            "\u001b[32m[I 210629 05:53:02 models:110]\u001b[39m 392 512 train loss: 0.0002709 valid loss: 0.2375736 P@1: 0.63000 P@3: 0.51000 P@5: 0.41200 N@3: 0.53898 N@5: 0.54472 early stop: 6\n",
            "\u001b[32m[I 210629 05:53:04 models:110]\u001b[39m 394 1024 train loss: 0.0001235 valid loss: 0.2380655 P@1: 0.63000 P@3: 0.51000 P@5: 0.41200 N@3: 0.53898 N@5: 0.54472 early stop: 7\n",
            "\u001b[32m[I 210629 05:53:07 models:110]\u001b[39m 397 512 train loss: 0.0001203 valid loss: 0.2385541 P@1: 0.63000 P@3: 0.51000 P@5: 0.41200 N@3: 0.53898 N@5: 0.54472 early stop: 8\n",
            "\u001b[32m[I 210629 05:53:09 models:110]\u001b[39m 399 1024 train loss: 0.0001064 valid loss: 0.2390317 P@1: 0.63000 P@3: 0.50667 P@5: 0.41200 N@3: 0.53663 N@5: 0.54439 early stop: 9\n",
            "\u001b[33m[W 210629 05:53:10 models:137]\u001b[39m Clipping gradients with total norm 0.06946 and max norm 0.00954\n",
            "\u001b[32m[I 210629 05:53:11 models:110]\u001b[39m 402 512 train loss: 0.0003718 valid loss: 0.2395090 P@1: 0.64000 P@3: 0.50667 P@5: 0.41200 N@3: 0.53837 N@5: 0.54569 early stop: 10\n",
            "\u001b[32m[I 210629 05:53:13 models:110]\u001b[39m 404 1024 train loss: 0.0002576 valid loss: 0.2399627 P@1: 0.64000 P@3: 0.50667 P@5: 0.41200 N@3: 0.53837 N@5: 0.54569 early stop: 11\n",
            "\u001b[32m[I 210629 05:53:16 models:110]\u001b[39m 407 512 train loss: 0.0002322 valid loss: 0.2404004 P@1: 0.64000 P@3: 0.50667 P@5: 0.41200 N@3: 0.53837 N@5: 0.54569 early stop: 12\n",
            "\u001b[32m[I 210629 05:53:18 models:110]\u001b[39m 409 1024 train loss: 0.0001592 valid loss: 0.2408693 P@1: 0.64000 P@3: 0.50667 P@5: 0.41200 N@3: 0.53837 N@5: 0.54569 early stop: 13\n",
            "\u001b[32m[I 210629 05:53:20 models:110]\u001b[39m 412 512 train loss: 0.0001719 valid loss: 0.2413355 P@1: 0.64000 P@3: 0.50667 P@5: 0.41200 N@3: 0.53837 N@5: 0.54569 early stop: 14\n",
            "\u001b[32m[I 210629 05:53:23 models:110]\u001b[39m 414 1024 train loss: 0.0000869 valid loss: 0.2418022 P@1: 0.64000 P@3: 0.50667 P@5: 0.41200 N@3: 0.53837 N@5: 0.54569 early stop: 15\n",
            "\u001b[32m[I 210629 05:53:25 models:110]\u001b[39m 417 512 train loss: 0.0001471 valid loss: 0.2422670 P@1: 0.63000 P@3: 0.50667 P@5: 0.41200 N@3: 0.53663 N@5: 0.54425 early stop: 16\n",
            "\u001b[32m[I 210629 05:53:27 models:110]\u001b[39m 419 1024 train loss: 0.0000917 valid loss: 0.2427250 P@1: 0.63000 P@3: 0.50667 P@5: 0.41200 N@3: 0.53663 N@5: 0.54425 early stop: 17\n",
            "\u001b[32m[I 210629 05:53:30 models:110]\u001b[39m 422 512 train loss: 0.0000875 valid loss: 0.2431833 P@1: 0.63000 P@3: 0.50667 P@5: 0.41200 N@3: 0.53663 N@5: 0.54425 early stop: 18\n",
            "\u001b[32m[I 210629 05:53:32 models:110]\u001b[39m 424 1024 train loss: 0.0001342 valid loss: 0.2436358 P@1: 0.63000 P@3: 0.51000 P@5: 0.41000 N@3: 0.53898 N@5: 0.54326 early stop: 19\n",
            "\u001b[32m[I 210629 05:53:34 models:110]\u001b[39m 427 512 train loss: 0.0001035 valid loss: 0.2440883 P@1: 0.63000 P@3: 0.51000 P@5: 0.41000 N@3: 0.53898 N@5: 0.54326 early stop: 20\n",
            "\u001b[32m[I 210629 05:53:36 models:110]\u001b[39m 429 1024 train loss: 0.0001171 valid loss: 0.2445448 P@1: 0.63000 P@3: 0.51000 P@5: 0.41000 N@3: 0.53898 N@5: 0.54326 early stop: 21\n",
            "\u001b[32m[I 210629 05:53:39 models:110]\u001b[39m 432 512 train loss: 0.0001222 valid loss: 0.2450018 P@1: 0.63000 P@3: 0.51000 P@5: 0.41000 N@3: 0.53898 N@5: 0.54326 early stop: 22\n",
            "\u001b[32m[I 210629 05:53:41 models:110]\u001b[39m 434 1024 train loss: 0.0000978 valid loss: 0.2454637 P@1: 0.63000 P@3: 0.51000 P@5: 0.41000 N@3: 0.53898 N@5: 0.54326 early stop: 23\n",
            "\u001b[32m[I 210629 05:53:44 models:110]\u001b[39m 437 512 train loss: 0.0001425 valid loss: 0.2459141 P@1: 0.63000 P@3: 0.51000 P@5: 0.41000 N@3: 0.53898 N@5: 0.54326 early stop: 24\n",
            "\u001b[33m[W 210629 05:53:45 models:137]\u001b[39m Clipping gradients with total norm 0.00406 and max norm 0.00076\n",
            "\u001b[33m[W 210629 05:53:45 models:137]\u001b[39m Clipping gradients with total norm 0.06145 and max norm 0.00152\n",
            "\u001b[32m[I 210629 05:53:46 models:110]\u001b[39m 439 1024 train loss: 0.0003131 valid loss: 0.2463582 P@1: 0.63000 P@3: 0.51000 P@5: 0.41200 N@3: 0.53898 N@5: 0.54507 early stop: 25\n",
            "\u001b[32m[I 210629 05:53:48 models:110]\u001b[39m 442 512 train loss: 0.0001292 valid loss: 0.2468050 P@1: 0.63000 P@3: 0.51000 P@5: 0.41200 N@3: 0.53898 N@5: 0.54507 early stop: 26\n",
            "\u001b[32m[I 210629 05:53:51 models:110]\u001b[39m 444 1024 train loss: 0.0000810 valid loss: 0.2472450 P@1: 0.63000 P@3: 0.51000 P@5: 0.41200 N@3: 0.53898 N@5: 0.54493 early stop: 27\n",
            "\u001b[32m[I 210629 05:53:54 models:110]\u001b[39m 447 512 train loss: 0.0000981 valid loss: 0.2476836 P@1: 0.63000 P@3: 0.51000 P@5: 0.41200 N@3: 0.53898 N@5: 0.54493 early stop: 28\n",
            "\u001b[32m[I 210629 05:53:56 models:110]\u001b[39m 449 1024 train loss: 0.0001159 valid loss: 0.2481218 P@1: 0.63000 P@3: 0.51000 P@5: 0.41200 N@3: 0.53898 N@5: 0.54493 early stop: 29\n",
            "\u001b[32m[I 210629 05:53:59 models:110]\u001b[39m 452 512 train loss: 0.0001377 valid loss: 0.2485571 P@1: 0.63000 P@3: 0.51000 P@5: 0.41200 N@3: 0.53898 N@5: 0.54493 early stop: 30\n",
            "\u001b[32m[I 210629 05:54:02 models:110]\u001b[39m 454 1024 train loss: 0.0000760 valid loss: 0.2489890 P@1: 0.63000 P@3: 0.51000 P@5: 0.41200 N@3: 0.53898 N@5: 0.54493 early stop: 31\n",
            "\u001b[32m[I 210629 05:54:05 models:110]\u001b[39m 457 512 train loss: 0.0001230 valid loss: 0.2494207 P@1: 0.63000 P@3: 0.51000 P@5: 0.41200 N@3: 0.53898 N@5: 0.54493 early stop: 32\n",
            "\u001b[32m[I 210629 05:54:07 models:110]\u001b[39m 459 1024 train loss: 0.0000958 valid loss: 0.2498541 P@1: 0.63000 P@3: 0.51000 P@5: 0.41200 N@3: 0.53898 N@5: 0.54493 early stop: 33\n",
            "\u001b[32m[I 210629 05:54:09 models:110]\u001b[39m 462 512 train loss: 0.0000841 valid loss: 0.2502863 P@1: 0.63000 P@3: 0.51000 P@5: 0.41200 N@3: 0.53898 N@5: 0.54493 early stop: 34\n",
            "\u001b[32m[I 210629 05:54:11 models:110]\u001b[39m 464 1024 train loss: 0.0001139 valid loss: 0.2507226 P@1: 0.63000 P@3: 0.51000 P@5: 0.41200 N@3: 0.53898 N@5: 0.54493 early stop: 35\n",
            "\u001b[33m[W 210629 05:54:12 models:137]\u001b[39m Clipping gradients with total norm 0.05482 and max norm 0.00513\n",
            "\u001b[32m[I 210629 05:54:14 models:110]\u001b[39m 467 512 train loss: 0.0003583 valid loss: 0.2511505 P@1: 0.63000 P@3: 0.51000 P@5: 0.41200 N@3: 0.53898 N@5: 0.54493 early stop: 36\n",
            "\u001b[32m[I 210629 05:54:16 models:110]\u001b[39m 469 1024 train loss: 0.0001055 valid loss: 0.2515806 P@1: 0.63000 P@3: 0.51000 P@5: 0.41200 N@3: 0.53898 N@5: 0.54493 early stop: 37\n",
            "\u001b[32m[I 210629 05:54:19 models:110]\u001b[39m 472 512 train loss: 0.0001226 valid loss: 0.2520118 P@1: 0.63000 P@3: 0.51000 P@5: 0.41200 N@3: 0.53898 N@5: 0.54513 early stop: 38\n",
            "\u001b[32m[I 210629 05:54:21 models:110]\u001b[39m 474 1024 train loss: 0.0000792 valid loss: 0.2524337 P@1: 0.63000 P@3: 0.51000 P@5: 0.41200 N@3: 0.53898 N@5: 0.54513 early stop: 39\n",
            "\u001b[33m[W 210629 05:54:22 models:137]\u001b[39m Clipping gradients with total norm 0.05199 and max norm 0.00543\n",
            "\u001b[32m[I 210629 05:54:23 models:110]\u001b[39m 477 512 train loss: 0.0002536 valid loss: 0.2528478 P@1: 0.63000 P@3: 0.51000 P@5: 0.41200 N@3: 0.53898 N@5: 0.54513 early stop: 40\n",
            "\u001b[32m[I 210629 05:54:25 models:110]\u001b[39m 479 1024 train loss: 0.0002617 valid loss: 0.2532471 P@1: 0.63000 P@3: 0.51000 P@5: 0.41200 N@3: 0.53898 N@5: 0.54513 early stop: 41\n",
            "\u001b[32m[I 210629 05:54:28 models:110]\u001b[39m 482 512 train loss: 0.0002394 valid loss: 0.2536453 P@1: 0.63000 P@3: 0.51000 P@5: 0.41200 N@3: 0.53898 N@5: 0.54513 early stop: 42\n",
            "\u001b[32m[I 210629 05:54:30 models:110]\u001b[39m 484 1024 train loss: 0.0001113 valid loss: 0.2540417 P@1: 0.63000 P@3: 0.51000 P@5: 0.41200 N@3: 0.53898 N@5: 0.54513 early stop: 43\n",
            "\u001b[33m[W 210629 05:54:31 models:137]\u001b[39m Clipping gradients with total norm 0.05438 and max norm 0.0102\n",
            "\u001b[32m[I 210629 05:54:32 models:110]\u001b[39m 487 512 train loss: 0.0004007 valid loss: 0.2544340 P@1: 0.63000 P@3: 0.51000 P@5: 0.41200 N@3: 0.53898 N@5: 0.54513 early stop: 44\n",
            "\u001b[32m[I 210629 05:54:35 models:110]\u001b[39m 489 1024 train loss: 0.0002142 valid loss: 0.2548375 P@1: 0.63000 P@3: 0.51000 P@5: 0.41200 N@3: 0.53898 N@5: 0.54513 early stop: 45\n",
            "\u001b[33m[W 210629 05:54:36 models:137]\u001b[39m Clipping gradients with total norm 0.07071 and max norm 0.00866\n",
            "\u001b[32m[I 210629 05:54:37 models:110]\u001b[39m 492 512 train loss: 0.0003966 valid loss: 0.2552347 P@1: 0.63000 P@3: 0.51000 P@5: 0.41200 N@3: 0.53898 N@5: 0.54513 early stop: 46\n",
            "\u001b[32m[I 210629 05:54:39 models:110]\u001b[39m 494 1024 train loss: 0.0001731 valid loss: 0.2556293 P@1: 0.63000 P@3: 0.51000 P@5: 0.41200 N@3: 0.53898 N@5: 0.54513 early stop: 47\n",
            "\u001b[32m[I 210629 05:54:42 models:110]\u001b[39m 497 512 train loss: 0.0001075 valid loss: 0.2560183 P@1: 0.64000 P@3: 0.51000 P@5: 0.41200 N@3: 0.54071 N@5: 0.54657 early stop: 0\n",
            "\u001b[32m[I 210629 05:54:44 models:110]\u001b[39m 499 1024 train loss: 0.0001639 valid loss: 0.2564253 P@1: 0.63000 P@3: 0.51000 P@5: 0.41200 N@3: 0.53898 N@5: 0.54513 early stop: 1\n",
            "\u001b[32m[I 210629 05:54:46 models:110]\u001b[39m 502 512 train loss: 0.0001361 valid loss: 0.2568405 P@1: 0.63000 P@3: 0.51000 P@5: 0.41200 N@3: 0.53898 N@5: 0.54513 early stop: 2\n",
            "\u001b[32m[I 210629 05:54:49 models:110]\u001b[39m 504 1024 train loss: 0.0001294 valid loss: 0.2572579 P@1: 0.63000 P@3: 0.51000 P@5: 0.41200 N@3: 0.53898 N@5: 0.54513 early stop: 3\n",
            "\u001b[32m[I 210629 05:54:51 models:110]\u001b[39m 507 512 train loss: 0.0000839 valid loss: 0.2576722 P@1: 0.63000 P@3: 0.51000 P@5: 0.41200 N@3: 0.53898 N@5: 0.54513 early stop: 4\n",
            "\u001b[32m[I 210629 05:54:53 models:110]\u001b[39m 509 1024 train loss: 0.0001260 valid loss: 0.2580762 P@1: 0.63000 P@3: 0.51333 P@5: 0.41200 N@3: 0.54133 N@5: 0.54540 early stop: 5\n",
            "\u001b[32m[I 210629 05:54:56 models:110]\u001b[39m 512 512 train loss: 0.0001377 valid loss: 0.2584798 P@1: 0.63000 P@3: 0.51333 P@5: 0.41200 N@3: 0.54133 N@5: 0.54540 early stop: 6\n",
            "\u001b[32m[I 210629 05:54:58 models:110]\u001b[39m 514 1024 train loss: 0.0000896 valid loss: 0.2588857 P@1: 0.63000 P@3: 0.51333 P@5: 0.41200 N@3: 0.54133 N@5: 0.54540 early stop: 7\n",
            "\u001b[32m[I 210629 05:55:00 models:110]\u001b[39m 517 512 train loss: 0.0001168 valid loss: 0.2592950 P@1: 0.63000 P@3: 0.51333 P@5: 0.41200 N@3: 0.54133 N@5: 0.54540 early stop: 8\n",
            "\u001b[32m[I 210629 05:55:02 models:110]\u001b[39m 519 1024 train loss: 0.0000643 valid loss: 0.2597111 P@1: 0.63000 P@3: 0.51333 P@5: 0.41200 N@3: 0.54194 N@5: 0.54591 early stop: 9\n",
            "\u001b[32m[I 210629 05:55:05 models:110]\u001b[39m 522 512 train loss: 0.0000936 valid loss: 0.2601332 P@1: 0.63000 P@3: 0.51333 P@5: 0.41200 N@3: 0.54194 N@5: 0.54591 early stop: 10\n",
            "\u001b[32m[I 210629 05:55:07 models:110]\u001b[39m 524 1024 train loss: 0.0001430 valid loss: 0.2605526 P@1: 0.63000 P@3: 0.51333 P@5: 0.41200 N@3: 0.54194 N@5: 0.54591 early stop: 11\n",
            "\u001b[33m[W 210629 05:55:09 models:137]\u001b[39m Clipping gradients with total norm 0.05276 and max norm 0.00487\n",
            "\u001b[32m[I 210629 05:55:10 models:110]\u001b[39m 527 512 train loss: 0.0002521 valid loss: 0.2609659 P@1: 0.63000 P@3: 0.51333 P@5: 0.41200 N@3: 0.54194 N@5: 0.54591 early stop: 12\n",
            "\u001b[32m[I 210629 05:55:12 models:110]\u001b[39m 529 1024 train loss: 0.0001016 valid loss: 0.2613763 P@1: 0.63000 P@3: 0.51333 P@5: 0.41200 N@3: 0.54194 N@5: 0.54591 early stop: 13\n",
            "\u001b[32m[I 210629 05:55:14 models:110]\u001b[39m 532 512 train loss: 0.0001183 valid loss: 0.2617816 P@1: 0.63000 P@3: 0.51333 P@5: 0.41200 N@3: 0.54256 N@5: 0.54653 early stop: 14\n",
            "\u001b[32m[I 210629 05:55:16 models:110]\u001b[39m 534 1024 train loss: 0.0000888 valid loss: 0.2621837 P@1: 0.63000 P@3: 0.51333 P@5: 0.41200 N@3: 0.54256 N@5: 0.54653 early stop: 15\n",
            "\u001b[32m[I 210629 05:55:19 models:110]\u001b[39m 537 512 train loss: 0.0001020 valid loss: 0.2625837 P@1: 0.63000 P@3: 0.51333 P@5: 0.41200 N@3: 0.54256 N@5: 0.54653 early stop: 16\n",
            "\u001b[32m[I 210629 05:55:21 models:110]\u001b[39m 539 1024 train loss: 0.0001023 valid loss: 0.2629788 P@1: 0.63000 P@3: 0.51333 P@5: 0.41200 N@3: 0.54256 N@5: 0.54653 early stop: 17\n",
            "\u001b[32m[I 210629 05:55:24 models:110]\u001b[39m 542 512 train loss: 0.0000776 valid loss: 0.2633724 P@1: 0.63000 P@3: 0.51333 P@5: 0.41200 N@3: 0.54256 N@5: 0.54653 early stop: 18\n",
            "\u001b[32m[I 210629 05:55:26 models:110]\u001b[39m 544 1024 train loss: 0.0004224 valid loss: 0.2637724 P@1: 0.63000 P@3: 0.51333 P@5: 0.41200 N@3: 0.54256 N@5: 0.54653 early stop: 19\n",
            "\u001b[32m[I 210629 05:55:28 models:110]\u001b[39m 547 512 train loss: 0.0003181 valid loss: 0.2641504 P@1: 0.63000 P@3: 0.51333 P@5: 0.41200 N@3: 0.54256 N@5: 0.54668 early stop: 0\n",
            "\u001b[32m[I 210629 05:55:30 models:110]\u001b[39m 549 1024 train loss: 0.0006030 valid loss: 0.2645326 P@1: 0.63000 P@3: 0.51333 P@5: 0.41200 N@3: 0.54256 N@5: 0.54668 early stop: 1\n",
            "\u001b[32m[I 210629 05:55:33 models:110]\u001b[39m 552 512 train loss: 0.0007096 valid loss: 0.2648802 P@1: 0.63000 P@3: 0.51333 P@5: 0.41200 N@3: 0.54256 N@5: 0.54653 early stop: 2\n",
            "\u001b[32m[I 210629 05:55:35 models:110]\u001b[39m 554 1024 train loss: 0.0061614 valid loss: 0.2651494 P@1: 0.63000 P@3: 0.52000 P@5: 0.41200 N@3: 0.54725 N@5: 0.54718 early stop: 0\n",
            "\u001b[32m[I 210629 05:55:38 models:110]\u001b[39m 557 512 train loss: 0.1358297 valid loss: 0.2638802 P@1: 0.63000 P@3: 0.51000 P@5: 0.41200 N@3: 0.54021 N@5: 0.54646 early stop: 1\n",
            "\u001b[32m[I 210629 05:55:40 models:110]\u001b[39m 559 1024 train loss: 0.0976255 valid loss: 0.2614076 P@1: 0.63000 P@3: 0.51000 P@5: 0.41200 N@3: 0.54021 N@5: 0.54646 early stop: 2\n",
            "\u001b[32m[I 210629 05:55:42 models:110]\u001b[39m 562 512 train loss: 0.0548141 valid loss: 0.2589341 P@1: 0.63000 P@3: 0.51000 P@5: 0.41200 N@3: 0.54021 N@5: 0.54609 early stop: 3\n",
            "\u001b[32m[I 210629 05:55:44 models:110]\u001b[39m 564 1024 train loss: 0.0303706 valid loss: 0.2567350 P@1: 0.63000 P@3: 0.51000 P@5: 0.41200 N@3: 0.54021 N@5: 0.54609 early stop: 4\n",
            "\u001b[32m[I 210629 05:55:47 models:110]\u001b[39m 567 512 train loss: 0.0189187 valid loss: 0.2548066 P@1: 0.63000 P@3: 0.51000 P@5: 0.41000 N@3: 0.54021 N@5: 0.54477 early stop: 5\n",
            "\u001b[32m[I 210629 05:55:49 models:110]\u001b[39m 569 1024 train loss: 0.0104009 valid loss: 0.2530620 P@1: 0.63000 P@3: 0.51000 P@5: 0.40800 N@3: 0.54021 N@5: 0.54303 early stop: 6\n",
            "\u001b[32m[I 210629 05:55:51 models:110]\u001b[39m 572 512 train loss: 0.0071614 valid loss: 0.2515176 P@1: 0.63000 P@3: 0.51000 P@5: 0.40800 N@3: 0.54082 N@5: 0.54364 early stop: 7\n",
            "\u001b[32m[I 210629 05:55:53 models:110]\u001b[39m 574 1024 train loss: 0.0063257 valid loss: 0.2500955 P@1: 0.63000 P@3: 0.51000 P@5: 0.41000 N@3: 0.54082 N@5: 0.54510 early stop: 8\n",
            "\u001b[32m[I 210629 05:55:56 models:110]\u001b[39m 577 512 train loss: 0.0050698 valid loss: 0.2488281 P@1: 0.63000 P@3: 0.51000 P@5: 0.41000 N@3: 0.54082 N@5: 0.54490 early stop: 9\n",
            "\u001b[32m[I 210629 05:55:58 models:110]\u001b[39m 579 1024 train loss: 0.0040894 valid loss: 0.2476561 P@1: 0.63000 P@3: 0.51333 P@5: 0.41000 N@3: 0.54317 N@5: 0.54478 early stop: 10\n",
            "\u001b[32m[I 210629 05:56:01 models:110]\u001b[39m 582 512 train loss: 0.0032714 valid loss: 0.2466348 P@1: 0.63000 P@3: 0.51333 P@5: 0.41200 N@3: 0.54317 N@5: 0.54609 early stop: 11\n",
            "\u001b[32m[I 210629 05:56:03 models:110]\u001b[39m 584 1024 train loss: 0.0024668 valid loss: 0.2456674 P@1: 0.63000 P@3: 0.51333 P@5: 0.41000 N@3: 0.54317 N@5: 0.54428 early stop: 12\n",
            "\u001b[32m[I 210629 05:56:05 models:110]\u001b[39m 587 512 train loss: 0.0020172 valid loss: 0.2447200 P@1: 0.64000 P@3: 0.51333 P@5: 0.40600 N@3: 0.54490 N@5: 0.54200 early stop: 13\n",
            "\u001b[32m[I 210629 05:56:07 models:110]\u001b[39m 589 1024 train loss: 0.0019964 valid loss: 0.2438523 P@1: 0.64000 P@3: 0.51000 P@5: 0.40600 N@3: 0.54194 N@5: 0.54128 early stop: 14\n",
            "\u001b[32m[I 210629 05:56:10 models:110]\u001b[39m 592 512 train loss: 0.0012838 valid loss: 0.2430392 P@1: 0.64000 P@3: 0.51333 P@5: 0.40600 N@3: 0.54367 N@5: 0.54107 early stop: 15\n",
            "\u001b[32m[I 210629 05:56:12 models:110]\u001b[39m 594 1024 train loss: 0.0006544 valid loss: 0.2422429 P@1: 0.64000 P@3: 0.51333 P@5: 0.41000 N@3: 0.54367 N@5: 0.54541 early stop: 16\n",
            "\u001b[32m[I 210629 05:56:14 models:110]\u001b[39m 597 512 train loss: 0.0004666 valid loss: 0.2415238 P@1: 0.64000 P@3: 0.51667 P@5: 0.40800 N@3: 0.54602 N@5: 0.54392 early stop: 17\n",
            "\u001b[32m[I 210629 05:56:17 models:110]\u001b[39m 599 1024 train loss: 0.0005737 valid loss: 0.2408443 P@1: 0.64000 P@3: 0.51667 P@5: 0.41000 N@3: 0.54602 N@5: 0.54538 early stop: 18\n",
            "\u001b[32m[I 210629 05:56:19 models:110]\u001b[39m 602 512 train loss: 0.0003301 valid loss: 0.2402031 P@1: 0.65000 P@3: 0.51667 P@5: 0.41000 N@3: 0.54837 N@5: 0.54726 early stop: 0\n",
            "\u001b[32m[I 210629 05:56:21 models:110]\u001b[39m 604 1024 train loss: 0.0003292 valid loss: 0.2395935 P@1: 0.65000 P@3: 0.51667 P@5: 0.41000 N@3: 0.54837 N@5: 0.54743 early stop: 0\n",
            "\u001b[32m[I 210629 05:56:24 models:110]\u001b[39m 607 512 train loss: 0.0003029 valid loss: 0.2390103 P@1: 0.65000 P@3: 0.51667 P@5: 0.41000 N@3: 0.54837 N@5: 0.54743 early stop: 1\n",
            "\u001b[32m[I 210629 05:56:26 models:110]\u001b[39m 609 1024 train loss: 0.0002613 valid loss: 0.2384636 P@1: 0.65000 P@3: 0.51333 P@5: 0.41200 N@3: 0.54541 N@5: 0.54807 early stop: 0\n",
            "\u001b[33m[W 210629 05:56:27 models:137]\u001b[39m Clipping gradients with total norm 0.07215 and max norm 0.01086\n",
            "\u001b[32m[I 210629 05:56:29 models:110]\u001b[39m 612 512 train loss: 0.0003742 valid loss: 0.2379471 P@1: 0.64000 P@3: 0.51000 P@5: 0.41200 N@3: 0.54133 N@5: 0.54624 early stop: 1\n",
            "\u001b[33m[W 210629 05:56:29 models:137]\u001b[39m Clipping gradients with total norm 0.17164 and max norm 0.01738\n",
            "\u001b[32m[I 210629 05:56:31 models:110]\u001b[39m 614 1024 train loss: 0.0010802 valid loss: 0.2374448 P@1: 0.64000 P@3: 0.51000 P@5: 0.41400 N@3: 0.54133 N@5: 0.54805 early stop: 2\n",
            "\u001b[32m[I 210629 05:56:33 models:110]\u001b[39m 617 512 train loss: 0.0002807 valid loss: 0.2369707 P@1: 0.64000 P@3: 0.51000 P@5: 0.41400 N@3: 0.54194 N@5: 0.54850 early stop: 0\n",
            "\u001b[32m[I 210629 05:56:36 models:110]\u001b[39m 619 1024 train loss: 0.0002308 valid loss: 0.2365233 P@1: 0.64000 P@3: 0.51333 P@5: 0.41400 N@3: 0.54429 N@5: 0.54882 early stop: 0\n",
            "\u001b[32m[I 210629 05:56:38 models:110]\u001b[39m 622 512 train loss: 0.0001822 valid loss: 0.2360944 P@1: 0.64000 P@3: 0.51333 P@5: 0.41400 N@3: 0.54429 N@5: 0.54882 early stop: 1\n",
            "\u001b[32m[I 210629 05:56:40 models:110]\u001b[39m 624 1024 train loss: 0.0001867 valid loss: 0.2356963 P@1: 0.64000 P@3: 0.51333 P@5: 0.41400 N@3: 0.54429 N@5: 0.54865 early stop: 2\n",
            "\u001b[32m[I 210629 05:56:43 models:110]\u001b[39m 627 512 train loss: 0.0001668 valid loss: 0.2353175 P@1: 0.64000 P@3: 0.51333 P@5: 0.41400 N@3: 0.54429 N@5: 0.54886 early stop: 0\n",
            "\u001b[33m[W 210629 05:56:44 models:137]\u001b[39m Clipping gradients with total norm 0.05042 and max norm 0.00872\n",
            "\u001b[32m[I 210629 05:56:45 models:110]\u001b[39m 629 1024 train loss: 0.0003452 valid loss: 0.2349508 P@1: 0.64000 P@3: 0.51333 P@5: 0.41600 N@3: 0.54490 N@5: 0.55093 early stop: 0\n",
            "\u001b[32m[I 210629 05:56:47 models:110]\u001b[39m 632 512 train loss: 0.0001830 valid loss: 0.2345949 P@1: 0.63000 P@3: 0.51333 P@5: 0.41600 N@3: 0.54317 N@5: 0.54968 early stop: 1\n",
            "\u001b[32m[I 210629 05:56:50 models:110]\u001b[39m 634 1024 train loss: 0.0002837 valid loss: 0.2342609 P@1: 0.63000 P@3: 0.51333 P@5: 0.41600 N@3: 0.54317 N@5: 0.54985 early stop: 2\n",
            "\u001b[32m[I 210629 05:56:52 models:110]\u001b[39m 637 512 train loss: 0.0001812 valid loss: 0.2339436 P@1: 0.63000 P@3: 0.51000 P@5: 0.41600 N@3: 0.54082 N@5: 0.54961 early stop: 3\n",
            "\u001b[32m[I 210629 05:56:54 models:110]\u001b[39m 639 1024 train loss: 0.0001651 valid loss: 0.2336419 P@1: 0.63000 P@3: 0.51000 P@5: 0.41600 N@3: 0.54082 N@5: 0.54944 early stop: 4\n",
            "\u001b[33m[W 210629 05:56:56 models:137]\u001b[39m Clipping gradients with total norm 0.04527 and max norm 0.00494\n",
            "\u001b[32m[I 210629 05:56:57 models:110]\u001b[39m 642 512 train loss: 0.0002637 valid loss: 0.2333549 P@1: 0.63000 P@3: 0.51000 P@5: 0.41600 N@3: 0.54082 N@5: 0.54961 early stop: 5\n",
            "\u001b[32m[I 210629 05:56:59 models:110]\u001b[39m 644 1024 train loss: 0.0001651 valid loss: 0.2330754 P@1: 0.62000 P@3: 0.50667 P@5: 0.41600 N@3: 0.53613 N@5: 0.54748 early stop: 6\n",
            "\u001b[32m[I 210629 05:57:01 models:110]\u001b[39m 647 512 train loss: 0.0001293 valid loss: 0.2328075 P@1: 0.62000 P@3: 0.51000 P@5: 0.41600 N@3: 0.53786 N@5: 0.54736 early stop: 7\n",
            "\u001b[32m[I 210629 05:57:03 models:110]\u001b[39m 649 1024 train loss: 0.0001762 valid loss: 0.2325606 P@1: 0.62000 P@3: 0.51333 P@5: 0.41600 N@3: 0.54021 N@5: 0.54763 early stop: 8\n",
            "\u001b[33m[W 210629 05:57:04 models:137]\u001b[39m Clipping gradients with total norm 0.04874 and max norm 0.00707\n",
            "\u001b[32m[I 210629 05:57:06 models:110]\u001b[39m 652 512 train loss: 0.0002516 valid loss: 0.2323190 P@1: 0.62000 P@3: 0.51333 P@5: 0.41600 N@3: 0.54021 N@5: 0.54746 early stop: 9\n",
            "\u001b[32m[I 210629 05:57:08 models:110]\u001b[39m 654 1024 train loss: 0.0002360 valid loss: 0.2320981 P@1: 0.62000 P@3: 0.51333 P@5: 0.41600 N@3: 0.54021 N@5: 0.54746 early stop: 10\n",
            "\u001b[32m[I 210629 05:57:11 models:110]\u001b[39m 657 512 train loss: 0.0001275 valid loss: 0.2318887 P@1: 0.62000 P@3: 0.51333 P@5: 0.41600 N@3: 0.54021 N@5: 0.54746 early stop: 11\n",
            "\u001b[33m[W 210629 05:57:12 models:137]\u001b[39m Clipping gradients with total norm 0.04999 and max norm 0.00613\n",
            "\u001b[32m[I 210629 05:57:13 models:110]\u001b[39m 659 1024 train loss: 0.0003509 valid loss: 0.2316892 P@1: 0.62000 P@3: 0.51333 P@5: 0.41200 N@3: 0.54021 N@5: 0.54444 early stop: 12\n",
            "\u001b[33m[W 210629 05:57:14 models:137]\u001b[39m Clipping gradients with total norm 0.06391 and max norm 0.01225\n",
            "\u001b[32m[I 210629 05:57:15 models:110]\u001b[39m 662 512 train loss: 0.0004208 valid loss: 0.2315015 P@1: 0.62000 P@3: 0.51333 P@5: 0.41200 N@3: 0.54021 N@5: 0.54444 early stop: 13\n",
            "\u001b[32m[I 210629 05:57:17 models:110]\u001b[39m 664 1024 train loss: 0.0001849 valid loss: 0.2313308 P@1: 0.62000 P@3: 0.51333 P@5: 0.41200 N@3: 0.54021 N@5: 0.54444 early stop: 14\n",
            "\u001b[32m[I 210629 05:57:20 models:110]\u001b[39m 667 512 train loss: 0.0001592 valid loss: 0.2311645 P@1: 0.62000 P@3: 0.51333 P@5: 0.41200 N@3: 0.53960 N@5: 0.54399 early stop: 15\n",
            "\u001b[32m[I 210629 05:57:22 models:110]\u001b[39m 669 1024 train loss: 0.0001562 valid loss: 0.2310023 P@1: 0.62000 P@3: 0.51333 P@5: 0.41200 N@3: 0.53960 N@5: 0.54399 early stop: 16\n",
            "\u001b[32m[I 210629 05:57:24 models:110]\u001b[39m 672 512 train loss: 0.0001457 valid loss: 0.2308508 P@1: 0.62000 P@3: 0.51333 P@5: 0.41200 N@3: 0.53960 N@5: 0.54399 early stop: 17\n",
            "\u001b[33m[W 210629 05:57:26 models:137]\u001b[39m Clipping gradients with total norm 0.04076 and max norm 0.00437\n",
            "\u001b[32m[I 210629 05:57:27 models:110]\u001b[39m 674 1024 train loss: 0.0002511 valid loss: 0.2307118 P@1: 0.62000 P@3: 0.51333 P@5: 0.41200 N@3: 0.53960 N@5: 0.54399 early stop: 18\n",
            "\u001b[32m[I 210629 05:57:29 models:110]\u001b[39m 677 512 train loss: 0.0001536 valid loss: 0.2305791 P@1: 0.62000 P@3: 0.51333 P@5: 0.41200 N@3: 0.53837 N@5: 0.54277 early stop: 19\n",
            "\u001b[32m[I 210629 05:57:31 models:110]\u001b[39m 679 1024 train loss: 0.0001060 valid loss: 0.2304508 P@1: 0.62000 P@3: 0.51333 P@5: 0.41400 N@3: 0.53837 N@5: 0.54514 early stop: 20\n",
            "\u001b[33m[W 210629 05:57:32 models:137]\u001b[39m Clipping gradients with total norm 0.07449 and max norm 0.0052\n",
            "\u001b[32m[I 210629 05:57:34 models:110]\u001b[39m 682 512 train loss: 0.0003777 valid loss: 0.2303378 P@1: 0.61000 P@3: 0.51333 P@5: 0.41400 N@3: 0.53663 N@5: 0.54341 early stop: 21\n",
            "\u001b[32m[I 210629 05:57:36 models:110]\u001b[39m 684 1024 train loss: 0.0001261 valid loss: 0.2302289 P@1: 0.61000 P@3: 0.51333 P@5: 0.41400 N@3: 0.53663 N@5: 0.54341 early stop: 22\n",
            "\u001b[32m[I 210629 05:57:38 models:110]\u001b[39m 687 512 train loss: 0.0001365 valid loss: 0.2301231 P@1: 0.61000 P@3: 0.51333 P@5: 0.41400 N@3: 0.53663 N@5: 0.54361 early stop: 23\n",
            "\u001b[32m[I 210629 05:57:40 models:110]\u001b[39m 689 1024 train loss: 0.0001012 valid loss: 0.2300315 P@1: 0.61000 P@3: 0.51333 P@5: 0.41400 N@3: 0.53663 N@5: 0.54361 early stop: 24\n",
            "\u001b[32m[I 210629 05:57:43 models:110]\u001b[39m 692 512 train loss: 0.0000962 valid loss: 0.2299496 P@1: 0.61000 P@3: 0.51333 P@5: 0.41400 N@3: 0.53725 N@5: 0.54423 early stop: 25\n",
            "\u001b[33m[W 210629 05:57:44 models:137]\u001b[39m Clipping gradients with total norm 0.05422 and max norm 0.00671\n",
            "\u001b[32m[I 210629 05:57:45 models:110]\u001b[39m 694 1024 train loss: 0.0003330 valid loss: 0.2298680 P@1: 0.61000 P@3: 0.51333 P@5: 0.41400 N@3: 0.53725 N@5: 0.54423 early stop: 26\n",
            "\u001b[32m[I 210629 05:57:48 models:110]\u001b[39m 697 512 train loss: 0.0001492 valid loss: 0.2297952 P@1: 0.61000 P@3: 0.52000 P@5: 0.41400 N@3: 0.54194 N@5: 0.54449 early stop: 27\n",
            "\u001b[32m[I 210629 05:57:50 models:110]\u001b[39m 699 1024 train loss: 0.0001451 valid loss: 0.2297329 P@1: 0.62000 P@3: 0.52000 P@5: 0.41400 N@3: 0.54367 N@5: 0.54578 early stop: 28\n",
            "\u001b[32m[I 210629 05:57:52 models:110]\u001b[39m 702 512 train loss: 0.0001228 valid loss: 0.2296751 P@1: 0.62000 P@3: 0.52000 P@5: 0.41400 N@3: 0.54367 N@5: 0.54578 early stop: 29\n",
            "\u001b[32m[I 210629 05:57:54 models:110]\u001b[39m 704 1024 train loss: 0.0001267 valid loss: 0.2296209 P@1: 0.62000 P@3: 0.52000 P@5: 0.41400 N@3: 0.54367 N@5: 0.54578 early stop: 30\n",
            "\u001b[32m[I 210629 05:57:57 models:110]\u001b[39m 707 512 train loss: 0.0002946 valid loss: 0.2295708 P@1: 0.62000 P@3: 0.52000 P@5: 0.41400 N@3: 0.54429 N@5: 0.54640 early stop: 31\n",
            "\u001b[32m[I 210629 05:57:59 models:110]\u001b[39m 709 1024 train loss: 0.0001585 valid loss: 0.2295250 P@1: 0.64000 P@3: 0.52333 P@5: 0.41400 N@3: 0.55010 N@5: 0.54940 early stop: 32\n",
            "\u001b[32m[I 210629 05:58:01 models:110]\u001b[39m 712 512 train loss: 0.0001603 valid loss: 0.2294862 P@1: 0.64000 P@3: 0.52333 P@5: 0.41400 N@3: 0.55010 N@5: 0.54920 early stop: 33\n",
            "\u001b[32m[I 210629 05:58:03 models:110]\u001b[39m 714 1024 train loss: 0.0000956 valid loss: 0.2294572 P@1: 0.64000 P@3: 0.52333 P@5: 0.41200 N@3: 0.55010 N@5: 0.54788 early stop: 34\n",
            "\u001b[32m[I 210629 05:58:06 models:110]\u001b[39m 717 512 train loss: 0.0001279 valid loss: 0.2294375 P@1: 0.64000 P@3: 0.52333 P@5: 0.41200 N@3: 0.55010 N@5: 0.54788 early stop: 35\n",
            "\u001b[32m[I 210629 05:58:08 models:110]\u001b[39m 719 1024 train loss: 0.0001202 valid loss: 0.2294203 P@1: 0.64000 P@3: 0.52333 P@5: 0.41200 N@3: 0.55010 N@5: 0.54788 early stop: 36\n",
            "\u001b[32m[I 210629 05:58:11 models:110]\u001b[39m 722 512 train loss: 0.0000909 valid loss: 0.2294048 P@1: 0.64000 P@3: 0.52333 P@5: 0.41200 N@3: 0.55071 N@5: 0.54833 early stop: 37\n",
            "\u001b[32m[I 210629 05:58:13 models:110]\u001b[39m 724 1024 train loss: 0.0001355 valid loss: 0.2294003 P@1: 0.64000 P@3: 0.52333 P@5: 0.41200 N@3: 0.55071 N@5: 0.54833 early stop: 38\n",
            "\u001b[32m[I 210629 05:58:15 models:110]\u001b[39m 727 512 train loss: 0.0001163 valid loss: 0.2294050 P@1: 0.65000 P@3: 0.52000 P@5: 0.41000 N@3: 0.55010 N@5: 0.54822 early stop: 39\n",
            "\u001b[32m[I 210629 05:58:17 models:110]\u001b[39m 729 1024 train loss: 0.0001049 valid loss: 0.2294158 P@1: 0.66000 P@3: 0.52000 P@5: 0.41000 N@3: 0.55183 N@5: 0.54995 early stop: 40\n",
            "\u001b[32m[I 210629 05:58:20 models:110]\u001b[39m 732 512 train loss: 0.0000894 valid loss: 0.2294296 P@1: 0.66000 P@3: 0.52000 P@5: 0.41000 N@3: 0.55183 N@5: 0.54995 early stop: 41\n",
            "\u001b[32m[I 210629 05:58:22 models:110]\u001b[39m 734 1024 train loss: 0.0001216 valid loss: 0.2294486 P@1: 0.66000 P@3: 0.52000 P@5: 0.41000 N@3: 0.55183 N@5: 0.54995 early stop: 42\n",
            "\u001b[32m[I 210629 05:58:25 models:110]\u001b[39m 737 512 train loss: 0.0001182 valid loss: 0.2294728 P@1: 0.66000 P@3: 0.52000 P@5: 0.41000 N@3: 0.55183 N@5: 0.54981 early stop: 43\n",
            "\u001b[32m[I 210629 05:58:27 models:110]\u001b[39m 739 1024 train loss: 0.0000984 valid loss: 0.2295018 P@1: 0.66000 P@3: 0.52000 P@5: 0.41000 N@3: 0.55183 N@5: 0.54981 early stop: 44\n",
            "\u001b[32m[I 210629 05:58:29 models:110]\u001b[39m 742 512 train loss: 0.0001073 valid loss: 0.2295348 P@1: 0.66000 P@3: 0.52000 P@5: 0.41000 N@3: 0.55183 N@5: 0.54981 early stop: 45\n",
            "\u001b[33m[W 210629 05:58:30 models:137]\u001b[39m Clipping gradients with total norm 0.03712 and max norm 0.00477\n",
            "\u001b[32m[I 210629 05:58:31 models:110]\u001b[39m 744 1024 train loss: 0.0002427 valid loss: 0.2295759 P@1: 0.66000 P@3: 0.52000 P@5: 0.41000 N@3: 0.55183 N@5: 0.54981 early stop: 46\n",
            "\u001b[32m[I 210629 05:58:34 models:110]\u001b[39m 747 512 train loss: 0.0001157 valid loss: 0.2296240 P@1: 0.66000 P@3: 0.52000 P@5: 0.41000 N@3: 0.55183 N@5: 0.54981 early stop: 47\n",
            "\u001b[32m[I 210629 05:58:36 models:110]\u001b[39m 749 1024 train loss: 0.0001415 valid loss: 0.2296745 P@1: 0.66000 P@3: 0.52000 P@5: 0.40800 N@3: 0.55183 N@5: 0.54849 early stop: 48\n",
            "\u001b[32m[I 210629 05:58:38 models:110]\u001b[39m 752 512 train loss: 0.0001447 valid loss: 0.2297278 P@1: 0.66000 P@3: 0.52000 P@5: 0.40800 N@3: 0.55183 N@5: 0.54849 early stop: 49\n",
            "\u001b[32m[I 210629 05:58:41 models:110]\u001b[39m 754 1024 train loss: 0.0000935 valid loss: 0.2297839 P@1: 0.66000 P@3: 0.52000 P@5: 0.40800 N@3: 0.55183 N@5: 0.54849 early stop: 50\n",
            "\u001b[32m[I 210629 05:58:43 main:76]\u001b[39m Finish Training\n",
            "\u001b[32m[I 210629 05:58:45 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210629 05:58:45 main:79]\u001b[39m Loading Test Set\n",
            "\u001b[32m[I 210629 05:58:45 main:83]\u001b[39m Size of Test Set: 100\n",
            "\u001b[32m[I 210629 05:58:45 main:85]\u001b[39m Predicting\n",
            "\u001b[32m[I 210629 05:58:49 main:91]\u001b[39m Finish Predicting\n",
            "Precision@1,3,5: 0.65 0.5066666666666667 0.398\n",
            "nDCG@1,3,5: 0.65 0.5462085302685622 0.5464655115226336\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsYoo4ymh-Dy"
      },
      "source": [
        "## Ablation study: Turn off hypernymy regularization\n",
        "\n",
        "We investigate the effect of the hierarachy (PeTaL/taxonomy.txt). The MATCH paper describes *hypernymy regularization*, which leverages taxonomy information to take into account the relationships between labels in training.\n",
        "\n",
        "This includes *regularization in the parameter space*, where a penalty is added to encourage the parameters of each label (e.g., `active_movement`) to be similar to its parent (e.g., `move`), and *regularization in the output space*, where a penalty is added if a child label occurs without its parent label (roughly speaking).\n",
        "\n",
        "The authors of `MATCH` were kind enough to include a CLI option, `--reg`, to toggle hypernymy regularization. `--reg 1` turns it on, and `--reg 0` turns it off."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z28moIQCiENw",
        "outputId": "bfb791e1-0563-440e-eef3-b272f43cf469"
      },
      "source": [
        "# note: --reg 0 turns off hypernymy regularization\n",
        "!PYTHONFAULTHANDLER=1 python main.py --data-cnf configure/datasets/PeTaL.yaml --model-cnf configure/models/MATCH-PeTaL.yaml --mode train --reg 0\n",
        "!PYTHONFAULTHANDLER=1 python main.py --data-cnf configure/datasets/PeTaL.yaml --model-cnf configure/models/MATCH-PeTaL.yaml --mode eval\n",
        "\n",
        "!python evaluation.py --results PeTaL/results/MATCH-PeTaL-labels.npy --targets PeTaL/test_labels.npy --train-labels PeTaL/train_labels.npy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 210624 15:11:07 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210624 15:11:07 main:35]\u001b[39m Loading Training and Validation Set\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['absorb_and/or_filter_solids', 'chemically_break_down_inorganic_compounds', 'detox/purify', 'manage_environmental_disturbances_in_a_community', 'protect_from_fire', 'protect_from_gases', 'send_vibratory_signals'] will be ignored\n",
            "  .format(sorted(unknown, key=str)))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['absorb_and/or_filter_solids', 'detox/purify', 'protect_from_gases'] will be ignored\n",
            "  .format(sorted(unknown, key=str)))\n",
            "\u001b[32m[I 210624 15:11:08 main:47]\u001b[39m Number of Labels: 124\n",
            "\u001b[32m[I 210624 15:11:08 main:48]\u001b[39m Size of Training Set: 800\n",
            "\u001b[32m[I 210624 15:11:08 main:49]\u001b[39m Size of Validation Set: 100\n",
            "\u001b[32m[I 210624 15:11:08 main:68]\u001b[39m Training\n",
            "\u001b[32m[I 210624 15:11:14 models:142]\u001b[39m SWA Initializing\n",
            "\u001b[32m[I 210624 15:11:35 models:110]\u001b[39m 24 1024 train loss: 0.1316223 valid loss: 0.1447000 P@1: 0.21000 P@3: 0.19333 P@5: 0.17200 N@3: 0.19867 N@5: 0.20960 early stop: 0\n",
            "\u001b[32m[I 210624 15:11:59 models:110]\u001b[39m 49 1024 train loss: 0.0376488 valid loss: 0.1427039 P@1: 0.37000 P@3: 0.26000 P@5: 0.24200 N@3: 0.28756 N@5: 0.31248 early stop: 0\n",
            "\u001b[32m[I 210624 15:12:22 models:110]\u001b[39m 74 1024 train loss: 0.0113640 valid loss: 0.1442004 P@1: 0.49000 P@3: 0.39000 P@5: 0.31600 N@3: 0.42163 N@5: 0.42813 early stop: 0\n",
            "\u001b[32m[I 210624 15:12:46 models:110]\u001b[39m 99 1024 train loss: 0.0058840 valid loss: 0.1494539 P@1: 0.61000 P@3: 0.47333 P@5: 0.36200 N@3: 0.51002 N@5: 0.50068 early stop: 0\n",
            "\u001b[32m[I 210624 15:13:10 models:110]\u001b[39m 124 1024 train loss: 0.0012710 valid loss: 0.1610811 P@1: 0.63000 P@3: 0.50667 P@5: 0.37800 N@3: 0.54266 N@5: 0.52604 early stop: 0\n",
            "\u001b[32m[I 210624 15:13:34 models:110]\u001b[39m 149 1024 train loss: 0.0008330 valid loss: 0.1722448 P@1: 0.67000 P@3: 0.51667 P@5: 0.38800 N@3: 0.55663 N@5: 0.54090 early stop: 0\n",
            "\u001b[32m[I 210624 15:13:57 models:110]\u001b[39m 174 1024 train loss: 0.0020319 valid loss: 0.1844395 P@1: 0.66000 P@3: 0.51333 P@5: 0.40000 N@3: 0.55306 N@5: 0.54949 early stop: 0\n",
            "\u001b[32m[I 210624 15:14:21 models:110]\u001b[39m 199 1024 train loss: 0.0007097 valid loss: 0.1958750 P@1: 0.66000 P@3: 0.52000 P@5: 0.40200 N@3: 0.55775 N@5: 0.55229 early stop: 0\n",
            "\u001b[32m[I 210624 15:14:45 models:110]\u001b[39m 224 1024 train loss: 0.0091214 valid loss: 0.1984477 P@1: 0.67000 P@3: 0.52333 P@5: 0.40600 N@3: 0.56183 N@5: 0.55719 early stop: 0\n",
            "\u001b[32m[I 210624 15:15:09 models:110]\u001b[39m 249 1024 train loss: 0.0009238 valid loss: 0.2055015 P@1: 0.66000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56244 N@5: 0.55483 early stop: 1\n",
            "\u001b[32m[I 210624 15:15:33 models:110]\u001b[39m 274 1024 train loss: 0.0000434 valid loss: 0.2125400 P@1: 0.66000 P@3: 0.52667 P@5: 0.40200 N@3: 0.56244 N@5: 0.55374 early stop: 2\n",
            "\u001b[32m[I 210624 15:15:57 models:110]\u001b[39m 299 1024 train loss: 0.0000251 valid loss: 0.2195170 P@1: 0.66000 P@3: 0.53000 P@5: 0.41200 N@3: 0.56541 N@5: 0.56033 early stop: 0\n",
            "\u001b[32m[I 210624 15:16:20 models:110]\u001b[39m 324 1024 train loss: 0.0000178 valid loss: 0.2263328 P@1: 0.65000 P@3: 0.53333 P@5: 0.41000 N@3: 0.56602 N@5: 0.55932 early stop: 1\n",
            "\u001b[33m[W 210624 15:16:23 models:137]\u001b[39m Clipping gradients with total norm 0.02378 and max norm 0.00173\n",
            "\u001b[32m[I 210624 15:16:44 models:110]\u001b[39m 349 1024 train loss: 0.0000151 valid loss: 0.2328805 P@1: 0.65000 P@3: 0.53333 P@5: 0.40800 N@3: 0.56541 N@5: 0.55762 early stop: 2\n",
            "\u001b[33m[W 210624 15:16:45 models:137]\u001b[39m Clipping gradients with total norm 0.00934 and max norm 0.00128\n",
            "\u001b[33m[W 210624 15:17:04 models:137]\u001b[39m Clipping gradients with total norm 0.00943 and max norm 0.00124\n",
            "\u001b[32m[I 210624 15:17:08 models:110]\u001b[39m 374 1024 train loss: 0.0000105 valid loss: 0.2391268 P@1: 0.66000 P@3: 0.53667 P@5: 0.40800 N@3: 0.57010 N@5: 0.56043 early stop: 0\n",
            "\u001b[33m[W 210624 15:17:11 models:137]\u001b[39m Clipping gradients with total norm 0.01799 and max norm 0.00061\n",
            "\u001b[33m[W 210624 15:17:28 models:137]\u001b[39m Clipping gradients with total norm 0.00906 and max norm 0.00107\n",
            "\u001b[32m[I 210624 15:17:31 models:110]\u001b[39m 399 1024 train loss: 0.0000091 valid loss: 0.2451547 P@1: 0.67000 P@3: 0.53667 P@5: 0.41400 N@3: 0.57244 N@5: 0.56731 early stop: 0\n",
            "\u001b[33m[W 210624 15:17:36 models:137]\u001b[39m Clipping gradients with total norm 0.00593 and max norm 0.00109\n",
            "\u001b[33m[W 210624 15:17:38 models:137]\u001b[39m Clipping gradients with total norm 0.00301 and max norm 0.00046\n",
            "\u001b[32m[I 210624 15:17:55 models:110]\u001b[39m 424 1024 train loss: 0.0000070 valid loss: 0.2508962 P@1: 0.67000 P@3: 0.54333 P@5: 0.41400 N@3: 0.57775 N@5: 0.56931 early stop: 0\n",
            "\u001b[32m[I 210624 15:18:19 models:110]\u001b[39m 449 1024 train loss: 0.0000053 valid loss: 0.2563647 P@1: 0.67000 P@3: 0.54667 P@5: 0.41200 N@3: 0.57948 N@5: 0.56781 early stop: 1\n",
            "\u001b[33m[W 210624 15:18:25 models:137]\u001b[39m Clipping gradients with total norm 0.00392 and max norm 0.00026\n",
            "\u001b[32m[I 210624 15:18:42 models:110]\u001b[39m 474 1024 train loss: 0.0000049 valid loss: 0.2616051 P@1: 0.67000 P@3: 0.54667 P@5: 0.41200 N@3: 0.57948 N@5: 0.56781 early stop: 2\n",
            "\u001b[32m[I 210624 15:19:06 models:110]\u001b[39m 499 1024 train loss: 0.0000041 valid loss: 0.2666784 P@1: 0.67000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58010 N@5: 0.57170 early stop: 0\n",
            "\u001b[32m[I 210624 15:19:30 models:110]\u001b[39m 524 1024 train loss: 0.0000035 valid loss: 0.2715280 P@1: 0.67000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58071 N@5: 0.57273 early stop: 0\n",
            "\u001b[32m[I 210624 15:19:54 models:110]\u001b[39m 549 1024 train loss: 0.0000031 valid loss: 0.2761523 P@1: 0.67000 P@3: 0.55333 P@5: 0.41800 N@3: 0.58541 N@5: 0.57464 early stop: 0\n",
            "\u001b[33m[W 210624 15:20:15 models:137]\u001b[39m Clipping gradients with total norm 0.00665 and max norm 0.00047\n",
            "\u001b[32m[I 210624 15:20:17 models:110]\u001b[39m 574 1024 train loss: 0.0000028 valid loss: 0.2805956 P@1: 0.66000 P@3: 0.55333 P@5: 0.41800 N@3: 0.58429 N@5: 0.57362 early stop: 1\n",
            "\u001b[32m[I 210624 15:20:41 models:110]\u001b[39m 599 1024 train loss: 0.0000024 valid loss: 0.2848959 P@1: 0.66000 P@3: 0.55333 P@5: 0.41800 N@3: 0.58429 N@5: 0.57351 early stop: 2\n",
            "\u001b[33m[W 210624 15:20:54 models:137]\u001b[39m Clipping gradients with total norm 0.00217 and max norm 0.0002\n",
            "\u001b[32m[I 210624 15:21:05 models:110]\u001b[39m 624 1024 train loss: 0.0000021 valid loss: 0.2890317 P@1: 0.66000 P@3: 0.55333 P@5: 0.42000 N@3: 0.58367 N@5: 0.57471 early stop: 0\n",
            "\u001b[32m[I 210624 15:21:29 models:110]\u001b[39m 649 1024 train loss: 0.0000020 valid loss: 0.2930167 P@1: 0.66000 P@3: 0.55333 P@5: 0.42000 N@3: 0.58367 N@5: 0.57471 early stop: 1\n",
            "\u001b[33m[W 210624 15:21:37 models:137]\u001b[39m Clipping gradients with total norm 0.0009 and max norm 0.00016\n",
            "\u001b[33m[W 210624 15:21:42 models:137]\u001b[39m Clipping gradients with total norm 0.00165 and max norm 0.00025\n",
            "\u001b[33m[W 210624 15:21:50 models:137]\u001b[39m Clipping gradients with total norm 0.00133 and max norm 0.00023\n",
            "\u001b[33m[W 210624 15:21:53 models:137]\u001b[39m Clipping gradients with total norm 0.00136 and max norm 0.00025\n",
            "\u001b[32m[I 210624 15:21:53 models:110]\u001b[39m 674 1024 train loss: 0.0000018 valid loss: 0.2968616 P@1: 0.65000 P@3: 0.55000 P@5: 0.42000 N@3: 0.57898 N@5: 0.57291 early stop: 2\n",
            "\u001b[33m[W 210624 15:22:06 models:137]\u001b[39m Clipping gradients with total norm 0.0052 and max norm 0.00044\n",
            "\u001b[33m[W 210624 15:22:13 models:137]\u001b[39m Clipping gradients with total norm 0.00504 and max norm 0.00012\n",
            "\u001b[32m[I 210624 15:22:17 models:110]\u001b[39m 699 1024 train loss: 0.0000017 valid loss: 0.3005809 P@1: 0.65000 P@3: 0.55000 P@5: 0.42000 N@3: 0.57898 N@5: 0.57291 early stop: 3\n",
            "\u001b[32m[I 210624 15:22:40 models:110]\u001b[39m 724 1024 train loss: 0.0000014 valid loss: 0.3041970 P@1: 0.65000 P@3: 0.55000 P@5: 0.41800 N@3: 0.57960 N@5: 0.57098 early stop: 4\n",
            "\u001b[33m[W 210624 15:22:43 models:137]\u001b[39m Clipping gradients with total norm 0.00235 and max norm 7e-05\n",
            "\u001b[33m[W 210624 15:22:46 models:137]\u001b[39m Clipping gradients with total norm 0.00094 and max norm 0.00014\n",
            "\u001b[33m[W 210624 15:22:58 models:137]\u001b[39m Clipping gradients with total norm 0.00589 and max norm 0.00025\n",
            "\u001b[32m[I 210624 15:23:04 models:110]\u001b[39m 749 1024 train loss: 0.0000014 valid loss: 0.3076957 P@1: 0.65000 P@3: 0.54667 P@5: 0.41800 N@3: 0.57725 N@5: 0.57075 early stop: 5\n",
            "\u001b[33m[W 210624 15:23:20 models:137]\u001b[39m Clipping gradients with total norm 0.00194 and max norm 0.00027\n",
            "\u001b[32m[I 210624 15:23:27 models:110]\u001b[39m 774 1024 train loss: 0.0000012 valid loss: 0.3110780 P@1: 0.65000 P@3: 0.54667 P@5: 0.41800 N@3: 0.57725 N@5: 0.57075 early stop: 6\n",
            "\u001b[33m[W 210624 15:23:38 models:137]\u001b[39m Clipping gradients with total norm 0.00133 and max norm 0.00011\n",
            "\u001b[33m[W 210624 15:23:43 models:137]\u001b[39m Clipping gradients with total norm 0.00084 and max norm 9e-05\n",
            "\u001b[32m[I 210624 15:23:51 models:110]\u001b[39m 799 1024 train loss: 0.0000011 valid loss: 0.3143861 P@1: 0.66000 P@3: 0.54667 P@5: 0.41800 N@3: 0.57898 N@5: 0.57248 early stop: 7\n",
            "\u001b[33m[W 210624 15:24:12 models:137]\u001b[39m Clipping gradients with total norm 0.00596 and max norm 7e-05\n",
            "\u001b[32m[I 210624 15:24:15 models:110]\u001b[39m 824 1024 train loss: 0.0000010 valid loss: 0.3175977 P@1: 0.66000 P@3: 0.54333 P@5: 0.42000 N@3: 0.57725 N@5: 0.57417 early stop: 8\n",
            "\u001b[33m[W 210624 15:24:24 models:137]\u001b[39m Clipping gradients with total norm 0.00767 and max norm 9e-05\n",
            "\u001b[33m[W 210624 15:24:33 models:137]\u001b[39m Clipping gradients with total norm 0.01256 and max norm 0.00042\n",
            "\u001b[33m[W 210624 15:24:38 models:137]\u001b[39m Clipping gradients with total norm 0.00293 and max norm 0.00025\n",
            "\u001b[32m[I 210624 15:24:38 models:110]\u001b[39m 849 1024 train loss: 0.0000013 valid loss: 0.3207212 P@1: 0.66000 P@3: 0.54333 P@5: 0.41800 N@3: 0.57725 N@5: 0.57271 early stop: 9\n",
            "\u001b[33m[W 210624 15:24:46 models:137]\u001b[39m Clipping gradients with total norm 0.0006 and max norm 0.00012\n",
            "\u001b[33m[W 210624 15:24:58 models:137]\u001b[39m Clipping gradients with total norm 0.00112 and max norm 0.00021\n",
            "\u001b[32m[I 210624 15:25:02 models:110]\u001b[39m 874 1024 train loss: 0.0000008 valid loss: 0.3237781 P@1: 0.67000 P@3: 0.54667 P@5: 0.42000 N@3: 0.58133 N@5: 0.57608 early stop: 0\n",
            "\u001b[32m[I 210624 15:25:26 models:110]\u001b[39m 899 1024 train loss: 0.0000007 valid loss: 0.3267716 P@1: 0.67000 P@3: 0.55000 P@5: 0.42000 N@3: 0.58367 N@5: 0.57640 early stop: 0\n",
            "\u001b[33m[W 210624 15:25:29 models:137]\u001b[39m Clipping gradients with total norm 0.00269 and max norm 0.00021\n",
            "\u001b[32m[I 210624 15:25:49 models:110]\u001b[39m 924 1024 train loss: 0.0000008 valid loss: 0.3297160 P@1: 0.67000 P@3: 0.55333 P@5: 0.42200 N@3: 0.58602 N@5: 0.57849 early stop: 0\n",
            "\u001b[33m[W 210624 15:26:02 models:137]\u001b[39m Clipping gradients with total norm 0.00169 and max norm 0.00016\n",
            "\u001b[32m[I 210624 15:26:13 models:110]\u001b[39m 949 1024 train loss: 0.0000007 valid loss: 0.3326059 P@1: 0.67000 P@3: 0.55333 P@5: 0.42400 N@3: 0.58602 N@5: 0.57980 early stop: 0\n",
            "\u001b[33m[W 210624 15:26:18 models:137]\u001b[39m Clipping gradients with total norm 0.00199 and max norm 0.00014\n",
            "\u001b[33m[W 210624 15:26:20 models:137]\u001b[39m Clipping gradients with total norm 0.02167 and max norm 0.00033\n",
            "\u001b[33m[W 210624 15:26:22 models:137]\u001b[39m Clipping gradients with total norm 0.00422 and max norm 0.00011\n",
            "\u001b[33m[W 210624 15:26:24 models:137]\u001b[39m Clipping gradients with total norm 0.0054 and max norm 0.00021\n",
            "\u001b[33m[W 210624 15:26:29 models:137]\u001b[39m Clipping gradients with total norm 0.00138 and max norm 0.00027\n",
            "\u001b[33m[W 210624 15:26:36 models:137]\u001b[39m Clipping gradients with total norm 0.00096 and max norm 0.00014\n",
            "\u001b[32m[I 210624 15:26:37 models:110]\u001b[39m 974 1024 train loss: 0.0000017 valid loss: 0.3354567 P@1: 0.67000 P@3: 0.55333 P@5: 0.42400 N@3: 0.58602 N@5: 0.57995 early stop: 0\n",
            "\u001b[33m[W 210624 15:26:47 models:137]\u001b[39m Clipping gradients with total norm 0.00035 and max norm 6e-05\n",
            "\u001b[33m[W 210624 15:26:49 models:137]\u001b[39m Clipping gradients with total norm 0.00077 and max norm 0.00015\n",
            "\u001b[32m[I 210624 15:27:01 models:110]\u001b[39m 999 1024 train loss: 0.0000006 valid loss: 0.3382077 P@1: 0.67000 P@3: 0.55333 P@5: 0.42400 N@3: 0.58602 N@5: 0.57995 early stop: 1\n",
            "\u001b[32m[I 210624 15:27:01 main:76]\u001b[39m Finish Training\n",
            "\u001b[32m[I 210624 15:27:02 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210624 15:27:02 main:79]\u001b[39m Loading Test Set\n",
            "\u001b[32m[I 210624 15:27:02 main:83]\u001b[39m Size of Test Set: 100\n",
            "\u001b[32m[I 210624 15:27:02 main:85]\u001b[39m Predicting\n",
            "\u001b[32m[I 210624 15:27:05 main:91]\u001b[39m Finish Predicting\n",
            "Precision@1,3,5: 0.66 0.53 0.42\n",
            "nDCG@1,3,5: 0.66 0.5620467187130749 0.5699775424270964\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBhMpPdH3DuI"
      },
      "source": [
        "## Study: Effect of Dataset Size on MATCH Performance\n",
        "\n",
        "| Train set size | P@1=nDCG@1 | P@3 | P@5 | nDCG@3 | nDCG@5 |\n",
        "| --- | --- | --- | --- | --- | --- |\n",
        "| 200 | 0.324 | 0.249 | 0.203 | 0.269 | 0.274 |\n",
        "| 300 | 0.424 | 0.337 | 0.275 | 0.362 | 0.364 |\n",
        "| 400 | 0.441 | 0.344 | 0.278 | 0.373 | 0.373 |\n",
        "| 500 | 0.547 | 0.419 | 0.328 | 0.454 | 0.447 |\n",
        "| 600 | 0.534 | 0.433 | 0.345 | 0.464 | 0.463 |\n",
        "| 700 | 0.555 | 0.434 | 0.342 | 0.466 | 0.472 |\n",
        "| 800 | 0.627 | 0.509 | 0.390 | 0.542 | 0.543 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEhF-2cM8wnk",
        "outputId": "eb151674-c029-4176-cfee-3e66cfaba3d6"
      },
      "source": [
        "%cd PeTaL/\n",
        "# Note: in order to vary traning set size, I adjusted the --train parameter (currently 0.8 for 0.8 * 1000 = 800 training examples)\n",
        "!python3 Split.py --train 0.8 --dev 0.1\n",
        "%cd ..\n",
        "# If the splitting went correctly, the first number in the wc output should be the number of training examples.\n",
        "!wc PeTaL/train.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shareddrives/MATCH Attempt/MATCH/PeTaL\n",
            "131\n",
            "/content/drive/Shareddrives/MATCH Attempt/MATCH\n",
            "    800  275452 4644161 PeTaL/train.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0ccg2sz_SCc",
        "outputId": "de48dd4e-e9f1-4b33-8aad-36b6ec405af8"
      },
      "source": [
        "# Slightly modified preprocess.sh\n",
        "\n",
        "!python3 transform_data_PeTaL.py --dataset $DATASET\n",
        "\n",
        "!python preprocess.py \\\n",
        "--text-path {DATASET}/train_texts.txt \\\n",
        "--label-path {DATASET}/train_labels.txt \\\n",
        "--vocab-path {DATASET}/vocab.npy \\\n",
        "--emb-path {DATASET}/emb_init.npy \\\n",
        "--w2v-model {DATASET}/{DATASET}.joint.emb \\\n",
        "\n",
        "!python preprocess.py \\\n",
        "--text-path {DATASET}/test_texts.txt \\\n",
        "--label-path {DATASET}/test_labels.txt \\\n",
        "--vocab-path {DATASET}/vocab.npy \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 210625 17:01:33 preprocess:28]\u001b[39m Vocab Size: 26834\n",
            "\u001b[32m[I 210625 17:01:33 preprocess:30]\u001b[39m Getting Dataset: PeTaL/train_texts.txt Max Length: 500\n",
            "\u001b[32m[I 210625 17:01:33 preprocess:32]\u001b[39m Size of Samples: 900\n",
            "\u001b[32m[I 210625 17:01:34 preprocess:28]\u001b[39m Vocab Size: 26834\n",
            "\u001b[32m[I 210625 17:01:34 preprocess:30]\u001b[39m Getting Dataset: PeTaL/test_texts.txt Max Length: 500\n",
            "\u001b[32m[I 210625 17:01:34 preprocess:32]\u001b[39m Size of Samples: 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dv1TKNj5_n3k",
        "outputId": "01c829d3-62a6-49f0-a74b-5021c4f01f69"
      },
      "source": [
        "# Slightly modified run_models.sh\n",
        "\n",
        "!PYTHONFAULTHANDLER=1 python main.py --data-cnf configure/datasets/{DATASET}.yaml --model-cnf configure/models/{MODEL}-{DATASET}.yaml --mode train --reg 1\n",
        "!PYTHONFAULTHANDLER=1 python main.py --data-cnf configure/datasets/{DATASET}.yaml --model-cnf configure/models/{MODEL}-{DATASET}.yaml --mode eval\n",
        "\n",
        "!python evaluation.py \\\n",
        "--results {DATASET}/results/{MODEL}-{DATASET}-labels.npy \\\n",
        "--targets {DATASET}/test_labels.npy \\\n",
        "--train-labels {DATASET}/train_labels.npy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 210625 17:01:38 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210625 17:01:38 main:35]\u001b[39m Loading Training and Validation Set\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['absorb_and/or_filter_solids', 'chemically_break_down_inorganic_compounds', 'detox/purify', 'manage_environmental_disturbances_in_a_community', 'protect_from_fire', 'protect_from_gases', 'send_vibratory_signals'] will be ignored\n",
            "  .format(sorted(unknown, key=str)))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['absorb_and/or_filter_solids', 'detox/purify', 'protect_from_gases'] will be ignored\n",
            "  .format(sorted(unknown, key=str)))\n",
            "\u001b[32m[I 210625 17:01:38 main:47]\u001b[39m Number of Labels: 124\n",
            "\u001b[32m[I 210625 17:01:38 main:48]\u001b[39m Size of Training Set: 800\n",
            "\u001b[32m[I 210625 17:01:38 main:49]\u001b[39m Size of Validation Set: 100\n",
            "\u001b[32m[I 210625 17:01:38 main:66]\u001b[39m Number of Edges: 101\n",
            "\u001b[32m[I 210625 17:01:38 main:68]\u001b[39m Training\n",
            "\u001b[32m[I 210625 17:01:45 models:142]\u001b[39m SWA Initializing\n",
            "\u001b[32m[I 210625 17:02:05 models:110]\u001b[39m 24 1024 train loss: 0.1285587 valid loss: 0.1464014 P@1: 0.43000 P@3: 0.27667 P@5: 0.23800 N@3: 0.31060 N@5: 0.32622 early stop: 0\n",
            "\u001b[32m[I 210625 17:02:29 models:110]\u001b[39m 49 1024 train loss: 0.0270746 valid loss: 0.1439123 P@1: 0.47000 P@3: 0.35667 P@5: 0.29600 N@3: 0.38437 N@5: 0.39816 early stop: 0\n",
            "\u001b[32m[I 210625 17:02:54 models:110]\u001b[39m 74 1024 train loss: 0.0104609 valid loss: 0.1469670 P@1: 0.50000 P@3: 0.38000 P@5: 0.32800 N@3: 0.41336 N@5: 0.43343 early stop: 0\n",
            "\u001b[32m[I 210625 17:03:19 models:110]\u001b[39m 99 1024 train loss: 0.0040509 valid loss: 0.1580660 P@1: 0.53000 P@3: 0.40333 P@5: 0.32800 N@3: 0.43682 N@5: 0.44306 early stop: 0\n",
            "\u001b[32m[I 210625 17:03:43 models:110]\u001b[39m 124 1024 train loss: 0.0046082 valid loss: 0.1709425 P@1: 0.53000 P@3: 0.40667 P@5: 0.34400 N@3: 0.43845 N@5: 0.45508 early stop: 0\n",
            "\u001b[32m[I 210625 17:04:08 models:110]\u001b[39m 149 1024 train loss: 0.0022198 valid loss: 0.1842978 P@1: 0.51000 P@3: 0.42000 P@5: 0.34800 N@3: 0.44560 N@5: 0.45866 early stop: 0\n",
            "\u001b[33m[W 210625 17:04:30 models:137]\u001b[39m Clipping gradients with total norm 0.30143 and max norm 0.03403\n",
            "\u001b[32m[I 210625 17:04:33 models:110]\u001b[39m 174 1024 train loss: 0.0004330 valid loss: 0.1975498 P@1: 0.51000 P@3: 0.42667 P@5: 0.35800 N@3: 0.45029 N@5: 0.46643 early stop: 0\n",
            "\u001b[32m[I 210625 17:04:57 models:110]\u001b[39m 199 1024 train loss: 0.0007769 valid loss: 0.2098621 P@1: 0.52000 P@3: 0.44000 P@5: 0.36200 N@3: 0.46325 N@5: 0.47581 early stop: 0\n",
            "\u001b[32m[I 210625 17:05:22 models:110]\u001b[39m 224 1024 train loss: 0.0000366 valid loss: 0.2219777 P@1: 0.53000 P@3: 0.45333 P@5: 0.36600 N@3: 0.47552 N@5: 0.48305 early stop: 0\n",
            "\u001b[33m[W 210625 17:05:46 models:137]\u001b[39m Clipping gradients with total norm 0.04468 and max norm 0.0008\n",
            "\u001b[32m[I 210625 17:05:47 models:110]\u001b[39m 249 1024 train loss: 0.0000136 valid loss: 0.2332115 P@1: 0.55000 P@3: 0.46000 P@5: 0.36600 N@3: 0.48367 N@5: 0.48704 early stop: 0\n",
            "\u001b[33m[W 210625 17:05:50 models:137]\u001b[39m Clipping gradients with total norm 0.00543 and max norm 0.00071\n",
            "\u001b[33m[W 210625 17:05:54 models:137]\u001b[39m Clipping gradients with total norm 0.01207 and max norm 0.0015\n",
            "\u001b[33m[W 210625 17:06:05 models:137]\u001b[39m Clipping gradients with total norm 0.01006 and max norm 0.00119\n",
            "\u001b[33m[W 210625 17:06:07 models:137]\u001b[39m Clipping gradients with total norm 0.01397 and max norm 0.00143\n",
            "\u001b[32m[I 210625 17:06:11 models:110]\u001b[39m 274 1024 train loss: 0.0000117 valid loss: 0.2436063 P@1: 0.55000 P@3: 0.46667 P@5: 0.37000 N@3: 0.48898 N@5: 0.49119 early stop: 0\n",
            "\u001b[33m[W 210625 17:06:24 models:137]\u001b[39m Clipping gradients with total norm 0.0116 and max norm 0.00221\n",
            "\u001b[33m[W 210625 17:06:32 models:137]\u001b[39m Clipping gradients with total norm 0.0037 and max norm 0.00071\n",
            "\u001b[32m[I 210625 17:06:36 models:110]\u001b[39m 299 1024 train loss: 0.0000081 valid loss: 0.2533838 P@1: 0.56000 P@3: 0.47000 P@5: 0.37400 N@3: 0.49244 N@5: 0.49545 early stop: 0\n",
            "\u001b[33m[W 210625 17:06:47 models:137]\u001b[39m Clipping gradients with total norm 0.00942 and max norm 0.00089\n",
            "\u001b[32m[I 210625 17:07:00 models:110]\u001b[39m 324 1024 train loss: 0.0000064 valid loss: 0.2622784 P@1: 0.57000 P@3: 0.47333 P@5: 0.37600 N@3: 0.49714 N@5: 0.49917 early stop: 0\n",
            "\u001b[33m[W 210625 17:07:14 models:137]\u001b[39m Clipping gradients with total norm 0.00307 and max norm 0.00058\n",
            "\u001b[33m[W 210625 17:07:21 models:137]\u001b[39m Clipping gradients with total norm 0.00417 and max norm 0.0008\n",
            "\u001b[32m[I 210625 17:07:25 models:110]\u001b[39m 349 1024 train loss: 0.0000055 valid loss: 0.2706714 P@1: 0.57000 P@3: 0.47333 P@5: 0.37800 N@3: 0.49714 N@5: 0.50152 early stop: 0\n",
            "\u001b[33m[W 210625 17:07:30 models:137]\u001b[39m Clipping gradients with total norm 0.00684 and max norm 0.00052\n",
            "\u001b[33m[W 210625 17:07:49 models:137]\u001b[39m Clipping gradients with total norm 0.00393 and max norm 0.00047\n",
            "\u001b[32m[I 210625 17:07:50 models:110]\u001b[39m 374 1024 train loss: 0.0000045 valid loss: 0.2785430 P@1: 0.57000 P@3: 0.47667 P@5: 0.37800 N@3: 0.50010 N@5: 0.50282 early stop: 0\n",
            "\u001b[33m[W 210625 17:08:00 models:137]\u001b[39m Clipping gradients with total norm 0.00343 and max norm 0.00041\n",
            "\u001b[32m[I 210625 17:08:14 models:110]\u001b[39m 399 1024 train loss: 0.0000039 valid loss: 0.2859447 P@1: 0.57000 P@3: 0.47333 P@5: 0.37800 N@3: 0.49775 N@5: 0.50255 early stop: 1\n",
            "\u001b[33m[W 210625 17:08:19 models:137]\u001b[39m Clipping gradients with total norm 0.00199 and max norm 0.00039\n",
            "\u001b[33m[W 210625 17:08:36 models:137]\u001b[39m Clipping gradients with total norm 0.02951 and max norm 0.00033\n",
            "\u001b[32m[I 210625 17:08:39 models:110]\u001b[39m 424 1024 train loss: 0.0000039 valid loss: 0.2928540 P@1: 0.57000 P@3: 0.47667 P@5: 0.38000 N@3: 0.50010 N@5: 0.50439 early stop: 0\n",
            "\u001b[33m[W 210625 17:08:53 models:137]\u001b[39m Clipping gradients with total norm 0.01288 and max norm 0.00135\n",
            "\u001b[32m[I 210625 17:09:03 models:110]\u001b[39m 449 1024 train loss: 0.0000031 valid loss: 0.2994399 P@1: 0.57000 P@3: 0.48000 P@5: 0.38400 N@3: 0.50244 N@5: 0.50790 early stop: 0\n",
            "\u001b[33m[W 210625 17:09:05 models:137]\u001b[39m Clipping gradients with total norm 0.00781 and max norm 0.00031\n",
            "\u001b[33m[W 210625 17:09:17 models:137]\u001b[39m Clipping gradients with total norm 0.00147 and max norm 0.00027\n",
            "\u001b[33m[W 210625 17:09:19 models:137]\u001b[39m Clipping gradients with total norm 0.00767 and max norm 0.00093\n",
            "\u001b[32m[I 210625 17:09:28 models:110]\u001b[39m 474 1024 train loss: 0.0000029 valid loss: 0.3056272 P@1: 0.57000 P@3: 0.48333 P@5: 0.38400 N@3: 0.50418 N@5: 0.50793 early stop: 0\n",
            "\u001b[33m[W 210625 17:09:33 models:137]\u001b[39m Clipping gradients with total norm 0.00391 and max norm 0.00055\n",
            "\u001b[33m[W 210625 17:09:39 models:137]\u001b[39m Clipping gradients with total norm 0.00757 and max norm 0.00066\n",
            "\u001b[32m[I 210625 17:09:53 models:110]\u001b[39m 499 1024 train loss: 0.0000027 valid loss: 0.3115870 P@1: 0.57000 P@3: 0.48333 P@5: 0.38400 N@3: 0.50479 N@5: 0.50844 early stop: 0\n",
            "\u001b[33m[W 210625 17:09:56 models:137]\u001b[39m Clipping gradients with total norm 0.00583 and max norm 0.00054\n",
            "\u001b[32m[I 210625 17:10:17 models:110]\u001b[39m 524 1024 train loss: 0.0000023 valid loss: 0.3172634 P@1: 0.57000 P@3: 0.48333 P@5: 0.38400 N@3: 0.50479 N@5: 0.50844 early stop: 1\n",
            "\u001b[33m[W 210625 17:10:19 models:137]\u001b[39m Clipping gradients with total norm 0.00333 and max norm 0.00032\n",
            "\u001b[33m[W 210625 17:10:25 models:137]\u001b[39m Clipping gradients with total norm 0.00594 and max norm 0.00063\n",
            "\u001b[33m[W 210625 17:10:33 models:137]\u001b[39m Clipping gradients with total norm 0.00277 and max norm 0.00051\n",
            "\u001b[32m[I 210625 17:10:42 models:110]\u001b[39m 549 1024 train loss: 0.0000021 valid loss: 0.3227161 P@1: 0.57000 P@3: 0.48000 P@5: 0.38600 N@3: 0.50244 N@5: 0.50943 early stop: 0\n",
            "\u001b[33m[W 210625 17:10:56 models:137]\u001b[39m Clipping gradients with total norm 0.00189 and max norm 0.00024\n",
            "\u001b[33m[W 210625 17:11:04 models:137]\u001b[39m Clipping gradients with total norm 0.00162 and max norm 0.00016\n",
            "\u001b[32m[I 210625 17:11:07 models:110]\u001b[39m 574 1024 train loss: 0.0000016 valid loss: 0.3278964 P@1: 0.57000 P@3: 0.48667 P@5: 0.38600 N@3: 0.50714 N@5: 0.51003 early stop: 0\n",
            "\u001b[33m[W 210625 17:11:09 models:137]\u001b[39m Clipping gradients with total norm 0.0042 and max norm 0.00065\n",
            "\u001b[33m[W 210625 17:11:11 models:137]\u001b[39m Clipping gradients with total norm 0.00995 and max norm 0.00103\n",
            "\u001b[33m[W 210625 17:11:18 models:137]\u001b[39m Clipping gradients with total norm 0.00715 and max norm 0.00079\n",
            "\u001b[33m[W 210625 17:11:20 models:137]\u001b[39m Clipping gradients with total norm 0.00165 and max norm 0.00027\n",
            "\u001b[33m[W 210625 17:11:24 models:137]\u001b[39m Clipping gradients with total norm 0.00151 and max norm 0.00027\n",
            "\u001b[32m[I 210625 17:11:32 models:110]\u001b[39m 599 1024 train loss: 0.0000022 valid loss: 0.3328350 P@1: 0.57000 P@3: 0.48667 P@5: 0.38600 N@3: 0.50714 N@5: 0.51003 early stop: 1\n",
            "\u001b[33m[W 210625 17:11:35 models:137]\u001b[39m Clipping gradients with total norm 0.0007 and max norm 0.00012\n",
            "\u001b[33m[W 210625 17:11:44 models:137]\u001b[39m Clipping gradients with total norm 0.00166 and max norm 0.00029\n",
            "\u001b[33m[W 210625 17:11:53 models:137]\u001b[39m Clipping gradients with total norm 0.00236 and max norm 0.00018\n",
            "\u001b[32m[I 210625 17:11:56 models:110]\u001b[39m 624 1024 train loss: 0.0000015 valid loss: 0.3375337 P@1: 0.57000 P@3: 0.49333 P@5: 0.38800 N@3: 0.51183 N@5: 0.51169 early stop: 0\n",
            "\u001b[33m[W 210625 17:12:11 models:137]\u001b[39m Clipping gradients with total norm 0.002 and max norm 0.00021\n",
            "\u001b[33m[W 210625 17:12:20 models:137]\u001b[39m Clipping gradients with total norm 0.00366 and max norm 0.00032\n",
            "\u001b[32m[I 210625 17:12:21 models:110]\u001b[39m 649 1024 train loss: 0.0000013 valid loss: 0.3420465 P@1: 0.57000 P@3: 0.49333 P@5: 0.38800 N@3: 0.51183 N@5: 0.51190 early stop: 0\n",
            "\u001b[33m[W 210625 17:12:24 models:137]\u001b[39m Clipping gradients with total norm 0.00152 and max norm 0.00027\n",
            "\u001b[33m[W 210625 17:12:29 models:137]\u001b[39m Clipping gradients with total norm 0.00511 and max norm 0.00046\n",
            "\u001b[33m[W 210625 17:12:31 models:137]\u001b[39m Clipping gradients with total norm 0.00379 and max norm 0.00018\n",
            "\u001b[32m[I 210625 17:12:45 models:110]\u001b[39m 674 1024 train loss: 0.0000014 valid loss: 0.3464078 P@1: 0.57000 P@3: 0.49333 P@5: 0.39200 N@3: 0.51183 N@5: 0.51568 early stop: 0\n",
            "\u001b[32m[I 210625 17:13:10 models:110]\u001b[39m 699 1024 train loss: 0.0000011 valid loss: 0.3505856 P@1: 0.57000 P@3: 0.49333 P@5: 0.39200 N@3: 0.51183 N@5: 0.51568 early stop: 1\n",
            "\u001b[33m[W 210625 17:13:16 models:137]\u001b[39m Clipping gradients with total norm 0.00104 and max norm 0.00014\n",
            "\u001b[33m[W 210625 17:13:35 models:137]\u001b[39m Clipping gradients with total norm 0.00159 and max norm 0.00018\n",
            "\u001b[32m[I 210625 17:13:35 models:110]\u001b[39m 724 1024 train loss: 0.0000010 valid loss: 0.3546435 P@1: 0.57000 P@3: 0.49333 P@5: 0.39400 N@3: 0.51244 N@5: 0.51760 early stop: 0\n",
            "\u001b[33m[W 210625 17:13:56 models:137]\u001b[39m Clipping gradients with total norm 0.00229 and max norm 0.00041\n",
            "\u001b[32m[I 210625 17:14:00 models:110]\u001b[39m 749 1024 train loss: 0.0000010 valid loss: 0.3585332 P@1: 0.57000 P@3: 0.49333 P@5: 0.39400 N@3: 0.51244 N@5: 0.51760 early stop: 1\n",
            "\u001b[33m[W 210625 17:14:19 models:137]\u001b[39m Clipping gradients with total norm 0.00137 and max norm 8e-05\n",
            "\u001b[32m[I 210625 17:14:24 models:110]\u001b[39m 774 1024 train loss: 0.0000008 valid loss: 0.3623462 P@1: 0.57000 P@3: 0.49333 P@5: 0.39400 N@3: 0.51306 N@5: 0.51822 early stop: 0\n",
            "\u001b[33m[W 210625 17:14:37 models:137]\u001b[39m Clipping gradients with total norm 0.00321 and max norm 0.00021\n",
            "\u001b[33m[W 210625 17:14:40 models:137]\u001b[39m Clipping gradients with total norm 0.00244 and max norm 9e-05\n",
            "\u001b[32m[I 210625 17:14:49 models:110]\u001b[39m 799 1024 train loss: 0.0000009 valid loss: 0.3659998 P@1: 0.57000 P@3: 0.49333 P@5: 0.39400 N@3: 0.51306 N@5: 0.51822 early stop: 1\n",
            "\u001b[33m[W 210625 17:14:50 models:137]\u001b[39m Clipping gradients with total norm 0.00427 and max norm 0.00022\n",
            "\u001b[33m[W 210625 17:14:53 models:137]\u001b[39m Clipping gradients with total norm 0.00128 and max norm 7e-05\n",
            "\u001b[33m[W 210625 17:14:58 models:137]\u001b[39m Clipping gradients with total norm 0.00063 and max norm 0.00012\n",
            "\u001b[33m[W 210625 17:15:04 models:137]\u001b[39m Clipping gradients with total norm 0.00098 and max norm 0.00016\n",
            "\u001b[32m[I 210625 17:15:13 models:110]\u001b[39m 824 1024 train loss: 0.0000008 valid loss: 0.3695699 P@1: 0.56000 P@3: 0.49333 P@5: 0.39400 N@3: 0.51194 N@5: 0.51693 early stop: 2\n",
            "\u001b[33m[W 210625 17:15:28 models:137]\u001b[39m Clipping gradients with total norm 0.01811 and max norm 6e-05\n",
            "\u001b[33m[W 210625 17:15:32 models:137]\u001b[39m Clipping gradients with total norm 0.00502 and max norm 0.00012\n",
            "\u001b[32m[I 210625 17:15:38 models:110]\u001b[39m 849 1024 train loss: 0.0000012 valid loss: 0.3730476 P@1: 0.55000 P@3: 0.49333 P@5: 0.39400 N@3: 0.51021 N@5: 0.51520 early stop: 3\n",
            "\u001b[33m[W 210625 17:15:53 models:137]\u001b[39m Clipping gradients with total norm 0.00197 and max norm 0.00015\n",
            "\u001b[32m[I 210625 17:16:03 models:110]\u001b[39m 874 1024 train loss: 0.0000007 valid loss: 0.3764400 P@1: 0.56000 P@3: 0.49333 P@5: 0.39400 N@3: 0.51194 N@5: 0.51645 early stop: 4\n",
            "\u001b[33m[W 210625 17:16:09 models:137]\u001b[39m Clipping gradients with total norm 0.00594 and max norm 8e-05\n",
            "\u001b[33m[W 210625 17:16:12 models:137]\u001b[39m Clipping gradients with total norm 0.01333 and max norm 0.00061\n",
            "\u001b[33m[W 210625 17:16:15 models:137]\u001b[39m Clipping gradients with total norm 0.00313 and max norm 0.00016\n",
            "\u001b[33m[W 210625 17:16:22 models:137]\u001b[39m Clipping gradients with total norm 0.00224 and max norm 0.00035\n",
            "\u001b[33m[W 210625 17:16:24 models:137]\u001b[39m Clipping gradients with total norm 0.0015 and max norm 0.00011\n",
            "\u001b[32m[I 210625 17:16:27 models:110]\u001b[39m 899 1024 train loss: 0.0000009 valid loss: 0.3797420 P@1: 0.55000 P@3: 0.49333 P@5: 0.39600 N@3: 0.51021 N@5: 0.51672 early stop: 5\n",
            "\u001b[33m[W 210625 17:16:41 models:137]\u001b[39m Clipping gradients with total norm 0.00097 and max norm 9e-05\n",
            "\u001b[33m[W 210625 17:16:45 models:137]\u001b[39m Clipping gradients with total norm 0.00043 and max norm 7e-05\n",
            "\u001b[33m[W 210625 17:16:51 models:137]\u001b[39m Clipping gradients with total norm 0.00044 and max norm 6e-05\n",
            "\u001b[32m[I 210625 17:16:52 models:110]\u001b[39m 924 1024 train loss: 0.0000005 valid loss: 0.3829843 P@1: 0.55000 P@3: 0.49000 P@5: 0.39400 N@3: 0.50786 N@5: 0.51517 early stop: 6\n",
            "\u001b[33m[W 210625 17:16:58 models:137]\u001b[39m Clipping gradients with total norm 0.00038 and max norm 5e-05\n",
            "\u001b[32m[I 210625 17:17:17 models:110]\u001b[39m 949 1024 train loss: 0.0000005 valid loss: 0.3861414 P@1: 0.55000 P@3: 0.49000 P@5: 0.39400 N@3: 0.50786 N@5: 0.51537 early stop: 7\n",
            "\u001b[33m[W 210625 17:17:29 models:137]\u001b[39m Clipping gradients with total norm 0.03717 and max norm 0.0001\n",
            "\u001b[32m[I 210625 17:17:41 models:110]\u001b[39m 974 1024 train loss: 0.0000020 valid loss: 0.3892423 P@1: 0.55000 P@3: 0.49000 P@5: 0.39400 N@3: 0.50848 N@5: 0.51599 early stop: 8\n",
            "\u001b[33m[W 210625 17:17:50 models:137]\u001b[39m Clipping gradients with total norm 0.00031 and max norm 5e-05\n",
            "\u001b[33m[W 210625 17:17:55 models:137]\u001b[39m Clipping gradients with total norm 0.00031 and max norm 6e-05\n",
            "\u001b[32m[I 210625 17:18:06 models:110]\u001b[39m 999 1024 train loss: 0.0000004 valid loss: 0.3922627 P@1: 0.55000 P@3: 0.49000 P@5: 0.39400 N@3: 0.50848 N@5: 0.51599 early stop: 9\n",
            "\u001b[32m[I 210625 17:18:06 main:76]\u001b[39m Finish Training\n",
            "\u001b[32m[I 210625 17:18:07 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210625 17:18:07 main:79]\u001b[39m Loading Test Set\n",
            "\u001b[32m[I 210625 17:18:07 main:83]\u001b[39m Size of Test Set: 100\n",
            "\u001b[32m[I 210625 17:18:07 main:85]\u001b[39m Predicting\n",
            "\u001b[32m[I 210625 17:18:10 main:91]\u001b[39m Finish Predicting\n",
            "Precision@1,3,5: 0.52 0.44 0.346\n",
            "nDCG@1,3,5: 0.52 0.4599936085661029 0.4694861074706761\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2qso0E6CLCl"
      },
      "source": [
        "idea: do k-fold cross validation?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4b9bR44_iyF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54Ur6eGk_q3C"
      },
      "source": [
        "# Results of MATCH Quick Start\n",
        "\n",
        "Running MATCH on MAG-CS dataset as described in the paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNemHI_6auA0",
        "outputId": "077ce78c-9144-413b-d5a2-59fc0a56bceb"
      },
      "source": [
        "!./preprocess.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 210617 15:44:06 preprocess:28]\u001b[39m Vocab Size: 500000\n",
            "\u001b[32m[I 210617 15:44:06 preprocess:30]\u001b[39m Getting Dataset: MAG/train_texts.txt Max Length: 500\n",
            "tcmalloc: large alloc 2539503616 bytes == 0x558fac048000 @  0x7f91aba831e7 0x7f91a934bea1 0x7f91a93b0928 0x7f91a93b4070 0x7f91a93b45e5 0x7f91a944d40d 0x558ed8136d54 0x558ed8136a50 0x558ed81ab105 0x558ed81a54ae 0x558ed81383ea 0x558ed81aa7f0 0x558ed81a57ad 0x558ed81383ea 0x558ed81a63b5 0x558ed81a57ad 0x558ed81383ea 0x558ed81a63b5 0x558ed81a54ae 0x558ed8077e2c 0x558ed81a7bb5 0x558ed81a54ae 0x558ed8138c9f 0x558ed8138ea1 0x558ed81a7bb5 0x558ed813830a 0x558ed81a660e 0x558ed81a54ae 0x558ed8138a81 0x558ed8138ea1 0x558ed81a7bb5\n",
            "\u001b[32m[I 210617 15:46:44 preprocess:32]\u001b[39m Size of Samples: 634874\n",
            "\u001b[32m[I 210617 15:47:41 preprocess:28]\u001b[39m Vocab Size: 500000\n",
            "\u001b[32m[I 210617 15:47:41 preprocess:30]\u001b[39m Getting Dataset: MAG/test_texts.txt Max Length: 500\n",
            "\u001b[32m[I 210617 15:47:56 preprocess:32]\u001b[39m Size of Samples: 70533\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "LahoKA3ofc2w",
        "outputId": "74303e99-cf22-4a18-da53-5d17d01a3eb5"
      },
      "source": [
        "!./run_models.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 210617 15:48:17 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210617 15:48:17 main:35]\u001b[39m Loading Training and Validation Set\n",
            "tcmalloc: large alloc 2539503616 bytes == 0x55777a22e000 @  0x7f4bd84601e7 0x7f4bd5d28ea1 0x7f4bd5d92b75 0x7f4bd5d9370e 0x7f4bd5e2c71e 0x55775d768d54 0x55775d768a50 0x55775d7dd105 0x55775d7d74ae 0x55775d76a3ea 0x55775d7d932a 0x55775d7d74ae 0x55775d76a3ea 0x55775d7d932a 0x55775d7d74ae 0x55775d76a3ea 0x55775d7d83b5 0x55775d7d74ae 0x55775d6a9e2c 0x55775d7d9bb5 0x55775d7d74ae 0x55775d76ac9f 0x55775d76aea1 0x55775d7d9bb5 0x55775d76a30a 0x55775d7d860e 0x55775d7d74ae 0x55775d76aa81 0x55775d76aea1 0x55775d7d9bb5 0x55775d7d74ae\n",
            "tcmalloc: large alloc 2257362944 bytes == 0x5578127f4000 @  0x7f4bd84601e7 0x7f4bd5d28ea1 0x7f4bd5d8d928 0x7f4bd5d8da43 0x7f4bd5ddd2d4 0x7f4bd5e1cb90 0x55775d768d54 0x55775d768a50 0x55775d7dd105 0x55775d7d77ad 0x55775d76a3ea 0x55775d7d83b5 0x55775d85aec8 0x55775d850d8e 0x55775d840b95 0x55775d777a34 0x55775d7a8cc4 0x55775d769462 0x55775d7dc715 0x55775d7d77ad 0x55775d76a3ea 0x55775d7d932a 0x55775d7d74ae 0x55775d6a9e2c 0x55775d7d9bb5 0x55775d7d74ae 0x55775d76ac9f 0x55775d76aea1 0x55775d7d9bb5 0x55775d76a30a 0x55775d7d860e\n",
            "\u001b[32m[I 210617 15:48:47 main:47]\u001b[39m Number of Labels: 15308\n",
            "\u001b[32m[I 210617 15:48:47 main:48]\u001b[39m Size of Training Set: 564340\n",
            "\u001b[32m[I 210617 15:48:47 main:49]\u001b[39m Size of Validation Set: 70534\n",
            "\u001b[32m[I 210617 15:48:54 main:66]\u001b[39m Number of Edges: 26491\n",
            "\u001b[32m[I 210617 15:48:54 main:68]\u001b[39m Training\n",
            "\u001b[32m[I 210617 15:52:33 models:110]\u001b[39m 0 25600 train loss: 0.0265692 valid loss: 0.0023821 P@1: 0.14950 P@3: 0.13431 P@5: 0.11884 N@3: 0.14536 N@5: 0.14271 early stop: 0\n",
            "\u001b[32m[I 210617 15:55:57 models:110]\u001b[39m 0 51200 train loss: 0.0023748 valid loss: 0.0023726 P@1: 0.15261 P@3: 0.13432 P@5: 0.11884 N@3: 0.14591 N@5: 0.14319 early stop: 0\n",
            "\u001b[32m[I 210617 15:59:21 models:110]\u001b[39m 0 76800 train loss: 0.0023770 valid loss: 0.0023646 P@1: 0.14959 P@3: 0.14534 P@5: 0.12222 N@3: 0.15358 N@5: 0.14813 early stop: 0\n",
            "\u001b[32m[I 210617 16:02:44 models:110]\u001b[39m 0 102400 train loss: 0.0023563 valid loss: 0.0023499 P@1: 0.16239 P@3: 0.14967 P@5: 0.12749 N@3: 0.16323 N@5: 0.15912 early stop: 0\n",
            "\u001b[32m[I 210617 16:06:08 models:110]\u001b[39m 0 128000 train loss: 0.0023532 valid loss: 0.0023100 P@1: 0.22043 P@3: 0.19512 P@5: 0.15106 N@3: 0.21118 N@5: 0.19175 early stop: 0\n",
            "\u001b[32m[I 210617 16:09:30 models:110]\u001b[39m 0 153600 train loss: 0.0022600 valid loss: 0.0022188 P@1: 0.28236 P@3: 0.22166 P@5: 0.17591 N@3: 0.24681 N@5: 0.22702 early stop: 0\n",
            "\u001b[32m[I 210617 16:12:52 models:110]\u001b[39m 0 179200 train loss: 0.0022004 valid loss: 0.0021808 P@1: 0.29547 P@3: 0.23071 P@5: 0.18188 N@3: 0.25665 N@5: 0.23578 early stop: 0\n",
            "\u001b[32m[I 210617 16:16:13 models:110]\u001b[39m 0 204800 train loss: 0.0021617 valid loss: 0.0021420 P@1: 0.32641 P@3: 0.23688 P@5: 0.18348 N@3: 0.26946 N@5: 0.24493 early stop: 0\n",
            "\u001b[32m[I 210617 16:19:36 models:110]\u001b[39m 0 230400 train loss: 0.0021457 valid loss: 0.0021426 P@1: 0.32132 P@3: 0.23919 P@5: 0.18677 N@3: 0.27180 N@5: 0.24823 early stop: 0\n",
            "\u001b[32m[I 210617 16:22:58 models:110]\u001b[39m 0 256000 train loss: 0.0021209 valid loss: 0.0020963 P@1: 0.35172 P@3: 0.26092 P@5: 0.19503 N@3: 0.29614 N@5: 0.26304 early stop: 0\n",
            "\u001b[32m[I 210617 16:26:20 models:110]\u001b[39m 0 281600 train loss: 0.0020804 valid loss: 0.0020582 P@1: 0.36778 P@3: 0.27336 P@5: 0.20879 N@3: 0.31183 N@5: 0.28089 early stop: 0\n",
            "\u001b[32m[I 210617 16:29:40 models:110]\u001b[39m 0 307200 train loss: 0.0020335 valid loss: 0.0019928 P@1: 0.42092 P@3: 0.29898 P@5: 0.22828 N@3: 0.34537 N@5: 0.31121 early stop: 0\n",
            "\u001b[32m[I 210617 16:33:02 models:110]\u001b[39m 0 332800 train loss: 0.0019721 valid loss: 0.0019144 P@1: 0.48201 P@3: 0.33649 P@5: 0.25268 N@3: 0.39146 N@5: 0.34977 early stop: 0\n",
            "\u001b[32m[I 210617 16:36:24 models:110]\u001b[39m 0 358400 train loss: 0.0019078 valid loss: 0.0018660 P@1: 0.50603 P@3: 0.34696 P@5: 0.26208 N@3: 0.40541 N@5: 0.36294 early stop: 0\n",
            "\u001b[32m[I 210617 16:39:47 models:110]\u001b[39m 0 384000 train loss: 0.0018624 valid loss: 0.0018374 P@1: 0.52631 P@3: 0.35881 P@5: 0.26892 N@3: 0.42091 N@5: 0.37551 early stop: 0\n",
            "\u001b[32m[I 210617 16:43:08 models:110]\u001b[39m 0 409600 train loss: 0.0018494 valid loss: 0.0018107 P@1: 0.54036 P@3: 0.36706 P@5: 0.27759 N@3: 0.43100 N@5: 0.38672 early stop: 0\n",
            "\u001b[32m[I 210617 16:46:30 models:110]\u001b[39m 0 435200 train loss: 0.0018314 valid loss: 0.0018014 P@1: 0.54781 P@3: 0.37196 P@5: 0.27996 N@3: 0.43717 N@5: 0.39110 early stop: 0\n",
            "\u001b[32m[I 210617 16:49:52 models:110]\u001b[39m 0 460800 train loss: 0.0018039 valid loss: 0.0017720 P@1: 0.56476 P@3: 0.37900 P@5: 0.28616 N@3: 0.44721 N@5: 0.40062 early stop: 0\n",
            "\u001b[32m[I 210617 16:53:14 models:110]\u001b[39m 0 486400 train loss: 0.0017857 valid loss: 0.0017528 P@1: 0.57863 P@3: 0.38857 P@5: 0.29338 N@3: 0.45818 N@5: 0.41038 early stop: 0\n",
            "\u001b[32m[I 210617 16:56:35 models:110]\u001b[39m 0 512000 train loss: 0.0017501 valid loss: 0.0017372 P@1: 0.58402 P@3: 0.39438 P@5: 0.29893 N@3: 0.46393 N@5: 0.41597 early stop: 0\n",
            "\u001b[32m[I 210617 16:59:57 models:110]\u001b[39m 0 537600 train loss: 0.0017435 valid loss: 0.0017118 P@1: 0.59217 P@3: 0.39886 P@5: 0.30266 N@3: 0.46961 N@5: 0.42156 early stop: 0\n",
            "\u001b[32m[I 210617 17:03:19 models:110]\u001b[39m 0 563200 train loss: 0.0017158 valid loss: 0.0016959 P@1: 0.59981 P@3: 0.40828 P@5: 0.30906 N@3: 0.47908 N@5: 0.42963 early stop: 0\n",
            "\u001b[32m[I 210617 17:06:42 models:110]\u001b[39m 1 24320 train loss: 0.0016824 valid loss: 0.0016590 P@1: 0.61375 P@3: 0.41818 P@5: 0.31990 N@3: 0.49059 N@5: 0.44245 early stop: 0\n",
            "\u001b[32m[I 210617 17:10:04 models:110]\u001b[39m 1 49920 train loss: 0.0016564 valid loss: 0.0016372 P@1: 0.62744 P@3: 0.42740 P@5: 0.32534 N@3: 0.50170 N@5: 0.45171 early stop: 0\n",
            "\u001b[32m[I 210617 17:13:27 models:110]\u001b[39m 1 75520 train loss: 0.0016280 valid loss: 0.0016063 P@1: 0.64141 P@3: 0.43951 P@5: 0.33472 N@3: 0.51480 N@5: 0.46378 early stop: 0\n",
            "\u001b[32m[I 210617 17:16:49 models:110]\u001b[39m 1 101120 train loss: 0.0016009 valid loss: 0.0015791 P@1: 0.65227 P@3: 0.44835 P@5: 0.34204 N@3: 0.52492 N@5: 0.47337 early stop: 0\n",
            "\u001b[32m[I 210617 17:20:12 models:110]\u001b[39m 1 126720 train loss: 0.0015781 valid loss: 0.0015600 P@1: 0.66033 P@3: 0.45637 P@5: 0.34943 N@3: 0.53316 N@5: 0.48174 early stop: 0\n",
            "\u001b[32m[I 210617 17:23:35 models:110]\u001b[39m 1 152320 train loss: 0.0015549 valid loss: 0.0015378 P@1: 0.67156 P@3: 0.46345 P@5: 0.35537 N@3: 0.54216 N@5: 0.49071 early stop: 0\n",
            "\u001b[32m[I 210617 17:26:58 models:110]\u001b[39m 1 177920 train loss: 0.0015375 valid loss: 0.0015130 P@1: 0.68136 P@3: 0.47286 P@5: 0.36404 N@3: 0.55178 N@5: 0.50038 early stop: 0\n",
            "\u001b[32m[I 210617 17:30:21 models:110]\u001b[39m 1 203520 train loss: 0.0015060 valid loss: 0.0014948 P@1: 0.68700 P@3: 0.47965 P@5: 0.36992 N@3: 0.55900 N@5: 0.50762 early stop: 0\n",
            "\u001b[32m[I 210617 17:33:46 models:110]\u001b[39m 1 229120 train loss: 0.0014988 valid loss: 0.0014775 P@1: 0.69199 P@3: 0.48550 P@5: 0.37434 N@3: 0.56471 N@5: 0.51295 early stop: 0\n",
            "\u001b[32m[I 210617 17:37:10 models:110]\u001b[39m 1 254720 train loss: 0.0014922 valid loss: 0.0014642 P@1: 0.69822 P@3: 0.49187 P@5: 0.37987 N@3: 0.57169 N@5: 0.51998 early stop: 0\n",
            "\u001b[32m[I 210617 17:40:34 models:110]\u001b[39m 1 280320 train loss: 0.0014746 valid loss: 0.0014556 P@1: 0.69997 P@3: 0.49482 P@5: 0.38262 N@3: 0.57524 N@5: 0.52398 early stop: 0\n",
            "\u001b[32m[I 210617 17:43:59 models:110]\u001b[39m 1 305920 train loss: 0.0014568 valid loss: 0.0014346 P@1: 0.70852 P@3: 0.50125 P@5: 0.38899 N@3: 0.58215 N@5: 0.53133 early stop: 0\n",
            "\u001b[32m[I 210617 17:47:24 models:110]\u001b[39m 1 331520 train loss: 0.0014452 valid loss: 0.0014233 P@1: 0.71346 P@3: 0.50791 P@5: 0.39326 N@3: 0.58895 N@5: 0.53687 early stop: 0\n",
            "\u001b[32m[I 210617 17:50:48 models:110]\u001b[39m 1 357120 train loss: 0.0014329 valid loss: 0.0014109 P@1: 0.71448 P@3: 0.50955 P@5: 0.39553 N@3: 0.59106 N@5: 0.53975 early stop: 0\n",
            "\u001b[32m[I 210617 17:54:13 models:110]\u001b[39m 1 382720 train loss: 0.0014245 valid loss: 0.0013965 P@1: 0.72138 P@3: 0.51770 P@5: 0.40278 N@3: 0.59938 N@5: 0.54824 early stop: 0\n",
            "\u001b[32m[I 210617 17:57:37 models:110]\u001b[39m 1 408320 train loss: 0.0014104 valid loss: 0.0013856 P@1: 0.72667 P@3: 0.52247 P@5: 0.40582 N@3: 0.60475 N@5: 0.55278 early stop: 0\n",
            "\u001b[32m[I 210617 18:01:01 models:110]\u001b[39m 1 433920 train loss: 0.0013963 valid loss: 0.0013689 P@1: 0.73074 P@3: 0.52864 P@5: 0.41113 N@3: 0.61083 N@5: 0.55897 early stop: 0\n",
            "\u001b[32m[I 210617 18:04:26 models:110]\u001b[39m 1 459520 train loss: 0.0013808 valid loss: 0.0013642 P@1: 0.73428 P@3: 0.52987 P@5: 0.41247 N@3: 0.61289 N@5: 0.56131 early stop: 0\n",
            "\u001b[32m[I 210617 18:07:51 models:110]\u001b[39m 1 485120 train loss: 0.0013666 valid loss: 0.0013488 P@1: 0.73950 P@3: 0.53607 P@5: 0.41801 N@3: 0.61949 N@5: 0.56798 early stop: 0\n",
            "\u001b[32m[I 210617 18:11:15 models:110]\u001b[39m 1 510720 train loss: 0.0013648 valid loss: 0.0013369 P@1: 0.74662 P@3: 0.54214 P@5: 0.42243 N@3: 0.62642 N@5: 0.57422 early stop: 0\n",
            "\u001b[32m[I 210617 18:14:40 models:110]\u001b[39m 1 536320 train loss: 0.0013452 valid loss: 0.0013256 P@1: 0.75074 P@3: 0.54653 P@5: 0.42656 N@3: 0.63072 N@5: 0.57880 early stop: 0\n",
            "\u001b[32m[I 210617 18:18:05 models:110]\u001b[39m 1 561920 train loss: 0.0013250 valid loss: 0.0013153 P@1: 0.75053 P@3: 0.55075 P@5: 0.42991 N@3: 0.63460 N@5: 0.58268 early stop: 0\n",
            "\u001b[32m[I 210617 18:21:31 models:110]\u001b[39m 2 23040 train loss: 0.0012830 valid loss: 0.0013054 P@1: 0.75583 P@3: 0.55582 P@5: 0.43489 N@3: 0.63969 N@5: 0.58816 early stop: 0\n",
            "\u001b[32m[I 210617 18:24:55 models:110]\u001b[39m 2 48640 train loss: 0.0012741 valid loss: 0.0012989 P@1: 0.76063 P@3: 0.55852 P@5: 0.43605 N@3: 0.64352 N@5: 0.59083 early stop: 0\n",
            "\u001b[32m[I 210617 18:28:19 models:110]\u001b[39m 2 74240 train loss: 0.0012666 valid loss: 0.0012838 P@1: 0.77105 P@3: 0.56806 P@5: 0.44364 N@3: 0.65364 N@5: 0.60039 early stop: 0\n",
            "\u001b[32m[I 210617 18:31:41 models:110]\u001b[39m 2 99840 train loss: 0.0012571 valid loss: 0.0012665 P@1: 0.77526 P@3: 0.57240 P@5: 0.44893 N@3: 0.65871 N@5: 0.60688 early stop: 0\n",
            "\u001b[32m[I 210617 18:35:04 models:110]\u001b[39m 2 125440 train loss: 0.0012584 valid loss: 0.0012588 P@1: 0.78029 P@3: 0.57746 P@5: 0.45228 N@3: 0.66394 N@5: 0.61118 early stop: 0\n",
            "\u001b[32m[I 210617 18:38:27 models:110]\u001b[39m 2 151040 train loss: 0.0012406 valid loss: 0.0012501 P@1: 0.78033 P@3: 0.58119 P@5: 0.45586 N@3: 0.66727 N@5: 0.61500 early stop: 0\n",
            "\u001b[32m[I 210617 18:41:50 models:110]\u001b[39m 2 176640 train loss: 0.0012300 valid loss: 0.0012360 P@1: 0.78829 P@3: 0.58915 P@5: 0.46187 N@3: 0.67578 N@5: 0.62260 early stop: 0\n",
            "\u001b[32m[I 210617 18:45:13 models:110]\u001b[39m 2 202240 train loss: 0.0012235 valid loss: 0.0012298 P@1: 0.79409 P@3: 0.59476 P@5: 0.46588 N@3: 0.68183 N@5: 0.62803 early stop: 0\n",
            "\u001b[32m[I 210617 18:48:35 models:110]\u001b[39m 2 227840 train loss: 0.0012101 valid loss: 0.0012141 P@1: 0.80238 P@3: 0.60216 P@5: 0.47143 N@3: 0.68977 N@5: 0.63518 early stop: 0\n",
            "\u001b[32m[I 210617 18:51:57 models:110]\u001b[39m 2 253440 train loss: 0.0011967 valid loss: 0.0012073 P@1: 0.81131 P@3: 0.60747 P@5: 0.47533 N@3: 0.69682 N@5: 0.64188 early stop: 0\n",
            "\u001b[32m[I 210617 18:55:20 models:110]\u001b[39m 2 279040 train loss: 0.0011927 valid loss: 0.0012004 P@1: 0.81344 P@3: 0.61079 P@5: 0.47872 N@3: 0.70005 N@5: 0.64531 early stop: 0\n",
            "\u001b[32m[I 210617 18:58:42 models:110]\u001b[39m 2 304640 train loss: 0.0011787 valid loss: 0.0011803 P@1: 0.81935 P@3: 0.61729 P@5: 0.48515 N@3: 0.70733 N@5: 0.65332 early stop: 0\n",
            "\u001b[32m[I 210617 19:02:04 models:110]\u001b[39m 2 330240 train loss: 0.0011766 valid loss: 0.0011776 P@1: 0.82491 P@3: 0.62133 P@5: 0.48763 N@3: 0.71186 N@5: 0.65699 early stop: 0\n",
            "\u001b[32m[I 210617 19:05:26 models:110]\u001b[39m 2 355840 train loss: 0.0011581 valid loss: 0.0011581 P@1: 0.82719 P@3: 0.62689 P@5: 0.49248 N@3: 0.71690 N@5: 0.66201 early stop: 0\n",
            "\u001b[32m[I 210617 19:08:49 models:110]\u001b[39m 2 381440 train loss: 0.0011503 valid loss: 0.0011487 P@1: 0.83531 P@3: 0.63216 P@5: 0.49667 N@3: 0.72335 N@5: 0.66820 early stop: 0\n",
            "\u001b[32m[I 210617 19:12:12 models:110]\u001b[39m 2 407040 train loss: 0.0011422 valid loss: 0.0011384 P@1: 0.83840 P@3: 0.63867 P@5: 0.50205 N@3: 0.72952 N@5: 0.67415 early stop: 0\n",
            "\u001b[32m[I 210617 19:15:34 models:110]\u001b[39m 2 432640 train loss: 0.0011397 valid loss: 0.0011285 P@1: 0.84339 P@3: 0.64390 P@5: 0.50727 N@3: 0.73458 N@5: 0.67964 early stop: 0\n",
            "\u001b[32m[I 210617 19:18:57 models:110]\u001b[39m 2 458240 train loss: 0.0011229 valid loss: 0.0011206 P@1: 0.84792 P@3: 0.64789 P@5: 0.51001 N@3: 0.73919 N@5: 0.68368 early stop: 0\n",
            "\u001b[32m[I 210617 19:22:20 models:110]\u001b[39m 2 483840 train loss: 0.0011186 valid loss: 0.0011147 P@1: 0.85226 P@3: 0.65158 P@5: 0.51345 N@3: 0.74335 N@5: 0.68790 early stop: 0\n",
            "\u001b[32m[I 210617 19:25:44 models:110]\u001b[39m 2 509440 train loss: 0.0011007 valid loss: 0.0010974 P@1: 0.85496 P@3: 0.65613 P@5: 0.51775 N@3: 0.74759 N@5: 0.69241 early stop: 0\n",
            "\u001b[32m[I 210617 19:29:07 models:110]\u001b[39m 2 535040 train loss: 0.0011005 valid loss: 0.0010905 P@1: 0.86056 P@3: 0.66233 P@5: 0.52277 N@3: 0.75404 N@5: 0.69851 early stop: 0\n",
            "\u001b[32m[I 210617 19:32:30 models:110]\u001b[39m 2 560640 train loss: 0.0010961 valid loss: 0.0010916 P@1: 0.86225 P@3: 0.66584 P@5: 0.52429 N@3: 0.75749 N@5: 0.70104 early stop: 0\n",
            "\u001b[32m[I 210617 19:35:55 models:110]\u001b[39m 3 21760 train loss: 0.0010145 valid loss: 0.0010886 P@1: 0.86313 P@3: 0.66647 P@5: 0.52666 N@3: 0.75797 N@5: 0.70273 early stop: 0\n",
            "\u001b[32m[I 210617 19:39:20 models:110]\u001b[39m 3 47360 train loss: 0.0009862 valid loss: 0.0010812 P@1: 0.86208 P@3: 0.67008 P@5: 0.53058 N@3: 0.76091 N@5: 0.70631 early stop: 0\n",
            "\u001b[32m[I 210617 19:42:42 models:110]\u001b[39m 3 72960 train loss: 0.0009927 valid loss: 0.0010753 P@1: 0.86313 P@3: 0.67141 P@5: 0.53181 N@3: 0.76174 N@5: 0.70687 early stop: 0\n",
            "\u001b[32m[I 210617 19:46:07 models:110]\u001b[39m 3 98560 train loss: 0.0009866 valid loss: 0.0010806 P@1: 0.86510 P@3: 0.67196 P@5: 0.53157 N@3: 0.76246 N@5: 0.70719 early stop: 0\n",
            "\u001b[32m[I 210617 19:49:32 models:110]\u001b[39m 3 124160 train loss: 0.0009774 valid loss: 0.0010677 P@1: 0.86860 P@3: 0.67909 P@5: 0.53724 N@3: 0.76967 N@5: 0.71397 early stop: 0\n",
            "\u001b[32m[I 210617 19:52:54 models:110]\u001b[39m 3 149760 train loss: 0.0009804 valid loss: 0.0010555 P@1: 0.87090 P@3: 0.68016 P@5: 0.53896 N@3: 0.77075 N@5: 0.71548 early stop: 0\n",
            "\u001b[32m[I 210617 19:56:15 models:110]\u001b[39m 3 175360 train loss: 0.0009753 valid loss: 0.0010526 P@1: 0.87322 P@3: 0.68519 P@5: 0.54404 N@3: 0.77538 N@5: 0.72071 early stop: 0\n",
            "\u001b[32m[I 210617 19:59:39 models:110]\u001b[39m 3 200960 train loss: 0.0009741 valid loss: 0.0010398 P@1: 0.87507 P@3: 0.68979 P@5: 0.54780 N@3: 0.77950 N@5: 0.72456 early stop: 0\n",
            "\u001b[32m[I 210617 20:03:02 models:110]\u001b[39m 3 226560 train loss: 0.0009578 valid loss: 0.0010343 P@1: 0.87827 P@3: 0.69188 P@5: 0.55013 N@3: 0.78214 N@5: 0.72750 early stop: 0\n",
            "\u001b[32m[I 210617 20:06:27 models:110]\u001b[39m 3 252160 train loss: 0.0009562 valid loss: 0.0010280 P@1: 0.88091 P@3: 0.69474 P@5: 0.55172 N@3: 0.78500 N@5: 0.72967 early stop: 0\n",
            "\u001b[32m[I 210617 20:09:50 models:110]\u001b[39m 3 277760 train loss: 0.0009481 valid loss: 0.0010191 P@1: 0.88289 P@3: 0.69971 P@5: 0.55683 N@3: 0.78985 N@5: 0.73521 early stop: 0\n",
            "\u001b[32m[I 210617 20:13:13 models:110]\u001b[39m 3 303360 train loss: 0.0009507 valid loss: 0.0010119 P@1: 0.88111 P@3: 0.70076 P@5: 0.55830 N@3: 0.78989 N@5: 0.73565 early stop: 0\n",
            "\u001b[32m[I 210617 20:16:39 models:110]\u001b[39m 3 328960 train loss: 0.0009392 valid loss: 0.0009986 P@1: 0.88567 P@3: 0.70567 P@5: 0.56280 N@3: 0.79496 N@5: 0.74072 early stop: 0\n",
            "\u001b[32m[I 210617 20:20:05 models:110]\u001b[39m 3 354560 train loss: 0.0009372 valid loss: 0.0009939 P@1: 0.88489 P@3: 0.70732 P@5: 0.56644 N@3: 0.79614 N@5: 0.74360 early stop: 0\n",
            "\u001b[32m[I 210617 20:23:28 models:110]\u001b[39m 3 380160 train loss: 0.0009356 valid loss: 0.0009856 P@1: 0.88811 P@3: 0.71212 P@5: 0.56981 N@3: 0.80094 N@5: 0.74789 early stop: 0\n",
            "\u001b[32m[I 210617 20:26:50 models:110]\u001b[39m 3 405760 train loss: 0.0009234 valid loss: 0.0009790 P@1: 0.88977 P@3: 0.71291 P@5: 0.57123 N@3: 0.80212 N@5: 0.74946 early stop: 0\n",
            "\u001b[32m[I 210617 20:30:12 models:110]\u001b[39m 3 431360 train loss: 0.0009157 valid loss: 0.0009711 P@1: 0.88959 P@3: 0.71445 P@5: 0.57281 N@3: 0.80301 N@5: 0.75078 early stop: 0\n",
            "\u001b[32m[I 210617 20:33:34 models:110]\u001b[39m 3 456960 train loss: 0.0009132 valid loss: 0.0009629 P@1: 0.89317 P@3: 0.72194 P@5: 0.58005 N@3: 0.81004 N@5: 0.75830 early stop: 0\n",
            "\u001b[32m[I 210617 20:36:54 models:110]\u001b[39m 3 482560 train loss: 0.0009110 valid loss: 0.0009569 P@1: 0.89285 P@3: 0.72096 P@5: 0.58038 N@3: 0.80921 N@5: 0.75818 early stop: 1\n",
            "\u001b[32m[I 210617 20:40:15 models:110]\u001b[39m 3 508160 train loss: 0.0009077 valid loss: 0.0009474 P@1: 0.89523 P@3: 0.72593 P@5: 0.58452 N@3: 0.81415 N@5: 0.76286 early stop: 0\n",
            "\u001b[32m[I 210617 20:43:37 models:110]\u001b[39m 3 533760 train loss: 0.0008964 valid loss: 0.0009401 P@1: 0.89666 P@3: 0.72788 P@5: 0.58682 N@3: 0.81564 N@5: 0.76504 early stop: 0\n",
            "\u001b[32m[I 210617 20:46:58 models:110]\u001b[39m 3 559360 train loss: 0.0008934 valid loss: 0.0009288 P@1: 0.89811 P@3: 0.73080 P@5: 0.58948 N@3: 0.81844 N@5: 0.76792 early stop: 0\n",
            "\u001b[32m[I 210617 20:47:34 models:142]\u001b[39m SWA Initializing\n",
            "\u001b[32m[I 210617 20:50:20 models:110]\u001b[39m 4 20480 train loss: 0.0007784 valid loss: 0.0009100 P@1: 0.90155 P@3: 0.73876 P@5: 0.59822 N@3: 0.82564 N@5: 0.77642 early stop: 0\n",
            "\u001b[32m[I 210617 20:53:40 models:110]\u001b[39m 4 46080 train loss: 0.0007484 valid loss: 0.0009058 P@1: 0.90256 P@3: 0.74187 P@5: 0.60096 N@3: 0.82841 N@5: 0.77909 early stop: 0\n",
            "\u001b[32m[I 210617 20:56:59 models:110]\u001b[39m 4 71680 train loss: 0.0007517 valid loss: 0.0008998 P@1: 0.90393 P@3: 0.74375 P@5: 0.60412 N@3: 0.83023 N@5: 0.78199 early stop: 0\n",
            "\u001b[32m[I 210617 21:00:18 models:110]\u001b[39m 4 97280 train loss: 0.0007541 valid loss: 0.0008971 P@1: 0.90423 P@3: 0.74486 P@5: 0.60574 N@3: 0.83118 N@5: 0.78347 early stop: 0\n",
            "\u001b[32m[I 210617 21:03:37 models:110]\u001b[39m 4 122880 train loss: 0.0007429 valid loss: 0.0008944 P@1: 0.90531 P@3: 0.74640 P@5: 0.60714 N@3: 0.83262 N@5: 0.78493 early stop: 0\n",
            "\u001b[32m[I 210617 21:06:54 models:110]\u001b[39m 4 148480 train loss: 0.0007556 valid loss: 0.0008904 P@1: 0.90579 P@3: 0.74751 P@5: 0.60857 N@3: 0.83363 N@5: 0.78626 early stop: 0\n",
            "\u001b[32m[I 210617 21:10:14 models:110]\u001b[39m 4 174080 train loss: 0.0007498 valid loss: 0.0008869 P@1: 0.90687 P@3: 0.74875 P@5: 0.60960 N@3: 0.83488 N@5: 0.78746 early stop: 0\n",
            "\u001b[32m[I 210617 21:13:34 models:110]\u001b[39m 4 199680 train loss: 0.0007422 valid loss: 0.0008839 P@1: 0.90726 P@3: 0.75003 P@5: 0.61076 N@3: 0.83607 N@5: 0.78874 early stop: 0\n",
            "\u001b[32m[I 210617 21:16:55 models:110]\u001b[39m 4 225280 train loss: 0.0007445 valid loss: 0.0008812 P@1: 0.90779 P@3: 0.75085 P@5: 0.61173 N@3: 0.83680 N@5: 0.78959 early stop: 0\n",
            "\u001b[32m[I 210617 21:20:16 models:110]\u001b[39m 4 250880 train loss: 0.0007428 valid loss: 0.0008780 P@1: 0.90803 P@3: 0.75143 P@5: 0.61276 N@3: 0.83737 N@5: 0.79056 early stop: 0\n",
            "\u001b[32m[I 210617 21:23:38 models:110]\u001b[39m 4 276480 train loss: 0.0007412 valid loss: 0.0008755 P@1: 0.90901 P@3: 0.75237 P@5: 0.61398 N@3: 0.83828 N@5: 0.79176 early stop: 0\n",
            "\u001b[32m[I 210617 21:27:00 models:110]\u001b[39m 4 302080 train loss: 0.0007415 valid loss: 0.0008720 P@1: 0.90962 P@3: 0.75381 P@5: 0.61512 N@3: 0.83962 N@5: 0.79297 early stop: 0\n",
            "\u001b[32m[I 210617 21:30:20 models:110]\u001b[39m 4 327680 train loss: 0.0007431 valid loss: 0.0008690 P@1: 0.90997 P@3: 0.75457 P@5: 0.61608 N@3: 0.84037 N@5: 0.79395 early stop: 0\n",
            "\u001b[32m[I 210617 21:33:40 models:110]\u001b[39m 4 353280 train loss: 0.0007445 valid loss: 0.0008660 P@1: 0.91045 P@3: 0.75539 P@5: 0.61724 N@3: 0.84112 N@5: 0.79500 early stop: 0\n",
            "\u001b[32m[I 210617 21:37:00 models:110]\u001b[39m 4 378880 train loss: 0.0007463 valid loss: 0.0008628 P@1: 0.91075 P@3: 0.75634 P@5: 0.61835 N@3: 0.84192 N@5: 0.79600 early stop: 0\n",
            "\u001b[32m[I 210617 21:40:19 models:110]\u001b[39m 4 404480 train loss: 0.0007477 valid loss: 0.0008595 P@1: 0.91104 P@3: 0.75694 P@5: 0.61926 N@3: 0.84256 N@5: 0.79693 early stop: 0\n",
            "\u001b[32m[I 210617 21:43:40 models:110]\u001b[39m 4 430080 train loss: 0.0007382 valid loss: 0.0008568 P@1: 0.91122 P@3: 0.75763 P@5: 0.62009 N@3: 0.84315 N@5: 0.79769 early stop: 0\n",
            "\u001b[32m[I 210617 21:47:01 models:110]\u001b[39m 4 455680 train loss: 0.0007361 valid loss: 0.0008543 P@1: 0.91150 P@3: 0.75807 P@5: 0.62095 N@3: 0.84357 N@5: 0.79843 early stop: 0\n",
            "\u001b[32m[I 210617 21:50:20 models:110]\u001b[39m 4 481280 train loss: 0.0007377 valid loss: 0.0008513 P@1: 0.91184 P@3: 0.75879 P@5: 0.62185 N@3: 0.84425 N@5: 0.79934 early stop: 0\n",
            "\u001b[32m[I 210617 21:53:39 models:110]\u001b[39m 4 506880 train loss: 0.0007387 valid loss: 0.0008486 P@1: 0.91237 P@3: 0.75965 P@5: 0.62288 N@3: 0.84514 N@5: 0.80039 early stop: 0\n",
            "\u001b[32m[I 210617 21:56:57 models:110]\u001b[39m 4 532480 train loss: 0.0007365 valid loss: 0.0008457 P@1: 0.91257 P@3: 0.76034 P@5: 0.62346 N@3: 0.84574 N@5: 0.80097 early stop: 0\n",
            "\u001b[32m[I 210617 22:00:16 models:110]\u001b[39m 4 558080 train loss: 0.0007327 valid loss: 0.0008433 P@1: 0.91295 P@3: 0.76108 P@5: 0.62435 N@3: 0.84638 N@5: 0.80178 early stop: 0\n",
            "\u001b[32m[I 210617 22:03:40 models:110]\u001b[39m 5 19200 train loss: 0.0006103 valid loss: 0.0008432 P@1: 0.91326 P@3: 0.76140 P@5: 0.62480 N@3: 0.84670 N@5: 0.80222 early stop: 0\n",
            "\u001b[32m[I 210617 22:07:00 models:110]\u001b[39m 5 44800 train loss: 0.0005825 valid loss: 0.0008429 P@1: 0.91356 P@3: 0.76176 P@5: 0.62510 N@3: 0.84700 N@5: 0.80254 early stop: 0\n",
            "\u001b[32m[I 210617 22:10:19 models:110]\u001b[39m 5 70400 train loss: 0.0005801 valid loss: 0.0008427 P@1: 0.91380 P@3: 0.76238 P@5: 0.62546 N@3: 0.84750 N@5: 0.80288 early stop: 0\n",
            "\u001b[32m[I 210617 22:13:40 models:110]\u001b[39m 5 96000 train loss: 0.0005884 valid loss: 0.0008424 P@1: 0.91376 P@3: 0.76257 P@5: 0.62580 N@3: 0.84765 N@5: 0.80316 early stop: 0\n",
            "\u001b[32m[I 210617 22:17:00 models:110]\u001b[39m 5 121600 train loss: 0.0005847 valid loss: 0.0008422 P@1: 0.91404 P@3: 0.76280 P@5: 0.62604 N@3: 0.84790 N@5: 0.80340 early stop: 0\n",
            "\u001b[32m[I 210617 22:20:23 models:110]\u001b[39m 5 147200 train loss: 0.0005903 valid loss: 0.0008419 P@1: 0.91417 P@3: 0.76310 P@5: 0.62651 N@3: 0.84815 N@5: 0.80383 early stop: 0\n",
            "\u001b[32m[I 210617 22:23:43 models:110]\u001b[39m 5 172800 train loss: 0.0005914 valid loss: 0.0008418 P@1: 0.91428 P@3: 0.76326 P@5: 0.62685 N@3: 0.84831 N@5: 0.80414 early stop: 0\n",
            "\u001b[32m[I 210617 22:27:06 models:110]\u001b[39m 5 198400 train loss: 0.0005954 valid loss: 0.0008413 P@1: 0.91411 P@3: 0.76355 P@5: 0.62715 N@3: 0.84847 N@5: 0.80434 early stop: 0\n",
            "\u001b[32m[I 210617 22:30:29 models:110]\u001b[39m 5 224000 train loss: 0.0005967 valid loss: 0.0008410 P@1: 0.91428 P@3: 0.76393 P@5: 0.62747 N@3: 0.84880 N@5: 0.80464 early stop: 0\n",
            "\u001b[32m[I 210617 22:33:51 models:110]\u001b[39m 5 249600 train loss: 0.0005987 valid loss: 0.0008403 P@1: 0.91451 P@3: 0.76426 P@5: 0.62780 N@3: 0.84908 N@5: 0.80495 early stop: 0\n",
            "\u001b[32m[I 210617 22:37:12 models:110]\u001b[39m 5 275200 train loss: 0.0006024 valid loss: 0.0008398 P@1: 0.91462 P@3: 0.76437 P@5: 0.62815 N@3: 0.84917 N@5: 0.80524 early stop: 0\n",
            "\u001b[32m[I 210617 22:40:35 models:110]\u001b[39m 5 300800 train loss: 0.0005998 valid loss: 0.0008393 P@1: 0.91491 P@3: 0.76452 P@5: 0.62846 N@3: 0.84934 N@5: 0.80554 early stop: 0\n",
            "\u001b[32m[I 210617 22:43:56 models:110]\u001b[39m 5 326400 train loss: 0.0006079 valid loss: 0.0008387 P@1: 0.91488 P@3: 0.76474 P@5: 0.62883 N@3: 0.84953 N@5: 0.80585 early stop: 0\n",
            "\u001b[32m[I 210617 22:47:18 models:110]\u001b[39m 5 352000 train loss: 0.0006045 valid loss: 0.0008384 P@1: 0.91502 P@3: 0.76505 P@5: 0.62900 N@3: 0.84980 N@5: 0.80602 early stop: 0\n",
            "\u001b[32m[I 210617 22:50:39 models:110]\u001b[39m 5 377600 train loss: 0.0006060 valid loss: 0.0008379 P@1: 0.91503 P@3: 0.76535 P@5: 0.62945 N@3: 0.85002 N@5: 0.80639 early stop: 0\n",
            "\u001b[32m[I 210617 22:54:02 models:110]\u001b[39m 5 403200 train loss: 0.0006094 valid loss: 0.0008371 P@1: 0.91520 P@3: 0.76560 P@5: 0.63002 N@3: 0.85024 N@5: 0.80683 early stop: 0\n",
            "\u001b[32m[I 210617 22:57:24 models:110]\u001b[39m 5 428800 train loss: 0.0006118 valid loss: 0.0008362 P@1: 0.91537 P@3: 0.76583 P@5: 0.63026 N@3: 0.85045 N@5: 0.80707 early stop: 0\n",
            "\u001b[32m[I 210617 23:00:46 models:110]\u001b[39m 5 454400 train loss: 0.0006081 valid loss: 0.0008354 P@1: 0.91559 P@3: 0.76616 P@5: 0.63061 N@3: 0.85079 N@5: 0.80743 early stop: 0\n",
            "\u001b[32m[I 210617 23:04:10 models:110]\u001b[39m 5 480000 train loss: 0.0006091 valid loss: 0.0008345 P@1: 0.91559 P@3: 0.76656 P@5: 0.63087 N@3: 0.85109 N@5: 0.80769 early stop: 0\n",
            "\u001b[32m[I 210617 23:07:31 models:110]\u001b[39m 5 505600 train loss: 0.0006183 valid loss: 0.0008337 P@1: 0.91563 P@3: 0.76692 P@5: 0.63123 N@3: 0.85140 N@5: 0.80801 early stop: 0\n",
            "\u001b[32m[I 210617 23:10:52 models:110]\u001b[39m 5 531200 train loss: 0.0006118 valid loss: 0.0008329 P@1: 0.91564 P@3: 0.76711 P@5: 0.63157 N@3: 0.85157 N@5: 0.80830 early stop: 0\n",
            "\u001b[32m[I 210617 23:14:14 models:110]\u001b[39m 5 556800 train loss: 0.0006150 valid loss: 0.0008319 P@1: 0.91596 P@3: 0.76737 P@5: 0.63186 N@3: 0.85181 N@5: 0.80861 early stop: 0\n",
            "\u001b[32m[I 210617 23:17:37 models:110]\u001b[39m 6 17920 train loss: 0.0005040 valid loss: 0.0008320 P@1: 0.91611 P@3: 0.76760 P@5: 0.63208 N@3: 0.85201 N@5: 0.80882 early stop: 0\n",
            "\u001b[32m[I 210617 23:20:58 models:110]\u001b[39m 6 43520 train loss: 0.0004629 valid loss: 0.0008323 P@1: 0.91620 P@3: 0.76774 P@5: 0.63223 N@3: 0.85211 N@5: 0.80895 early stop: 0\n",
            "\u001b[32m[I 210617 23:24:19 models:110]\u001b[39m 6 69120 train loss: 0.0004694 valid loss: 0.0008326 P@1: 0.91621 P@3: 0.76786 P@5: 0.63253 N@3: 0.85222 N@5: 0.80919 early stop: 0\n",
            "\u001b[32m[I 210617 23:27:40 models:110]\u001b[39m 6 94720 train loss: 0.0004745 valid loss: 0.0008330 P@1: 0.91617 P@3: 0.76804 P@5: 0.63270 N@3: 0.85232 N@5: 0.80927 early stop: 0\n",
            "\u001b[32m[I 210617 23:31:01 models:110]\u001b[39m 6 120320 train loss: 0.0004792 valid loss: 0.0008335 P@1: 0.91610 P@3: 0.76823 P@5: 0.63292 N@3: 0.85243 N@5: 0.80942 early stop: 0\n",
            "\u001b[32m[I 210617 23:34:20 models:110]\u001b[39m 6 145920 train loss: 0.0004814 valid loss: 0.0008339 P@1: 0.91604 P@3: 0.76834 P@5: 0.63310 N@3: 0.85250 N@5: 0.80955 early stop: 0\n",
            "\u001b[32m[I 210617 23:37:39 models:110]\u001b[39m 6 171520 train loss: 0.0004856 valid loss: 0.0008342 P@1: 0.91620 P@3: 0.76840 P@5: 0.63337 N@3: 0.85255 N@5: 0.80977 early stop: 0\n",
            "\u001b[32m[I 210617 23:40:59 models:110]\u001b[39m 6 197120 train loss: 0.0004848 valid loss: 0.0008347 P@1: 0.91625 P@3: 0.76860 P@5: 0.63345 N@3: 0.85274 N@5: 0.80984 early stop: 0\n",
            "\u001b[32m[I 210617 23:44:17 models:110]\u001b[39m 6 222720 train loss: 0.0004948 valid loss: 0.0008350 P@1: 0.91608 P@3: 0.76875 P@5: 0.63368 N@3: 0.85282 N@5: 0.81000 early stop: 0\n",
            "\u001b[32m[I 210617 23:47:38 models:110]\u001b[39m 6 248320 train loss: 0.0004913 valid loss: 0.0008352 P@1: 0.91622 P@3: 0.76884 P@5: 0.63384 N@3: 0.85295 N@5: 0.81015 early stop: 0\n",
            "\u001b[32m[I 210617 23:51:01 models:110]\u001b[39m 6 273920 train loss: 0.0004954 valid loss: 0.0008354 P@1: 0.91624 P@3: 0.76904 P@5: 0.63390 N@3: 0.85312 N@5: 0.81025 early stop: 0\n",
            "\u001b[32m[I 210617 23:54:22 models:110]\u001b[39m 6 299520 train loss: 0.0005021 valid loss: 0.0008356 P@1: 0.91638 P@3: 0.76928 P@5: 0.63407 N@3: 0.85332 N@5: 0.81040 early stop: 0\n",
            "\u001b[32m[I 210617 23:57:42 models:110]\u001b[39m 6 325120 train loss: 0.0005009 valid loss: 0.0008358 P@1: 0.91632 P@3: 0.76936 P@5: 0.63423 N@3: 0.85339 N@5: 0.81057 early stop: 0\n",
            "\u001b[32m[I 210618 00:01:02 models:110]\u001b[39m 6 350720 train loss: 0.0005023 valid loss: 0.0008359 P@1: 0.91630 P@3: 0.76941 P@5: 0.63434 N@3: 0.85340 N@5: 0.81063 early stop: 0\n",
            "\u001b[32m[I 210618 00:04:23 models:110]\u001b[39m 6 376320 train loss: 0.0005116 valid loss: 0.0008360 P@1: 0.91634 P@3: 0.76944 P@5: 0.63443 N@3: 0.85343 N@5: 0.81073 early stop: 0\n",
            "\u001b[32m[I 210618 00:07:44 models:110]\u001b[39m 6 401920 train loss: 0.0005106 valid loss: 0.0008362 P@1: 0.91642 P@3: 0.76941 P@5: 0.63457 N@3: 0.85345 N@5: 0.81086 early stop: 0\n",
            "\u001b[32m[I 210618 00:11:06 models:110]\u001b[39m 6 427520 train loss: 0.0005120 valid loss: 0.0008363 P@1: 0.91652 P@3: 0.76956 P@5: 0.63475 N@3: 0.85357 N@5: 0.81101 early stop: 0\n",
            "\u001b[32m[I 210618 00:14:27 models:110]\u001b[39m 6 453120 train loss: 0.0005148 valid loss: 0.0008364 P@1: 0.91661 P@3: 0.76972 P@5: 0.63505 N@3: 0.85371 N@5: 0.81127 early stop: 0\n",
            "\u001b[32m[I 210618 00:17:48 models:110]\u001b[39m 6 478720 train loss: 0.0005191 valid loss: 0.0008363 P@1: 0.91689 P@3: 0.76978 P@5: 0.63517 N@3: 0.85377 N@5: 0.81139 early stop: 0\n",
            "\u001b[32m[I 210618 00:21:09 models:110]\u001b[39m 6 504320 train loss: 0.0005146 valid loss: 0.0008361 P@1: 0.91693 P@3: 0.77005 P@5: 0.63550 N@3: 0.85400 N@5: 0.81168 early stop: 0\n",
            "\u001b[32m[I 210618 00:24:29 models:110]\u001b[39m 6 529920 train loss: 0.0005200 valid loss: 0.0008360 P@1: 0.91706 P@3: 0.77013 P@5: 0.63576 N@3: 0.85410 N@5: 0.81190 early stop: 0\n",
            "\u001b[32m[I 210618 00:27:50 models:110]\u001b[39m 6 555520 train loss: 0.0005172 valid loss: 0.0008359 P@1: 0.91708 P@3: 0.77031 P@5: 0.63597 N@3: 0.85424 N@5: 0.81204 early stop: 0\n",
            "\u001b[32m[I 210618 00:31:14 models:110]\u001b[39m 7 16640 train loss: 0.0004291 valid loss: 0.0008365 P@1: 0.91712 P@3: 0.77043 P@5: 0.63604 N@3: 0.85434 N@5: 0.81209 early stop: 0\n",
            "\u001b[32m[I 210618 00:34:37 models:110]\u001b[39m 7 42240 train loss: 0.0003800 valid loss: 0.0008370 P@1: 0.91712 P@3: 0.77049 P@5: 0.63606 N@3: 0.85438 N@5: 0.81210 early stop: 0\n",
            "\u001b[32m[I 210618 00:38:01 models:110]\u001b[39m 7 67840 train loss: 0.0003812 valid loss: 0.0008378 P@1: 0.91706 P@3: 0.77059 P@5: 0.63622 N@3: 0.85445 N@5: 0.81221 early stop: 0\n",
            "\u001b[32m[I 210618 00:41:22 models:110]\u001b[39m 7 93440 train loss: 0.0003912 valid loss: 0.0008385 P@1: 0.91709 P@3: 0.77080 P@5: 0.63633 N@3: 0.85459 N@5: 0.81232 early stop: 0\n",
            "\u001b[32m[I 210618 00:44:41 models:110]\u001b[39m 7 119040 train loss: 0.0003994 valid loss: 0.0008392 P@1: 0.91705 P@3: 0.77078 P@5: 0.63651 N@3: 0.85457 N@5: 0.81245 early stop: 0\n",
            "\u001b[32m[I 210618 00:47:58 models:110]\u001b[39m 7 144640 train loss: 0.0004050 valid loss: 0.0008398 P@1: 0.91720 P@3: 0.77095 P@5: 0.63658 N@3: 0.85476 N@5: 0.81258 early stop: 0\n",
            "\u001b[32m[I 210618 00:51:17 models:110]\u001b[39m 7 170240 train loss: 0.0004054 valid loss: 0.0008404 P@1: 0.91712 P@3: 0.77103 P@5: 0.63675 N@3: 0.85481 N@5: 0.81268 early stop: 0\n",
            "\u001b[32m[I 210618 00:54:36 models:110]\u001b[39m 7 195840 train loss: 0.0004108 valid loss: 0.0008411 P@1: 0.91686 P@3: 0.77112 P@5: 0.63683 N@3: 0.85483 N@5: 0.81270 early stop: 0\n",
            "\u001b[32m[I 210618 00:57:54 models:110]\u001b[39m 7 221440 train loss: 0.0004130 valid loss: 0.0008416 P@1: 0.91689 P@3: 0.77118 P@5: 0.63683 N@3: 0.85490 N@5: 0.81268 early stop: 1\n",
            "\u001b[32m[I 210618 01:01:12 models:110]\u001b[39m 7 247040 train loss: 0.0004155 valid loss: 0.0008422 P@1: 0.91689 P@3: 0.77117 P@5: 0.63696 N@3: 0.85490 N@5: 0.81278 early stop: 0\n",
            "\u001b[32m[I 210618 01:04:31 models:110]\u001b[39m 7 272640 train loss: 0.0004189 valid loss: 0.0008428 P@1: 0.91674 P@3: 0.77132 P@5: 0.63709 N@3: 0.85495 N@5: 0.81284 early stop: 0\n",
            "\u001b[32m[I 210618 01:07:49 models:110]\u001b[39m 7 298240 train loss: 0.0004203 valid loss: 0.0008435 P@1: 0.91669 P@3: 0.77130 P@5: 0.63710 N@3: 0.85494 N@5: 0.81285 early stop: 0\n",
            "\u001b[32m[I 210618 01:11:08 models:110]\u001b[39m 7 323840 train loss: 0.0004232 valid loss: 0.0008441 P@1: 0.91674 P@3: 0.77139 P@5: 0.63721 N@3: 0.85502 N@5: 0.81296 early stop: 0\n",
            "\u001b[32m[I 210618 01:14:27 models:110]\u001b[39m 7 349440 train loss: 0.0004298 valid loss: 0.0008447 P@1: 0.91679 P@3: 0.77139 P@5: 0.63733 N@3: 0.85501 N@5: 0.81302 early stop: 0\n",
            "\u001b[32m[I 210618 01:17:46 models:110]\u001b[39m 7 375040 train loss: 0.0004293 valid loss: 0.0008452 P@1: 0.91674 P@3: 0.77155 P@5: 0.63744 N@3: 0.85512 N@5: 0.81311 early stop: 0\n",
            "\u001b[32m[I 210618 01:21:06 models:110]\u001b[39m 7 400640 train loss: 0.0004313 valid loss: 0.0008456 P@1: 0.91682 P@3: 0.77155 P@5: 0.63760 N@3: 0.85517 N@5: 0.81328 early stop: 0\n",
            "\u001b[32m[I 210618 01:24:25 models:110]\u001b[39m 7 426240 train loss: 0.0004309 valid loss: 0.0008460 P@1: 0.91689 P@3: 0.77160 P@5: 0.63767 N@3: 0.85522 N@5: 0.81335 early stop: 0\n",
            "\u001b[32m[I 210618 01:27:44 models:110]\u001b[39m 7 451840 train loss: 0.0004411 valid loss: 0.0008464 P@1: 0.91685 P@3: 0.77165 P@5: 0.63777 N@3: 0.85526 N@5: 0.81342 early stop: 0\n",
            "\u001b[32m[I 210618 01:31:01 models:110]\u001b[39m 7 477440 train loss: 0.0004382 valid loss: 0.0008469 P@1: 0.91669 P@3: 0.77169 P@5: 0.63784 N@3: 0.85524 N@5: 0.81346 early stop: 0\n",
            "\u001b[32m[I 210618 01:34:19 models:110]\u001b[39m 7 503040 train loss: 0.0004409 valid loss: 0.0008471 P@1: 0.91668 P@3: 0.77170 P@5: 0.63801 N@3: 0.85527 N@5: 0.81360 early stop: 0\n",
            "\u001b[32m[I 210618 01:37:37 models:110]\u001b[39m 7 528640 train loss: 0.0004435 valid loss: 0.0008474 P@1: 0.91675 P@3: 0.77188 P@5: 0.63813 N@3: 0.85540 N@5: 0.81370 early stop: 0\n",
            "\u001b[32m[I 210618 01:40:56 models:110]\u001b[39m 7 554240 train loss: 0.0004443 valid loss: 0.0008477 P@1: 0.91669 P@3: 0.77193 P@5: 0.63815 N@3: 0.85542 N@5: 0.81371 early stop: 0\n",
            "\u001b[32m[I 210618 01:44:18 models:110]\u001b[39m 8 15360 train loss: 0.0003721 valid loss: 0.0008486 P@1: 0.91662 P@3: 0.77195 P@5: 0.63823 N@3: 0.85543 N@5: 0.81375 early stop: 0\n",
            "\u001b[32m[I 210618 01:47:36 models:110]\u001b[39m 8 40960 train loss: 0.0003174 valid loss: 0.0008494 P@1: 0.91651 P@3: 0.77195 P@5: 0.63834 N@3: 0.85541 N@5: 0.81380 early stop: 0\n",
            "\u001b[32m[I 210618 01:50:55 models:110]\u001b[39m 8 66560 train loss: 0.0003286 valid loss: 0.0008503 P@1: 0.91651 P@3: 0.77205 P@5: 0.63840 N@3: 0.85548 N@5: 0.81383 early stop: 0\n",
            "\u001b[32m[I 210618 01:54:14 models:110]\u001b[39m 8 92160 train loss: 0.0003289 valid loss: 0.0008513 P@1: 0.91654 P@3: 0.77207 P@5: 0.63855 N@3: 0.85549 N@5: 0.81392 early stop: 0\n",
            "\u001b[32m[I 210618 01:57:34 models:110]\u001b[39m 8 117760 train loss: 0.0003364 valid loss: 0.0008522 P@1: 0.91659 P@3: 0.77214 P@5: 0.63858 N@3: 0.85557 N@5: 0.81397 early stop: 0\n",
            "\u001b[32m[I 210618 02:00:53 models:110]\u001b[39m 8 143360 train loss: 0.0003436 valid loss: 0.0008531 P@1: 0.91655 P@3: 0.77223 P@5: 0.63872 N@3: 0.85564 N@5: 0.81408 early stop: 0\n",
            "\u001b[32m[I 210618 02:04:11 models:110]\u001b[39m 8 168960 train loss: 0.0003458 valid loss: 0.0008538 P@1: 0.91654 P@3: 0.77226 P@5: 0.63883 N@3: 0.85566 N@5: 0.81417 early stop: 0\n",
            "\u001b[32m[I 210618 02:07:31 models:110]\u001b[39m 8 194560 train loss: 0.0003432 valid loss: 0.0008546 P@1: 0.91662 P@3: 0.77218 P@5: 0.63888 N@3: 0.85561 N@5: 0.81422 early stop: 0\n",
            "\u001b[32m[I 210618 02:10:47 models:110]\u001b[39m 8 220160 train loss: 0.0003516 valid loss: 0.0008553 P@1: 0.91662 P@3: 0.77222 P@5: 0.63885 N@3: 0.85561 N@5: 0.81418 early stop: 1\n",
            "\u001b[32m[I 210618 02:14:03 models:110]\u001b[39m 8 245760 train loss: 0.0003523 valid loss: 0.0008561 P@1: 0.91655 P@3: 0.77222 P@5: 0.63879 N@3: 0.85559 N@5: 0.81411 early stop: 2\n",
            "\u001b[32m[I 210618 02:17:21 models:110]\u001b[39m 8 271360 train loss: 0.0003596 valid loss: 0.0008569 P@1: 0.91647 P@3: 0.77218 P@5: 0.63880 N@3: 0.85554 N@5: 0.81409 early stop: 3\n",
            "\u001b[32m[I 210618 02:20:37 models:110]\u001b[39m 8 296960 train loss: 0.0003582 valid loss: 0.0008577 P@1: 0.91645 P@3: 0.77228 P@5: 0.63886 N@3: 0.85562 N@5: 0.81415 early stop: 4\n",
            "\u001b[32m[I 210618 02:23:53 models:110]\u001b[39m 8 322560 train loss: 0.0003640 valid loss: 0.0008585 P@1: 0.91655 P@3: 0.77242 P@5: 0.63887 N@3: 0.85575 N@5: 0.81419 early stop: 5\n",
            "\u001b[32m[I 210618 02:27:12 models:110]\u001b[39m 8 348160 train loss: 0.0003660 valid loss: 0.0008591 P@1: 0.91652 P@3: 0.77253 P@5: 0.63892 N@3: 0.85583 N@5: 0.81424 early stop: 0\n",
            "\u001b[32m[I 210618 02:30:29 models:110]\u001b[39m 8 373760 train loss: 0.0003670 valid loss: 0.0008597 P@1: 0.91652 P@3: 0.77249 P@5: 0.63902 N@3: 0.85581 N@5: 0.81431 early stop: 0\n",
            "\u001b[32m[I 210618 02:33:48 models:110]\u001b[39m 8 399360 train loss: 0.0003705 valid loss: 0.0008604 P@1: 0.91645 P@3: 0.77263 P@5: 0.63908 N@3: 0.85591 N@5: 0.81434 early stop: 0\n",
            "\u001b[32m[I 210618 02:37:07 models:110]\u001b[39m 8 424960 train loss: 0.0003744 valid loss: 0.0008611 P@1: 0.91638 P@3: 0.77256 P@5: 0.63918 N@3: 0.85584 N@5: 0.81441 early stop: 0\n",
            "\u001b[32m[I 210618 02:40:25 models:110]\u001b[39m 8 450560 train loss: 0.0003781 valid loss: 0.0008617 P@1: 0.91639 P@3: 0.77260 P@5: 0.63919 N@3: 0.85589 N@5: 0.81444 early stop: 0\n",
            "\u001b[32m[I 210618 02:43:43 models:110]\u001b[39m 8 476160 train loss: 0.0003797 valid loss: 0.0008623 P@1: 0.91625 P@3: 0.77261 P@5: 0.63918 N@3: 0.85585 N@5: 0.81440 early stop: 1\n",
            "\u001b[32m[I 210618 02:46:59 models:110]\u001b[39m 8 501760 train loss: 0.0003808 valid loss: 0.0008629 P@1: 0.91625 P@3: 0.77261 P@5: 0.63923 N@3: 0.85586 N@5: 0.81443 early stop: 2\n",
            "\u001b[32m[I 210618 02:50:15 models:110]\u001b[39m 8 527360 train loss: 0.0003855 valid loss: 0.0008634 P@1: 0.91624 P@3: 0.77262 P@5: 0.63927 N@3: 0.85589 N@5: 0.81449 early stop: 0\n",
            "\u001b[32m[I 210618 02:53:33 models:110]\u001b[39m 8 552960 train loss: 0.0003866 valid loss: 0.0008640 P@1: 0.91621 P@3: 0.77259 P@5: 0.63931 N@3: 0.85587 N@5: 0.81451 early stop: 0\n",
            "\u001b[32m[I 210618 02:56:53 models:110]\u001b[39m 9 14080 train loss: 0.0003286 valid loss: 0.0008648 P@1: 0.91627 P@3: 0.77266 P@5: 0.63947 N@3: 0.85591 N@5: 0.81464 early stop: 0\n",
            "\u001b[32m[I 210618 03:00:09 models:110]\u001b[39m 9 39680 train loss: 0.0002730 valid loss: 0.0008658 P@1: 0.91627 P@3: 0.77263 P@5: 0.63953 N@3: 0.85590 N@5: 0.81470 early stop: 0\n",
            "\u001b[32m[I 210618 03:03:26 models:110]\u001b[39m 9 65280 train loss: 0.0002801 valid loss: 0.0008670 P@1: 0.91625 P@3: 0.77262 P@5: 0.63951 N@3: 0.85589 N@5: 0.81468 early stop: 1\n",
            "\u001b[32m[I 210618 03:06:41 models:110]\u001b[39m 9 90880 train loss: 0.0002865 valid loss: 0.0008680 P@1: 0.91631 P@3: 0.77259 P@5: 0.63958 N@3: 0.85588 N@5: 0.81471 early stop: 0\n",
            "\u001b[32m[I 210618 03:09:58 models:110]\u001b[39m 9 116480 train loss: 0.0002899 valid loss: 0.0008689 P@1: 0.91630 P@3: 0.77263 P@5: 0.63965 N@3: 0.85590 N@5: 0.81475 early stop: 0\n",
            "\u001b[32m[I 210618 03:13:15 models:110]\u001b[39m 9 142080 train loss: 0.0002951 valid loss: 0.0008700 P@1: 0.91618 P@3: 0.77266 P@5: 0.63960 N@3: 0.85590 N@5: 0.81471 early stop: 1\n",
            "\u001b[32m[I 210618 03:16:31 models:110]\u001b[39m 9 167680 train loss: 0.0002942 valid loss: 0.0008710 P@1: 0.91622 P@3: 0.77266 P@5: 0.63970 N@3: 0.85590 N@5: 0.81478 early stop: 0\n",
            "\u001b[32m[I 210618 03:19:51 models:110]\u001b[39m 9 193280 train loss: 0.0003016 valid loss: 0.0008720 P@1: 0.91615 P@3: 0.77268 P@5: 0.63975 N@3: 0.85590 N@5: 0.81482 early stop: 0\n",
            "\u001b[32m[I 210618 03:23:08 models:110]\u001b[39m 9 218880 train loss: 0.0003021 valid loss: 0.0008731 P@1: 0.91608 P@3: 0.77273 P@5: 0.63977 N@3: 0.85593 N@5: 0.81484 early stop: 0\n",
            "\u001b[32m[I 210618 03:26:26 models:110]\u001b[39m 9 244480 train loss: 0.0003085 valid loss: 0.0008740 P@1: 0.91604 P@3: 0.77283 P@5: 0.63985 N@3: 0.85598 N@5: 0.81488 early stop: 0\n",
            "\u001b[32m[I 210618 03:29:45 models:110]\u001b[39m 9 270080 train loss: 0.0003107 valid loss: 0.0008750 P@1: 0.91615 P@3: 0.77284 P@5: 0.63989 N@3: 0.85599 N@5: 0.81491 early stop: 0\n",
            "\u001b[32m[I 210618 03:33:06 models:110]\u001b[39m 9 295680 train loss: 0.0003124 valid loss: 0.0008758 P@1: 0.91625 P@3: 0.77287 P@5: 0.63996 N@3: 0.85604 N@5: 0.81500 early stop: 0\n",
            "\u001b[32m[I 210618 03:36:24 models:110]\u001b[39m 9 321280 train loss: 0.0003144 valid loss: 0.0008768 P@1: 0.91622 P@3: 0.77291 P@5: 0.63998 N@3: 0.85605 N@5: 0.81499 early stop: 1\n",
            "\u001b[32m[I 210618 03:39:40 models:110]\u001b[39m 9 346880 train loss: 0.0003190 valid loss: 0.0008777 P@1: 0.91620 P@3: 0.77302 P@5: 0.64010 N@3: 0.85611 N@5: 0.81509 early stop: 0\n",
            "\u001b[32m[I 210618 03:42:59 models:110]\u001b[39m 9 372480 train loss: 0.0003228 valid loss: 0.0008786 P@1: 0.91632 P@3: 0.77313 P@5: 0.64019 N@3: 0.85624 N@5: 0.81520 early stop: 0\n",
            "\u001b[32m[I 210618 03:46:17 models:110]\u001b[39m 9 398080 train loss: 0.0003219 valid loss: 0.0008794 P@1: 0.91624 P@3: 0.77319 P@5: 0.64020 N@3: 0.85628 N@5: 0.81523 early stop: 0\n",
            "\u001b[32m[I 210618 03:49:38 models:110]\u001b[39m 9 423680 train loss: 0.0003261 valid loss: 0.0008803 P@1: 0.91620 P@3: 0.77325 P@5: 0.64027 N@3: 0.85631 N@5: 0.81525 early stop: 0\n",
            "\u001b[32m[I 210618 03:52:59 models:110]\u001b[39m 9 449280 train loss: 0.0003291 valid loss: 0.0008811 P@1: 0.91611 P@3: 0.77329 P@5: 0.64038 N@3: 0.85634 N@5: 0.81532 early stop: 0\n",
            "\u001b[32m[I 210618 03:56:20 models:110]\u001b[39m 9 474880 train loss: 0.0003297 valid loss: 0.0008819 P@1: 0.91613 P@3: 0.77319 P@5: 0.64035 N@3: 0.85626 N@5: 0.81530 early stop: 1\n",
            "\u001b[32m[I 210618 03:59:39 models:110]\u001b[39m 9 500480 train loss: 0.0003328 valid loss: 0.0008826 P@1: 0.91617 P@3: 0.77321 P@5: 0.64040 N@3: 0.85627 N@5: 0.81532 early stop: 0\n",
            "\u001b[32m[I 210618 04:02:57 models:110]\u001b[39m 9 526080 train loss: 0.0003348 valid loss: 0.0008833 P@1: 0.91611 P@3: 0.77325 P@5: 0.64041 N@3: 0.85627 N@5: 0.81531 early stop: 1\n",
            "\u001b[32m[I 210618 04:06:15 models:110]\u001b[39m 9 551680 train loss: 0.0003400 valid loss: 0.0008840 P@1: 0.91621 P@3: 0.77330 P@5: 0.64047 N@3: 0.85634 N@5: 0.81537 early stop: 0\n",
            "\u001b[32m[I 210618 04:09:35 models:110]\u001b[39m 10 12800 train loss: 0.0002882 valid loss: 0.0008850 P@1: 0.91625 P@3: 0.77340 P@5: 0.64051 N@3: 0.85643 N@5: 0.81543 early stop: 0\n",
            "\u001b[32m[I 210618 04:12:54 models:110]\u001b[39m 10 38400 train loss: 0.0002400 valid loss: 0.0008861 P@1: 0.91613 P@3: 0.77336 P@5: 0.64056 N@3: 0.85634 N@5: 0.81540 early stop: 1\n",
            "\u001b[32m[I 210618 04:16:11 models:110]\u001b[39m 10 64000 train loss: 0.0002460 valid loss: 0.0008872 P@1: 0.91604 P@3: 0.77340 P@5: 0.64053 N@3: 0.85636 N@5: 0.81538 early stop: 2\n",
            "\u001b[32m[I 210618 04:19:28 models:110]\u001b[39m 10 89600 train loss: 0.0002503 valid loss: 0.0008884 P@1: 0.91603 P@3: 0.77334 P@5: 0.64052 N@3: 0.85632 N@5: 0.81538 early stop: 3\n",
            "\u001b[32m[I 210618 04:22:45 models:110]\u001b[39m 10 115200 train loss: 0.0002527 valid loss: 0.0008895 P@1: 0.91603 P@3: 0.77335 P@5: 0.64054 N@3: 0.85634 N@5: 0.81541 early stop: 4\n",
            "\u001b[32m[I 210618 04:26:01 models:110]\u001b[39m 10 140800 train loss: 0.0002551 valid loss: 0.0008906 P@1: 0.91596 P@3: 0.77335 P@5: 0.64054 N@3: 0.85629 N@5: 0.81536 early stop: 5\n",
            "\u001b[32m[I 210618 04:29:15 models:110]\u001b[39m 10 166400 train loss: 0.0002585 valid loss: 0.0008917 P@1: 0.91594 P@3: 0.77335 P@5: 0.64054 N@3: 0.85628 N@5: 0.81536 early stop: 6\n",
            "\u001b[32m[I 210618 04:32:31 models:110]\u001b[39m 10 192000 train loss: 0.0002629 valid loss: 0.0008928 P@1: 0.91588 P@3: 0.77337 P@5: 0.64054 N@3: 0.85629 N@5: 0.81535 early stop: 7\n",
            "\u001b[32m[I 210618 04:35:47 models:110]\u001b[39m 10 217600 train loss: 0.0002662 valid loss: 0.0008939 P@1: 0.91588 P@3: 0.77333 P@5: 0.64053 N@3: 0.85625 N@5: 0.81533 early stop: 8\n",
            "\u001b[32m[I 210618 04:39:02 models:110]\u001b[39m 10 243200 train loss: 0.0002673 valid loss: 0.0008950 P@1: 0.91586 P@3: 0.77329 P@5: 0.64056 N@3: 0.85620 N@5: 0.81534 early stop: 9\n",
            "\u001b[32m[I 210618 04:42:18 models:110]\u001b[39m 10 268800 train loss: 0.0002713 valid loss: 0.0008960 P@1: 0.91574 P@3: 0.77322 P@5: 0.64063 N@3: 0.85613 N@5: 0.81536 early stop: 10\n",
            "\u001b[32m[I 210618 04:45:34 models:110]\u001b[39m 10 294400 train loss: 0.0002742 valid loss: 0.0008970 P@1: 0.91570 P@3: 0.77325 P@5: 0.64064 N@3: 0.85615 N@5: 0.81537 early stop: 11\n",
            "\u001b[32m[I 210618 04:48:49 models:110]\u001b[39m 10 320000 train loss: 0.0002811 valid loss: 0.0008982 P@1: 0.91571 P@3: 0.77322 P@5: 0.64055 N@3: 0.85611 N@5: 0.81529 early stop: 12\n",
            "\u001b[32m[I 210618 04:52:04 models:110]\u001b[39m 10 345600 train loss: 0.0002788 valid loss: 0.0008992 P@1: 0.91570 P@3: 0.77319 P@5: 0.64062 N@3: 0.85609 N@5: 0.81533 early stop: 13\n",
            "\u001b[32m[I 210618 04:55:20 models:110]\u001b[39m 10 371200 train loss: 0.0002844 valid loss: 0.0009002 P@1: 0.91567 P@3: 0.77323 P@5: 0.64068 N@3: 0.85609 N@5: 0.81535 early stop: 14\n",
            "\u001b[32m[I 210618 04:58:38 models:110]\u001b[39m 10 396800 train loss: 0.0002839 valid loss: 0.0009012 P@1: 0.91577 P@3: 0.77319 P@5: 0.64069 N@3: 0.85609 N@5: 0.81537 early stop: 15\n",
            "\u001b[32m[I 210618 05:01:54 models:110]\u001b[39m 10 422400 train loss: 0.0002879 valid loss: 0.0009021 P@1: 0.91574 P@3: 0.77321 P@5: 0.64075 N@3: 0.85610 N@5: 0.81542 early stop: 16\n",
            "\u001b[32m[I 210618 05:05:11 models:110]\u001b[39m 10 448000 train loss: 0.0002850 valid loss: 0.0009030 P@1: 0.91577 P@3: 0.77325 P@5: 0.64087 N@3: 0.85613 N@5: 0.81551 early stop: 0\n",
            "\u001b[32m[I 210618 05:08:28 models:110]\u001b[39m 10 473600 train loss: 0.0002942 valid loss: 0.0009039 P@1: 0.91587 P@3: 0.77330 P@5: 0.64085 N@3: 0.85616 N@5: 0.81548 early stop: 1\n",
            "\u001b[32m[I 210618 05:11:44 models:110]\u001b[39m 10 499200 train loss: 0.0002922 valid loss: 0.0009049 P@1: 0.91573 P@3: 0.77323 P@5: 0.64082 N@3: 0.85609 N@5: 0.81543 early stop: 2\n",
            "\u001b[32m[I 210618 05:14:59 models:110]\u001b[39m 10 524800 train loss: 0.0002951 valid loss: 0.0009058 P@1: 0.91559 P@3: 0.77325 P@5: 0.64083 N@3: 0.85607 N@5: 0.81543 early stop: 3\n",
            "\u001b[32m[I 210618 05:18:14 models:110]\u001b[39m 10 550400 train loss: 0.0002992 valid loss: 0.0009066 P@1: 0.91562 P@3: 0.77323 P@5: 0.64087 N@3: 0.85604 N@5: 0.81543 early stop: 4\n",
            "\u001b[32m[I 210618 05:21:30 models:110]\u001b[39m 11 11520 train loss: 0.0002634 valid loss: 0.0009077 P@1: 0.91560 P@3: 0.77317 P@5: 0.64093 N@3: 0.85601 N@5: 0.81549 early stop: 5\n",
            "\u001b[32m[I 210618 05:24:47 models:110]\u001b[39m 11 37120 train loss: 0.0002148 valid loss: 0.0009089 P@1: 0.91567 P@3: 0.77319 P@5: 0.64094 N@3: 0.85602 N@5: 0.81549 early stop: 6\n",
            "\u001b[32m[I 210618 05:28:07 models:110]\u001b[39m 11 62720 train loss: 0.0002138 valid loss: 0.0009101 P@1: 0.91584 P@3: 0.77312 P@5: 0.64094 N@3: 0.85600 N@5: 0.81552 early stop: 0\n",
            "\u001b[32m[I 210618 05:31:25 models:110]\u001b[39m 11 88320 train loss: 0.0002180 valid loss: 0.0009112 P@1: 0.91576 P@3: 0.77315 P@5: 0.64101 N@3: 0.85602 N@5: 0.81555 early stop: 0\n",
            "\u001b[32m[I 210618 05:34:44 models:110]\u001b[39m 11 113920 train loss: 0.0002237 valid loss: 0.0009124 P@1: 0.91573 P@3: 0.77319 P@5: 0.64108 N@3: 0.85603 N@5: 0.81560 early stop: 0\n",
            "\u001b[32m[I 210618 05:38:01 models:110]\u001b[39m 11 139520 train loss: 0.0002240 valid loss: 0.0009136 P@1: 0.91570 P@3: 0.77319 P@5: 0.64109 N@3: 0.85601 N@5: 0.81558 early stop: 1\n",
            "\u001b[32m[I 210618 05:41:14 models:110]\u001b[39m 11 165120 train loss: 0.0002317 valid loss: 0.0009147 P@1: 0.91570 P@3: 0.77319 P@5: 0.64105 N@3: 0.85599 N@5: 0.81554 early stop: 2\n",
            "\u001b[32m[I 210618 05:44:28 models:110]\u001b[39m 11 190720 train loss: 0.0002337 valid loss: 0.0009159 P@1: 0.91560 P@3: 0.77316 P@5: 0.64112 N@3: 0.85595 N@5: 0.81556 early stop: 3\n",
            "\u001b[32m[I 210618 05:47:42 models:110]\u001b[39m 11 216320 train loss: 0.0002372 valid loss: 0.0009170 P@1: 0.91574 P@3: 0.77309 P@5: 0.64109 N@3: 0.85592 N@5: 0.81555 early stop: 4\n",
            "\u001b[32m[I 210618 05:50:56 models:110]\u001b[39m 11 241920 train loss: 0.0002379 valid loss: 0.0009182 P@1: 0.91560 P@3: 0.77306 P@5: 0.64112 N@3: 0.85585 N@5: 0.81553 early stop: 5\n",
            "\u001b[32m[I 210618 05:54:11 models:110]\u001b[39m 11 267520 train loss: 0.0002439 valid loss: 0.0009192 P@1: 0.91567 P@3: 0.77303 P@5: 0.64112 N@3: 0.85584 N@5: 0.81553 early stop: 6\n",
            "\u001b[32m[I 210618 05:57:25 models:110]\u001b[39m 11 293120 train loss: 0.0002449 valid loss: 0.0009204 P@1: 0.91553 P@3: 0.77302 P@5: 0.64114 N@3: 0.85581 N@5: 0.81552 early stop: 7\n",
            "\u001b[32m[I 210618 06:00:40 models:110]\u001b[39m 11 318720 train loss: 0.0002490 valid loss: 0.0009214 P@1: 0.91550 P@3: 0.77297 P@5: 0.64108 N@3: 0.85576 N@5: 0.81547 early stop: 8\n",
            "\u001b[32m[I 210618 06:03:54 models:110]\u001b[39m 11 344320 train loss: 0.0002502 valid loss: 0.0009224 P@1: 0.91545 P@3: 0.77288 P@5: 0.64114 N@3: 0.85569 N@5: 0.81549 early stop: 9\n",
            "\u001b[32m[I 210618 06:07:08 models:110]\u001b[39m 11 369920 train loss: 0.0002489 valid loss: 0.0009235 P@1: 0.91547 P@3: 0.77287 P@5: 0.64116 N@3: 0.85570 N@5: 0.81551 early stop: 10\n",
            "\u001b[32m[I 210618 06:10:22 models:110]\u001b[39m 11 395520 train loss: 0.0002535 valid loss: 0.0009245 P@1: 0.91547 P@3: 0.77289 P@5: 0.64116 N@3: 0.85569 N@5: 0.81550 early stop: 11\n",
            "\u001b[32m[I 210618 06:13:36 models:110]\u001b[39m 11 421120 train loss: 0.0002548 valid loss: 0.0009254 P@1: 0.91545 P@3: 0.77285 P@5: 0.64116 N@3: 0.85564 N@5: 0.81550 early stop: 12\n",
            "\u001b[32m[I 210618 06:16:50 models:110]\u001b[39m 11 446720 train loss: 0.0002584 valid loss: 0.0009264 P@1: 0.91537 P@3: 0.77282 P@5: 0.64117 N@3: 0.85561 N@5: 0.81549 early stop: 13\n",
            "\u001b[32m[I 210618 06:20:05 models:110]\u001b[39m 11 472320 train loss: 0.0002642 valid loss: 0.0009273 P@1: 0.91546 P@3: 0.77285 P@5: 0.64120 N@3: 0.85565 N@5: 0.81552 early stop: 14\n",
            "\u001b[32m[I 210618 06:23:19 models:110]\u001b[39m 11 497920 train loss: 0.0002593 valid loss: 0.0009282 P@1: 0.91543 P@3: 0.77285 P@5: 0.64119 N@3: 0.85564 N@5: 0.81550 early stop: 15\n",
            "\u001b[32m[I 210618 06:26:37 models:110]\u001b[39m 11 523520 train loss: 0.0002641 valid loss: 0.0009291 P@1: 0.91542 P@3: 0.77284 P@5: 0.64123 N@3: 0.85563 N@5: 0.81554 early stop: 16\n",
            "\u001b[32m[I 210618 06:29:53 models:110]\u001b[39m 11 549120 train loss: 0.0002661 valid loss: 0.0009301 P@1: 0.91539 P@3: 0.77283 P@5: 0.64128 N@3: 0.85562 N@5: 0.81557 early stop: 17\n",
            "\u001b[32m[I 210618 06:33:11 models:110]\u001b[39m 12 10240 train loss: 0.0002382 valid loss: 0.0009312 P@1: 0.91535 P@3: 0.77287 P@5: 0.64129 N@3: 0.85563 N@5: 0.81557 early stop: 18\n",
            "\u001b[32m[I 210618 06:36:28 models:110]\u001b[39m 12 35840 train loss: 0.0001891 valid loss: 0.0009323 P@1: 0.91539 P@3: 0.77285 P@5: 0.64133 N@3: 0.85564 N@5: 0.81559 early stop: 19\n",
            "\u001b[32m[I 210618 06:39:42 models:110]\u001b[39m 12 61440 train loss: 0.0001937 valid loss: 0.0009334 P@1: 0.91540 P@3: 0.77285 P@5: 0.64131 N@3: 0.85563 N@5: 0.81558 early stop: 20\n",
            "\u001b[32m[I 210618 06:43:00 models:110]\u001b[39m 12 87040 train loss: 0.0001939 valid loss: 0.0009347 P@1: 0.91554 P@3: 0.77286 P@5: 0.64133 N@3: 0.85567 N@5: 0.81562 early stop: 0\n",
            "\u001b[32m[I 210618 06:46:18 models:110]\u001b[39m 12 112640 train loss: 0.0002008 valid loss: 0.0009359 P@1: 0.91545 P@3: 0.77287 P@5: 0.64132 N@3: 0.85566 N@5: 0.81559 early stop: 1\n",
            "\u001b[32m[I 210618 06:49:35 models:110]\u001b[39m 12 138240 train loss: 0.0002039 valid loss: 0.0009370 P@1: 0.91547 P@3: 0.77281 P@5: 0.64130 N@3: 0.85562 N@5: 0.81557 early stop: 2\n",
            "\u001b[32m[I 210618 06:52:53 models:110]\u001b[39m 12 163840 train loss: 0.0002051 valid loss: 0.0009382 P@1: 0.91563 P@3: 0.77280 P@5: 0.64130 N@3: 0.85564 N@5: 0.81559 early stop: 3\n",
            "\u001b[32m[I 210618 06:56:10 models:110]\u001b[39m 12 189440 train loss: 0.0002087 valid loss: 0.0009394 P@1: 0.91557 P@3: 0.77278 P@5: 0.64129 N@3: 0.85560 N@5: 0.81554 early stop: 4\n",
            "\u001b[32m[I 210618 06:59:30 models:110]\u001b[39m 12 215040 train loss: 0.0002097 valid loss: 0.0009405 P@1: 0.91557 P@3: 0.77278 P@5: 0.64129 N@3: 0.85559 N@5: 0.81554 early stop: 5\n",
            "\u001b[32m[I 210618 07:02:47 models:110]\u001b[39m 12 240640 train loss: 0.0002147 valid loss: 0.0009417 P@1: 0.91563 P@3: 0.77276 P@5: 0.64129 N@3: 0.85559 N@5: 0.81555 early stop: 6\n",
            "\u001b[32m[I 210618 07:06:04 models:110]\u001b[39m 12 266240 train loss: 0.0002148 valid loss: 0.0009429 P@1: 0.91557 P@3: 0.77280 P@5: 0.64131 N@3: 0.85562 N@5: 0.81556 early stop: 7\n",
            "\u001b[32m[I 210618 07:09:20 models:110]\u001b[39m 12 291840 train loss: 0.0002177 valid loss: 0.0009441 P@1: 0.91552 P@3: 0.77279 P@5: 0.64123 N@3: 0.85559 N@5: 0.81549 early stop: 8\n",
            "\u001b[32m[I 210618 07:12:36 models:110]\u001b[39m 12 317440 train loss: 0.0002202 valid loss: 0.0009452 P@1: 0.91550 P@3: 0.77278 P@5: 0.64128 N@3: 0.85557 N@5: 0.81550 early stop: 9\n",
            "\u001b[32m[I 210618 07:15:53 models:110]\u001b[39m 12 343040 train loss: 0.0002239 valid loss: 0.0009463 P@1: 0.91559 P@3: 0.77277 P@5: 0.64128 N@3: 0.85558 N@5: 0.81553 early stop: 10\n",
            "\u001b[32m[I 210618 07:19:08 models:110]\u001b[39m 12 368640 train loss: 0.0002263 valid loss: 0.0009474 P@1: 0.91566 P@3: 0.77281 P@5: 0.64126 N@3: 0.85561 N@5: 0.81552 early stop: 11\n",
            "\u001b[32m[I 210618 07:22:24 models:110]\u001b[39m 12 394240 train loss: 0.0002290 valid loss: 0.0009484 P@1: 0.91554 P@3: 0.77290 P@5: 0.64127 N@3: 0.85566 N@5: 0.81553 early stop: 12\n",
            "\u001b[32m[I 210618 07:25:39 models:110]\u001b[39m 12 419840 train loss: 0.0002313 valid loss: 0.0009495 P@1: 0.91559 P@3: 0.77291 P@5: 0.64128 N@3: 0.85569 N@5: 0.81555 early stop: 13\n",
            "\u001b[32m[I 210618 07:28:55 models:110]\u001b[39m 12 445440 train loss: 0.0002345 valid loss: 0.0009506 P@1: 0.91552 P@3: 0.77291 P@5: 0.64125 N@3: 0.85568 N@5: 0.81551 early stop: 14\n",
            "\u001b[32m[I 210618 07:32:11 models:110]\u001b[39m 12 471040 train loss: 0.0002312 valid loss: 0.0009517 P@1: 0.91547 P@3: 0.77295 P@5: 0.64124 N@3: 0.85570 N@5: 0.81551 early stop: 15\n",
            "\u001b[32m[I 210618 07:35:26 models:110]\u001b[39m 12 496640 train loss: 0.0002367 valid loss: 0.0009527 P@1: 0.91553 P@3: 0.77297 P@5: 0.64129 N@3: 0.85572 N@5: 0.81554 early stop: 16\n",
            "\u001b[32m[I 210618 07:38:43 models:110]\u001b[39m 12 522240 train loss: 0.0002391 valid loss: 0.0009538 P@1: 0.91549 P@3: 0.77292 P@5: 0.64132 N@3: 0.85566 N@5: 0.81554 early stop: 17\n",
            "\u001b[32m[I 210618 07:41:59 models:110]\u001b[39m 12 547840 train loss: 0.0002406 valid loss: 0.0009547 P@1: 0.91549 P@3: 0.77289 P@5: 0.64134 N@3: 0.85563 N@5: 0.81554 early stop: 18\n",
            "\u001b[32m[I 210618 07:45:20 models:110]\u001b[39m 13 8960 train loss: 0.0002209 valid loss: 0.0009557 P@1: 0.91562 P@3: 0.77295 P@5: 0.64139 N@3: 0.85569 N@5: 0.81559 early stop: 19\n",
            "\u001b[32m[I 210618 07:48:38 models:110]\u001b[39m 13 34560 train loss: 0.0001721 valid loss: 0.0009569 P@1: 0.91563 P@3: 0.77284 P@5: 0.64138 N@3: 0.85562 N@5: 0.81559 early stop: 20\n",
            "\u001b[32m[I 210618 07:51:57 models:110]\u001b[39m 13 60160 train loss: 0.0001749 valid loss: 0.0009582 P@1: 0.91564 P@3: 0.77275 P@5: 0.64141 N@3: 0.85556 N@5: 0.81559 early stop: 21\n",
            "\u001b[32m[I 210618 07:55:19 models:110]\u001b[39m 13 85760 train loss: 0.0001785 valid loss: 0.0009593 P@1: 0.91570 P@3: 0.77270 P@5: 0.64139 N@3: 0.85554 N@5: 0.81560 early stop: 22\n",
            "\u001b[32m[I 210618 07:58:38 models:110]\u001b[39m 13 111360 train loss: 0.0001786 valid loss: 0.0009605 P@1: 0.91574 P@3: 0.77276 P@5: 0.64144 N@3: 0.85560 N@5: 0.81566 early stop: 0\n",
            "\u001b[32m[I 210618 08:01:56 models:110]\u001b[39m 13 136960 train loss: 0.0001828 valid loss: 0.0009617 P@1: 0.91570 P@3: 0.77279 P@5: 0.64143 N@3: 0.85559 N@5: 0.81563 early stop: 1\n",
            "\u001b[32m[I 210618 08:05:15 models:110]\u001b[39m 13 162560 train loss: 0.0001861 valid loss: 0.0009629 P@1: 0.91574 P@3: 0.77274 P@5: 0.64145 N@3: 0.85557 N@5: 0.81563 early stop: 2\n",
            "\u001b[32m[I 210618 08:08:34 models:110]\u001b[39m 13 188160 train loss: 0.0001886 valid loss: 0.0009640 P@1: 0.91574 P@3: 0.77277 P@5: 0.64149 N@3: 0.85559 N@5: 0.81566 early stop: 0\n",
            "\u001b[32m[I 210618 08:11:53 models:110]\u001b[39m 13 213760 train loss: 0.0001933 valid loss: 0.0009651 P@1: 0.91567 P@3: 0.77278 P@5: 0.64151 N@3: 0.85557 N@5: 0.81566 early stop: 1\n",
            "\u001b[32m[I 210618 08:15:11 models:110]\u001b[39m 13 239360 train loss: 0.0001952 valid loss: 0.0009662 P@1: 0.91554 P@3: 0.77279 P@5: 0.64149 N@3: 0.85555 N@5: 0.81562 early stop: 2\n",
            "\u001b[32m[I 210618 08:18:28 models:110]\u001b[39m 13 264960 train loss: 0.0001957 valid loss: 0.0009673 P@1: 0.91549 P@3: 0.77278 P@5: 0.64148 N@3: 0.85554 N@5: 0.81561 early stop: 3\n",
            "\u001b[32m[I 210618 08:21:45 models:110]\u001b[39m 13 290560 train loss: 0.0001962 valid loss: 0.0009685 P@1: 0.91554 P@3: 0.77265 P@5: 0.64152 N@3: 0.85546 N@5: 0.81563 early stop: 4\n",
            "\u001b[32m[I 210618 08:25:02 models:110]\u001b[39m 13 316160 train loss: 0.0002003 valid loss: 0.0009696 P@1: 0.91549 P@3: 0.77263 P@5: 0.64155 N@3: 0.85543 N@5: 0.81564 early stop: 5\n",
            "\u001b[32m[I 210618 08:28:20 models:110]\u001b[39m 13 341760 train loss: 0.0002014 valid loss: 0.0009708 P@1: 0.91540 P@3: 0.77261 P@5: 0.64152 N@3: 0.85539 N@5: 0.81559 early stop: 6\n",
            "\u001b[32m[I 210618 08:31:36 models:110]\u001b[39m 13 367360 train loss: 0.0002045 valid loss: 0.0009719 P@1: 0.91545 P@3: 0.77261 P@5: 0.64152 N@3: 0.85541 N@5: 0.81561 early stop: 7\n",
            "\u001b[32m[I 210618 08:34:55 models:110]\u001b[39m 13 392960 train loss: 0.0002085 valid loss: 0.0009730 P@1: 0.91547 P@3: 0.77264 P@5: 0.64151 N@3: 0.85544 N@5: 0.81560 early stop: 8\n",
            "\u001b[32m[I 210618 08:38:13 models:110]\u001b[39m 13 418560 train loss: 0.0002088 valid loss: 0.0009742 P@1: 0.91549 P@3: 0.77263 P@5: 0.64156 N@3: 0.85542 N@5: 0.81562 early stop: 9\n",
            "\u001b[32m[I 210618 08:41:29 models:110]\u001b[39m 13 444160 train loss: 0.0002106 valid loss: 0.0009752 P@1: 0.91560 P@3: 0.77262 P@5: 0.64151 N@3: 0.85544 N@5: 0.81560 early stop: 10\n",
            "\u001b[32m[I 210618 08:44:46 models:110]\u001b[39m 13 469760 train loss: 0.0002152 valid loss: 0.0009762 P@1: 0.91545 P@3: 0.77264 P@5: 0.64157 N@3: 0.85542 N@5: 0.81560 early stop: 11\n",
            "\u001b[32m[I 210618 08:48:03 models:110]\u001b[39m 13 495360 train loss: 0.0002146 valid loss: 0.0009773 P@1: 0.91550 P@3: 0.77263 P@5: 0.64151 N@3: 0.85543 N@5: 0.81557 early stop: 12\n",
            "\u001b[32m[I 210618 08:51:19 models:110]\u001b[39m 13 520960 train loss: 0.0002186 valid loss: 0.0009782 P@1: 0.91547 P@3: 0.77263 P@5: 0.64152 N@3: 0.85543 N@5: 0.81558 early stop: 13\n",
            "\u001b[32m[I 210618 08:54:36 models:110]\u001b[39m 13 546560 train loss: 0.0002188 valid loss: 0.0009792 P@1: 0.91547 P@3: 0.77260 P@5: 0.64153 N@3: 0.85541 N@5: 0.81559 early stop: 14\n",
            "\u001b[32m[I 210618 08:57:53 models:110]\u001b[39m 14 7680 train loss: 0.0002018 valid loss: 0.0009803 P@1: 0.91543 P@3: 0.77264 P@5: 0.64156 N@3: 0.85543 N@5: 0.81560 early stop: 15\n",
            "\u001b[32m[I 210618 09:01:10 models:110]\u001b[39m 14 33280 train loss: 0.0001563 valid loss: 0.0009815 P@1: 0.91545 P@3: 0.77264 P@5: 0.64159 N@3: 0.85543 N@5: 0.81562 early stop: 16\n",
            "\u001b[32m[I 210618 09:04:25 models:110]\u001b[39m 14 58880 train loss: 0.0001598 valid loss: 0.0009827 P@1: 0.91549 P@3: 0.77274 P@5: 0.64163 N@3: 0.85552 N@5: 0.81566 early stop: 17\n",
            "\u001b[32m[I 210618 09:07:40 models:110]\u001b[39m 14 84480 train loss: 0.0001618 valid loss: 0.0009839 P@1: 0.91547 P@3: 0.77270 P@5: 0.64155 N@3: 0.85548 N@5: 0.81560 early stop: 18\n",
            "\u001b[32m[I 210618 09:10:58 models:110]\u001b[39m 14 110080 train loss: 0.0001652 valid loss: 0.0009852 P@1: 0.91546 P@3: 0.77272 P@5: 0.64156 N@3: 0.85551 N@5: 0.81561 early stop: 19\n",
            "\u001b[32m[I 210618 09:14:14 models:110]\u001b[39m 14 135680 train loss: 0.0001675 valid loss: 0.0009864 P@1: 0.91540 P@3: 0.77278 P@5: 0.64159 N@3: 0.85554 N@5: 0.81562 early stop: 20\n",
            "\u001b[32m[I 210618 09:17:30 models:110]\u001b[39m 14 161280 train loss: 0.0001730 valid loss: 0.0009876 P@1: 0.91535 P@3: 0.77278 P@5: 0.64153 N@3: 0.85552 N@5: 0.81558 early stop: 21\n",
            "\u001b[32m[I 210618 09:20:45 models:110]\u001b[39m 14 186880 train loss: 0.0001729 valid loss: 0.0009888 P@1: 0.91542 P@3: 0.77275 P@5: 0.64156 N@3: 0.85551 N@5: 0.81560 early stop: 22\n",
            "\u001b[32m[I 210618 09:24:01 models:110]\u001b[39m 14 212480 train loss: 0.0001759 valid loss: 0.0009900 P@1: 0.91543 P@3: 0.77275 P@5: 0.64152 N@3: 0.85549 N@5: 0.81556 early stop: 23\n",
            "\u001b[32m[I 210618 09:27:18 models:110]\u001b[39m 14 238080 train loss: 0.0001783 valid loss: 0.0009912 P@1: 0.91547 P@3: 0.77277 P@5: 0.64155 N@3: 0.85552 N@5: 0.81558 early stop: 24\n",
            "\u001b[32m[I 210618 09:30:34 models:110]\u001b[39m 14 263680 train loss: 0.0001798 valid loss: 0.0009923 P@1: 0.91545 P@3: 0.77273 P@5: 0.64153 N@3: 0.85550 N@5: 0.81557 early stop: 25\n",
            "\u001b[32m[I 210618 09:33:52 models:110]\u001b[39m 14 289280 train loss: 0.0001833 valid loss: 0.0009934 P@1: 0.91543 P@3: 0.77274 P@5: 0.64155 N@3: 0.85550 N@5: 0.81558 early stop: 26\n",
            "\u001b[32m[I 210618 09:37:10 models:110]\u001b[39m 14 314880 train loss: 0.0001864 valid loss: 0.0009946 P@1: 0.91543 P@3: 0.77271 P@5: 0.64150 N@3: 0.85547 N@5: 0.81554 early stop: 27\n",
            "\u001b[32m[I 210618 09:40:28 models:110]\u001b[39m 14 340480 train loss: 0.0001847 valid loss: 0.0009957 P@1: 0.91539 P@3: 0.77269 P@5: 0.64153 N@3: 0.85543 N@5: 0.81554 early stop: 28\n",
            "\u001b[32m[I 210618 09:43:44 models:110]\u001b[39m 14 366080 train loss: 0.0001861 valid loss: 0.0009968 P@1: 0.91542 P@3: 0.77264 P@5: 0.64157 N@3: 0.85541 N@5: 0.81557 early stop: 29\n",
            "\u001b[32m[I 210618 09:47:00 models:110]\u001b[39m 14 391680 train loss: 0.0001915 valid loss: 0.0009979 P@1: 0.91537 P@3: 0.77261 P@5: 0.64155 N@3: 0.85537 N@5: 0.81553 early stop: 30\n",
            "\u001b[32m[I 210618 09:50:17 models:110]\u001b[39m 14 417280 train loss: 0.0001923 valid loss: 0.0009991 P@1: 0.91527 P@3: 0.77263 P@5: 0.64152 N@3: 0.85536 N@5: 0.81548 early stop: 31\n",
            "\u001b[32m[I 210618 09:53:35 models:110]\u001b[39m 14 442880 train loss: 0.0001932 valid loss: 0.0010001 P@1: 0.91530 P@3: 0.77255 P@5: 0.64153 N@3: 0.85531 N@5: 0.81549 early stop: 32\n",
            "\u001b[32m[I 210618 09:56:51 models:110]\u001b[39m 14 468480 train loss: 0.0001938 valid loss: 0.0010012 P@1: 0.91527 P@3: 0.77250 P@5: 0.64154 N@3: 0.85525 N@5: 0.81548 early stop: 33\n",
            "\u001b[32m[I 210618 10:00:07 models:110]\u001b[39m 14 494080 train loss: 0.0001954 valid loss: 0.0010022 P@1: 0.91527 P@3: 0.77252 P@5: 0.64154 N@3: 0.85526 N@5: 0.81548 early stop: 34\n",
            "\u001b[32m[I 210618 10:03:23 models:110]\u001b[39m 14 519680 train loss: 0.0001962 valid loss: 0.0010032 P@1: 0.91530 P@3: 0.77251 P@5: 0.64153 N@3: 0.85526 N@5: 0.81548 early stop: 35\n",
            "\u001b[32m[I 210618 10:06:39 models:110]\u001b[39m 14 545280 train loss: 0.0002007 valid loss: 0.0010042 P@1: 0.91522 P@3: 0.77254 P@5: 0.64155 N@3: 0.85528 N@5: 0.81548 early stop: 36\n",
            "\u001b[32m[I 210618 10:08:46 main:76]\u001b[39m Finish Training\n",
            "\u001b[32m[I 210618 10:08:57 main:32]\u001b[39m Model Name: MATCH\n",
            "\u001b[32m[I 210618 10:08:57 main:79]\u001b[39m Loading Test Set\n",
            "\u001b[32m[I 210618 10:09:01 main:83]\u001b[39m Size of Test Set: 70533\n",
            "\u001b[32m[I 210618 10:09:01 main:85]\u001b[39m Predicting\n",
            "\u001b[32m[I 210618 10:09:30 main:91]\u001b[39m Finish Predicting\n",
            "Precision@1,3,5: 0.9148909021309174 0.7706416381929971 0.6392610551089561\n",
            "nDCG@1,3,5: 0.9148909021309174 0.8551977454602886 0.8154306415429102\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}