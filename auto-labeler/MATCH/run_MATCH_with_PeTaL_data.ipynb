{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"run_MATCH_with_PeTaL_data.ipynb","provenance":[],"collapsed_sections":["znKf8GyxO3VC","ktf4hCBZJK6A","UB0SRyKqPGaw","feVctRCvVwQV","-4gUb3gaZeUJ","GBhMpPdH3DuI","54Ur6eGk_q3C"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"6mUF_9SjH8-6"},"source":["# Run MATCH with PeTaL data\n","\n","Created by Eric Kong on 21 June 2021.\n","\n","In this notebook we run the MATCH algorithm (https://github.com/yuzhimanhua/MATCH) on Lens data labelled with PeTaL's taxonomy.\n","\n","This notebook was originally run in Google Colaboratory with GPU acceleration."]},{"cell_type":"code","metadata":{"id":"UU4GmAtqzAWd","executionInfo":{"status":"ok","timestamp":1624645058411,"user_tz":420,"elapsed":314,"user":{"displayName":"Eric Kong","photoUrl":"","userId":"07341482379174037459"}}},"source":["import os"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rOZTaB7SzZ4D","executionInfo":{"status":"ok","timestamp":1624645059110,"user_tz":420,"elapsed":4,"user":{"displayName":"Eric Kong","photoUrl":"","userId":"07341482379174037459"}},"outputId":"4c3b96b8-dbb5-4d48-ac18-2a80163307a2"},"source":["# If running on Google Colab: Mount drive and cd to workspace.\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# NOTE: Replace DRIVE_PATH with the path you plan to clone MATCH into.\n","DRIVE_PATH = '/content/drive/Shareddrives/NASA'\n","%cd $DRIVE_PATH"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/Shareddrives/NASA\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1o1d5YtSag3y","executionInfo":{"status":"ok","timestamp":1624645062090,"user_tz":420,"elapsed":278,"user":{"displayName":"Eric Kong","photoUrl":"","userId":"07341482379174037459"}},"outputId":"3ae055d5-b7ce-47d1-d9be-9994a8b87cf4"},"source":["!nvidia-smi"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Fri Jun 25 18:17:41 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   42C    P0    25W / 300W |      0MiB / 16160MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JzcNuQ-lyK7m","executionInfo":{"status":"ok","timestamp":1624645063993,"user_tz":420,"elapsed":308,"user":{"displayName":"Eric Kong","photoUrl":"","userId":"07341482379174037459"}},"outputId":"9e5c690a-4077-44af-81d3-5b121cfbb56d"},"source":["if not os.path.exists('MATCH/'):\n","    !git clone https://github.com/yuzhimanhua/MATCH.git\n","else:\n","    print(\"You have already cloned the MATCH repository.\")"],"execution_count":4,"outputs":[{"output_type":"stream","text":["You have already cloned the MATCH repository.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TH4Sftq2eT5E","executionInfo":{"status":"ok","timestamp":1624645070053,"user_tz":420,"elapsed":320,"user":{"displayName":"Eric Kong","photoUrl":"","userId":"07341482379174037459"}},"outputId":"754fde93-1e45-4c9e-d671-a0b255b7fdb6"},"source":["%cd ./MATCH\n","!ls"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/content/drive/Shareddrives/NASA/MATCH\n","configure      main.py\t     PeTaL-062414      run_models.sh\n","deepxml        MeSH\t     predictions.txt   transform_data_PeTaL.py\n","evaluation.py  PeTaL\t     preprocess.py     transform_data.py\n","joint\t       PeTaL-062309  preprocess.sh\n","LICENSE        PeTaL-062312  README.md\n","MAG\t       PeTaL-062315  requirements.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BXEInyjhbPDy","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1624645001424,"user_tz":420,"elapsed":125615,"user":{"displayName":"Eric Kong","photoUrl":"","userId":"07341482379174037459"}},"outputId":"40444e46-59c8-447d-e57d-93180aad97f9"},"source":["# Install requirements in requirements.txt\n","!chmod 755 -R .\n","!pip3 install -r requirements.txt"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Collecting torch==1.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/65/5248be50c55ab7429dd5c11f5e2f9f5865606b80e854ca63139ad1a584f2/torch-1.2.0-cp37-cp37m-manylinux1_x86_64.whl (748.9MB)\n","\u001b[K     |████████████████████████████████| 748.9MB 25kB/s \n","\u001b[?25hCollecting torchvision==0.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/83/2d77d040e34bd8f70dcb4770f7eb7d0aa71e07738abf6831be863ade00db/torchvision-0.4.0-cp37-cp37m-manylinux1_x86_64.whl (8.8MB)\n","\u001b[K     |████████████████████████████████| 8.8MB 7.3MB/s \n","\u001b[?25hCollecting torchgpipe==0.0.5\n","  Downloading https://files.pythonhosted.org/packages/47/ac/8c4f6d058e87403643c49c00bc18c97b553b6d6d60a295a4b9168710a93d/torchgpipe-0.0.5.tar.gz\n","Collecting click==7.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/37/45185cb5abbc30d7257104c434fe0b07e5a195a6847506c074527aa599ec/Click-7.0-py2.py3-none-any.whl (81kB)\n","\u001b[K     |████████████████████████████████| 81kB 11.0MB/s \n","\u001b[?25hCollecting ruamel.yaml==0.16.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/90/ecff85a2e9c497e2fa7142496e10233556b5137db5bd46f3f3b006935ca8/ruamel.yaml-0.16.5-py2.py3-none-any.whl (123kB)\n","\u001b[K     |████████████████████████████████| 133kB 56.4MB/s \n","\u001b[?25hCollecting numpy==1.16.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/e7/6c780e612d245cca62bc3ba8e263038f7c144a96a54f877f3714a0e8427e/numpy-1.16.2-cp37-cp37m-manylinux1_x86_64.whl (17.3MB)\n","\u001b[K     |████████████████████████████████| 17.3MB 186kB/s \n","\u001b[?25hCollecting scipy==1.2.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/7e/5cee36eee5b3194687232f6150a89a38f784883c612db7f4da2ab190980d/scipy-1.2.1-cp37-cp37m-manylinux1_x86_64.whl (24.8MB)\n","\u001b[K     |████████████████████████████████| 24.8MB 1.5MB/s \n","\u001b[?25hCollecting scikit-learn==0.20.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/cc/a84e1748a2a70d0f3e081f56cefc634f3b57013b16faa6926d3a6f0598df/scikit_learn-0.20.3-cp37-cp37m-manylinux1_x86_64.whl (5.4MB)\n","\u001b[K     |████████████████████████████████| 5.4MB 33.4MB/s \n","\u001b[?25hCollecting gensim==3.7.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0d/94/05785b66fb679ca9c906bad8bf27f6e8ac00c3d41061aca5911dae997fd3/gensim-3.7.2-cp37-cp37m-manylinux1_x86_64.whl (24.2MB)\n","\u001b[K     |████████████████████████████████| 24.2MB 52.4MB/s \n","\u001b[?25hCollecting tqdm==4.31.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl (48kB)\n","\u001b[K     |████████████████████████████████| 51kB 7.5MB/s \n","\u001b[?25hCollecting joblib==0.13.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/c1/50a758e8247561e58cb87305b1e90b171b8c767b15b12a1734001f41d356/joblib-0.13.2-py2.py3-none-any.whl (278kB)\n","\u001b[K     |████████████████████████████████| 286kB 53.8MB/s \n","\u001b[?25hCollecting logzero==1.5.0\n","  Downloading https://files.pythonhosted.org/packages/97/24/27295d318ea8976b12cf9cc51d82e7c7129220f6a3cc9e3443df3be8afdb/logzero-1.5.0-py2.py3-none-any.whl\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.4.0->-r requirements.txt (line 2)) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchvision==0.4.0->-r requirements.txt (line 2)) (1.15.0)\n","Collecting ruamel.yaml.clib>=0.1.2; platform_python_implementation == \"CPython\" and python_version < \"3.8\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/8e/4d77d58d398bcf2d608d8e953fa739f975fbf2c882f5150ee41f544d638b/ruamel.yaml.clib-0.2.4-cp37-cp37m-manylinux1_x86_64.whl (546kB)\n","\u001b[K     |████████████████████████████████| 552kB 52.0MB/s \n","\u001b[?25hRequirement already satisfied: smart-open>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from gensim==3.7.2->-r requirements.txt (line 9)) (5.1.0)\n","Building wheels for collected packages: torchgpipe\n","  Building wheel for torchgpipe (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torchgpipe: filename=torchgpipe-0.0.5-cp37-none-any.whl size=37366 sha256=d472ee9a9749189e8e2a93485e6c8f46cf4efa911e2fcd210401e268acd881d9\n","  Stored in directory: /root/.cache/pip/wheels/3c/a9/48/2c05d34e5b2d6ac008536f5d940176fe0ea6b6dc792b97e121\n","Successfully built torchgpipe\n","\u001b[31mERROR: xarray 0.18.2 has requirement numpy>=1.17, but you'll have numpy 1.16.2 which is incompatible.\u001b[0m\n","\u001b[31mERROR: torchtext 0.10.0 has requirement torch==1.9.0, but you'll have torch 1.2.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow 2.5.0 has requirement numpy~=1.19.2, but you'll have numpy 1.16.2 which is incompatible.\u001b[0m\n","\u001b[31mERROR: spacy 2.2.4 has requirement tqdm<5.0.0,>=4.38.0, but you'll have tqdm 4.31.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: pyerfa 2.0.0 has requirement numpy>=1.17, but you'll have numpy 1.16.2 which is incompatible.\u001b[0m\n","\u001b[31mERROR: pyarrow 3.0.0 has requirement numpy>=1.16.6, but you'll have numpy 1.16.2 which is incompatible.\u001b[0m\n","\u001b[31mERROR: librosa 0.8.1 has requirement joblib>=0.14, but you'll have joblib 0.13.2 which is incompatible.\u001b[0m\n","\u001b[31mERROR: kapre 0.3.5 has requirement numpy>=1.18.5, but you'll have numpy 1.16.2 which is incompatible.\u001b[0m\n","\u001b[31mERROR: fbprophet 0.7.1 has requirement tqdm>=4.36.1, but you'll have tqdm 4.31.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: cupy-cuda101 9.1.0 has requirement numpy>=1.17, but you'll have numpy 1.16.2 which is incompatible.\u001b[0m\n","\u001b[31mERROR: astropy 4.2.1 has requirement numpy>=1.17, but you'll have numpy 1.16.2 which is incompatible.\u001b[0m\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Installing collected packages: numpy, torch, torchvision, torchgpipe, click, ruamel.yaml.clib, ruamel.yaml, scipy, scikit-learn, gensim, tqdm, joblib, logzero\n","  Found existing installation: numpy 1.19.5\n","    Uninstalling numpy-1.19.5:\n","      Successfully uninstalled numpy-1.19.5\n","  Found existing installation: torch 1.9.0+cu102\n","    Uninstalling torch-1.9.0+cu102:\n","      Successfully uninstalled torch-1.9.0+cu102\n","  Found existing installation: torchvision 0.10.0+cu102\n","    Uninstalling torchvision-0.10.0+cu102:\n","      Successfully uninstalled torchvision-0.10.0+cu102\n","  Found existing installation: click 7.1.2\n","    Uninstalling click-7.1.2:\n","      Successfully uninstalled click-7.1.2\n","  Found existing installation: scipy 1.4.1\n","    Uninstalling scipy-1.4.1:\n","      Successfully uninstalled scipy-1.4.1\n","  Found existing installation: scikit-learn 0.22.2.post1\n","    Uninstalling scikit-learn-0.22.2.post1:\n","      Successfully uninstalled scikit-learn-0.22.2.post1\n","  Found existing installation: gensim 3.6.0\n","    Uninstalling gensim-3.6.0:\n","      Successfully uninstalled gensim-3.6.0\n","  Found existing installation: tqdm 4.41.1\n","    Uninstalling tqdm-4.41.1:\n","      Successfully uninstalled tqdm-4.41.1\n","  Found existing installation: joblib 1.0.1\n","    Uninstalling joblib-1.0.1:\n","      Successfully uninstalled joblib-1.0.1\n","Successfully installed click-7.0 gensim-3.7.2 joblib-0.13.2 logzero-1.5.0 numpy-1.16.2 ruamel.yaml-0.16.5 ruamel.yaml.clib-0.2.4 scikit-learn-0.20.3 scipy-1.2.1 torch-1.2.0 torchgpipe-0.0.5 torchvision-0.4.0 tqdm-4.31.1\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"__quj3W6O1Np","executionInfo":{"status":"ok","timestamp":1624645075143,"user_tz":420,"elapsed":271,"user":{"displayName":"Eric Kong","photoUrl":"","userId":"07341482379174037459"}}},"source":["DATASET = \"PeTaL\"\n","MODEL = \"MATCH\""],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vXPqI8bqLnEd","executionInfo":{"status":"ok","timestamp":1624645092047,"user_tz":420,"elapsed":13126,"user":{"displayName":"Eric Kong","photoUrl":"","userId":"07341482379174037459"}},"outputId":"822ab53d-5c1f-42f6-e726-5abccbd3212d"},"source":["# Slightly modified preprocess.sh\n","\n","!python3 transform_data_PeTaL.py --dataset $DATASET\n","\n","!python preprocess.py \\\n","--text-path {DATASET}/train_texts.txt \\\n","--label-path {DATASET}/train_labels.txt \\\n","--vocab-path {DATASET}/vocab.npy \\\n","--emb-path {DATASET}/emb_init.npy \\\n","--w2v-model {DATASET}/{DATASET}.joint.emb \\\n","\n","!python preprocess.py \\\n","--text-path {DATASET}/test_texts.txt \\\n","--label-path {DATASET}/test_labels.txt \\\n","--vocab-path {DATASET}/vocab.npy \\"],"execution_count":7,"outputs":[{"output_type":"stream","text":["\u001b[32m[I 210625 18:18:07 preprocess:28]\u001b[39m Vocab Size: 26834\n","\u001b[32m[I 210625 18:18:07 preprocess:30]\u001b[39m Getting Dataset: PeTaL/train_texts.txt Max Length: 500\n","\u001b[32m[I 210625 18:18:07 preprocess:32]\u001b[39m Size of Samples: 900\n","\u001b[32m[I 210625 18:18:10 preprocess:28]\u001b[39m Vocab Size: 26834\n","\u001b[32m[I 210625 18:18:10 preprocess:30]\u001b[39m Getting Dataset: PeTaL/test_texts.txt Max Length: 500\n","\u001b[32m[I 210625 18:18:10 preprocess:32]\u001b[39m Size of Samples: 100\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b5nUH8rtLsyP","executionInfo":{"status":"ok","timestamp":1624642701638,"user_tz":420,"elapsed":992955,"user":{"displayName":"Eric Kong","photoUrl":"","userId":"07341482379174037459"}},"outputId":"82b1565a-c05a-443b-f1bf-37274c4b9f86"},"source":["# Slightly modified run_models.sh\n","\n","!PYTHONFAULTHANDLER=1 python main.py --data-cnf configure/datasets/{DATASET}.yaml --model-cnf configure/models/{MODEL}-{DATASET}.yaml --mode train --reg 1\n","!PYTHONFAULTHANDLER=1 python main.py --data-cnf configure/datasets/{DATASET}.yaml --model-cnf configure/models/{MODEL}-{DATASET}.yaml --mode eval\n","\n","!python evaluation.py \\\n","--results {DATASET}/results/{MODEL}-{DATASET}-labels.npy \\\n","--targets {DATASET}/test_labels.npy \\\n","--train-labels {DATASET}/train_labels.npy"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[32m[I 210625 17:21:49 main:32]\u001b[39m Model Name: MATCH\n","\u001b[32m[I 210625 17:21:49 main:35]\u001b[39m Loading Training and Validation Set\n","/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['absorb_and/or_filter_solids', 'chemically_break_down_inorganic_compounds', 'detox/purify', 'manage_environmental_disturbances_in_a_community', 'protect_from_fire', 'protect_from_gases', 'send_vibratory_signals'] will be ignored\n","  .format(sorted(unknown, key=str)))\n","/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['absorb_and/or_filter_solids', 'detox/purify', 'protect_from_gases'] will be ignored\n","  .format(sorted(unknown, key=str)))\n","\u001b[32m[I 210625 17:21:49 main:47]\u001b[39m Number of Labels: 124\n","\u001b[32m[I 210625 17:21:49 main:48]\u001b[39m Size of Training Set: 800\n","\u001b[32m[I 210625 17:21:49 main:49]\u001b[39m Size of Validation Set: 100\n","\u001b[32m[I 210625 17:21:49 main:66]\u001b[39m Number of Edges: 101\n","\u001b[32m[I 210625 17:21:49 main:68]\u001b[39m Training\n","\u001b[32m[I 210625 17:21:56 models:142]\u001b[39m SWA Initializing\n","\u001b[32m[I 210625 17:22:16 models:110]\u001b[39m 24 1024 train loss: 0.1276313 valid loss: 0.1360693 P@1: 0.53000 P@3: 0.32000 P@5: 0.26800 N@3: 0.37020 N@5: 0.38405 early stop: 0\n","\u001b[32m[I 210625 17:22:41 models:110]\u001b[39m 49 1024 train loss: 0.0285277 valid loss: 0.1314530 P@1: 0.57000 P@3: 0.42333 P@5: 0.34800 N@3: 0.46408 N@5: 0.47563 early stop: 0\n","\u001b[32m[I 210625 17:23:05 models:110]\u001b[39m 74 1024 train loss: 0.0110766 valid loss: 0.1356467 P@1: 0.59000 P@3: 0.45333 P@5: 0.37000 N@3: 0.49243 N@5: 0.50024 early stop: 0\n","\u001b[32m[I 210625 17:23:30 models:110]\u001b[39m 99 1024 train loss: 0.0075656 valid loss: 0.1446022 P@1: 0.64000 P@3: 0.45667 P@5: 0.37400 N@3: 0.50448 N@5: 0.51622 early stop: 0\n","\u001b[32m[I 210625 17:23:55 models:110]\u001b[39m 124 1024 train loss: 0.0020164 valid loss: 0.1570960 P@1: 0.63000 P@3: 0.48667 P@5: 0.37400 N@3: 0.52632 N@5: 0.51987 early stop: 0\n","\u001b[32m[I 210625 17:24:19 models:110]\u001b[39m 149 1024 train loss: 0.0009423 valid loss: 0.1713175 P@1: 0.63000 P@3: 0.50333 P@5: 0.37600 N@3: 0.53663 N@5: 0.52094 early stop: 0\n","\u001b[33m[W 210625 17:24:42 models:137]\u001b[39m Clipping gradients with total norm 0.05607 and max norm 0.00665\n","\u001b[32m[I 210625 17:24:44 models:110]\u001b[39m 174 1024 train loss: 0.0001085 valid loss: 0.1861679 P@1: 0.62000 P@3: 0.50333 P@5: 0.38800 N@3: 0.53429 N@5: 0.52783 early stop: 0\n","\u001b[33m[W 210625 17:24:56 models:137]\u001b[39m Clipping gradients with total norm 0.03656 and max norm 0.00386\n","\u001b[32m[I 210625 17:25:08 models:110]\u001b[39m 199 1024 train loss: 0.0000269 valid loss: 0.2010610 P@1: 0.61000 P@3: 0.51000 P@5: 0.38800 N@3: 0.53663 N@5: 0.52637 early stop: 1\n","\u001b[32m[I 210625 17:25:33 models:110]\u001b[39m 224 1024 train loss: 0.0000165 valid loss: 0.2151444 P@1: 0.59000 P@3: 0.51000 P@5: 0.39200 N@3: 0.53256 N@5: 0.52714 early stop: 2\n","\u001b[33m[W 210625 17:25:34 models:137]\u001b[39m Clipping gradients with total norm 0.01262 and max norm 0.00201\n","\u001b[33m[W 210625 17:25:41 models:137]\u001b[39m Clipping gradients with total norm 0.02998 and max norm 0.00178\n","\u001b[33m[W 210625 17:25:42 models:137]\u001b[39m Clipping gradients with total norm 0.00706 and max norm 0.00111\n","\u001b[32m[I 210625 17:25:58 models:110]\u001b[39m 249 1024 train loss: 0.0000118 valid loss: 0.2285607 P@1: 0.59000 P@3: 0.51000 P@5: 0.39600 N@3: 0.53317 N@5: 0.53148 early stop: 0\n","\u001b[33m[W 210625 17:25:58 models:137]\u001b[39m Clipping gradients with total norm 0.00874 and max norm 0.00122\n","\u001b[33m[W 210625 17:26:01 models:137]\u001b[39m Clipping gradients with total norm 0.02677 and max norm 0.00151\n","\u001b[33m[W 210625 17:26:03 models:137]\u001b[39m Clipping gradients with total norm 0.00743 and max norm 0.00116\n","\u001b[32m[I 210625 17:26:22 models:110]\u001b[39m 274 1024 train loss: 0.0000119 valid loss: 0.2410402 P@1: 0.60000 P@3: 0.51000 P@5: 0.39600 N@3: 0.53480 N@5: 0.53329 early stop: 0\n","\u001b[33m[W 210625 17:26:25 models:137]\u001b[39m Clipping gradients with total norm 0.02013 and max norm 0.0024\n","\u001b[33m[W 210625 17:26:45 models:137]\u001b[39m Clipping gradients with total norm 0.00504 and max norm 0.00096\n","\u001b[32m[I 210625 17:26:47 models:110]\u001b[39m 299 1024 train loss: 0.0000088 valid loss: 0.2528553 P@1: 0.60000 P@3: 0.51333 P@5: 0.39400 N@3: 0.53714 N@5: 0.53091 early stop: 1\n","\u001b[33m[W 210625 17:26:48 models:137]\u001b[39m Clipping gradients with total norm 0.01399 and max norm 0.00181\n","\u001b[32m[I 210625 17:27:11 models:110]\u001b[39m 324 1024 train loss: 0.0000062 valid loss: 0.2639082 P@1: 0.60000 P@3: 0.51333 P@5: 0.40000 N@3: 0.53714 N@5: 0.53514 early stop: 0\n","\u001b[33m[W 210625 17:27:22 models:137]\u001b[39m Clipping gradients with total norm 0.00863 and max norm 0.00069\n","\u001b[32m[I 210625 17:27:36 models:110]\u001b[39m 349 1024 train loss: 0.0000054 valid loss: 0.2743636 P@1: 0.61000 P@3: 0.51667 P@5: 0.40000 N@3: 0.54184 N@5: 0.53740 early stop: 0\n","\u001b[33m[W 210625 17:27:39 models:137]\u001b[39m Clipping gradients with total norm 0.01801 and max norm 0.00213\n","\u001b[33m[W 210625 17:27:41 models:137]\u001b[39m Clipping gradients with total norm 0.00522 and max norm 0.00089\n","\u001b[33m[W 210625 17:27:50 models:137]\u001b[39m Clipping gradients with total norm 0.01109 and max norm 0.00199\n","\u001b[33m[W 210625 17:27:56 models:137]\u001b[39m Clipping gradients with total norm 0.00933 and max norm 0.00105\n","\u001b[32m[I 210625 17:28:00 models:110]\u001b[39m 374 1024 train loss: 0.0000054 valid loss: 0.2842191 P@1: 0.62000 P@3: 0.52000 P@5: 0.39800 N@3: 0.54592 N@5: 0.53887 early stop: 0\n","\u001b[33m[W 210625 17:28:04 models:137]\u001b[39m Clipping gradients with total norm 0.00859 and max norm 0.00083\n","\u001b[33m[W 210625 17:28:16 models:137]\u001b[39m Clipping gradients with total norm 0.01209 and max norm 0.00125\n","\u001b[33m[W 210625 17:28:17 models:137]\u001b[39m Clipping gradients with total norm 0.01291 and max norm 0.0025\n","\u001b[33m[W 210625 17:28:20 models:137]\u001b[39m Clipping gradients with total norm 0.01728 and max norm 0.00267\n","\u001b[32m[I 210625 17:28:25 models:110]\u001b[39m 399 1024 train loss: 0.0015812 valid loss: 0.2901856 P@1: 0.62000 P@3: 0.51333 P@5: 0.39200 N@3: 0.54328 N@5: 0.53679 early stop: 1\n","\u001b[33m[W 210625 17:28:26 models:137]\u001b[39m Clipping gradients with total norm 9.05533 and max norm 1.0\n","\u001b[33m[W 210625 17:28:26 models:137]\u001b[39m Clipping gradients with total norm 11.91264 and max norm 1.0\n","\u001b[33m[W 210625 17:28:26 models:137]\u001b[39m Clipping gradients with total norm 10.11368 and max norm 1.0\n","\u001b[33m[W 210625 17:28:26 models:137]\u001b[39m Clipping gradients with total norm 5.73885 and max norm 1.0\n","\u001b[32m[I 210625 17:28:50 models:110]\u001b[39m 424 1024 train loss: 0.0731590 valid loss: 0.2621575 P@1: 0.60000 P@3: 0.49333 P@5: 0.39400 N@3: 0.52328 N@5: 0.53027 early stop: 2\n","\u001b[32m[I 210625 17:29:14 models:110]\u001b[39m 449 1024 train loss: 0.0045496 valid loss: 0.2507608 P@1: 0.58000 P@3: 0.48667 P@5: 0.37800 N@3: 0.51336 N@5: 0.51158 early stop: 3\n","\u001b[32m[I 210625 17:29:39 models:110]\u001b[39m 474 1024 train loss: 0.0007478 valid loss: 0.2460060 P@1: 0.57000 P@3: 0.48333 P@5: 0.36800 N@3: 0.50805 N@5: 0.50163 early stop: 4\n","\u001b[32m[I 210625 17:30:03 models:110]\u001b[39m 499 1024 train loss: 0.0001102 valid loss: 0.2438970 P@1: 0.58000 P@3: 0.47667 P@5: 0.35800 N@3: 0.50448 N@5: 0.49441 early stop: 5\n","\u001b[33m[W 210625 17:30:11 models:137]\u001b[39m Clipping gradients with total norm 0.03615 and max norm 0.00704\n","\u001b[33m[W 210625 17:30:13 models:137]\u001b[39m Clipping gradients with total norm 0.04554 and max norm 0.00444\n","\u001b[33m[W 210625 17:30:18 models:137]\u001b[39m Clipping gradients with total norm 0.02745 and max norm 0.00542\n","\u001b[33m[W 210625 17:30:20 models:137]\u001b[39m Clipping gradients with total norm 0.05009 and max norm 0.00366\n","\u001b[32m[I 210625 17:30:28 models:110]\u001b[39m 524 1024 train loss: 0.0000696 valid loss: 0.2430516 P@1: 0.59000 P@3: 0.46333 P@5: 0.35400 N@3: 0.49559 N@5: 0.49049 early stop: 6\n","\u001b[33m[W 210625 17:30:32 models:137]\u001b[39m Clipping gradients with total norm 0.01946 and max norm 0.00387\n","\u001b[33m[W 210625 17:30:52 models:137]\u001b[39m Clipping gradients with total norm 0.04385 and max norm 0.00554\n","\u001b[32m[I 210625 17:30:53 models:110]\u001b[39m 549 1024 train loss: 0.0000489 valid loss: 0.2432262 P@1: 0.57000 P@3: 0.45667 P@5: 0.35000 N@3: 0.48498 N@5: 0.48186 early stop: 7\n","\u001b[33m[W 210625 17:30:59 models:137]\u001b[39m Clipping gradients with total norm 0.02182 and max norm 0.00308\n","\u001b[32m[I 210625 17:31:17 models:110]\u001b[39m 574 1024 train loss: 0.0000335 valid loss: 0.2440199 P@1: 0.57000 P@3: 0.45333 P@5: 0.34800 N@3: 0.48202 N@5: 0.47875 early stop: 8\n","\u001b[32m[I 210625 17:31:42 models:110]\u001b[39m 599 1024 train loss: 0.0000222 valid loss: 0.2452846 P@1: 0.56000 P@3: 0.45000 P@5: 0.34000 N@3: 0.47794 N@5: 0.47044 early stop: 9\n","\u001b[32m[I 210625 17:32:06 models:110]\u001b[39m 624 1024 train loss: 0.0000174 valid loss: 0.2469588 P@1: 0.54000 P@3: 0.44000 P@5: 0.34400 N@3: 0.46824 N@5: 0.46949 early stop: 10\n","\u001b[33m[W 210625 17:32:07 models:137]\u001b[39m Clipping gradients with total norm 0.10549 and max norm 0.00225\n","\u001b[33m[W 210625 17:32:23 models:137]\u001b[39m Clipping gradients with total norm 0.02073 and max norm 0.00213\n","\u001b[32m[I 210625 17:32:31 models:110]\u001b[39m 649 1024 train loss: 0.0000151 valid loss: 0.2489256 P@1: 0.54000 P@3: 0.44333 P@5: 0.33800 N@3: 0.47059 N@5: 0.46523 early stop: 11\n","\u001b[33m[W 210625 17:32:36 models:137]\u001b[39m Clipping gradients with total norm 0.00905 and max norm 0.00171\n","\u001b[33m[W 210625 17:32:44 models:137]\u001b[39m Clipping gradients with total norm 0.00787 and max norm 0.00141\n","\u001b[33m[W 210625 17:32:48 models:137]\u001b[39m Clipping gradients with total norm 0.01419 and max norm 0.00265\n","\u001b[33m[W 210625 17:32:52 models:137]\u001b[39m Clipping gradients with total norm 0.01504 and max norm 0.00237\n","\u001b[32m[I 210625 17:32:55 models:110]\u001b[39m 674 1024 train loss: 0.0000146 valid loss: 0.2511272 P@1: 0.54000 P@3: 0.44333 P@5: 0.34000 N@3: 0.47059 N@5: 0.46693 early stop: 12\n","\u001b[33m[W 210625 17:32:58 models:137]\u001b[39m Clipping gradients with total norm 0.03535 and max norm 0.00638\n","\u001b[32m[I 210625 17:33:20 models:110]\u001b[39m 699 1024 train loss: 0.0113575 valid loss: 0.2442007 P@1: 0.53000 P@3: 0.44333 P@5: 0.34200 N@3: 0.46744 N@5: 0.46648 early stop: 13\n","\u001b[32m[I 210625 17:33:45 models:110]\u001b[39m 724 1024 train loss: 0.0033794 valid loss: 0.2416938 P@1: 0.53000 P@3: 0.44333 P@5: 0.34200 N@3: 0.46744 N@5: 0.46613 early stop: 14\n","\u001b[32m[I 210625 17:34:10 models:110]\u001b[39m 749 1024 train loss: 0.0000931 valid loss: 0.2403389 P@1: 0.54000 P@3: 0.44333 P@5: 0.34400 N@3: 0.46978 N@5: 0.46916 early stop: 15\n","\u001b[32m[I 210625 17:34:34 models:110]\u001b[39m 774 1024 train loss: 0.0000451 valid loss: 0.2396922 P@1: 0.54000 P@3: 0.44333 P@5: 0.34800 N@3: 0.46978 N@5: 0.47277 early stop: 16\n","\u001b[33m[W 210625 17:34:54 models:137]\u001b[39m Clipping gradients with total norm 0.00566 and max norm 0.00106\n","\u001b[32m[I 210625 17:34:59 models:110]\u001b[39m 799 1024 train loss: 0.0000298 valid loss: 0.2395504 P@1: 0.55000 P@3: 0.44667 P@5: 0.34800 N@3: 0.47386 N@5: 0.47455 early stop: 17\n","\u001b[33m[W 210625 17:35:02 models:137]\u001b[39m Clipping gradients with total norm 0.057 and max norm 0.00227\n","\u001b[32m[I 210625 17:35:23 models:110]\u001b[39m 824 1024 train loss: 0.0000232 valid loss: 0.2398903 P@1: 0.55000 P@3: 0.45000 P@5: 0.35400 N@3: 0.47621 N@5: 0.47979 early stop: 18\n","\u001b[33m[W 210625 17:35:38 models:137]\u001b[39m Clipping gradients with total norm 0.00276 and max norm 0.00055\n","\u001b[33m[W 210625 17:35:41 models:137]\u001b[39m Clipping gradients with total norm 0.00465 and max norm 0.00072\n","\u001b[32m[I 210625 17:35:48 models:110]\u001b[39m 849 1024 train loss: 0.0000169 valid loss: 0.2406016 P@1: 0.56000 P@3: 0.46000 P@5: 0.35600 N@3: 0.48559 N@5: 0.48436 early stop: 19\n","\u001b[33m[W 210625 17:35:49 models:137]\u001b[39m Clipping gradients with total norm 0.00657 and max norm 0.00082\n","\u001b[32m[I 210625 17:36:12 models:110]\u001b[39m 874 1024 train loss: 0.0000140 valid loss: 0.2415577 P@1: 0.56000 P@3: 0.46000 P@5: 0.35400 N@3: 0.48682 N@5: 0.48410 early stop: 20\n","\u001b[33m[W 210625 17:36:21 models:137]\u001b[39m Clipping gradients with total norm 0.00269 and max norm 0.00052\n","\u001b[33m[W 210625 17:36:23 models:137]\u001b[39m Clipping gradients with total norm 0.00575 and max norm 0.00062\n","\u001b[32m[I 210625 17:36:37 models:110]\u001b[39m 899 1024 train loss: 0.0000116 valid loss: 0.2427775 P@1: 0.56000 P@3: 0.46000 P@5: 0.35600 N@3: 0.48682 N@5: 0.48506 early stop: 21\n","\u001b[32m[I 210625 17:37:02 models:110]\u001b[39m 924 1024 train loss: 0.0000093 valid loss: 0.2442360 P@1: 0.57000 P@3: 0.46000 P@5: 0.35600 N@3: 0.48794 N@5: 0.48656 early stop: 22\n","\u001b[33m[W 210625 17:37:24 models:137]\u001b[39m Clipping gradients with total norm 0.01309 and max norm 0.00087\n","\u001b[32m[I 210625 17:37:26 models:110]\u001b[39m 949 1024 train loss: 0.0000089 valid loss: 0.2458636 P@1: 0.58000 P@3: 0.46667 P@5: 0.35600 N@3: 0.49498 N@5: 0.48896 early stop: 23\n","\u001b[33m[W 210625 17:37:50 models:137]\u001b[39m Clipping gradients with total norm 0.00262 and max norm 0.00036\n","\u001b[32m[I 210625 17:37:51 models:110]\u001b[39m 974 1024 train loss: 0.0000073 valid loss: 0.2476160 P@1: 0.58000 P@3: 0.46667 P@5: 0.36000 N@3: 0.49621 N@5: 0.49344 early stop: 24\n","\u001b[32m[I 210625 17:38:15 models:110]\u001b[39m 999 1024 train loss: 0.0000064 valid loss: 0.2494735 P@1: 0.58000 P@3: 0.46667 P@5: 0.36400 N@3: 0.49621 N@5: 0.49591 early stop: 25\n","\u001b[32m[I 210625 17:38:15 main:76]\u001b[39m Finish Training\n","\u001b[32m[I 210625 17:38:17 main:32]\u001b[39m Model Name: MATCH\n","\u001b[32m[I 210625 17:38:17 main:79]\u001b[39m Loading Test Set\n","\u001b[32m[I 210625 17:38:17 main:83]\u001b[39m Size of Test Set: 100\n","\u001b[32m[I 210625 17:38:17 main:85]\u001b[39m Predicting\n","\u001b[32m[I 210625 17:38:20 main:91]\u001b[39m Finish Predicting\n","Precision@1,3,5: 0.62 0.4633333333333333 0.362\n","nDCG@1,3,5: 0.62 0.5016206598625033 0.5088425243132072\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DYbr2McO78q7"},"source":["## Ablation study: Effect of adding MAG and MeSH labels to text\n","\n","Relevant to PeTaL Labeller Issue #53 (https://github.com/nasa-petal/PeTaL-labeller/issues/53).\n","\n","Note: to turn on/off including MAG fields of study and MeSH terms I mucked about with the source file ./transform_data_PeTaL.py. For future it may be more convenient to add CLI options specifying such."]},{"cell_type":"markdown","metadata":{"id":"znKf8GyxO3VC"},"source":["### Attempt with 1000 epochs, step = 10"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TKmpQEVan81b","executionInfo":{"status":"ok","timestamp":1624489115118,"user_tz":420,"elapsed":796012,"user":{"displayName":"Eric Kong","photoUrl":"","userId":"07341482379174037459"}},"outputId":"7b796293-8913-4242-e05d-3598cb1e0b94"},"source":["!PYTHONFAULTHANDLER=1 python main.py --data-cnf configure/datasets/PeTaL.yaml --model-cnf configure/models/MATCH-PeTaL.yaml --mode train --reg 1"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[32m[I 210623 22:45:20 main:32]\u001b[39m Model Name: MATCH\n","\u001b[32m[I 210623 22:45:20 main:35]\u001b[39m Loading Training and Validation Set\n","/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['absorb_and/or_filter_solids', 'chemically_break_down_inorganic_compounds', 'detox/purify', 'manage_environmental_disturbances_in_a_community', 'protect_from_fire', 'protect_from_gases', 'send_vibratory_signals'] will be ignored\n","  .format(sorted(unknown, key=str)))\n","/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['absorb_and/or_filter_solids', 'detox/purify', 'protect_from_gases'] will be ignored\n","  .format(sorted(unknown, key=str)))\n","\u001b[32m[I 210623 22:45:20 main:47]\u001b[39m Number of Labels: 124\n","\u001b[32m[I 210623 22:45:20 main:48]\u001b[39m Size of Training Set: 800\n","\u001b[32m[I 210623 22:45:20 main:49]\u001b[39m Size of Validation Set: 100\n","\u001b[32m[I 210623 22:45:20 main:66]\u001b[39m Number of Edges: 101\n","\u001b[32m[I 210623 22:45:20 main:68]\u001b[39m Training\n","\u001b[32m[I 210623 22:45:25 models:110]\u001b[39m 2 512 train loss: 0.2828015 valid loss: 0.1504736 P@1: 0.13000 P@3: 0.14333 P@5: 0.15200 N@3: 0.13877 N@5: 0.16712 early stop: 0\n","\u001b[32m[I 210623 22:45:26 models:142]\u001b[39m SWA Initializing\n","\u001b[32m[I 210623 22:45:27 models:110]\u001b[39m 4 1024 train loss: 0.1472312 valid loss: 0.1420337 P@1: 0.29000 P@3: 0.22000 P@5: 0.19200 N@3: 0.23735 N@5: 0.25692 early stop: 0\n","\u001b[32m[I 210623 22:45:29 models:110]\u001b[39m 7 512 train loss: 0.1413717 valid loss: 0.1408076 P@1: 0.29000 P@3: 0.21333 P@5: 0.19800 N@3: 0.23224 N@5: 0.26085 early stop: 0\n","\u001b[32m[I 210623 22:45:31 models:110]\u001b[39m 9 1024 train loss: 0.1415736 valid loss: 0.1401886 P@1: 0.29000 P@3: 0.22333 P@5: 0.19800 N@3: 0.23927 N@5: 0.26515 early stop: 0\n","\u001b[32m[I 210623 22:45:34 models:110]\u001b[39m 12 512 train loss: 0.1390659 valid loss: 0.1401599 P@1: 0.29000 P@3: 0.21667 P@5: 0.20600 N@3: 0.23642 N@5: 0.27087 early stop: 0\n","\u001b[32m[I 210623 22:45:35 models:110]\u001b[39m 14 1024 train loss: 0.1312067 valid loss: 0.1396144 P@1: 0.29000 P@3: 0.22333 P@5: 0.21600 N@3: 0.24358 N@5: 0.28302 early stop: 0\n","\u001b[32m[I 210623 22:45:38 models:110]\u001b[39m 17 512 train loss: 0.1092956 valid loss: 0.1391667 P@1: 0.29000 P@3: 0.24333 P@5: 0.22200 N@3: 0.25520 N@5: 0.28721 early stop: 0\n","\u001b[32m[I 210623 22:45:40 models:110]\u001b[39m 19 1024 train loss: 0.0877103 valid loss: 0.1384966 P@1: 0.31000 P@3: 0.29333 P@5: 0.23000 N@3: 0.29182 N@5: 0.30022 early stop: 0\n","\u001b[32m[I 210623 22:45:42 models:110]\u001b[39m 22 512 train loss: 0.0721453 valid loss: 0.1377209 P@1: 0.34000 P@3: 0.29333 P@5: 0.25400 N@3: 0.30143 N@5: 0.32670 early stop: 0\n","\u001b[32m[I 210623 22:45:44 models:110]\u001b[39m 24 1024 train loss: 0.0567940 valid loss: 0.1367496 P@1: 0.33000 P@3: 0.31333 P@5: 0.27200 N@3: 0.32186 N@5: 0.35008 early stop: 0\n","\u001b[32m[I 210623 22:45:46 models:110]\u001b[39m 27 512 train loss: 0.0477226 valid loss: 0.1355672 P@1: 0.38000 P@3: 0.35000 P@5: 0.27200 N@3: 0.35807 N@5: 0.36240 early stop: 0\n","\u001b[32m[I 210623 22:45:48 models:110]\u001b[39m 29 1024 train loss: 0.0372079 valid loss: 0.1344171 P@1: 0.39000 P@3: 0.35333 P@5: 0.28400 N@3: 0.36418 N@5: 0.37517 early stop: 0\n","\u001b[32m[I 210623 22:45:50 models:110]\u001b[39m 32 512 train loss: 0.0300921 valid loss: 0.1333332 P@1: 0.45000 P@3: 0.37000 P@5: 0.29200 N@3: 0.39010 N@5: 0.39525 early stop: 0\n","\u001b[32m[I 210623 22:45:52 models:110]\u001b[39m 34 1024 train loss: 0.0260908 valid loss: 0.1325421 P@1: 0.45000 P@3: 0.37667 P@5: 0.29200 N@3: 0.39805 N@5: 0.39936 early stop: 0\n","\u001b[32m[I 210623 22:45:54 models:110]\u001b[39m 37 512 train loss: 0.0230909 valid loss: 0.1316033 P@1: 0.51000 P@3: 0.37667 P@5: 0.29600 N@3: 0.41048 N@5: 0.41246 early stop: 0\n","\u001b[32m[I 210623 22:45:56 models:110]\u001b[39m 39 1024 train loss: 0.0229914 valid loss: 0.1307570 P@1: 0.54000 P@3: 0.39667 P@5: 0.31600 N@3: 0.43282 N@5: 0.44155 early stop: 0\n","\u001b[32m[I 210623 22:45:59 models:110]\u001b[39m 42 512 train loss: 0.0196234 valid loss: 0.1302794 P@1: 0.55000 P@3: 0.40000 P@5: 0.32200 N@3: 0.43690 N@5: 0.44568 early stop: 0\n","\u001b[32m[I 210623 22:46:00 models:110]\u001b[39m 44 1024 train loss: 0.0173238 valid loss: 0.1299516 P@1: 0.55000 P@3: 0.41667 P@5: 0.32600 N@3: 0.45109 N@5: 0.45361 early stop: 0\n","\u001b[32m[I 210623 22:46:03 models:110]\u001b[39m 47 512 train loss: 0.0149172 valid loss: 0.1297022 P@1: 0.56000 P@3: 0.42333 P@5: 0.33400 N@3: 0.46059 N@5: 0.46418 early stop: 0\n","\u001b[32m[I 210623 22:46:04 models:110]\u001b[39m 49 1024 train loss: 0.0133846 valid loss: 0.1293354 P@1: 0.58000 P@3: 0.44333 P@5: 0.35600 N@3: 0.47813 N@5: 0.48710 early stop: 0\n","\u001b[32m[I 210623 22:46:07 models:110]\u001b[39m 52 512 train loss: 0.0124470 valid loss: 0.1294590 P@1: 0.59000 P@3: 0.43667 P@5: 0.36200 N@3: 0.47693 N@5: 0.49474 early stop: 0\n","\u001b[32m[I 210623 22:46:09 models:110]\u001b[39m 54 1024 train loss: 0.0097449 valid loss: 0.1299029 P@1: 0.60000 P@3: 0.44333 P@5: 0.35600 N@3: 0.48450 N@5: 0.49327 early stop: 1\n","\u001b[32m[I 210623 22:46:11 models:110]\u001b[39m 57 512 train loss: 0.0087734 valid loss: 0.1301338 P@1: 0.61000 P@3: 0.45333 P@5: 0.37400 N@3: 0.49327 N@5: 0.50910 early stop: 0\n","\u001b[32m[I 210623 22:46:13 models:110]\u001b[39m 59 1024 train loss: 0.0099266 valid loss: 0.1303918 P@1: 0.62000 P@3: 0.45667 P@5: 0.38200 N@3: 0.49919 N@5: 0.51987 early stop: 0\n","\u001b[32m[I 210623 22:46:15 models:110]\u001b[39m 62 512 train loss: 0.0106908 valid loss: 0.1308213 P@1: 0.63000 P@3: 0.47667 P@5: 0.38400 N@3: 0.51684 N@5: 0.52587 early stop: 0\n","\u001b[32m[I 210623 22:46:17 models:110]\u001b[39m 64 1024 train loss: 0.0081676 valid loss: 0.1315280 P@1: 0.61000 P@3: 0.47667 P@5: 0.38800 N@3: 0.51338 N@5: 0.52654 early stop: 0\n","\u001b[32m[I 210623 22:46:19 models:110]\u001b[39m 67 512 train loss: 0.0071341 valid loss: 0.1324403 P@1: 0.62000 P@3: 0.48667 P@5: 0.39200 N@3: 0.52277 N@5: 0.53152 early stop: 0\n","\u001b[32m[I 210623 22:46:21 models:110]\u001b[39m 69 1024 train loss: 0.0078963 valid loss: 0.1326268 P@1: 0.63000 P@3: 0.48667 P@5: 0.39400 N@3: 0.52327 N@5: 0.53501 early stop: 0\n","\u001b[32m[I 210623 22:46:23 models:110]\u001b[39m 72 512 train loss: 0.0072790 valid loss: 0.1331672 P@1: 0.63000 P@3: 0.49667 P@5: 0.39600 N@3: 0.53031 N@5: 0.53790 early stop: 0\n","\u001b[32m[I 210623 22:46:25 models:110]\u001b[39m 74 1024 train loss: 0.0062511 valid loss: 0.1340420 P@1: 0.63000 P@3: 0.49333 P@5: 0.40000 N@3: 0.52981 N@5: 0.54217 early stop: 0\n","\u001b[32m[I 210623 22:46:27 models:110]\u001b[39m 77 512 train loss: 0.0046199 valid loss: 0.1350762 P@1: 0.62000 P@3: 0.49000 P@5: 0.40400 N@3: 0.52634 N@5: 0.54590 early stop: 0\n","\u001b[32m[I 210623 22:46:29 models:110]\u001b[39m 79 1024 train loss: 0.0031119 valid loss: 0.1365904 P@1: 0.62000 P@3: 0.49000 P@5: 0.41400 N@3: 0.52573 N@5: 0.55355 early stop: 0\n","\u001b[32m[I 210623 22:46:32 models:110]\u001b[39m 82 512 train loss: 0.0025406 valid loss: 0.1377593 P@1: 0.63000 P@3: 0.49000 P@5: 0.41600 N@3: 0.52869 N@5: 0.55749 early stop: 0\n","\u001b[32m[I 210623 22:46:34 models:110]\u001b[39m 84 1024 train loss: 0.0030506 valid loss: 0.1390296 P@1: 0.63000 P@3: 0.49667 P@5: 0.41600 N@3: 0.53399 N@5: 0.55918 early stop: 0\n","\u001b[32m[I 210623 22:46:36 models:110]\u001b[39m 87 512 train loss: 0.0038510 valid loss: 0.1404527 P@1: 0.63000 P@3: 0.50333 P@5: 0.41200 N@3: 0.53992 N@5: 0.55795 early stop: 1\n","\u001b[32m[I 210623 22:46:38 models:110]\u001b[39m 89 1024 train loss: 0.0025371 valid loss: 0.1419401 P@1: 0.63000 P@3: 0.51000 P@5: 0.41400 N@3: 0.54399 N@5: 0.56009 early stop: 0\n","\u001b[32m[I 210623 22:46:40 models:110]\u001b[39m 92 512 train loss: 0.0019458 valid loss: 0.1435195 P@1: 0.63000 P@3: 0.51667 P@5: 0.41200 N@3: 0.54930 N@5: 0.56006 early stop: 1\n","\u001b[32m[I 210623 22:46:42 models:110]\u001b[39m 94 1024 train loss: 0.0014409 valid loss: 0.1451182 P@1: 0.62000 P@3: 0.51667 P@5: 0.41200 N@3: 0.54706 N@5: 0.55839 early stop: 2\n","\u001b[32m[I 210623 22:46:44 models:110]\u001b[39m 97 512 train loss: 0.0016518 valid loss: 0.1463895 P@1: 0.62000 P@3: 0.51333 P@5: 0.41200 N@3: 0.54410 N@5: 0.55737 early stop: 3\n","\u001b[32m[I 210623 22:46:46 models:110]\u001b[39m 99 1024 train loss: 0.0025220 valid loss: 0.1477252 P@1: 0.62000 P@3: 0.51000 P@5: 0.41000 N@3: 0.54175 N@5: 0.55608 early stop: 4\n","\u001b[32m[I 210623 22:46:48 models:110]\u001b[39m 102 512 train loss: 0.0028910 valid loss: 0.1492322 P@1: 0.62000 P@3: 0.50333 P@5: 0.41000 N@3: 0.53706 N@5: 0.55629 early stop: 5\n","\u001b[32m[I 210623 22:46:50 models:110]\u001b[39m 104 1024 train loss: 0.0033150 valid loss: 0.1507863 P@1: 0.62000 P@3: 0.50000 P@5: 0.41200 N@3: 0.53471 N@5: 0.55772 early stop: 6\n","\u001b[32m[I 210623 22:46:52 models:110]\u001b[39m 107 512 train loss: 0.0025369 valid loss: 0.1522192 P@1: 0.62000 P@3: 0.50333 P@5: 0.41200 N@3: 0.53645 N@5: 0.55763 early stop: 7\n","\u001b[32m[I 210623 22:46:54 models:110]\u001b[39m 109 1024 train loss: 0.0037009 valid loss: 0.1537179 P@1: 0.62000 P@3: 0.50000 P@5: 0.41600 N@3: 0.53410 N@5: 0.56123 early stop: 0\n","\u001b[32m[I 210623 22:46:56 models:110]\u001b[39m 112 512 train loss: 0.0028273 valid loss: 0.1550791 P@1: 0.62000 P@3: 0.50333 P@5: 0.41800 N@3: 0.53645 N@5: 0.56363 early stop: 0\n","\u001b[32m[I 210623 22:46:58 models:110]\u001b[39m 114 1024 train loss: 0.0023991 valid loss: 0.1565566 P@1: 0.62000 P@3: 0.50667 P@5: 0.42200 N@3: 0.53879 N@5: 0.56765 early stop: 0\n","\u001b[32m[I 210623 22:47:00 models:110]\u001b[39m 117 512 train loss: 0.0044341 valid loss: 0.1580565 P@1: 0.62000 P@3: 0.51000 P@5: 0.42200 N@3: 0.54247 N@5: 0.56886 early stop: 0\n","\u001b[32m[I 210623 22:47:02 models:110]\u001b[39m 119 1024 train loss: 0.0055735 valid loss: 0.1592336 P@1: 0.62000 P@3: 0.50667 P@5: 0.41800 N@3: 0.54013 N@5: 0.56597 early stop: 1\n","\u001b[32m[I 210623 22:47:04 models:110]\u001b[39m 122 512 train loss: 0.0049872 valid loss: 0.1604301 P@1: 0.62000 P@3: 0.51000 P@5: 0.41600 N@3: 0.54247 N@5: 0.56527 early stop: 2\n","\u001b[32m[I 210623 22:47:06 models:110]\u001b[39m 124 1024 train loss: 0.0072934 valid loss: 0.1613507 P@1: 0.63000 P@3: 0.51333 P@5: 0.42000 N@3: 0.54655 N@5: 0.56983 early stop: 0\n","\u001b[32m[I 210623 22:47:08 models:110]\u001b[39m 127 512 train loss: 0.0075645 valid loss: 0.1624922 P@1: 0.63000 P@3: 0.51667 P@5: 0.42200 N@3: 0.54890 N@5: 0.57147 early stop: 0\n","\u001b[32m[I 210623 22:47:10 models:110]\u001b[39m 129 1024 train loss: 0.0060054 valid loss: 0.1638388 P@1: 0.63000 P@3: 0.51667 P@5: 0.42200 N@3: 0.54890 N@5: 0.57147 early stop: 1\n","\u001b[32m[I 210623 22:47:12 models:110]\u001b[39m 132 512 train loss: 0.0073010 valid loss: 0.1649591 P@1: 0.63000 P@3: 0.51333 P@5: 0.42000 N@3: 0.54594 N@5: 0.56867 early stop: 2\n","\u001b[32m[I 210623 22:47:14 models:110]\u001b[39m 134 1024 train loss: 0.0058112 valid loss: 0.1658425 P@1: 0.63000 P@3: 0.51333 P@5: 0.41800 N@3: 0.54797 N@5: 0.56911 early stop: 3\n","\u001b[32m[I 210623 22:47:16 models:110]\u001b[39m 137 512 train loss: 0.0047911 valid loss: 0.1667938 P@1: 0.63000 P@3: 0.51000 P@5: 0.41800 N@3: 0.54562 N@5: 0.56908 early stop: 4\n","\u001b[32m[I 210623 22:47:18 models:110]\u001b[39m 139 1024 train loss: 0.0037641 valid loss: 0.1675359 P@1: 0.63000 P@3: 0.51000 P@5: 0.41800 N@3: 0.54562 N@5: 0.56899 early stop: 5\n","\u001b[32m[I 210623 22:47:20 models:110]\u001b[39m 142 512 train loss: 0.0023915 valid loss: 0.1685021 P@1: 0.63000 P@3: 0.51333 P@5: 0.41800 N@3: 0.54797 N@5: 0.56922 early stop: 6\n","\u001b[32m[I 210623 22:47:22 models:110]\u001b[39m 144 1024 train loss: 0.0016447 valid loss: 0.1693847 P@1: 0.63000 P@3: 0.51000 P@5: 0.42000 N@3: 0.54562 N@5: 0.57101 early stop: 7\n","\u001b[32m[I 210623 22:47:25 models:110]\u001b[39m 147 512 train loss: 0.0012429 valid loss: 0.1703670 P@1: 0.64000 P@3: 0.51333 P@5: 0.42000 N@3: 0.54970 N@5: 0.57258 early stop: 0\n","\u001b[32m[I 210623 22:47:26 models:110]\u001b[39m 149 1024 train loss: 0.0010092 valid loss: 0.1713086 P@1: 0.64000 P@3: 0.51333 P@5: 0.42000 N@3: 0.54970 N@5: 0.57301 early stop: 0\n","\u001b[32m[I 210623 22:47:29 models:110]\u001b[39m 152 512 train loss: 0.0009248 valid loss: 0.1723873 P@1: 0.64000 P@3: 0.51333 P@5: 0.41800 N@3: 0.54970 N@5: 0.57164 early stop: 1\n","\u001b[32m[I 210623 22:47:30 models:110]\u001b[39m 154 1024 train loss: 0.0004845 valid loss: 0.1736306 P@1: 0.64000 P@3: 0.51000 P@5: 0.42000 N@3: 0.54735 N@5: 0.57242 early stop: 2\n","\u001b[32m[I 210623 22:47:33 models:110]\u001b[39m 157 512 train loss: 0.0002594 valid loss: 0.1747746 P@1: 0.64000 P@3: 0.51333 P@5: 0.42000 N@3: 0.54970 N@5: 0.57317 early stop: 0\n","\u001b[32m[I 210623 22:47:35 models:110]\u001b[39m 159 1024 train loss: 0.0001450 valid loss: 0.1759714 P@1: 0.64000 P@3: 0.51333 P@5: 0.42200 N@3: 0.54970 N@5: 0.57471 early stop: 0\n","\u001b[32m[I 210623 22:47:37 models:110]\u001b[39m 162 512 train loss: 0.0001126 valid loss: 0.1772127 P@1: 0.63000 P@3: 0.51333 P@5: 0.42200 N@3: 0.54797 N@5: 0.57283 early stop: 1\n","\u001b[32m[I 210623 22:47:39 models:110]\u001b[39m 164 1024 train loss: 0.0000751 valid loss: 0.1784562 P@1: 0.63000 P@3: 0.51333 P@5: 0.42200 N@3: 0.54797 N@5: 0.57283 early stop: 2\n","\u001b[32m[I 210623 22:47:41 models:110]\u001b[39m 167 512 train loss: 0.0000650 valid loss: 0.1796996 P@1: 0.63000 P@3: 0.51000 P@5: 0.42200 N@3: 0.54562 N@5: 0.57250 early stop: 3\n","\u001b[32m[I 210623 22:47:43 models:110]\u001b[39m 169 1024 train loss: 0.0000506 valid loss: 0.1809405 P@1: 0.63000 P@3: 0.51000 P@5: 0.42200 N@3: 0.54562 N@5: 0.57250 early stop: 4\n","\u001b[32m[I 210623 22:47:45 models:110]\u001b[39m 172 512 train loss: 0.0000471 valid loss: 0.1822005 P@1: 0.63000 P@3: 0.51000 P@5: 0.42200 N@3: 0.54562 N@5: 0.57236 early stop: 5\n","\u001b[32m[I 210623 22:47:47 models:110]\u001b[39m 174 1024 train loss: 0.0000430 valid loss: 0.1834400 P@1: 0.63000 P@3: 0.52000 P@5: 0.41800 N@3: 0.55205 N@5: 0.57000 early stop: 6\n","\u001b[32m[I 210623 22:47:49 models:110]\u001b[39m 177 512 train loss: 0.0000389 valid loss: 0.1846584 P@1: 0.63000 P@3: 0.51667 P@5: 0.41600 N@3: 0.54970 N@5: 0.56837 early stop: 7\n","\u001b[32m[I 210623 22:47:51 models:110]\u001b[39m 179 1024 train loss: 0.0000313 valid loss: 0.1858623 P@1: 0.63000 P@3: 0.51667 P@5: 0.41600 N@3: 0.54970 N@5: 0.56837 early stop: 8\n","\u001b[32m[I 210623 22:47:53 models:110]\u001b[39m 182 512 train loss: 0.0000328 valid loss: 0.1870710 P@1: 0.63000 P@3: 0.52000 P@5: 0.41800 N@3: 0.55215 N@5: 0.56949 early stop: 9\n","\u001b[32m[I 210623 22:47:55 models:110]\u001b[39m 184 1024 train loss: 0.0000371 valid loss: 0.1882880 P@1: 0.63000 P@3: 0.52000 P@5: 0.41800 N@3: 0.55215 N@5: 0.56949 early stop: 10\n","\u001b[32m[I 210623 22:47:57 models:110]\u001b[39m 187 512 train loss: 0.0000314 valid loss: 0.1894951 P@1: 0.63000 P@3: 0.51333 P@5: 0.41600 N@3: 0.54746 N@5: 0.56767 early stop: 11\n","\u001b[32m[I 210623 22:47:59 models:110]\u001b[39m 189 1024 train loss: 0.0000340 valid loss: 0.1906805 P@1: 0.63000 P@3: 0.51333 P@5: 0.41600 N@3: 0.54746 N@5: 0.56750 early stop: 12\n","\u001b[32m[I 210623 22:48:01 models:110]\u001b[39m 192 512 train loss: 0.0000268 valid loss: 0.1918564 P@1: 0.63000 P@3: 0.52000 P@5: 0.41600 N@3: 0.55215 N@5: 0.56804 early stop: 13\n","\u001b[33m[W 210623 22:48:02 models:137]\u001b[39m Clipping gradients with total norm 0.01622 and max norm 0.00252\n","\u001b[32m[I 210623 22:48:03 models:110]\u001b[39m 194 1024 train loss: 0.0000300 valid loss: 0.1930411 P@1: 0.63000 P@3: 0.52000 P@5: 0.41600 N@3: 0.55154 N@5: 0.56763 early stop: 14\n","\u001b[32m[I 210623 22:48:05 models:110]\u001b[39m 197 512 train loss: 0.0000250 valid loss: 0.1942319 P@1: 0.63000 P@3: 0.52000 P@5: 0.41800 N@3: 0.55154 N@5: 0.56880 early stop: 15\n","\u001b[32m[I 210623 22:48:07 models:110]\u001b[39m 199 1024 train loss: 0.0000260 valid loss: 0.1953984 P@1: 0.63000 P@3: 0.52000 P@5: 0.41800 N@3: 0.55154 N@5: 0.56880 early stop: 16\n","\u001b[32m[I 210623 22:48:09 models:110]\u001b[39m 202 512 train loss: 0.0000240 valid loss: 0.1965470 P@1: 0.64000 P@3: 0.52000 P@5: 0.41800 N@3: 0.55327 N@5: 0.57053 early stop: 17\n","\u001b[32m[I 210623 22:48:11 models:110]\u001b[39m 204 1024 train loss: 0.0000207 valid loss: 0.1976856 P@1: 0.64000 P@3: 0.52000 P@5: 0.41600 N@3: 0.55327 N@5: 0.56892 early stop: 18\n","\u001b[32m[I 210623 22:48:13 models:110]\u001b[39m 207 512 train loss: 0.0000213 valid loss: 0.1988186 P@1: 0.64000 P@3: 0.52333 P@5: 0.41600 N@3: 0.55562 N@5: 0.56915 early stop: 19\n","\u001b[32m[I 210623 22:48:15 models:110]\u001b[39m 209 1024 train loss: 0.0000190 valid loss: 0.1999494 P@1: 0.64000 P@3: 0.52333 P@5: 0.41600 N@3: 0.55642 N@5: 0.56996 early stop: 20\n","\u001b[32m[I 210623 22:48:17 models:110]\u001b[39m 212 512 train loss: 0.0000168 valid loss: 0.2010769 P@1: 0.63000 P@3: 0.52667 P@5: 0.41800 N@3: 0.55703 N@5: 0.57036 early stop: 21\n","\u001b[32m[I 210623 22:48:19 models:110]\u001b[39m 214 1024 train loss: 0.0000170 valid loss: 0.2021891 P@1: 0.63000 P@3: 0.52667 P@5: 0.41800 N@3: 0.55703 N@5: 0.57036 early stop: 22\n","\u001b[32m[I 210623 22:48:21 models:110]\u001b[39m 217 512 train loss: 0.0000179 valid loss: 0.2032884 P@1: 0.64000 P@3: 0.52667 P@5: 0.41400 N@3: 0.55815 N@5: 0.56816 early stop: 23\n","\u001b[32m[I 210623 22:48:23 models:110]\u001b[39m 219 1024 train loss: 0.0000162 valid loss: 0.2043686 P@1: 0.64000 P@3: 0.52667 P@5: 0.41400 N@3: 0.55815 N@5: 0.56836 early stop: 24\n","\u001b[32m[I 210623 22:48:25 models:110]\u001b[39m 222 512 train loss: 0.0000166 valid loss: 0.2054390 P@1: 0.64000 P@3: 0.52333 P@5: 0.41400 N@3: 0.55580 N@5: 0.56804 early stop: 25\n","\u001b[32m[I 210623 22:48:27 models:110]\u001b[39m 224 1024 train loss: 0.0000135 valid loss: 0.2065056 P@1: 0.64000 P@3: 0.52333 P@5: 0.41400 N@3: 0.55580 N@5: 0.56804 early stop: 26\n","\u001b[32m[I 210623 22:48:29 models:110]\u001b[39m 227 512 train loss: 0.0000149 valid loss: 0.2075673 P@1: 0.64000 P@3: 0.52333 P@5: 0.41400 N@3: 0.55580 N@5: 0.56818 early stop: 27\n","\u001b[32m[I 210623 22:48:31 models:110]\u001b[39m 229 1024 train loss: 0.0000156 valid loss: 0.2086188 P@1: 0.64000 P@3: 0.52333 P@5: 0.41400 N@3: 0.55580 N@5: 0.56818 early stop: 28\n","\u001b[32m[I 210623 22:48:33 models:110]\u001b[39m 232 512 train loss: 0.0000135 valid loss: 0.2096689 P@1: 0.64000 P@3: 0.52000 P@5: 0.41400 N@3: 0.55346 N@5: 0.56791 early stop: 29\n","\u001b[32m[I 210623 22:48:35 models:110]\u001b[39m 234 1024 train loss: 0.0000143 valid loss: 0.2107162 P@1: 0.64000 P@3: 0.52000 P@5: 0.41400 N@3: 0.55346 N@5: 0.56771 early stop: 30\n","\u001b[32m[I 210623 22:48:37 models:110]\u001b[39m 237 512 train loss: 0.0000129 valid loss: 0.2117573 P@1: 0.64000 P@3: 0.52000 P@5: 0.41400 N@3: 0.55407 N@5: 0.56815 early stop: 31\n","\u001b[32m[I 210623 22:48:39 models:110]\u001b[39m 239 1024 train loss: 0.0000139 valid loss: 0.2127829 P@1: 0.64000 P@3: 0.52333 P@5: 0.41400 N@3: 0.55703 N@5: 0.56889 early stop: 32\n","\u001b[32m[I 210623 22:48:42 models:110]\u001b[39m 242 512 train loss: 0.0000122 valid loss: 0.2137908 P@1: 0.64000 P@3: 0.52333 P@5: 0.41400 N@3: 0.55703 N@5: 0.56909 early stop: 33\n","\u001b[32m[I 210623 22:48:43 models:110]\u001b[39m 244 1024 train loss: 0.0000181 valid loss: 0.2147850 P@1: 0.64000 P@3: 0.52333 P@5: 0.41600 N@3: 0.55703 N@5: 0.57020 early stop: 34\n","\u001b[32m[I 210623 22:48:46 models:110]\u001b[39m 247 512 train loss: 0.0000153 valid loss: 0.2157747 P@1: 0.64000 P@3: 0.52333 P@5: 0.41600 N@3: 0.55703 N@5: 0.57020 early stop: 35\n","\u001b[32m[I 210623 22:48:47 models:110]\u001b[39m 249 1024 train loss: 0.0000151 valid loss: 0.2167583 P@1: 0.64000 P@3: 0.52333 P@5: 0.41600 N@3: 0.55703 N@5: 0.57020 early stop: 36\n","\u001b[32m[I 210623 22:48:50 models:110]\u001b[39m 252 512 train loss: 0.0000117 valid loss: 0.2177306 P@1: 0.64000 P@3: 0.52333 P@5: 0.41600 N@3: 0.55765 N@5: 0.57081 early stop: 37\n","\u001b[32m[I 210623 22:48:51 models:110]\u001b[39m 254 1024 train loss: 0.0000098 valid loss: 0.2186962 P@1: 0.64000 P@3: 0.52333 P@5: 0.41600 N@3: 0.55765 N@5: 0.57081 early stop: 38\n","\u001b[32m[I 210623 22:48:54 models:110]\u001b[39m 257 512 train loss: 0.0000120 valid loss: 0.2196659 P@1: 0.64000 P@3: 0.52667 P@5: 0.41600 N@3: 0.56061 N@5: 0.57164 early stop: 39\n","\u001b[32m[I 210623 22:48:55 models:110]\u001b[39m 259 1024 train loss: 0.0000103 valid loss: 0.2206358 P@1: 0.64000 P@3: 0.52667 P@5: 0.41600 N@3: 0.56061 N@5: 0.57164 early stop: 40\n","\u001b[32m[I 210623 22:48:58 models:110]\u001b[39m 262 512 train loss: 0.0000094 valid loss: 0.2215997 P@1: 0.64000 P@3: 0.52667 P@5: 0.41600 N@3: 0.56061 N@5: 0.57149 early stop: 41\n","\u001b[32m[I 210623 22:49:00 models:110]\u001b[39m 264 1024 train loss: 0.0000103 valid loss: 0.2225573 P@1: 0.64000 P@3: 0.52667 P@5: 0.41800 N@3: 0.56061 N@5: 0.57280 early stop: 42\n","\u001b[32m[I 210623 22:49:02 models:110]\u001b[39m 267 512 train loss: 0.0000085 valid loss: 0.2235059 P@1: 0.64000 P@3: 0.52667 P@5: 0.41800 N@3: 0.56061 N@5: 0.57280 early stop: 43\n","\u001b[32m[I 210623 22:49:04 models:110]\u001b[39m 269 1024 train loss: 0.0000078 valid loss: 0.2244453 P@1: 0.64000 P@3: 0.52667 P@5: 0.41800 N@3: 0.56061 N@5: 0.57280 early stop: 44\n","\u001b[32m[I 210623 22:49:06 models:110]\u001b[39m 272 512 train loss: 0.0000091 valid loss: 0.2253772 P@1: 0.64000 P@3: 0.53000 P@5: 0.41800 N@3: 0.56295 N@5: 0.57304 early stop: 45\n","\u001b[32m[I 210623 22:49:08 models:110]\u001b[39m 274 1024 train loss: 0.0000084 valid loss: 0.2263013 P@1: 0.64000 P@3: 0.53000 P@5: 0.41800 N@3: 0.56295 N@5: 0.57254 early stop: 46\n","\u001b[32m[I 210623 22:49:10 models:110]\u001b[39m 277 512 train loss: 0.0000087 valid loss: 0.2272167 P@1: 0.64000 P@3: 0.53000 P@5: 0.41800 N@3: 0.56295 N@5: 0.57254 early stop: 47\n","\u001b[33m[W 210623 22:49:11 models:137]\u001b[39m Clipping gradients with total norm 0.0063 and max norm 0.0012\n","\u001b[32m[I 210623 22:49:12 models:110]\u001b[39m 279 1024 train loss: 0.0000088 valid loss: 0.2281239 P@1: 0.64000 P@3: 0.53000 P@5: 0.41800 N@3: 0.56295 N@5: 0.57254 early stop: 48\n","\u001b[32m[I 210623 22:49:14 models:110]\u001b[39m 282 512 train loss: 0.0000082 valid loss: 0.2290301 P@1: 0.64000 P@3: 0.53667 P@5: 0.41800 N@3: 0.56765 N@5: 0.57319 early stop: 49\n","\u001b[32m[I 210623 22:49:16 models:110]\u001b[39m 284 1024 train loss: 0.0000071 valid loss: 0.2299288 P@1: 0.64000 P@3: 0.53667 P@5: 0.41800 N@3: 0.56765 N@5: 0.57319 early stop: 50\n","\u001b[32m[I 210623 22:49:18 models:110]\u001b[39m 287 512 train loss: 0.0000095 valid loss: 0.2308187 P@1: 0.64000 P@3: 0.53333 P@5: 0.41800 N@3: 0.56530 N@5: 0.57286 early stop: 51\n","\u001b[32m[I 210623 22:49:20 models:110]\u001b[39m 289 1024 train loss: 0.0000089 valid loss: 0.2317001 P@1: 0.64000 P@3: 0.53333 P@5: 0.41800 N@3: 0.56530 N@5: 0.57286 early stop: 52\n","\u001b[32m[I 210623 22:49:22 models:110]\u001b[39m 292 512 train loss: 0.0000071 valid loss: 0.2325743 P@1: 0.63000 P@3: 0.53333 P@5: 0.41800 N@3: 0.56357 N@5: 0.57113 early stop: 53\n","\u001b[32m[I 210623 22:49:24 models:110]\u001b[39m 294 1024 train loss: 0.0000076 valid loss: 0.2334415 P@1: 0.63000 P@3: 0.53333 P@5: 0.42000 N@3: 0.56357 N@5: 0.57244 early stop: 54\n","\u001b[32m[I 210623 22:49:26 models:110]\u001b[39m 297 512 train loss: 0.0000077 valid loss: 0.2343048 P@1: 0.64000 P@3: 0.53333 P@5: 0.42000 N@3: 0.56530 N@5: 0.57417 early stop: 55\n","\u001b[32m[I 210623 22:49:28 models:110]\u001b[39m 299 1024 train loss: 0.0000077 valid loss: 0.2351632 P@1: 0.64000 P@3: 0.53333 P@5: 0.41800 N@3: 0.56530 N@5: 0.57236 early stop: 56\n","\u001b[32m[I 210623 22:49:30 models:110]\u001b[39m 302 512 train loss: 0.0000062 valid loss: 0.2360134 P@1: 0.64000 P@3: 0.53333 P@5: 0.41800 N@3: 0.56530 N@5: 0.57236 early stop: 57\n","\u001b[32m[I 210623 22:49:32 models:110]\u001b[39m 304 1024 train loss: 0.0000078 valid loss: 0.2368590 P@1: 0.64000 P@3: 0.53333 P@5: 0.41800 N@3: 0.56530 N@5: 0.57236 early stop: 58\n","\u001b[32m[I 210623 22:49:34 models:110]\u001b[39m 307 512 train loss: 0.0000056 valid loss: 0.2377033 P@1: 0.64000 P@3: 0.53333 P@5: 0.41800 N@3: 0.56530 N@5: 0.57236 early stop: 59\n","\u001b[32m[I 210623 22:49:36 models:110]\u001b[39m 309 1024 train loss: 0.0000066 valid loss: 0.2385412 P@1: 0.64000 P@3: 0.53333 P@5: 0.42000 N@3: 0.56530 N@5: 0.57367 early stop: 60\n","\u001b[32m[I 210623 22:49:38 models:110]\u001b[39m 312 512 train loss: 0.0000066 valid loss: 0.2393714 P@1: 0.64000 P@3: 0.53333 P@5: 0.42000 N@3: 0.56530 N@5: 0.57367 early stop: 61\n","\u001b[32m[I 210623 22:49:40 models:110]\u001b[39m 314 1024 train loss: 0.0000068 valid loss: 0.2401965 P@1: 0.64000 P@3: 0.53333 P@5: 0.42000 N@3: 0.56530 N@5: 0.57367 early stop: 62\n","\u001b[32m[I 210623 22:49:42 models:110]\u001b[39m 317 512 train loss: 0.0000058 valid loss: 0.2410143 P@1: 0.64000 P@3: 0.53333 P@5: 0.42000 N@3: 0.56530 N@5: 0.57382 early stop: 63\n","\u001b[32m[I 210623 22:49:44 models:110]\u001b[39m 319 1024 train loss: 0.0000064 valid loss: 0.2418249 P@1: 0.64000 P@3: 0.53333 P@5: 0.42000 N@3: 0.56530 N@5: 0.57382 early stop: 64\n","\u001b[32m[I 210623 22:49:46 models:110]\u001b[39m 322 512 train loss: 0.0000061 valid loss: 0.2426304 P@1: 0.64000 P@3: 0.53333 P@5: 0.42000 N@3: 0.56530 N@5: 0.57382 early stop: 65\n","\u001b[32m[I 210623 22:49:48 models:110]\u001b[39m 324 1024 train loss: 0.0000057 valid loss: 0.2434291 P@1: 0.64000 P@3: 0.53333 P@5: 0.42000 N@3: 0.56530 N@5: 0.57382 early stop: 66\n","\u001b[32m[I 210623 22:49:50 models:110]\u001b[39m 327 512 train loss: 0.0000058 valid loss: 0.2442217 P@1: 0.64000 P@3: 0.53333 P@5: 0.42000 N@3: 0.56530 N@5: 0.57365 early stop: 67\n","\u001b[32m[I 210623 22:49:52 models:110]\u001b[39m 329 1024 train loss: 0.0000055 valid loss: 0.2450087 P@1: 0.64000 P@3: 0.53333 P@5: 0.42000 N@3: 0.56530 N@5: 0.57365 early stop: 68\n","\u001b[32m[I 210623 22:49:54 models:110]\u001b[39m 332 512 train loss: 0.0000052 valid loss: 0.2457911 P@1: 0.64000 P@3: 0.53333 P@5: 0.42000 N@3: 0.56592 N@5: 0.57409 early stop: 69\n","\u001b[32m[I 210623 22:49:56 models:110]\u001b[39m 334 1024 train loss: 0.0000052 valid loss: 0.2465654 P@1: 0.65000 P@3: 0.53333 P@5: 0.42000 N@3: 0.56765 N@5: 0.57534 early stop: 0\n","\u001b[32m[I 210623 22:49:58 models:110]\u001b[39m 337 512 train loss: 0.0000047 valid loss: 0.2473327 P@1: 0.65000 P@3: 0.53333 P@5: 0.42000 N@3: 0.56765 N@5: 0.57534 early stop: 1\n","\u001b[32m[I 210623 22:50:00 models:110]\u001b[39m 339 1024 train loss: 0.0000053 valid loss: 0.2480980 P@1: 0.65000 P@3: 0.53333 P@5: 0.42000 N@3: 0.56765 N@5: 0.57534 early stop: 2\n","\u001b[32m[I 210623 22:50:02 models:110]\u001b[39m 342 512 train loss: 0.0000061 valid loss: 0.2488641 P@1: 0.65000 P@3: 0.53333 P@5: 0.41800 N@3: 0.56765 N@5: 0.57368 early stop: 3\n","\u001b[32m[I 210623 22:50:04 models:110]\u001b[39m 344 1024 train loss: 0.0000048 valid loss: 0.2496265 P@1: 0.65000 P@3: 0.53333 P@5: 0.41800 N@3: 0.56765 N@5: 0.57368 early stop: 4\n","\u001b[32m[I 210623 22:50:06 models:110]\u001b[39m 347 512 train loss: 0.0000048 valid loss: 0.2503776 P@1: 0.66000 P@3: 0.53333 P@5: 0.41800 N@3: 0.56938 N@5: 0.57541 early stop: 0\n","\u001b[32m[I 210623 22:50:08 models:110]\u001b[39m 349 1024 train loss: 0.0000043 valid loss: 0.2511179 P@1: 0.66000 P@3: 0.53333 P@5: 0.41800 N@3: 0.56938 N@5: 0.57541 early stop: 1\n","\u001b[33m[W 210623 22:50:10 models:137]\u001b[39m Clipping gradients with total norm 0.00571 and max norm 0.00069\n","\u001b[32m[I 210623 22:50:10 models:110]\u001b[39m 352 512 train loss: 0.0000053 valid loss: 0.2518554 P@1: 0.66000 P@3: 0.53333 P@5: 0.41800 N@3: 0.56938 N@5: 0.57541 early stop: 2\n","\u001b[32m[I 210623 22:50:12 models:110]\u001b[39m 354 1024 train loss: 0.0000049 valid loss: 0.2525899 P@1: 0.65000 P@3: 0.53333 P@5: 0.41800 N@3: 0.56765 N@5: 0.57431 early stop: 3\n","\u001b[32m[I 210623 22:50:14 models:110]\u001b[39m 357 512 train loss: 0.0000045 valid loss: 0.2533217 P@1: 0.65000 P@3: 0.53333 P@5: 0.41800 N@3: 0.56765 N@5: 0.57431 early stop: 4\n","\u001b[33m[W 210623 22:50:15 models:137]\u001b[39m Clipping gradients with total norm 0.00227 and max norm 0.00041\n","\u001b[32m[I 210623 22:50:16 models:110]\u001b[39m 359 1024 train loss: 0.0000043 valid loss: 0.2540501 P@1: 0.65000 P@3: 0.53333 P@5: 0.41800 N@3: 0.56765 N@5: 0.57431 early stop: 5\n","\u001b[33m[W 210623 22:50:17 models:137]\u001b[39m Clipping gradients with total norm 0.00345 and max norm 0.00067\n","\u001b[32m[I 210623 22:50:19 models:110]\u001b[39m 362 512 train loss: 0.0000050 valid loss: 0.2547746 P@1: 0.65000 P@3: 0.53000 P@5: 0.41800 N@3: 0.56530 N@5: 0.57407 early stop: 6\n","\u001b[32m[I 210623 22:50:20 models:110]\u001b[39m 364 1024 train loss: 0.0000044 valid loss: 0.2554976 P@1: 0.65000 P@3: 0.53000 P@5: 0.41800 N@3: 0.56530 N@5: 0.57407 early stop: 7\n","\u001b[32m[I 210623 22:50:23 models:110]\u001b[39m 367 512 train loss: 0.0000039 valid loss: 0.2562153 P@1: 0.65000 P@3: 0.53000 P@5: 0.41800 N@3: 0.56592 N@5: 0.57469 early stop: 8\n","\u001b[32m[I 210623 22:50:24 models:110]\u001b[39m 369 1024 train loss: 0.0000046 valid loss: 0.2569265 P@1: 0.65000 P@3: 0.53000 P@5: 0.41800 N@3: 0.56592 N@5: 0.57469 early stop: 9\n","\u001b[32m[I 210623 22:50:27 models:110]\u001b[39m 372 512 train loss: 0.0000037 valid loss: 0.2576337 P@1: 0.65000 P@3: 0.53000 P@5: 0.41800 N@3: 0.56592 N@5: 0.57469 early stop: 10\n","\u001b[32m[I 210623 22:50:29 models:110]\u001b[39m 374 1024 train loss: 0.0000047 valid loss: 0.2583380 P@1: 0.65000 P@3: 0.53000 P@5: 0.41800 N@3: 0.56592 N@5: 0.57454 early stop: 11\n","\u001b[32m[I 210623 22:50:31 models:110]\u001b[39m 377 512 train loss: 0.0000038 valid loss: 0.2590371 P@1: 0.65000 P@3: 0.53667 P@5: 0.41800 N@3: 0.57061 N@5: 0.57501 early stop: 12\n","\u001b[32m[I 210623 22:50:33 models:110]\u001b[39m 379 1024 train loss: 0.0000040 valid loss: 0.2597322 P@1: 0.65000 P@3: 0.53667 P@5: 0.41800 N@3: 0.57061 N@5: 0.57501 early stop: 13\n","\u001b[32m[I 210623 22:50:35 models:110]\u001b[39m 382 512 train loss: 0.0000038 valid loss: 0.2604232 P@1: 0.65000 P@3: 0.53667 P@5: 0.41800 N@3: 0.57061 N@5: 0.57501 early stop: 14\n","\u001b[32m[I 210623 22:50:37 models:110]\u001b[39m 384 1024 train loss: 0.0000039 valid loss: 0.2611116 P@1: 0.65000 P@3: 0.53667 P@5: 0.41800 N@3: 0.57061 N@5: 0.57501 early stop: 15\n","\u001b[32m[I 210623 22:50:39 models:110]\u001b[39m 387 512 train loss: 0.0000036 valid loss: 0.2617992 P@1: 0.65000 P@3: 0.53667 P@5: 0.41800 N@3: 0.57061 N@5: 0.57501 early stop: 16\n","\u001b[32m[I 210623 22:50:41 models:110]\u001b[39m 389 1024 train loss: 0.0000068 valid loss: 0.2624799 P@1: 0.65000 P@3: 0.53667 P@5: 0.41800 N@3: 0.57122 N@5: 0.57545 early stop: 0\n","\u001b[32m[I 210623 22:50:43 models:110]\u001b[39m 392 512 train loss: 0.0000049 valid loss: 0.2631593 P@1: 0.65000 P@3: 0.53667 P@5: 0.41800 N@3: 0.57122 N@5: 0.57545 early stop: 1\n","\u001b[32m[I 210623 22:50:45 models:110]\u001b[39m 394 1024 train loss: 0.0000037 valid loss: 0.2638339 P@1: 0.65000 P@3: 0.53667 P@5: 0.41800 N@3: 0.57122 N@5: 0.57545 early stop: 2\n","\u001b[33m[W 210623 22:50:47 models:137]\u001b[39m Clipping gradients with total norm 0.00297 and max norm 0.00052\n","\u001b[32m[I 210623 22:50:47 models:110]\u001b[39m 397 512 train loss: 0.0000041 valid loss: 0.2645004 P@1: 0.65000 P@3: 0.53667 P@5: 0.41800 N@3: 0.57122 N@5: 0.57545 early stop: 3\n","\u001b[32m[I 210623 22:50:49 models:110]\u001b[39m 399 1024 train loss: 0.0000048 valid loss: 0.2651585 P@1: 0.65000 P@3: 0.53667 P@5: 0.41800 N@3: 0.57122 N@5: 0.57545 early stop: 4\n","\u001b[32m[I 210623 22:50:51 models:110]\u001b[39m 402 512 train loss: 0.0000040 valid loss: 0.2658070 P@1: 0.65000 P@3: 0.53667 P@5: 0.41800 N@3: 0.57122 N@5: 0.57545 early stop: 5\n","\u001b[33m[W 210623 22:50:52 models:137]\u001b[39m Clipping gradients with total norm 0.01376 and max norm 0.00075\n","\u001b[32m[I 210623 22:50:53 models:110]\u001b[39m 404 1024 train loss: 0.0000053 valid loss: 0.2664554 P@1: 0.65000 P@3: 0.53667 P@5: 0.41800 N@3: 0.57122 N@5: 0.57560 early stop: 0\n","\u001b[32m[I 210623 22:50:55 models:110]\u001b[39m 407 512 train loss: 0.0000041 valid loss: 0.2671040 P@1: 0.65000 P@3: 0.54000 P@5: 0.41800 N@3: 0.57357 N@5: 0.57593 early stop: 0\n","\u001b[32m[I 210623 22:50:57 models:110]\u001b[39m 409 1024 train loss: 0.0000035 valid loss: 0.2677508 P@1: 0.65000 P@3: 0.54333 P@5: 0.41600 N@3: 0.57592 N@5: 0.57494 early stop: 1\n","\u001b[33m[W 210623 22:50:58 models:137]\u001b[39m Clipping gradients with total norm 0.0062 and max norm 0.00092\n","\u001b[32m[I 210623 22:50:59 models:110]\u001b[39m 412 512 train loss: 0.0000040 valid loss: 0.2683933 P@1: 0.65000 P@3: 0.54333 P@5: 0.41600 N@3: 0.57592 N@5: 0.57494 early stop: 2\n","\u001b[32m[I 210623 22:51:01 models:110]\u001b[39m 414 1024 train loss: 0.0000037 valid loss: 0.2690327 P@1: 0.65000 P@3: 0.54333 P@5: 0.41800 N@3: 0.57592 N@5: 0.57625 early stop: 0\n","\u001b[32m[I 210623 22:51:03 models:110]\u001b[39m 417 512 train loss: 0.0000031 valid loss: 0.2696692 P@1: 0.65000 P@3: 0.54333 P@5: 0.41800 N@3: 0.57592 N@5: 0.57604 early stop: 1\n","\u001b[32m[I 210623 22:51:05 models:110]\u001b[39m 419 1024 train loss: 0.0000032 valid loss: 0.2703035 P@1: 0.65000 P@3: 0.54333 P@5: 0.41800 N@3: 0.57592 N@5: 0.57604 early stop: 2\n","\u001b[32m[I 210623 22:51:08 models:110]\u001b[39m 422 512 train loss: 0.0000033 valid loss: 0.2709363 P@1: 0.65000 P@3: 0.54333 P@5: 0.41800 N@3: 0.57592 N@5: 0.57604 early stop: 3\n","\u001b[32m[I 210623 22:51:09 models:110]\u001b[39m 424 1024 train loss: 0.0000029 valid loss: 0.2715654 P@1: 0.65000 P@3: 0.54333 P@5: 0.41800 N@3: 0.57592 N@5: 0.57604 early stop: 4\n","\u001b[32m[I 210623 22:51:12 models:110]\u001b[39m 427 512 train loss: 0.0000028 valid loss: 0.2721905 P@1: 0.65000 P@3: 0.54333 P@5: 0.41800 N@3: 0.57592 N@5: 0.57604 early stop: 5\n","\u001b[32m[I 210623 22:51:13 models:110]\u001b[39m 429 1024 train loss: 0.0000035 valid loss: 0.2728113 P@1: 0.65000 P@3: 0.54333 P@5: 0.41800 N@3: 0.57592 N@5: 0.57604 early stop: 6\n","\u001b[32m[I 210623 22:51:16 models:110]\u001b[39m 432 512 train loss: 0.0000032 valid loss: 0.2734306 P@1: 0.65000 P@3: 0.54000 P@5: 0.41800 N@3: 0.57357 N@5: 0.57572 early stop: 7\n","\u001b[32m[I 210623 22:51:18 models:110]\u001b[39m 434 1024 train loss: 0.0000029 valid loss: 0.2740459 P@1: 0.65000 P@3: 0.54000 P@5: 0.41800 N@3: 0.57357 N@5: 0.57572 early stop: 8\n","\u001b[32m[I 210623 22:51:20 models:110]\u001b[39m 437 512 train loss: 0.0000027 valid loss: 0.2746583 P@1: 0.65000 P@3: 0.53667 P@5: 0.41800 N@3: 0.57122 N@5: 0.57569 early stop: 9\n","\u001b[32m[I 210623 22:51:22 models:110]\u001b[39m 439 1024 train loss: 0.0000031 valid loss: 0.2752711 P@1: 0.65000 P@3: 0.53667 P@5: 0.41800 N@3: 0.57122 N@5: 0.57569 early stop: 10\n","\u001b[32m[I 210623 22:51:24 models:110]\u001b[39m 442 512 train loss: 0.0000026 valid loss: 0.2758806 P@1: 0.65000 P@3: 0.53667 P@5: 0.41800 N@3: 0.57122 N@5: 0.57569 early stop: 11\n","\u001b[32m[I 210623 22:51:26 models:110]\u001b[39m 444 1024 train loss: 0.0000023 valid loss: 0.2764850 P@1: 0.65000 P@3: 0.53667 P@5: 0.41800 N@3: 0.57122 N@5: 0.57569 early stop: 12\n","\u001b[32m[I 210623 22:51:28 models:110]\u001b[39m 447 512 train loss: 0.0000028 valid loss: 0.2770859 P@1: 0.65000 P@3: 0.54000 P@5: 0.42000 N@3: 0.57357 N@5: 0.57724 early stop: 0\n","\u001b[32m[I 210623 22:51:30 models:110]\u001b[39m 449 1024 train loss: 0.0000025 valid loss: 0.2776825 P@1: 0.65000 P@3: 0.54000 P@5: 0.42000 N@3: 0.57357 N@5: 0.57724 early stop: 1\n","\u001b[32m[I 210623 22:51:33 models:110]\u001b[39m 452 512 train loss: 0.0000028 valid loss: 0.2782739 P@1: 0.65000 P@3: 0.54000 P@5: 0.42000 N@3: 0.57357 N@5: 0.57724 early stop: 2\n","\u001b[32m[I 210623 22:51:35 models:110]\u001b[39m 454 1024 train loss: 0.0000022 valid loss: 0.2788621 P@1: 0.65000 P@3: 0.54000 P@5: 0.42000 N@3: 0.57357 N@5: 0.57724 early stop: 3\n","\u001b[32m[I 210623 22:51:38 models:110]\u001b[39m 457 512 train loss: 0.0000023 valid loss: 0.2794479 P@1: 0.65000 P@3: 0.53667 P@5: 0.42000 N@3: 0.57122 N@5: 0.57671 early stop: 4\n","\u001b[32m[I 210623 22:51:39 models:110]\u001b[39m 459 1024 train loss: 0.0000021 valid loss: 0.2800322 P@1: 0.65000 P@3: 0.53667 P@5: 0.42000 N@3: 0.57122 N@5: 0.57671 early stop: 5\n","\u001b[33m[W 210623 22:51:41 models:137]\u001b[39m Clipping gradients with total norm 0.00617 and max norm 0.0004\n","\u001b[32m[I 210623 22:51:42 models:110]\u001b[39m 462 512 train loss: 0.0000032 valid loss: 0.2806147 P@1: 0.65000 P@3: 0.53667 P@5: 0.42000 N@3: 0.57122 N@5: 0.57671 early stop: 6\n","\u001b[32m[I 210623 22:51:43 models:110]\u001b[39m 464 1024 train loss: 0.0000031 valid loss: 0.2811922 P@1: 0.65000 P@3: 0.53667 P@5: 0.42000 N@3: 0.57184 N@5: 0.57715 early stop: 7\n","\u001b[32m[I 210623 22:51:46 models:110]\u001b[39m 467 512 train loss: 0.0000033 valid loss: 0.2817638 P@1: 0.65000 P@3: 0.53667 P@5: 0.42000 N@3: 0.57184 N@5: 0.57715 early stop: 8\n","\u001b[32m[I 210623 22:51:48 models:110]\u001b[39m 469 1024 train loss: 0.0000028 valid loss: 0.2823306 P@1: 0.65000 P@3: 0.53667 P@5: 0.42000 N@3: 0.57184 N@5: 0.57715 early stop: 9\n","\u001b[32m[I 210623 22:51:50 models:110]\u001b[39m 472 512 train loss: 0.0000023 valid loss: 0.2828957 P@1: 0.65000 P@3: 0.53667 P@5: 0.42000 N@3: 0.57184 N@5: 0.57715 early stop: 10\n","\u001b[32m[I 210623 22:51:52 models:110]\u001b[39m 474 1024 train loss: 0.0000021 valid loss: 0.2834594 P@1: 0.65000 P@3: 0.53667 P@5: 0.42000 N@3: 0.57184 N@5: 0.57715 early stop: 11\n","\u001b[32m[I 210623 22:51:54 models:110]\u001b[39m 477 512 train loss: 0.0000022 valid loss: 0.2840212 P@1: 0.65000 P@3: 0.53667 P@5: 0.42000 N@3: 0.57184 N@5: 0.57715 early stop: 12\n","\u001b[33m[W 210623 22:51:55 models:137]\u001b[39m Clipping gradients with total norm 0.00407 and max norm 0.00033\n","\u001b[32m[I 210623 22:51:56 models:110]\u001b[39m 479 1024 train loss: 0.0000022 valid loss: 0.2845797 P@1: 0.65000 P@3: 0.53667 P@5: 0.42000 N@3: 0.57184 N@5: 0.57715 early stop: 13\n","\u001b[32m[I 210623 22:51:58 models:110]\u001b[39m 482 512 train loss: 0.0000023 valid loss: 0.2851352 P@1: 0.65000 P@3: 0.53667 P@5: 0.42000 N@3: 0.57245 N@5: 0.57791 early stop: 0\n","\u001b[32m[I 210623 22:52:00 models:110]\u001b[39m 484 1024 train loss: 0.0000018 valid loss: 0.2856855 P@1: 0.65000 P@3: 0.53667 P@5: 0.42000 N@3: 0.57245 N@5: 0.57812 early stop: 0\n","\u001b[32m[I 210623 22:52:02 models:110]\u001b[39m 487 512 train loss: 0.0000019 valid loss: 0.2862317 P@1: 0.65000 P@3: 0.53667 P@5: 0.42000 N@3: 0.57245 N@5: 0.57812 early stop: 1\n","\u001b[32m[I 210623 22:52:04 models:110]\u001b[39m 489 1024 train loss: 0.0000020 valid loss: 0.2867756 P@1: 0.66000 P@3: 0.53667 P@5: 0.42000 N@3: 0.57418 N@5: 0.57937 early stop: 0\n","\u001b[32m[I 210623 22:52:06 models:110]\u001b[39m 492 512 train loss: 0.0000021 valid loss: 0.2873172 P@1: 0.66000 P@3: 0.53667 P@5: 0.42000 N@3: 0.57418 N@5: 0.57937 early stop: 1\n","\u001b[32m[I 210623 22:52:08 models:110]\u001b[39m 494 1024 train loss: 0.0000023 valid loss: 0.2878577 P@1: 0.66000 P@3: 0.53667 P@5: 0.42000 N@3: 0.57418 N@5: 0.57937 early stop: 2\n","\u001b[33m[W 210623 22:52:09 models:137]\u001b[39m Clipping gradients with total norm 0.00128 and max norm 0.0002\n","\u001b[32m[I 210623 22:52:10 models:110]\u001b[39m 497 512 train loss: 0.0000022 valid loss: 0.2883967 P@1: 0.66000 P@3: 0.53667 P@5: 0.42000 N@3: 0.57418 N@5: 0.57922 early stop: 3\n","\u001b[32m[I 210623 22:52:12 models:110]\u001b[39m 499 1024 train loss: 0.0000026 valid loss: 0.2889325 P@1: 0.66000 P@3: 0.53667 P@5: 0.42000 N@3: 0.57418 N@5: 0.57922 early stop: 4\n","\u001b[32m[I 210623 22:52:15 models:110]\u001b[39m 502 512 train loss: 0.0000022 valid loss: 0.2894661 P@1: 0.66000 P@3: 0.53667 P@5: 0.42000 N@3: 0.57418 N@5: 0.57922 early stop: 5\n","\u001b[33m[W 210623 22:52:16 models:137]\u001b[39m Clipping gradients with total norm 0.00271 and max norm 0.00029\n","\u001b[32m[I 210623 22:52:16 models:110]\u001b[39m 504 1024 train loss: 0.0000023 valid loss: 0.2900000 P@1: 0.66000 P@3: 0.53667 P@5: 0.42000 N@3: 0.57418 N@5: 0.57922 early stop: 6\n","\u001b[32m[I 210623 22:52:19 models:110]\u001b[39m 507 512 train loss: 0.0000024 valid loss: 0.2905336 P@1: 0.66000 P@3: 0.53667 P@5: 0.42000 N@3: 0.57418 N@5: 0.57902 early stop: 7\n","\u001b[33m[W 210623 22:52:20 models:137]\u001b[39m Clipping gradients with total norm 0.00889 and max norm 0.0005\n","\u001b[32m[I 210623 22:52:20 models:110]\u001b[39m 509 1024 train loss: 0.0000036 valid loss: 0.2910640 P@1: 0.66000 P@3: 0.53667 P@5: 0.42000 N@3: 0.57418 N@5: 0.57902 early stop: 8\n","\u001b[32m[I 210623 22:52:23 models:110]\u001b[39m 512 512 train loss: 0.0000021 valid loss: 0.2915892 P@1: 0.66000 P@3: 0.53667 P@5: 0.42000 N@3: 0.57418 N@5: 0.57902 early stop: 9\n","\u001b[32m[I 210623 22:52:25 models:110]\u001b[39m 514 1024 train loss: 0.0000018 valid loss: 0.2921107 P@1: 0.66000 P@3: 0.53667 P@5: 0.42000 N@3: 0.57418 N@5: 0.57902 early stop: 10\n","\u001b[32m[I 210623 22:52:27 models:110]\u001b[39m 517 512 train loss: 0.0000032 valid loss: 0.2926301 P@1: 0.66000 P@3: 0.53667 P@5: 0.42000 N@3: 0.57418 N@5: 0.57902 early stop: 11\n","\u001b[32m[I 210623 22:52:29 models:110]\u001b[39m 519 1024 train loss: 0.0000023 valid loss: 0.2931475 P@1: 0.66000 P@3: 0.53667 P@5: 0.42000 N@3: 0.57418 N@5: 0.57902 early stop: 12\n","\u001b[32m[I 210623 22:52:31 models:110]\u001b[39m 522 512 train loss: 0.0000027 valid loss: 0.2936607 P@1: 0.66000 P@3: 0.53667 P@5: 0.42000 N@3: 0.57418 N@5: 0.57902 early stop: 13\n","\u001b[32m[I 210623 22:52:33 models:110]\u001b[39m 524 1024 train loss: 0.0000022 valid loss: 0.2941695 P@1: 0.66000 P@3: 0.53667 P@5: 0.41800 N@3: 0.57418 N@5: 0.57720 early stop: 14\n","\u001b[32m[I 210623 22:52:35 models:110]\u001b[39m 527 512 train loss: 0.0000023 valid loss: 0.2946807 P@1: 0.66000 P@3: 0.54000 P@5: 0.41800 N@3: 0.57653 N@5: 0.57744 early stop: 15\n","\u001b[32m[I 210623 22:52:37 models:110]\u001b[39m 529 1024 train loss: 0.0000021 valid loss: 0.2951911 P@1: 0.66000 P@3: 0.54000 P@5: 0.41800 N@3: 0.57653 N@5: 0.57744 early stop: 16\n","\u001b[32m[I 210623 22:52:39 models:110]\u001b[39m 532 512 train loss: 0.0000019 valid loss: 0.2956993 P@1: 0.66000 P@3: 0.54000 P@5: 0.41800 N@3: 0.57653 N@5: 0.57744 early stop: 17\n","\u001b[33m[W 210623 22:52:39 models:137]\u001b[39m Clipping gradients with total norm 0.00369 and max norm 0.00046\n","\u001b[32m[I 210623 22:52:41 models:110]\u001b[39m 534 1024 train loss: 0.0000029 valid loss: 0.2962058 P@1: 0.66000 P@3: 0.54000 P@5: 0.41800 N@3: 0.57653 N@5: 0.57723 early stop: 18\n","\u001b[32m[I 210623 22:52:43 models:110]\u001b[39m 537 512 train loss: 0.0000019 valid loss: 0.2967112 P@1: 0.66000 P@3: 0.54000 P@5: 0.41800 N@3: 0.57653 N@5: 0.57723 early stop: 19\n","\u001b[32m[I 210623 22:52:45 models:110]\u001b[39m 539 1024 train loss: 0.0000018 valid loss: 0.2972154 P@1: 0.66000 P@3: 0.54000 P@5: 0.41800 N@3: 0.57653 N@5: 0.57723 early stop: 20\n","\u001b[32m[I 210623 22:52:47 models:110]\u001b[39m 542 512 train loss: 0.0000017 valid loss: 0.2977179 P@1: 0.66000 P@3: 0.54000 P@5: 0.41800 N@3: 0.57653 N@5: 0.57723 early stop: 21\n","\u001b[32m[I 210623 22:52:49 models:110]\u001b[39m 544 1024 train loss: 0.0000019 valid loss: 0.2982158 P@1: 0.66000 P@3: 0.54000 P@5: 0.41800 N@3: 0.57653 N@5: 0.57723 early stop: 22\n","\u001b[33m[W 210623 22:52:50 models:137]\u001b[39m Clipping gradients with total norm 0.00489 and max norm 0.00068\n","\u001b[32m[I 210623 22:52:51 models:110]\u001b[39m 547 512 train loss: 0.0000023 valid loss: 0.2987110 P@1: 0.66000 P@3: 0.54333 P@5: 0.41800 N@3: 0.57888 N@5: 0.57747 early stop: 23\n","\u001b[32m[I 210623 22:52:53 models:110]\u001b[39m 549 1024 train loss: 0.0000016 valid loss: 0.2992055 P@1: 0.66000 P@3: 0.54333 P@5: 0.41800 N@3: 0.57888 N@5: 0.57747 early stop: 24\n","\u001b[32m[I 210623 22:52:55 models:110]\u001b[39m 552 512 train loss: 0.0000019 valid loss: 0.2996978 P@1: 0.66000 P@3: 0.54333 P@5: 0.41800 N@3: 0.57888 N@5: 0.57747 early stop: 25\n","\u001b[32m[I 210623 22:52:57 models:110]\u001b[39m 554 1024 train loss: 0.0000017 valid loss: 0.3001873 P@1: 0.66000 P@3: 0.54333 P@5: 0.41800 N@3: 0.57888 N@5: 0.57747 early stop: 26\n","\u001b[32m[I 210623 22:52:59 models:110]\u001b[39m 557 512 train loss: 0.0000017 valid loss: 0.3006743 P@1: 0.66000 P@3: 0.54333 P@5: 0.41800 N@3: 0.57888 N@5: 0.57747 early stop: 27\n","\u001b[33m[W 210623 22:53:01 models:137]\u001b[39m Clipping gradients with total norm 0.11791 and max norm 0.00026\n","\u001b[32m[I 210623 22:53:01 models:110]\u001b[39m 559 1024 train loss: 0.0000136 valid loss: 0.3011582 P@1: 0.66000 P@3: 0.54333 P@5: 0.41800 N@3: 0.57888 N@5: 0.57767 early stop: 28\n","\u001b[32m[I 210623 22:53:03 models:110]\u001b[39m 562 512 train loss: 0.0000014 valid loss: 0.3016402 P@1: 0.66000 P@3: 0.54333 P@5: 0.41800 N@3: 0.57888 N@5: 0.57767 early stop: 29\n","\u001b[32m[I 210623 22:53:05 models:110]\u001b[39m 564 1024 train loss: 0.0000013 valid loss: 0.3021198 P@1: 0.66000 P@3: 0.54333 P@5: 0.41800 N@3: 0.57888 N@5: 0.57767 early stop: 30\n","\u001b[32m[I 210623 22:53:07 models:110]\u001b[39m 567 512 train loss: 0.0000017 valid loss: 0.3025952 P@1: 0.66000 P@3: 0.54333 P@5: 0.41800 N@3: 0.57888 N@5: 0.57788 early stop: 31\n","\u001b[32m[I 210623 22:53:09 models:110]\u001b[39m 569 1024 train loss: 0.0000014 valid loss: 0.3030685 P@1: 0.66000 P@3: 0.54333 P@5: 0.41800 N@3: 0.57888 N@5: 0.57788 early stop: 32\n","\u001b[32m[I 210623 22:53:11 models:110]\u001b[39m 572 512 train loss: 0.0000013 valid loss: 0.3035404 P@1: 0.66000 P@3: 0.54333 P@5: 0.41800 N@3: 0.57888 N@5: 0.57788 early stop: 33\n","\u001b[33m[W 210623 22:53:12 models:137]\u001b[39m Clipping gradients with total norm 0.0036 and max norm 0.00021\n","\u001b[32m[I 210623 22:53:13 models:110]\u001b[39m 574 1024 train loss: 0.0000018 valid loss: 0.3040114 P@1: 0.67000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58295 N@5: 0.57803 early stop: 34\n","\u001b[32m[I 210623 22:53:15 models:110]\u001b[39m 577 512 train loss: 0.0000012 valid loss: 0.3044804 P@1: 0.67000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58295 N@5: 0.57803 early stop: 35\n","\u001b[32m[I 210623 22:53:17 models:110]\u001b[39m 579 1024 train loss: 0.0000014 valid loss: 0.3049478 P@1: 0.67000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58295 N@5: 0.57803 early stop: 36\n","\u001b[32m[I 210623 22:53:19 models:110]\u001b[39m 582 512 train loss: 0.0000015 valid loss: 0.3054131 P@1: 0.67000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58295 N@5: 0.57803 early stop: 37\n","\u001b[32m[I 210623 22:53:21 models:110]\u001b[39m 584 1024 train loss: 0.0000013 valid loss: 0.3058758 P@1: 0.68000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58469 N@5: 0.57928 early stop: 38\n","\u001b[32m[I 210623 22:53:23 models:110]\u001b[39m 587 512 train loss: 0.0000013 valid loss: 0.3063366 P@1: 0.68000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58469 N@5: 0.57928 early stop: 39\n","\u001b[32m[I 210623 22:53:25 models:110]\u001b[39m 589 1024 train loss: 0.0000011 valid loss: 0.3067954 P@1: 0.68000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58469 N@5: 0.57928 early stop: 40\n","\u001b[32m[I 210623 22:53:28 models:110]\u001b[39m 592 512 train loss: 0.0000011 valid loss: 0.3072532 P@1: 0.68000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58469 N@5: 0.57928 early stop: 41\n","\u001b[33m[W 210623 22:53:28 models:137]\u001b[39m Clipping gradients with total norm 0.00133 and max norm 0.00022\n","\u001b[32m[I 210623 22:53:29 models:110]\u001b[39m 594 1024 train loss: 0.0000015 valid loss: 0.3077081 P@1: 0.68000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58469 N@5: 0.57928 early stop: 42\n","\u001b[32m[I 210623 22:53:32 models:110]\u001b[39m 597 512 train loss: 0.0000014 valid loss: 0.3081619 P@1: 0.68000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58469 N@5: 0.57928 early stop: 43\n","\u001b[32m[I 210623 22:53:33 models:110]\u001b[39m 599 1024 train loss: 0.0000014 valid loss: 0.3086150 P@1: 0.68000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58469 N@5: 0.57928 early stop: 44\n","\u001b[32m[I 210623 22:53:36 models:110]\u001b[39m 602 512 train loss: 0.0000012 valid loss: 0.3090687 P@1: 0.68000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58469 N@5: 0.57928 early stop: 45\n","\u001b[32m[I 210623 22:53:37 models:110]\u001b[39m 604 1024 train loss: 0.0000011 valid loss: 0.3095207 P@1: 0.68000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58469 N@5: 0.57928 early stop: 46\n","\u001b[32m[I 210623 22:53:40 models:110]\u001b[39m 607 512 train loss: 0.0000013 valid loss: 0.3099716 P@1: 0.68000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58469 N@5: 0.57928 early stop: 47\n","\u001b[32m[I 210623 22:53:41 models:110]\u001b[39m 609 1024 train loss: 0.0000012 valid loss: 0.3104214 P@1: 0.68000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58469 N@5: 0.57928 early stop: 48\n","\u001b[32m[I 210623 22:53:44 models:110]\u001b[39m 612 512 train loss: 0.0000014 valid loss: 0.3108691 P@1: 0.68000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58469 N@5: 0.57928 early stop: 49\n","\u001b[32m[I 210623 22:53:46 models:110]\u001b[39m 614 1024 train loss: 0.0000012 valid loss: 0.3113133 P@1: 0.68000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58530 N@5: 0.57972 early stop: 0\n","\u001b[32m[I 210623 22:53:48 models:110]\u001b[39m 617 512 train loss: 0.0000013 valid loss: 0.3117547 P@1: 0.68000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58530 N@5: 0.57972 early stop: 1\n","\u001b[32m[I 210623 22:53:50 models:110]\u001b[39m 619 1024 train loss: 0.0000017 valid loss: 0.3121940 P@1: 0.68000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58530 N@5: 0.57972 early stop: 2\n","\u001b[32m[I 210623 22:53:52 models:110]\u001b[39m 622 512 train loss: 0.0000012 valid loss: 0.3126340 P@1: 0.68000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58530 N@5: 0.57972 early stop: 3\n","\u001b[32m[I 210623 22:53:54 models:110]\u001b[39m 624 1024 train loss: 0.0000013 valid loss: 0.3130727 P@1: 0.68000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58530 N@5: 0.57972 early stop: 4\n","\u001b[32m[I 210623 22:53:56 models:110]\u001b[39m 627 512 train loss: 0.0000012 valid loss: 0.3135090 P@1: 0.68000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58530 N@5: 0.57972 early stop: 5\n","\u001b[32m[I 210623 22:53:58 models:110]\u001b[39m 629 1024 train loss: 0.0000010 valid loss: 0.3139431 P@1: 0.68000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58530 N@5: 0.57972 early stop: 6\n","\u001b[32m[I 210623 22:54:00 models:110]\u001b[39m 632 512 train loss: 0.0000011 valid loss: 0.3143756 P@1: 0.68000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58530 N@5: 0.57972 early stop: 7\n","\u001b[32m[I 210623 22:54:02 models:110]\u001b[39m 634 1024 train loss: 0.0000010 valid loss: 0.3148082 P@1: 0.68000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58530 N@5: 0.57972 early stop: 8\n","\u001b[33m[W 210623 22:54:03 models:137]\u001b[39m Clipping gradients with total norm 0.00777 and max norm 0.00021\n","\u001b[32m[I 210623 22:54:04 models:110]\u001b[39m 637 512 train loss: 0.0000025 valid loss: 0.3152389 P@1: 0.68000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58530 N@5: 0.57972 early stop: 9\n","\u001b[32m[I 210623 22:54:06 models:110]\u001b[39m 639 1024 train loss: 0.0000016 valid loss: 0.3156649 P@1: 0.68000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58530 N@5: 0.57972 early stop: 10\n","\u001b[32m[I 210623 22:54:08 models:110]\u001b[39m 642 512 train loss: 0.0000018 valid loss: 0.3160875 P@1: 0.68000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58530 N@5: 0.57972 early stop: 11\n","\u001b[32m[I 210623 22:54:10 models:110]\u001b[39m 644 1024 train loss: 0.0000014 valid loss: 0.3165076 P@1: 0.68000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58530 N@5: 0.57972 early stop: 12\n","\u001b[33m[W 210623 22:54:11 models:137]\u001b[39m Clipping gradients with total norm 0.00676 and max norm 0.00098\n","\u001b[32m[I 210623 22:54:12 models:110]\u001b[39m 647 512 train loss: 0.0000021 valid loss: 0.3169299 P@1: 0.68000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58530 N@5: 0.57972 early stop: 13\n","\u001b[32m[I 210623 22:54:14 models:110]\u001b[39m 649 1024 train loss: 0.0000011 valid loss: 0.3173524 P@1: 0.68000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58530 N@5: 0.57972 early stop: 14\n","\u001b[33m[W 210623 22:54:15 models:137]\u001b[39m Clipping gradients with total norm 0.00134 and max norm 0.00021\n","\u001b[32m[I 210623 22:54:16 models:110]\u001b[39m 652 512 train loss: 0.0000011 valid loss: 0.3177740 P@1: 0.68000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58530 N@5: 0.57972 early stop: 15\n","\u001b[33m[W 210623 22:54:17 models:137]\u001b[39m Clipping gradients with total norm 0.00075 and max norm 0.00015\n","\u001b[32m[I 210623 22:54:18 models:110]\u001b[39m 654 1024 train loss: 0.0000011 valid loss: 0.3181939 P@1: 0.68000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58530 N@5: 0.57972 early stop: 16\n","\u001b[32m[I 210623 22:54:20 models:110]\u001b[39m 657 512 train loss: 0.0000011 valid loss: 0.3186129 P@1: 0.68000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58530 N@5: 0.57972 early stop: 17\n","\u001b[32m[I 210623 22:54:22 models:110]\u001b[39m 659 1024 train loss: 0.0000011 valid loss: 0.3190333 P@1: 0.68000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58530 N@5: 0.57972 early stop: 18\n","\u001b[33m[W 210623 22:54:24 models:137]\u001b[39m Clipping gradients with total norm 0.00282 and max norm 0.00055\n","\u001b[32m[I 210623 22:54:25 models:110]\u001b[39m 662 512 train loss: 0.0000015 valid loss: 0.3194519 P@1: 0.68000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58530 N@5: 0.57972 early stop: 19\n","\u001b[32m[I 210623 22:54:26 models:110]\u001b[39m 664 1024 train loss: 0.0000012 valid loss: 0.3198711 P@1: 0.69000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58703 N@5: 0.58146 early stop: 0\n","\u001b[32m[I 210623 22:54:29 models:110]\u001b[39m 667 512 train loss: 0.0000025 valid loss: 0.3202753 P@1: 0.69000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58703 N@5: 0.58146 early stop: 1\n","\u001b[32m[I 210623 22:54:30 models:110]\u001b[39m 669 1024 train loss: 0.0000017 valid loss: 0.3206733 P@1: 0.69000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58703 N@5: 0.58146 early stop: 2\n","\u001b[32m[I 210623 22:54:33 models:110]\u001b[39m 672 512 train loss: 0.0000011 valid loss: 0.3210721 P@1: 0.69000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58703 N@5: 0.58146 early stop: 3\n","\u001b[32m[I 210623 22:54:35 models:110]\u001b[39m 674 1024 train loss: 0.0000012 valid loss: 0.3214716 P@1: 0.69000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58642 N@5: 0.58101 early stop: 4\n","\u001b[33m[W 210623 22:54:35 models:137]\u001b[39m Clipping gradients with total norm 0.01255 and max norm 0.00049\n","\u001b[32m[I 210623 22:54:37 models:110]\u001b[39m 677 512 train loss: 0.0000026 valid loss: 0.3218718 P@1: 0.69000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58642 N@5: 0.58101 early stop: 5\n","\u001b[32m[I 210623 22:54:39 models:110]\u001b[39m 679 1024 train loss: 0.0000013 valid loss: 0.3222710 P@1: 0.69000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58642 N@5: 0.58101 early stop: 6\n","\u001b[33m[W 210623 22:54:40 models:137]\u001b[39m Clipping gradients with total norm 0.02269 and max norm 0.00035\n","\u001b[32m[I 210623 22:54:41 models:110]\u001b[39m 682 512 train loss: 0.0000063 valid loss: 0.3226704 P@1: 0.69000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58642 N@5: 0.58101 early stop: 7\n","\u001b[32m[I 210623 22:54:43 models:110]\u001b[39m 684 1024 train loss: 0.0000010 valid loss: 0.3230708 P@1: 0.69000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58642 N@5: 0.58101 early stop: 8\n","\u001b[33m[W 210623 22:54:43 models:137]\u001b[39m Clipping gradients with total norm 0.00211 and max norm 0.00041\n","\u001b[33m[W 210623 22:54:44 models:137]\u001b[39m Clipping gradients with total norm 0.00253 and max norm 0.00032\n","\u001b[32m[I 210623 22:54:45 models:110]\u001b[39m 687 512 train loss: 0.0000018 valid loss: 0.3234687 P@1: 0.69000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58642 N@5: 0.58101 early stop: 9\n","\u001b[32m[I 210623 22:54:47 models:110]\u001b[39m 689 1024 train loss: 0.0000016 valid loss: 0.3238595 P@1: 0.69000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58642 N@5: 0.58101 early stop: 10\n","\u001b[33m[W 210623 22:54:49 models:137]\u001b[39m Clipping gradients with total norm 0.00471 and max norm 0.0002\n","\u001b[32m[I 210623 22:54:49 models:110]\u001b[39m 692 512 train loss: 0.0000013 valid loss: 0.3242471 P@1: 0.69000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58642 N@5: 0.58101 early stop: 11\n","\u001b[32m[I 210623 22:54:51 models:110]\u001b[39m 694 1024 train loss: 0.0000010 valid loss: 0.3246338 P@1: 0.69000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58642 N@5: 0.58101 early stop: 12\n","\u001b[32m[I 210623 22:54:53 models:110]\u001b[39m 697 512 train loss: 0.0000009 valid loss: 0.3250206 P@1: 0.69000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58642 N@5: 0.58101 early stop: 13\n","\u001b[32m[I 210623 22:54:55 models:110]\u001b[39m 699 1024 train loss: 0.0000010 valid loss: 0.3254062 P@1: 0.69000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58642 N@5: 0.58101 early stop: 14\n","\u001b[32m[I 210623 22:54:57 models:110]\u001b[39m 702 512 train loss: 0.0000010 valid loss: 0.3257908 P@1: 0.69000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58642 N@5: 0.58101 early stop: 15\n","\u001b[33m[W 210623 22:54:58 models:137]\u001b[39m Clipping gradients with total norm 0.00124 and max norm 0.00023\n","\u001b[32m[I 210623 22:54:59 models:110]\u001b[39m 704 1024 train loss: 0.0000010 valid loss: 0.3261772 P@1: 0.69000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58642 N@5: 0.58101 early stop: 16\n","\u001b[32m[I 210623 22:55:01 models:110]\u001b[39m 707 512 train loss: 0.0000009 valid loss: 0.3265645 P@1: 0.69000 P@3: 0.55000 P@5: 0.41600 N@3: 0.58876 N@5: 0.58134 early stop: 17\n","\u001b[32m[I 210623 22:55:03 models:110]\u001b[39m 709 1024 train loss: 0.0000012 valid loss: 0.3269507 P@1: 0.69000 P@3: 0.55000 P@5: 0.41600 N@3: 0.58876 N@5: 0.58134 early stop: 18\n","\u001b[32m[I 210623 22:55:05 models:110]\u001b[39m 712 512 train loss: 0.0000009 valid loss: 0.3273362 P@1: 0.69000 P@3: 0.55000 P@5: 0.41600 N@3: 0.58876 N@5: 0.58134 early stop: 19\n","\u001b[32m[I 210623 22:55:07 models:110]\u001b[39m 714 1024 train loss: 0.0000010 valid loss: 0.3277192 P@1: 0.69000 P@3: 0.55000 P@5: 0.41600 N@3: 0.58876 N@5: 0.58134 early stop: 20\n","\u001b[32m[I 210623 22:55:09 models:110]\u001b[39m 717 512 train loss: 0.0000010 valid loss: 0.3280990 P@1: 0.69000 P@3: 0.55000 P@5: 0.41600 N@3: 0.58938 N@5: 0.58178 early stop: 0\n","\u001b[32m[I 210623 22:55:11 models:110]\u001b[39m 719 1024 train loss: 0.0000008 valid loss: 0.3284765 P@1: 0.69000 P@3: 0.55000 P@5: 0.41600 N@3: 0.58938 N@5: 0.58178 early stop: 1\n","\u001b[33m[W 210623 22:55:12 models:137]\u001b[39m Clipping gradients with total norm 0.0232 and max norm 0.00026\n","\u001b[32m[I 210623 22:55:13 models:110]\u001b[39m 722 512 train loss: 0.0000062 valid loss: 0.3288503 P@1: 0.69000 P@3: 0.55000 P@5: 0.41600 N@3: 0.58938 N@5: 0.58178 early stop: 2\n","\u001b[33m[W 210623 22:55:15 models:137]\u001b[39m Clipping gradients with total norm 0.00078 and max norm 0.00013\n","\u001b[32m[I 210623 22:55:15 models:110]\u001b[39m 724 1024 train loss: 0.0000010 valid loss: 0.3292218 P@1: 0.69000 P@3: 0.55000 P@5: 0.41600 N@3: 0.58938 N@5: 0.58178 early stop: 3\n","\u001b[32m[I 210623 22:55:17 models:110]\u001b[39m 727 512 train loss: 0.0000007 valid loss: 0.3295925 P@1: 0.69000 P@3: 0.55000 P@5: 0.41600 N@3: 0.58938 N@5: 0.58178 early stop: 4\n","\u001b[32m[I 210623 22:55:19 models:110]\u001b[39m 729 1024 train loss: 0.0000009 valid loss: 0.3299631 P@1: 0.69000 P@3: 0.55000 P@5: 0.41600 N@3: 0.58938 N@5: 0.58178 early stop: 5\n","\u001b[33m[W 210623 22:55:20 models:137]\u001b[39m Clipping gradients with total norm 0.0013 and max norm 0.00024\n","\u001b[32m[I 210623 22:55:22 models:110]\u001b[39m 732 512 train loss: 0.0000009 valid loss: 0.3303329 P@1: 0.69000 P@3: 0.55000 P@5: 0.41600 N@3: 0.58938 N@5: 0.58178 early stop: 6\n","\u001b[32m[I 210623 22:55:23 models:110]\u001b[39m 734 1024 train loss: 0.0000009 valid loss: 0.3307018 P@1: 0.69000 P@3: 0.55000 P@5: 0.41600 N@3: 0.58938 N@5: 0.58178 early stop: 7\n","\u001b[32m[I 210623 22:55:26 models:110]\u001b[39m 737 512 train loss: 0.0000009 valid loss: 0.3310724 P@1: 0.69000 P@3: 0.55000 P@5: 0.41600 N@3: 0.58938 N@5: 0.58178 early stop: 8\n","\u001b[32m[I 210623 22:55:27 models:110]\u001b[39m 739 1024 train loss: 0.0000008 valid loss: 0.3314440 P@1: 0.69000 P@3: 0.55000 P@5: 0.41600 N@3: 0.58938 N@5: 0.58178 early stop: 9\n","\u001b[33m[W 210623 22:55:28 models:137]\u001b[39m Clipping gradients with total norm 0.00171 and max norm 0.00018\n","\u001b[32m[I 210623 22:55:30 models:110]\u001b[39m 742 512 train loss: 0.0000011 valid loss: 0.3318154 P@1: 0.69000 P@3: 0.55000 P@5: 0.41600 N@3: 0.58938 N@5: 0.58178 early stop: 10\n","\u001b[32m[I 210623 22:55:32 models:110]\u001b[39m 744 1024 train loss: 0.0000008 valid loss: 0.3321860 P@1: 0.69000 P@3: 0.55000 P@5: 0.41600 N@3: 0.58938 N@5: 0.58178 early stop: 11\n","\u001b[32m[I 210623 22:55:34 models:110]\u001b[39m 747 512 train loss: 0.0000007 valid loss: 0.3325552 P@1: 0.69000 P@3: 0.55000 P@5: 0.41600 N@3: 0.58938 N@5: 0.58178 early stop: 12\n","\u001b[32m[I 210623 22:55:36 models:110]\u001b[39m 749 1024 train loss: 0.0000007 valid loss: 0.3329221 P@1: 0.69000 P@3: 0.55000 P@5: 0.41600 N@3: 0.58938 N@5: 0.58178 early stop: 13\n","\u001b[32m[I 210623 22:55:38 models:110]\u001b[39m 752 512 train loss: 0.0000009 valid loss: 0.3332855 P@1: 0.69000 P@3: 0.55000 P@5: 0.41600 N@3: 0.58938 N@5: 0.58178 early stop: 14\n","\u001b[32m[I 210623 22:55:40 models:110]\u001b[39m 754 1024 train loss: 0.0000007 valid loss: 0.3336479 P@1: 0.69000 P@3: 0.55000 P@5: 0.41600 N@3: 0.58938 N@5: 0.58178 early stop: 15\n","\u001b[32m[I 210623 22:55:42 models:110]\u001b[39m 757 512 train loss: 0.0000008 valid loss: 0.3340096 P@1: 0.69000 P@3: 0.55000 P@5: 0.41600 N@3: 0.58938 N@5: 0.58178 early stop: 16\n","\u001b[33m[W 210623 22:55:43 models:137]\u001b[39m Clipping gradients with total norm 0.00302 and max norm 0.00019\n","\u001b[33m[W 210623 22:55:44 models:137]\u001b[39m Clipping gradients with total norm 0.00573 and max norm 0.00038\n","\u001b[32m[I 210623 22:55:44 models:110]\u001b[39m 759 1024 train loss: 0.0000018 valid loss: 0.3343708 P@1: 0.69000 P@3: 0.55000 P@5: 0.41600 N@3: 0.58938 N@5: 0.58178 early stop: 17\n","\u001b[32m[I 210623 22:55:46 models:110]\u001b[39m 762 512 train loss: 0.0000007 valid loss: 0.3347322 P@1: 0.69000 P@3: 0.55000 P@5: 0.41600 N@3: 0.58938 N@5: 0.58178 early stop: 18\n","\u001b[33m[W 210623 22:55:47 models:137]\u001b[39m Clipping gradients with total norm 0.00489 and max norm 0.00013\n","\u001b[32m[I 210623 22:55:48 models:110]\u001b[39m 764 1024 train loss: 0.0000011 valid loss: 0.3350925 P@1: 0.69000 P@3: 0.55000 P@5: 0.41600 N@3: 0.58938 N@5: 0.58178 early stop: 19\n","\u001b[32m[I 210623 22:55:50 models:110]\u001b[39m 767 512 train loss: 0.0000007 valid loss: 0.3354509 P@1: 0.69000 P@3: 0.55000 P@5: 0.41600 N@3: 0.58938 N@5: 0.58178 early stop: 20\n","\u001b[33m[W 210623 22:55:51 models:137]\u001b[39m Clipping gradients with total norm 0.01565 and max norm 0.00013\n","\u001b[32m[I 210623 22:55:52 models:110]\u001b[39m 769 1024 train loss: 0.0000046 valid loss: 0.3358077 P@1: 0.69000 P@3: 0.55000 P@5: 0.41600 N@3: 0.58938 N@5: 0.58178 early stop: 21\n","\u001b[33m[W 210623 22:55:53 models:137]\u001b[39m Clipping gradients with total norm 0.00148 and max norm 0.00027\n","\u001b[32m[I 210623 22:55:54 models:110]\u001b[39m 772 512 train loss: 0.0000009 valid loss: 0.3361631 P@1: 0.69000 P@3: 0.55000 P@5: 0.41400 N@3: 0.58938 N@5: 0.58047 early stop: 22\n","\u001b[33m[W 210623 22:55:55 models:137]\u001b[39m Clipping gradients with total norm 0.00106 and max norm 0.00014\n","\u001b[32m[I 210623 22:55:56 models:110]\u001b[39m 774 1024 train loss: 0.0000008 valid loss: 0.3365172 P@1: 0.69000 P@3: 0.55000 P@5: 0.41400 N@3: 0.58938 N@5: 0.58047 early stop: 23\n","\u001b[32m[I 210623 22:55:58 models:110]\u001b[39m 777 512 train loss: 0.0000006 valid loss: 0.3368702 P@1: 0.69000 P@3: 0.55000 P@5: 0.41400 N@3: 0.58938 N@5: 0.58047 early stop: 24\n","\u001b[33m[W 210623 22:55:59 models:137]\u001b[39m Clipping gradients with total norm 0.02016 and max norm 0.00015\n","\u001b[32m[I 210623 22:56:00 models:110]\u001b[39m 779 1024 train loss: 0.0000045 valid loss: 0.3372220 P@1: 0.69000 P@3: 0.55000 P@5: 0.41400 N@3: 0.58938 N@5: 0.58047 early stop: 25\n","\u001b[33m[W 210623 22:56:01 models:137]\u001b[39m Clipping gradients with total norm 0.00126 and max norm 0.00011\n","\u001b[32m[I 210623 22:56:02 models:110]\u001b[39m 782 512 train loss: 0.0000008 valid loss: 0.3375747 P@1: 0.69000 P@3: 0.55000 P@5: 0.41400 N@3: 0.58938 N@5: 0.58047 early stop: 26\n","\u001b[32m[I 210623 22:56:04 models:110]\u001b[39m 784 1024 train loss: 0.0000009 valid loss: 0.3379265 P@1: 0.69000 P@3: 0.55000 P@5: 0.41400 N@3: 0.58938 N@5: 0.58047 early stop: 27\n","\u001b[32m[I 210623 22:56:06 models:110]\u001b[39m 787 512 train loss: 0.0000008 valid loss: 0.3382784 P@1: 0.69000 P@3: 0.55000 P@5: 0.41400 N@3: 0.58938 N@5: 0.58047 early stop: 28\n","\u001b[33m[W 210623 22:56:07 models:137]\u001b[39m Clipping gradients with total norm 0.0024 and max norm 0.00015\n","\u001b[32m[I 210623 22:56:08 models:110]\u001b[39m 789 1024 train loss: 0.0000010 valid loss: 0.3386325 P@1: 0.69000 P@3: 0.55000 P@5: 0.41400 N@3: 0.58999 N@5: 0.58098 early stop: 29\n","\u001b[32m[I 210623 22:56:10 models:110]\u001b[39m 792 512 train loss: 0.0000007 valid loss: 0.3389864 P@1: 0.69000 P@3: 0.55000 P@5: 0.41400 N@3: 0.58999 N@5: 0.58098 early stop: 30\n","\u001b[32m[I 210623 22:56:12 models:110]\u001b[39m 794 1024 train loss: 0.0000007 valid loss: 0.3393382 P@1: 0.69000 P@3: 0.55000 P@5: 0.41400 N@3: 0.58999 N@5: 0.58098 early stop: 31\n","\u001b[33m[W 210623 22:56:13 models:137]\u001b[39m Clipping gradients with total norm 0.00141 and max norm 0.00014\n","\u001b[32m[I 210623 22:56:14 models:110]\u001b[39m 797 512 train loss: 0.0000008 valid loss: 0.3396883 P@1: 0.69000 P@3: 0.55000 P@5: 0.41400 N@3: 0.58999 N@5: 0.58098 early stop: 32\n","\u001b[33m[W 210623 22:56:15 models:137]\u001b[39m Clipping gradients with total norm 0.00639 and max norm 0.00034\n","\u001b[32m[I 210623 22:56:16 models:110]\u001b[39m 799 1024 train loss: 0.0000017 valid loss: 0.3400371 P@1: 0.69000 P@3: 0.55000 P@5: 0.41400 N@3: 0.58999 N@5: 0.58098 early stop: 33\n","\u001b[33m[W 210623 22:56:18 models:137]\u001b[39m Clipping gradients with total norm 0.00349 and max norm 0.00051\n","\u001b[32m[I 210623 22:56:18 models:110]\u001b[39m 802 512 train loss: 0.0000017 valid loss: 0.3403856 P@1: 0.69000 P@3: 0.55000 P@5: 0.41400 N@3: 0.58999 N@5: 0.58098 early stop: 34\n","\u001b[33m[W 210623 22:56:20 models:137]\u001b[39m Clipping gradients with total norm 0.00317 and max norm 0.00015\n","\u001b[32m[I 210623 22:56:20 models:110]\u001b[39m 804 1024 train loss: 0.0000011 valid loss: 0.3407358 P@1: 0.69000 P@3: 0.55000 P@5: 0.41400 N@3: 0.58999 N@5: 0.58098 early stop: 35\n","\u001b[32m[I 210623 22:56:22 models:110]\u001b[39m 807 512 train loss: 0.0000011 valid loss: 0.3410842 P@1: 0.69000 P@3: 0.55000 P@5: 0.41400 N@3: 0.58999 N@5: 0.58098 early stop: 36\n","\u001b[32m[I 210623 22:56:24 models:110]\u001b[39m 809 1024 train loss: 0.0000010 valid loss: 0.3414305 P@1: 0.69000 P@3: 0.55000 P@5: 0.41400 N@3: 0.58999 N@5: 0.58098 early stop: 37\n","\u001b[33m[W 210623 22:56:26 models:137]\u001b[39m Clipping gradients with total norm 0.00212 and max norm 0.00026\n","\u001b[32m[I 210623 22:56:26 models:110]\u001b[39m 812 512 train loss: 0.0000011 valid loss: 0.3417773 P@1: 0.69000 P@3: 0.55000 P@5: 0.41400 N@3: 0.58999 N@5: 0.58098 early stop: 38\n","\u001b[33m[W 210623 22:56:27 models:137]\u001b[39m Clipping gradients with total norm 0.00105 and max norm 0.00019\n","\u001b[32m[I 210623 22:56:28 models:110]\u001b[39m 814 1024 train loss: 0.0000009 valid loss: 0.3421239 P@1: 0.69000 P@3: 0.55000 P@5: 0.41400 N@3: 0.58999 N@5: 0.58098 early stop: 39\n","\u001b[33m[W 210623 22:56:29 models:137]\u001b[39m Clipping gradients with total norm 0.00895 and max norm 0.00018\n","\u001b[32m[I 210623 22:56:30 models:110]\u001b[39m 817 512 train loss: 0.0000020 valid loss: 0.3424672 P@1: 0.69000 P@3: 0.55000 P@5: 0.41400 N@3: 0.58999 N@5: 0.58098 early stop: 40\n","\u001b[32m[I 210623 22:56:32 models:110]\u001b[39m 819 1024 train loss: 0.0000007 valid loss: 0.3428057 P@1: 0.69000 P@3: 0.55000 P@5: 0.41400 N@3: 0.58999 N@5: 0.58098 early stop: 41\n","\u001b[32m[I 210623 22:56:35 models:110]\u001b[39m 822 512 train loss: 0.0000006 valid loss: 0.3431409 P@1: 0.69000 P@3: 0.55000 P@5: 0.41400 N@3: 0.58999 N@5: 0.58098 early stop: 42\n","\u001b[33m[W 210623 22:56:35 models:137]\u001b[39m Clipping gradients with total norm 0.00057 and max norm 0.0001\n","\u001b[32m[I 210623 22:56:36 models:110]\u001b[39m 824 1024 train loss: 0.0000006 valid loss: 0.3434761 P@1: 0.69000 P@3: 0.55000 P@5: 0.41400 N@3: 0.58999 N@5: 0.58098 early stop: 43\n","\u001b[32m[I 210623 22:56:39 models:110]\u001b[39m 827 512 train loss: 0.0000005 valid loss: 0.3438111 P@1: 0.69000 P@3: 0.55000 P@5: 0.41400 N@3: 0.58999 N@5: 0.58098 early stop: 44\n","\u001b[33m[W 210623 22:56:39 models:137]\u001b[39m Clipping gradients with total norm 0.00131 and max norm 0.00021\n","\u001b[32m[I 210623 22:56:40 models:110]\u001b[39m 829 1024 train loss: 0.0000008 valid loss: 0.3441450 P@1: 0.69000 P@3: 0.55000 P@5: 0.41400 N@3: 0.58999 N@5: 0.58083 early stop: 45\n","\u001b[33m[W 210623 22:56:41 models:137]\u001b[39m Clipping gradients with total norm 0.00082 and max norm 0.00011\n","\u001b[32m[I 210623 22:56:43 models:110]\u001b[39m 832 512 train loss: 0.0000008 valid loss: 0.3444773 P@1: 0.69000 P@3: 0.55000 P@5: 0.41400 N@3: 0.58938 N@5: 0.58039 early stop: 46\n","\u001b[32m[I 210623 22:56:44 models:110]\u001b[39m 834 1024 train loss: 0.0000008 valid loss: 0.3448106 P@1: 0.69000 P@3: 0.55000 P@5: 0.41400 N@3: 0.58938 N@5: 0.58039 early stop: 47\n","\u001b[32m[I 210623 22:56:47 models:110]\u001b[39m 837 512 train loss: 0.0000007 valid loss: 0.3451456 P@1: 0.69000 P@3: 0.55000 P@5: 0.41400 N@3: 0.58938 N@5: 0.58039 early stop: 48\n","\u001b[33m[W 210623 22:56:48 models:137]\u001b[39m Clipping gradients with total norm 0.00185 and max norm 0.00026\n","\u001b[32m[I 210623 22:56:49 models:110]\u001b[39m 839 1024 train loss: 0.0000009 valid loss: 0.3454794 P@1: 0.69000 P@3: 0.55000 P@5: 0.41400 N@3: 0.58876 N@5: 0.57977 early stop: 49\n","\u001b[32m[I 210623 22:56:51 models:110]\u001b[39m 842 512 train loss: 0.0000007 valid loss: 0.3458132 P@1: 0.69000 P@3: 0.55000 P@5: 0.41600 N@3: 0.58876 N@5: 0.58159 early stop: 50\n","\u001b[32m[I 210623 22:56:53 models:110]\u001b[39m 844 1024 train loss: 0.0000005 valid loss: 0.3461472 P@1: 0.69000 P@3: 0.55000 P@5: 0.41600 N@3: 0.58876 N@5: 0.58159 early stop: 51\n","\u001b[32m[I 210623 22:56:55 models:110]\u001b[39m 847 512 train loss: 0.0000007 valid loss: 0.3464789 P@1: 0.69000 P@3: 0.55000 P@5: 0.41600 N@3: 0.58876 N@5: 0.58159 early stop: 52\n","\u001b[32m[I 210623 22:56:57 models:110]\u001b[39m 849 1024 train loss: 0.0000005 valid loss: 0.3468087 P@1: 0.69000 P@3: 0.55000 P@5: 0.41600 N@3: 0.58876 N@5: 0.58159 early stop: 53\n","\u001b[32m[I 210623 22:56:59 models:110]\u001b[39m 852 512 train loss: 0.0000005 valid loss: 0.3471374 P@1: 0.69000 P@3: 0.55000 P@5: 0.41600 N@3: 0.58876 N@5: 0.58159 early stop: 54\n","\u001b[32m[I 210623 22:57:01 models:110]\u001b[39m 854 1024 train loss: 0.0000005 valid loss: 0.3474653 P@1: 0.69000 P@3: 0.55000 P@5: 0.41600 N@3: 0.58876 N@5: 0.58159 early stop: 55\n","\u001b[32m[I 210623 22:57:03 models:110]\u001b[39m 857 512 train loss: 0.0000006 valid loss: 0.3477921 P@1: 0.69000 P@3: 0.55000 P@5: 0.41600 N@3: 0.58876 N@5: 0.58159 early stop: 56\n","\u001b[32m[I 210623 22:57:05 models:110]\u001b[39m 859 1024 train loss: 0.0000005 valid loss: 0.3481176 P@1: 0.69000 P@3: 0.55000 P@5: 0.41600 N@3: 0.58876 N@5: 0.58159 early stop: 57\n","\u001b[32m[I 210623 22:57:07 models:110]\u001b[39m 862 512 train loss: 0.0000006 valid loss: 0.3484417 P@1: 0.69000 P@3: 0.55000 P@5: 0.41600 N@3: 0.58876 N@5: 0.58159 early stop: 58\n","\u001b[32m[I 210623 22:57:09 models:110]\u001b[39m 864 1024 train loss: 0.0000005 valid loss: 0.3487651 P@1: 0.69000 P@3: 0.55000 P@5: 0.41400 N@3: 0.58876 N@5: 0.58028 early stop: 59\n","\u001b[32m[I 210623 22:57:11 models:110]\u001b[39m 867 512 train loss: 0.0000005 valid loss: 0.3490881 P@1: 0.69000 P@3: 0.55000 P@5: 0.41400 N@3: 0.58876 N@5: 0.58028 early stop: 60\n","\u001b[32m[I 210623 22:57:13 models:110]\u001b[39m 869 1024 train loss: 0.0000005 valid loss: 0.3494113 P@1: 0.69000 P@3: 0.55000 P@5: 0.41400 N@3: 0.58876 N@5: 0.58028 early stop: 61\n","\u001b[32m[I 210623 22:57:15 models:110]\u001b[39m 872 512 train loss: 0.0000006 valid loss: 0.3497340 P@1: 0.69000 P@3: 0.55000 P@5: 0.41400 N@3: 0.58876 N@5: 0.58048 early stop: 62\n","\u001b[32m[I 210623 22:57:17 models:110]\u001b[39m 874 1024 train loss: 0.0000005 valid loss: 0.3500551 P@1: 0.69000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58642 N@5: 0.58025 early stop: 63\n","\u001b[32m[I 210623 22:57:19 models:110]\u001b[39m 877 512 train loss: 0.0000005 valid loss: 0.3503759 P@1: 0.69000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58642 N@5: 0.58025 early stop: 64\n","\u001b[33m[W 210623 22:57:20 models:137]\u001b[39m Clipping gradients with total norm 0.00248 and max norm 0.00011\n","\u001b[32m[I 210623 22:57:21 models:110]\u001b[39m 879 1024 train loss: 0.0000008 valid loss: 0.3506956 P@1: 0.69000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58642 N@5: 0.58025 early stop: 65\n","\u001b[33m[W 210623 22:57:22 models:137]\u001b[39m Clipping gradients with total norm 0.00325 and max norm 7e-05\n","\u001b[32m[I 210623 22:57:23 models:110]\u001b[39m 882 512 train loss: 0.0000007 valid loss: 0.3510149 P@1: 0.69000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58642 N@5: 0.58025 early stop: 66\n","\u001b[32m[I 210623 22:57:25 models:110]\u001b[39m 884 1024 train loss: 0.0000004 valid loss: 0.3513340 P@1: 0.69000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58642 N@5: 0.58025 early stop: 67\n","\u001b[32m[I 210623 22:57:27 models:110]\u001b[39m 887 512 train loss: 0.0000006 valid loss: 0.3516525 P@1: 0.69000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58642 N@5: 0.58025 early stop: 68\n","\u001b[32m[I 210623 22:57:29 models:110]\u001b[39m 889 1024 train loss: 0.0000004 valid loss: 0.3519714 P@1: 0.69000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58642 N@5: 0.58025 early stop: 69\n","\u001b[32m[I 210623 22:57:31 models:110]\u001b[39m 892 512 train loss: 0.0000005 valid loss: 0.3522895 P@1: 0.69000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58642 N@5: 0.58025 early stop: 70\n","\u001b[32m[I 210623 22:57:33 models:110]\u001b[39m 894 1024 train loss: 0.0000005 valid loss: 0.3526063 P@1: 0.69000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58642 N@5: 0.58025 early stop: 71\n","\u001b[32m[I 210623 22:57:35 models:110]\u001b[39m 897 512 train loss: 0.0000005 valid loss: 0.3529213 P@1: 0.69000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58642 N@5: 0.58025 early stop: 72\n","\u001b[32m[I 210623 22:57:37 models:110]\u001b[39m 899 1024 train loss: 0.0000005 valid loss: 0.3532349 P@1: 0.69000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58642 N@5: 0.58025 early stop: 73\n","\u001b[32m[I 210623 22:57:39 models:110]\u001b[39m 902 512 train loss: 0.0000005 valid loss: 0.3535474 P@1: 0.69000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58642 N@5: 0.58025 early stop: 74\n","\u001b[32m[I 210623 22:57:41 models:110]\u001b[39m 904 1024 train loss: 0.0000005 valid loss: 0.3538581 P@1: 0.69000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58642 N@5: 0.58025 early stop: 75\n","\u001b[32m[I 210623 22:57:43 models:110]\u001b[39m 907 512 train loss: 0.0000004 valid loss: 0.3541682 P@1: 0.69000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58642 N@5: 0.58025 early stop: 76\n","\u001b[33m[W 210623 22:57:44 models:137]\u001b[39m Clipping gradients with total norm 0.00061 and max norm 0.00012\n","\u001b[33m[W 210623 22:57:45 models:137]\u001b[39m Clipping gradients with total norm 0.00098 and max norm 6e-05\n","\u001b[32m[I 210623 22:57:45 models:110]\u001b[39m 909 1024 train loss: 0.0000006 valid loss: 0.3544783 P@1: 0.69000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58642 N@5: 0.58025 early stop: 77\n","\u001b[32m[I 210623 22:57:48 models:110]\u001b[39m 912 512 train loss: 0.0000005 valid loss: 0.3547894 P@1: 0.69000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58642 N@5: 0.58004 early stop: 78\n","\u001b[32m[I 210623 22:57:49 models:110]\u001b[39m 914 1024 train loss: 0.0000004 valid loss: 0.3551008 P@1: 0.69000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58642 N@5: 0.58004 early stop: 79\n","\u001b[32m[I 210623 22:57:52 models:110]\u001b[39m 917 512 train loss: 0.0000005 valid loss: 0.3554120 P@1: 0.69000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58642 N@5: 0.58004 early stop: 80\n","\u001b[32m[I 210623 22:57:53 models:110]\u001b[39m 919 1024 train loss: 0.0000004 valid loss: 0.3557218 P@1: 0.69000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58642 N@5: 0.58004 early stop: 81\n","\u001b[32m[I 210623 22:57:56 models:110]\u001b[39m 922 512 train loss: 0.0000004 valid loss: 0.3560302 P@1: 0.69000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58642 N@5: 0.58004 early stop: 82\n","\u001b[32m[I 210623 22:57:57 models:110]\u001b[39m 924 1024 train loss: 0.0000004 valid loss: 0.3563369 P@1: 0.69000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58642 N@5: 0.58004 early stop: 83\n","\u001b[32m[I 210623 22:58:00 models:110]\u001b[39m 927 512 train loss: 0.0000004 valid loss: 0.3566417 P@1: 0.69000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58642 N@5: 0.58004 early stop: 84\n","\u001b[32m[I 210623 22:58:01 models:110]\u001b[39m 929 1024 train loss: 0.0000004 valid loss: 0.3569447 P@1: 0.69000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58642 N@5: 0.58004 early stop: 85\n","\u001b[33m[W 210623 22:58:03 models:137]\u001b[39m Clipping gradients with total norm 0.00121 and max norm 0.0001\n","\u001b[32m[I 210623 22:58:04 models:110]\u001b[39m 932 512 train loss: 0.0000005 valid loss: 0.3572467 P@1: 0.69000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58642 N@5: 0.58004 early stop: 86\n","\u001b[32m[I 210623 22:58:05 models:110]\u001b[39m 934 1024 train loss: 0.0000005 valid loss: 0.3575472 P@1: 0.69000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58642 N@5: 0.58004 early stop: 87\n","\u001b[33m[W 210623 22:58:06 models:137]\u001b[39m Clipping gradients with total norm 0.00051 and max norm 8e-05\n","\u001b[32m[I 210623 22:58:08 models:110]\u001b[39m 937 512 train loss: 0.0000005 valid loss: 0.3578472 P@1: 0.69000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58642 N@5: 0.58004 early stop: 88\n","\u001b[32m[I 210623 22:58:09 models:110]\u001b[39m 939 1024 train loss: 0.0000004 valid loss: 0.3581473 P@1: 0.69000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58642 N@5: 0.58004 early stop: 89\n","\u001b[32m[I 210623 22:58:12 models:110]\u001b[39m 942 512 train loss: 0.0000005 valid loss: 0.3584481 P@1: 0.69000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58642 N@5: 0.58004 early stop: 90\n","\u001b[33m[W 210623 22:58:12 models:137]\u001b[39m Clipping gradients with total norm 0.001 and max norm 0.00016\n","\u001b[32m[I 210623 22:58:13 models:110]\u001b[39m 944 1024 train loss: 0.0000006 valid loss: 0.3587477 P@1: 0.69000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58642 N@5: 0.58004 early stop: 91\n","\u001b[32m[I 210623 22:58:16 models:110]\u001b[39m 947 512 train loss: 0.0000004 valid loss: 0.3590475 P@1: 0.69000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58642 N@5: 0.58004 early stop: 92\n","\u001b[33m[W 210623 22:58:17 models:137]\u001b[39m Clipping gradients with total norm 0.0014 and max norm 0.00028\n","\u001b[32m[I 210623 22:58:18 models:110]\u001b[39m 949 1024 train loss: 0.0000006 valid loss: 0.3593495 P@1: 0.69000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58642 N@5: 0.58004 early stop: 93\n","\u001b[32m[I 210623 22:58:20 models:110]\u001b[39m 952 512 train loss: 0.0000006 valid loss: 0.3596533 P@1: 0.69000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58642 N@5: 0.58019 early stop: 94\n","\u001b[32m[I 210623 22:58:22 models:110]\u001b[39m 954 1024 train loss: 0.0000007 valid loss: 0.3599563 P@1: 0.69000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58642 N@5: 0.58019 early stop: 95\n","\u001b[32m[I 210623 22:58:24 models:110]\u001b[39m 957 512 train loss: 0.0000007 valid loss: 0.3602579 P@1: 0.69000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58642 N@5: 0.58019 early stop: 96\n","\u001b[32m[I 210623 22:58:26 models:110]\u001b[39m 959 1024 train loss: 0.0000005 valid loss: 0.3605587 P@1: 0.69000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58642 N@5: 0.58019 early stop: 97\n","\u001b[32m[I 210623 22:58:28 models:110]\u001b[39m 962 512 train loss: 0.0000004 valid loss: 0.3608588 P@1: 0.69000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58642 N@5: 0.58019 early stop: 98\n","\u001b[32m[I 210623 22:58:30 models:110]\u001b[39m 964 1024 train loss: 0.0000005 valid loss: 0.3611593 P@1: 0.69000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58642 N@5: 0.58019 early stop: 99\n","\u001b[32m[I 210623 22:58:32 models:110]\u001b[39m 967 512 train loss: 0.0000006 valid loss: 0.3614617 P@1: 0.69000 P@3: 0.54667 P@5: 0.41400 N@3: 0.58642 N@5: 0.58019 early stop: 100\n","\u001b[33m[W 210623 22:58:33 models:137]\u001b[39m Clipping gradients with total norm 0.00051 and max norm 8e-05\n","\u001b[32m[I 210623 22:58:34 main:76]\u001b[39m Finish Training\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u1CyH-MLqjZC","executionInfo":{"status":"ok","timestamp":1624489203263,"user_tz":420,"elapsed":6005,"user":{"displayName":"Eric Kong","photoUrl":"","userId":"07341482379174037459"}},"outputId":"c2d15cd3-d9ca-407a-a002-88787b258620"},"source":["!PYTHONFAULTHANDLER=1 python main.py --data-cnf configure/datasets/PeTaL.yaml --model-cnf configure/models/MATCH-PeTaL.yaml --mode eval"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[32m[I 210623 22:59:58 main:32]\u001b[39m Model Name: MATCH\n","\u001b[32m[I 210623 22:59:58 main:79]\u001b[39m Loading Test Set\n","\u001b[32m[I 210623 22:59:58 main:83]\u001b[39m Size of Test Set: 100\n","\u001b[32m[I 210623 22:59:58 main:85]\u001b[39m Predicting\n","\u001b[32m[I 210623 23:00:02 main:91]\u001b[39m Finish Predicting\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ob7v6qWZjpKU","executionInfo":{"status":"ok","timestamp":1624489207481,"user_tz":420,"elapsed":2857,"user":{"displayName":"Eric Kong","photoUrl":"","userId":"07341482379174037459"}},"outputId":"ae581745-a49e-4405-d196-05c3af201851"},"source":["!python evaluation.py --results PeTaL/results/MATCH-PeTaL-labels.npy --targets PeTaL/test_labels.npy --train-labels PeTaL/train_labels.npy"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Precision@1,3,5: 0.61 0.49666666666666665 0.382\n","nDCG@1,3,5: 0.61 0.5242603369757589 0.5284999798212123\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ktf4hCBZJK6A"},"source":["### Attempt with 1000 epochs, step = 100"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BCE2oQc-JKHN","executionInfo":{"status":"ok","timestamp":1624490769540,"user_tz":420,"elapsed":719702,"user":{"displayName":"Eric Kong","photoUrl":"","userId":"07341482379174037459"}},"outputId":"1de38af8-f2b5-42d9-b1b7-915050cc3da3"},"source":["!PYTHONFAULTHANDLER=1 python main.py --data-cnf configure/datasets/PeTaL.yaml --model-cnf configure/models/MATCH-PeTaL.yaml --mode train --reg 1"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[32m[I 210623 23:14:10 main:32]\u001b[39m Model Name: MATCH\n","\u001b[32m[I 210623 23:14:10 main:35]\u001b[39m Loading Training and Validation Set\n","/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['absorb_and/or_filter_solids', 'chemically_break_down_inorganic_compounds', 'detox/purify', 'manage_environmental_disturbances_in_a_community', 'protect_from_fire', 'protect_from_gases', 'send_vibratory_signals'] will be ignored\n","  .format(sorted(unknown, key=str)))\n","/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['absorb_and/or_filter_solids', 'detox/purify', 'protect_from_gases'] will be ignored\n","  .format(sorted(unknown, key=str)))\n","\u001b[32m[I 210623 23:14:10 main:47]\u001b[39m Number of Labels: 124\n","\u001b[32m[I 210623 23:14:10 main:48]\u001b[39m Size of Training Set: 800\n","\u001b[32m[I 210623 23:14:10 main:49]\u001b[39m Size of Validation Set: 100\n","\u001b[32m[I 210623 23:14:10 main:66]\u001b[39m Number of Edges: 101\n","\u001b[32m[I 210623 23:14:10 main:68]\u001b[39m Training\n","\u001b[32m[I 210623 23:14:17 models:142]\u001b[39m SWA Initializing\n","\u001b[32m[I 210623 23:14:32 models:110]\u001b[39m 24 1024 train loss: 0.1303363 valid loss: 0.1415193 P@1: 0.43000 P@3: 0.26000 P@5: 0.18200 N@3: 0.30282 N@5: 0.27964 early stop: 0\n","\u001b[32m[I 210623 23:14:50 models:110]\u001b[39m 49 1024 train loss: 0.0259115 valid loss: 0.1349336 P@1: 0.54000 P@3: 0.37000 P@5: 0.28200 N@3: 0.41327 N@5: 0.40468 early stop: 0\n","\u001b[32m[I 210623 23:15:07 models:110]\u001b[39m 74 1024 train loss: 0.0111599 valid loss: 0.1365098 P@1: 0.57000 P@3: 0.44333 P@5: 0.35000 N@3: 0.48224 N@5: 0.48113 early stop: 0\n","\u001b[32m[I 210623 23:15:25 models:110]\u001b[39m 99 1024 train loss: 0.0030574 valid loss: 0.1480756 P@1: 0.61000 P@3: 0.47667 P@5: 0.36400 N@3: 0.51509 N@5: 0.50378 early stop: 0\n","\u001b[32m[I 210623 23:15:43 models:110]\u001b[39m 124 1024 train loss: 0.0021582 valid loss: 0.1609345 P@1: 0.60000 P@3: 0.47333 P@5: 0.36800 N@3: 0.50978 N@5: 0.50584 early stop: 0\n","\u001b[32m[I 210623 23:16:01 models:110]\u001b[39m 149 1024 train loss: 0.0046083 valid loss: 0.1716277 P@1: 0.63000 P@3: 0.49000 P@5: 0.38000 N@3: 0.52855 N@5: 0.52399 early stop: 0\n","\u001b[32m[I 210623 23:16:19 models:110]\u001b[39m 174 1024 train loss: 0.0004270 valid loss: 0.1839084 P@1: 0.62000 P@3: 0.49333 P@5: 0.38800 N@3: 0.52855 N@5: 0.52802 early stop: 0\n","\u001b[32m[I 210623 23:16:37 models:110]\u001b[39m 199 1024 train loss: 0.0000347 valid loss: 0.1957750 P@1: 0.63000 P@3: 0.49333 P@5: 0.39600 N@3: 0.52948 N@5: 0.53588 early stop: 0\n","\u001b[32m[I 210623 23:16:55 models:110]\u001b[39m 224 1024 train loss: 0.0000195 valid loss: 0.2071029 P@1: 0.63000 P@3: 0.50000 P@5: 0.39800 N@3: 0.53479 N@5: 0.53912 early stop: 0\n","\u001b[32m[I 210623 23:17:13 models:110]\u001b[39m 249 1024 train loss: 0.0000152 valid loss: 0.2177515 P@1: 0.63000 P@3: 0.49333 P@5: 0.39600 N@3: 0.53010 N@5: 0.53741 early stop: 1\n","\u001b[33m[W 210623 23:17:17 models:137]\u001b[39m Clipping gradients with total norm 0.00811 and max norm 0.00114\n","\u001b[33m[W 210623 23:17:24 models:137]\u001b[39m Clipping gradients with total norm 0.00702 and max norm 0.001\n","\u001b[33m[W 210623 23:17:25 models:137]\u001b[39m Clipping gradients with total norm 0.01258 and max norm 0.00237\n","\u001b[32m[I 210623 23:17:31 models:110]\u001b[39m 274 1024 train loss: 0.0000134 valid loss: 0.2278254 P@1: 0.62000 P@3: 0.50333 P@5: 0.39400 N@3: 0.53621 N@5: 0.53620 early stop: 2\n","\u001b[33m[W 210623 23:17:42 models:137]\u001b[39m Clipping gradients with total norm 0.01879 and max norm 0.00151\n","\u001b[33m[W 210623 23:17:46 models:137]\u001b[39m Clipping gradients with total norm 0.01238 and max norm 0.00113\n","\u001b[32m[I 210623 23:17:49 models:110]\u001b[39m 299 1024 train loss: 0.0000100 valid loss: 0.2373018 P@1: 0.62000 P@3: 0.50667 P@5: 0.39600 N@3: 0.53794 N@5: 0.53818 early stop: 3\n","\u001b[33m[W 210623 23:17:57 models:137]\u001b[39m Clipping gradients with total norm 0.00476 and max norm 0.00073\n","\u001b[32m[I 210623 23:18:07 models:110]\u001b[39m 324 1024 train loss: 0.0000073 valid loss: 0.2462835 P@1: 0.62000 P@3: 0.50667 P@5: 0.40000 N@3: 0.53855 N@5: 0.54240 early stop: 0\n","\u001b[32m[I 210623 23:18:24 models:110]\u001b[39m 349 1024 train loss: 0.0000054 valid loss: 0.2546201 P@1: 0.62000 P@3: 0.50333 P@5: 0.40000 N@3: 0.53621 N@5: 0.54237 early stop: 1\n","\u001b[33m[W 210623 23:18:34 models:137]\u001b[39m Clipping gradients with total norm 0.00371 and max norm 0.00067\n","\u001b[33m[W 210623 23:18:37 models:137]\u001b[39m Clipping gradients with total norm 0.01919 and max norm 0.00145\n","\u001b[32m[I 210623 23:18:42 models:110]\u001b[39m 374 1024 train loss: 0.0000048 valid loss: 0.2624709 P@1: 0.62000 P@3: 0.51000 P@5: 0.40000 N@3: 0.54152 N@5: 0.54346 early stop: 0\n","\u001b[32m[I 210623 23:19:00 models:110]\u001b[39m 399 1024 train loss: 0.0000040 valid loss: 0.2699245 P@1: 0.62000 P@3: 0.51000 P@5: 0.40000 N@3: 0.54141 N@5: 0.54407 early stop: 0\n","\u001b[33m[W 210623 23:19:04 models:137]\u001b[39m Clipping gradients with total norm 0.05564 and max norm 0.00144\n","\u001b[33m[W 210623 23:19:07 models:137]\u001b[39m Clipping gradients with total norm 0.00384 and max norm 0.0005\n","\u001b[33m[W 210623 23:19:07 models:137]\u001b[39m Clipping gradients with total norm 0.01174 and max norm 0.00101\n","\u001b[33m[W 210623 23:19:17 models:137]\u001b[39m Clipping gradients with total norm 0.00867 and max norm 0.00117\n","\u001b[32m[I 210623 23:19:18 models:110]\u001b[39m 424 1024 train loss: 0.0000046 valid loss: 0.2768732 P@1: 0.61000 P@3: 0.50667 P@5: 0.39800 N@3: 0.53733 N@5: 0.54029 early stop: 1\n","\u001b[33m[W 210623 23:19:28 models:137]\u001b[39m Clipping gradients with total norm 0.01016 and max norm 0.001\n","\u001b[32m[I 210623 23:19:36 models:110]\u001b[39m 449 1024 train loss: 0.0000036 valid loss: 0.2834906 P@1: 0.61000 P@3: 0.50667 P@5: 0.39400 N@3: 0.53733 N@5: 0.53766 early stop: 2\n","\u001b[33m[W 210623 23:19:40 models:137]\u001b[39m Clipping gradients with total norm 0.0087 and max norm 0.00126\n","\u001b[32m[I 210623 23:19:53 models:110]\u001b[39m 474 1024 train loss: 0.0000037 valid loss: 0.2898347 P@1: 0.61000 P@3: 0.50667 P@5: 0.39400 N@3: 0.53672 N@5: 0.53730 early stop: 3\n","\u001b[32m[I 210623 23:20:11 models:110]\u001b[39m 499 1024 train loss: 0.0000025 valid loss: 0.2959807 P@1: 0.61000 P@3: 0.50333 P@5: 0.39200 N@3: 0.53376 N@5: 0.53412 early stop: 4\n","\u001b[33m[W 210623 23:20:20 models:137]\u001b[39m Clipping gradients with total norm 0.00461 and max norm 0.00046\n","\u001b[32m[I 210623 23:20:29 models:110]\u001b[39m 524 1024 train loss: 0.0000023 valid loss: 0.3017532 P@1: 0.61000 P@3: 0.50333 P@5: 0.39200 N@3: 0.53376 N@5: 0.53462 early stop: 5\n","\u001b[33m[W 210623 23:20:37 models:137]\u001b[39m Clipping gradients with total norm 0.00341 and max norm 0.00045\n","\u001b[33m[W 210623 23:20:39 models:137]\u001b[39m Clipping gradients with total norm 0.02566 and max norm 0.00026\n","\u001b[32m[I 210623 23:20:47 models:110]\u001b[39m 549 1024 train loss: 0.0000026 valid loss: 0.3073660 P@1: 0.61000 P@3: 0.51333 P@5: 0.39400 N@3: 0.54080 N@5: 0.53655 early stop: 6\n","\u001b[33m[W 210623 23:21:02 models:137]\u001b[39m Clipping gradients with total norm 0.00481 and max norm 0.00067\n","\u001b[32m[I 210623 23:21:05 models:110]\u001b[39m 574 1024 train loss: 0.0000019 valid loss: 0.3126516 P@1: 0.61000 P@3: 0.51667 P@5: 0.39800 N@3: 0.54234 N@5: 0.53861 early stop: 7\n","\u001b[33m[W 210623 23:21:06 models:137]\u001b[39m Clipping gradients with total norm 0.00484 and max norm 0.00042\n","\u001b[33m[W 210623 23:21:10 models:137]\u001b[39m Clipping gradients with total norm 0.04465 and max norm 0.00096\n","\u001b[33m[W 210623 23:21:12 models:137]\u001b[39m Clipping gradients with total norm 0.0234 and max norm 0.00295\n","\u001b[33m[W 210623 23:21:16 models:137]\u001b[39m Clipping gradients with total norm 1.99441 and max norm 0.16587\n","\u001b[33m[W 210623 23:21:16 models:137]\u001b[39m Clipping gradients with total norm 2.2826 and max norm 0.33173\n","\u001b[33m[W 210623 23:21:16 models:137]\u001b[39m Clipping gradients with total norm 3.7751 and max norm 0.66347\n","\u001b[33m[W 210623 23:21:16 models:137]\u001b[39m Clipping gradients with total norm 5.21066 and max norm 1.0\n","\u001b[33m[W 210623 23:21:16 models:137]\u001b[39m Clipping gradients with total norm 8.60285 and max norm 1.0\n","\u001b[33m[W 210623 23:21:17 models:137]\u001b[39m Clipping gradients with total norm 6.66375 and max norm 1.0\n","\u001b[33m[W 210623 23:21:17 models:137]\u001b[39m Clipping gradients with total norm 9.7272 and max norm 1.0\n","\u001b[33m[W 210623 23:21:17 models:137]\u001b[39m Clipping gradients with total norm 5.61784 and max norm 1.0\n","\u001b[33m[W 210623 23:21:17 models:137]\u001b[39m Clipping gradients with total norm 7.90646 and max norm 1.0\n","\u001b[32m[I 210623 23:21:23 models:110]\u001b[39m 599 1024 train loss: 0.0762113 valid loss: 0.2730016 P@1: 0.63000 P@3: 0.52333 P@5: 0.38800 N@3: 0.55439 N@5: 0.53787 early stop: 8\n","\u001b[32m[I 210623 23:21:41 models:110]\u001b[39m 624 1024 train loss: 0.0263548 valid loss: 0.2555712 P@1: 0.62000 P@3: 0.50333 P@5: 0.39800 N@3: 0.53795 N@5: 0.54312 early stop: 9\n","\u001b[32m[I 210623 23:21:58 models:110]\u001b[39m 649 1024 train loss: 0.0035953 valid loss: 0.2457935 P@1: 0.61000 P@3: 0.49000 P@5: 0.38200 N@3: 0.52571 N@5: 0.52281 early stop: 10\n","\u001b[32m[I 210623 23:22:16 models:110]\u001b[39m 674 1024 train loss: 0.0014165 valid loss: 0.2395107 P@1: 0.61000 P@3: 0.48667 P@5: 0.36600 N@3: 0.52520 N@5: 0.51183 early stop: 11\n","\u001b[32m[I 210623 23:22:34 models:110]\u001b[39m 699 1024 train loss: 0.0003433 valid loss: 0.2356114 P@1: 0.62000 P@3: 0.46333 P@5: 0.35400 N@3: 0.51032 N@5: 0.50030 early stop: 12\n","\u001b[33m[W 210623 23:22:45 models:137]\u001b[39m Clipping gradients with total norm 0.04449 and max norm 0.00372\n","\u001b[33m[W 210623 23:22:48 models:137]\u001b[39m Clipping gradients with total norm 0.06797 and max norm 0.00646\n","\u001b[33m[W 210623 23:22:49 models:137]\u001b[39m Clipping gradients with total norm 0.04171 and max norm 0.00596\n","\u001b[32m[I 210623 23:22:52 models:110]\u001b[39m 724 1024 train loss: 0.0000784 valid loss: 0.2335374 P@1: 0.62000 P@3: 0.46333 P@5: 0.35000 N@3: 0.51043 N@5: 0.49650 early stop: 13\n","\u001b[33m[W 210623 23:23:02 models:137]\u001b[39m Clipping gradients with total norm 0.0229 and max norm 0.00371\n","\u001b[33m[W 210623 23:23:07 models:137]\u001b[39m Clipping gradients with total norm 0.02138 and max norm 0.00205\n","\u001b[32m[I 210623 23:23:10 models:110]\u001b[39m 749 1024 train loss: 0.0000526 valid loss: 0.2326673 P@1: 0.62000 P@3: 0.46667 P@5: 0.35200 N@3: 0.51013 N@5: 0.49644 early stop: 14\n","\u001b[33m[W 210623 23:23:16 models:137]\u001b[39m Clipping gradients with total norm 0.03519 and max norm 0.00625\n","\u001b[32m[I 210623 23:23:28 models:110]\u001b[39m 774 1024 train loss: 0.0000442 valid loss: 0.2326050 P@1: 0.60000 P@3: 0.46333 P@5: 0.35600 N@3: 0.50122 N@5: 0.49353 early stop: 15\n","\u001b[33m[W 210623 23:23:31 models:137]\u001b[39m Clipping gradients with total norm 0.03441 and max norm 0.00344\n","\u001b[32m[I 210623 23:23:46 models:110]\u001b[39m 799 1024 train loss: 0.0000296 valid loss: 0.2331086 P@1: 0.61000 P@3: 0.45333 P@5: 0.34800 N@3: 0.49388 N@5: 0.48624 early stop: 16\n","\u001b[32m[I 210623 23:24:03 models:110]\u001b[39m 824 1024 train loss: 0.0000276 valid loss: 0.2340070 P@1: 0.60000 P@3: 0.45000 P@5: 0.34000 N@3: 0.49042 N@5: 0.47904 early stop: 17\n","\u001b[33m[W 210623 23:24:10 models:137]\u001b[39m Clipping gradients with total norm 0.00915 and max norm 0.00147\n","\u001b[32m[I 210623 23:24:21 models:110]\u001b[39m 849 1024 train loss: 0.0000206 valid loss: 0.2352343 P@1: 0.57000 P@3: 0.45000 P@5: 0.33800 N@3: 0.48442 N@5: 0.47227 early stop: 18\n","\u001b[33m[W 210623 23:24:35 models:137]\u001b[39m Clipping gradients with total norm 0.02207 and max norm 0.00098\n","\u001b[32m[I 210623 23:24:39 models:110]\u001b[39m 874 1024 train loss: 0.0000146 valid loss: 0.2366574 P@1: 0.57000 P@3: 0.45000 P@5: 0.34000 N@3: 0.48319 N@5: 0.47152 early stop: 19\n","\u001b[33m[W 210623 23:24:56 models:137]\u001b[39m Clipping gradients with total norm 0.00854 and max norm 0.0013\n","\u001b[32m[I 210623 23:24:57 models:110]\u001b[39m 899 1024 train loss: 0.0000117 valid loss: 0.2382847 P@1: 0.57000 P@3: 0.45000 P@5: 0.34200 N@3: 0.48319 N@5: 0.47315 early stop: 20\n","\u001b[32m[I 210623 23:25:15 models:110]\u001b[39m 924 1024 train loss: 0.0020094 valid loss: 0.2377156 P@1: 0.59000 P@3: 0.45333 P@5: 0.33800 N@3: 0.48767 N@5: 0.47311 early stop: 21\n","\u001b[32m[I 210623 23:25:33 models:110]\u001b[39m 949 1024 train loss: 0.0089087 valid loss: 0.2357241 P@1: 0.60000 P@3: 0.44667 P@5: 0.34400 N@3: 0.48471 N@5: 0.47783 early stop: 22\n","\u001b[33m[W 210623 23:25:43 models:137]\u001b[39m Clipping gradients with total norm 0.04688 and max norm 0.00825\n","\u001b[32m[I 210623 23:25:51 models:110]\u001b[39m 974 1024 train loss: 0.0001187 valid loss: 0.2344745 P@1: 0.61000 P@3: 0.44000 P@5: 0.34400 N@3: 0.48236 N@5: 0.47962 early stop: 23\n","\u001b[32m[I 210623 23:26:08 models:110]\u001b[39m 999 1024 train loss: 0.0000449 valid loss: 0.2336695 P@1: 0.61000 P@3: 0.44667 P@5: 0.34200 N@3: 0.48767 N@5: 0.47849 early stop: 24\n","\u001b[32m[I 210623 23:26:08 main:76]\u001b[39m Finish Training\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sQup8umfJnjC","executionInfo":{"status":"ok","timestamp":1624490797193,"user_tz":420,"elapsed":5698,"user":{"displayName":"Eric Kong","photoUrl":"","userId":"07341482379174037459"}},"outputId":"efc5b762-862f-4477-9e36-fbe4dab856af"},"source":["!PYTHONFAULTHANDLER=1 python main.py --data-cnf configure/datasets/PeTaL.yaml --model-cnf configure/models/MATCH-PeTaL.yaml --mode eval"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[32m[I 210623 23:26:32 main:32]\u001b[39m Model Name: MATCH\n","\u001b[32m[I 210623 23:26:32 main:79]\u001b[39m Loading Test Set\n","\u001b[32m[I 210623 23:26:32 main:83]\u001b[39m Size of Test Set: 100\n","\u001b[32m[I 210623 23:26:32 main:85]\u001b[39m Predicting\n","\u001b[32m[I 210623 23:26:35 main:91]\u001b[39m Finish Predicting\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OBfyqfsVJr7v","executionInfo":{"status":"ok","timestamp":1624490797709,"user_tz":420,"elapsed":522,"user":{"displayName":"Eric Kong","photoUrl":"","userId":"07341482379174037459"}},"outputId":"6c773d85-33b5-4bc5-a054-8a371303e233"},"source":["!python evaluation.py --results PeTaL/results/MATCH-PeTaL-labels.npy --targets PeTaL/test_labels.npy --train-labels PeTaL/train_labels.npy"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Precision@1,3,5: 0.67 0.49333333333333335 0.392\n","nDCG@1,3,5: 0.67 0.5408242591878821 0.5537992919593395\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UB0SRyKqPGaw"},"source":["### Results with MAG labels, without MeSH labels"]},{"cell_type":"code","metadata":{"id":"ASAeS4SqPEmD"},"source":["# !cp -r PeTaL-062315 PeTaL\n","\n","!python3 transform_data_PeTaL.py --dataset $DATASET"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q9pe2-41RW1H","executionInfo":{"status":"ok","timestamp":1624492316020,"user_tz":420,"elapsed":2771,"user":{"displayName":"Eric Kong","photoUrl":"","userId":"07341482379174037459"}},"outputId":"21304b0f-a916-4c27-9d6e-5cd99608d74b"},"source":["!python preprocess.py \\\n","--text-path {DATASET}/train_texts.txt \\\n","--label-path {DATASET}/train_labels.txt \\\n","--vocab-path {DATASET}/vocab.npy \\\n","--emb-path {DATASET}/emb_init.npy \\\n","--w2v-model {DATASET}/{DATASET}.joint.emb \\\n","\n","!python preprocess.py \\\n","--text-path {DATASET}/test_texts.txt \\\n","--label-path {DATASET}/test_labels.txt \\\n","--vocab-path {DATASET}/vocab.npy \\"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[32m[I 210623 23:51:54 preprocess:28]\u001b[39m Vocab Size: 26834\n","\u001b[32m[I 210623 23:51:54 preprocess:30]\u001b[39m Getting Dataset: PeTaL/train_texts.txt Max Length: 500\n","\u001b[32m[I 210623 23:51:54 preprocess:32]\u001b[39m Size of Samples: 900\n","\u001b[32m[I 210623 23:51:55 preprocess:28]\u001b[39m Vocab Size: 26834\n","\u001b[32m[I 210623 23:51:55 preprocess:30]\u001b[39m Getting Dataset: PeTaL/test_texts.txt Max Length: 500\n","\u001b[32m[I 210623 23:51:55 preprocess:32]\u001b[39m Size of Samples: 100\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dZRxGf3pSddy","executionInfo":{"status":"ok","timestamp":1624493130239,"user_tz":420,"elapsed":724719,"user":{"displayName":"Eric Kong","photoUrl":"","userId":"07341482379174037459"}},"outputId":"304cbbb0-6eb5-43b5-b1e7-4fac930bd276"},"source":["!PYTHONFAULTHANDLER=1 python main.py --data-cnf configure/datasets/{DATASET}.yaml --model-cnf configure/models/{MODEL}-{DATASET}.yaml --mode train --reg 1\n","!PYTHONFAULTHANDLER=1 python main.py --data-cnf configure/datasets/{DATASET}.yaml --model-cnf configure/models/{MODEL}-{DATASET}.yaml --mode eval\n","\n","!python evaluation.py \\\n","--results {DATASET}/results/{MODEL}-{DATASET}-labels.npy \\\n","--targets {DATASET}/test_labels.npy \\\n","--train-labels {DATASET}/train_labels.npy"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[32m[I 210623 23:53:26 main:32]\u001b[39m Model Name: MATCH\n","\u001b[32m[I 210623 23:53:26 main:35]\u001b[39m Loading Training and Validation Set\n","/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['absorb_and/or_filter_solids', 'chemically_break_down_inorganic_compounds', 'detox/purify', 'manage_environmental_disturbances_in_a_community', 'protect_from_fire', 'protect_from_gases', 'send_vibratory_signals'] will be ignored\n","  .format(sorted(unknown, key=str)))\n","/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['absorb_and/or_filter_solids', 'detox/purify', 'protect_from_gases'] will be ignored\n","  .format(sorted(unknown, key=str)))\n","\u001b[32m[I 210623 23:53:26 main:47]\u001b[39m Number of Labels: 124\n","\u001b[32m[I 210623 23:53:26 main:48]\u001b[39m Size of Training Set: 800\n","\u001b[32m[I 210623 23:53:26 main:49]\u001b[39m Size of Validation Set: 100\n","\u001b[32m[I 210623 23:53:26 main:66]\u001b[39m Number of Edges: 101\n","\u001b[32m[I 210623 23:53:26 main:68]\u001b[39m Training\n","\u001b[32m[I 210623 23:53:32 models:142]\u001b[39m SWA Initializing\n","\u001b[32m[I 210623 23:53:47 models:110]\u001b[39m 24 1024 train loss: 0.1282653 valid loss: 0.1520345 P@1: 0.27000 P@3: 0.21000 P@5: 0.18600 N@3: 0.22714 N@5: 0.25315 early stop: 0\n","\u001b[32m[I 210623 23:54:05 models:110]\u001b[39m 49 1024 train loss: 0.0306060 valid loss: 0.1493586 P@1: 0.40000 P@3: 0.29667 P@5: 0.24800 N@3: 0.32327 N@5: 0.33705 early stop: 0\n","\u001b[32m[I 210623 23:54:23 models:110]\u001b[39m 74 1024 train loss: 0.0121740 valid loss: 0.1538170 P@1: 0.48000 P@3: 0.35333 P@5: 0.29200 N@3: 0.38439 N@5: 0.39602 early stop: 0\n","\u001b[32m[I 210623 23:54:41 models:110]\u001b[39m 99 1024 train loss: 0.0036369 valid loss: 0.1629476 P@1: 0.51000 P@3: 0.39667 P@5: 0.31600 N@3: 0.42695 N@5: 0.43331 early stop: 0\n","\u001b[32m[I 210623 23:54:59 models:110]\u001b[39m 124 1024 train loss: 0.0011694 valid loss: 0.1755957 P@1: 0.53000 P@3: 0.41667 P@5: 0.33000 N@3: 0.44943 N@5: 0.45384 early stop: 0\n","\u001b[32m[I 210623 23:55:16 models:110]\u001b[39m 149 1024 train loss: 0.0061330 valid loss: 0.1817662 P@1: 0.54000 P@3: 0.43333 P@5: 0.35200 N@3: 0.46298 N@5: 0.47602 early stop: 0\n","\u001b[32m[I 210623 23:55:34 models:110]\u001b[39m 174 1024 train loss: 0.0018362 valid loss: 0.1918411 P@1: 0.56000 P@3: 0.45667 P@5: 0.37000 N@3: 0.48409 N@5: 0.49706 early stop: 0\n","\u001b[32m[I 210623 23:55:52 models:110]\u001b[39m 199 1024 train loss: 0.0015366 valid loss: 0.2025761 P@1: 0.57000 P@3: 0.46333 P@5: 0.37200 N@3: 0.49236 N@5: 0.50291 early stop: 0\n","\u001b[32m[I 210623 23:56:10 models:110]\u001b[39m 224 1024 train loss: 0.0057352 valid loss: 0.2101495 P@1: 0.56000 P@3: 0.46667 P@5: 0.37400 N@3: 0.49298 N@5: 0.50486 early stop: 0\n","\u001b[32m[I 210623 23:56:28 models:110]\u001b[39m 249 1024 train loss: 0.0024801 valid loss: 0.2156245 P@1: 0.58000 P@3: 0.47333 P@5: 0.38200 N@3: 0.50530 N@5: 0.51658 early stop: 0\n","\u001b[32m[I 210623 23:56:46 models:110]\u001b[39m 274 1024 train loss: 0.0000528 valid loss: 0.2220434 P@1: 0.61000 P@3: 0.48333 P@5: 0.38800 N@3: 0.51754 N@5: 0.52532 early stop: 0\n","\u001b[32m[I 210623 23:57:04 models:110]\u001b[39m 299 1024 train loss: 0.0000312 valid loss: 0.2285830 P@1: 0.61000 P@3: 0.49333 P@5: 0.38800 N@3: 0.52519 N@5: 0.52734 early stop: 0\n","\u001b[33m[W 210623 23:57:09 models:137]\u001b[39m Clipping gradients with total norm 0.01335 and max norm 0.00178\n","\u001b[32m[I 210623 23:57:21 models:110]\u001b[39m 324 1024 train loss: 0.0000222 valid loss: 0.2350696 P@1: 0.60000 P@3: 0.50000 P@5: 0.38800 N@3: 0.52876 N@5: 0.52775 early stop: 0\n","\u001b[33m[W 210623 23:57:22 models:137]\u001b[39m Clipping gradients with total norm 0.01015 and max norm 0.00117\n","\u001b[33m[W 210623 23:57:27 models:137]\u001b[39m Clipping gradients with total norm 0.00842 and max norm 0.00093\n","\u001b[32m[I 210623 23:57:39 models:110]\u001b[39m 349 1024 train loss: 0.0000163 valid loss: 0.2413728 P@1: 0.60000 P@3: 0.50000 P@5: 0.38600 N@3: 0.52876 N@5: 0.52618 early stop: 1\n","\u001b[32m[I 210623 23:57:57 models:110]\u001b[39m 374 1024 train loss: 0.0000127 valid loss: 0.2475551 P@1: 0.60000 P@3: 0.49667 P@5: 0.38400 N@3: 0.52642 N@5: 0.52406 early stop: 2\n","\u001b[33m[W 210623 23:58:03 models:137]\u001b[39m Clipping gradients with total norm 0.0058 and max norm 0.00114\n","\u001b[32m[I 210623 23:58:15 models:110]\u001b[39m 399 1024 train loss: 0.0000103 valid loss: 0.2535094 P@1: 0.60000 P@3: 0.49667 P@5: 0.38400 N@3: 0.52703 N@5: 0.52470 early stop: 3\n","\u001b[33m[W 210623 23:58:19 models:137]\u001b[39m Clipping gradients with total norm 0.00265 and max norm 0.00053\n","\u001b[32m[I 210623 23:58:33 models:110]\u001b[39m 424 1024 train loss: 0.0000086 valid loss: 0.2593060 P@1: 0.60000 P@3: 0.50000 P@5: 0.38600 N@3: 0.52938 N@5: 0.52687 early stop: 4\n","\u001b[32m[I 210623 23:58:51 models:110]\u001b[39m 449 1024 train loss: 0.0000067 valid loss: 0.2648663 P@1: 0.61000 P@3: 0.50000 P@5: 0.38400 N@3: 0.53111 N@5: 0.52712 early stop: 5\n","\u001b[33m[W 210623 23:58:55 models:137]\u001b[39m Clipping gradients with total norm 0.00869 and max norm 0.00126\n","\u001b[33m[W 210623 23:59:01 models:137]\u001b[39m Clipping gradients with total norm 0.00712 and max norm 0.00041\n","\u001b[32m[I 210623 23:59:09 models:110]\u001b[39m 474 1024 train loss: 0.0000067 valid loss: 0.2700994 P@1: 0.61000 P@3: 0.49667 P@5: 0.38600 N@3: 0.53018 N@5: 0.52936 early stop: 0\n","\u001b[33m[W 210623 23:59:21 models:137]\u001b[39m Clipping gradients with total norm 0.00688 and max norm 0.00048\n","\u001b[32m[I 210623 23:59:27 models:110]\u001b[39m 499 1024 train loss: 0.0000052 valid loss: 0.2752510 P@1: 0.61000 P@3: 0.49667 P@5: 0.38400 N@3: 0.52938 N@5: 0.52694 early stop: 1\n","\u001b[33m[W 210623 23:59:30 models:137]\u001b[39m Clipping gradients with total norm 0.00219 and max norm 0.00033\n","\u001b[32m[I 210623 23:59:44 models:110]\u001b[39m 524 1024 train loss: 0.0000045 valid loss: 0.2802268 P@1: 0.61000 P@3: 0.50000 P@5: 0.38400 N@3: 0.53173 N@5: 0.52734 early stop: 2\n","\u001b[32m[I 210624 00:00:02 models:110]\u001b[39m 549 1024 train loss: 0.0000042 valid loss: 0.2849983 P@1: 0.61000 P@3: 0.49667 P@5: 0.38400 N@3: 0.52938 N@5: 0.52707 early stop: 3\n","\u001b[33m[W 210624 00:00:07 models:137]\u001b[39m Clipping gradients with total norm 0.00243 and max norm 0.00044\n","\u001b[32m[I 210624 00:00:20 models:110]\u001b[39m 574 1024 train loss: 0.0000035 valid loss: 0.2896004 P@1: 0.61000 P@3: 0.49667 P@5: 0.38400 N@3: 0.52938 N@5: 0.52702 early stop: 4\n","\u001b[33m[W 210624 00:00:30 models:137]\u001b[39m Clipping gradients with total norm 0.00299 and max norm 0.0004\n","\u001b[32m[I 210624 00:00:38 models:110]\u001b[39m 599 1024 train loss: 0.0000031 valid loss: 0.2940446 P@1: 0.61000 P@3: 0.49667 P@5: 0.38200 N@3: 0.52938 N@5: 0.52570 early stop: 5\n","\u001b[32m[I 210624 00:00:55 models:110]\u001b[39m 624 1024 train loss: 0.0000027 valid loss: 0.2983665 P@1: 0.61000 P@3: 0.49333 P@5: 0.38200 N@3: 0.52703 N@5: 0.52523 early stop: 6\n","\u001b[33m[W 210624 00:01:09 models:137]\u001b[39m Clipping gradients with total norm 0.00215 and max norm 0.00041\n","\u001b[33m[W 210624 00:01:11 models:137]\u001b[39m Clipping gradients with total norm 0.0037 and max norm 0.00059\n","\u001b[33m[W 210624 00:01:13 models:137]\u001b[39m Clipping gradients with total norm 0.00125 and max norm 0.00018\n","\u001b[32m[I 210624 00:01:13 models:110]\u001b[39m 649 1024 train loss: 0.0000026 valid loss: 0.3025186 P@1: 0.60000 P@3: 0.49667 P@5: 0.38800 N@3: 0.52826 N@5: 0.52876 early stop: 7\n","\u001b[33m[W 210624 00:01:19 models:137]\u001b[39m Clipping gradients with total norm 0.00428 and max norm 0.00039\n","\u001b[32m[I 210624 00:01:31 models:110]\u001b[39m 674 1024 train loss: 0.0000024 valid loss: 0.3065800 P@1: 0.60000 P@3: 0.50000 P@5: 0.38800 N@3: 0.53061 N@5: 0.52892 early stop: 8\n","\u001b[33m[W 210624 00:01:39 models:137]\u001b[39m Clipping gradients with total norm 0.00186 and max norm 0.0003\n","\u001b[32m[I 210624 00:01:49 models:110]\u001b[39m 699 1024 train loss: 0.0000020 valid loss: 0.3104965 P@1: 0.60000 P@3: 0.50000 P@5: 0.38800 N@3: 0.53061 N@5: 0.52892 early stop: 9\n","\u001b[33m[W 210624 00:01:51 models:137]\u001b[39m Clipping gradients with total norm 0.00167 and max norm 0.00015\n","\u001b[33m[W 210624 00:02:02 models:137]\u001b[39m Clipping gradients with total norm 0.00106 and max norm 0.00012\n","\u001b[32m[I 210624 00:02:07 models:110]\u001b[39m 724 1024 train loss: 0.0000018 valid loss: 0.3142850 P@1: 0.60000 P@3: 0.50000 P@5: 0.38800 N@3: 0.53061 N@5: 0.52892 early stop: 10\n","\u001b[33m[W 210624 00:02:15 models:137]\u001b[39m Clipping gradients with total norm 0.00391 and max norm 0.00056\n","\u001b[32m[I 210624 00:02:25 models:110]\u001b[39m 749 1024 train loss: 0.0000017 valid loss: 0.3179969 P@1: 0.60000 P@3: 0.50000 P@5: 0.38600 N@3: 0.53061 N@5: 0.52775 early stop: 11\n","\u001b[33m[W 210624 00:02:33 models:137]\u001b[39m Clipping gradients with total norm 0.00122 and max norm 0.00022\n","\u001b[33m[W 210624 00:02:37 models:137]\u001b[39m Clipping gradients with total norm 0.00176 and max norm 0.00024\n","\u001b[33m[W 210624 00:02:41 models:137]\u001b[39m Clipping gradients with total norm 0.00264 and max norm 0.00022\n","\u001b[32m[I 210624 00:02:42 models:110]\u001b[39m 774 1024 train loss: 0.0000017 valid loss: 0.3215601 P@1: 0.60000 P@3: 0.50000 P@5: 0.38600 N@3: 0.53061 N@5: 0.52775 early stop: 12\n","\u001b[33m[W 210624 00:02:44 models:137]\u001b[39m Clipping gradients with total norm 0.00846 and max norm 0.00017\n","\u001b[32m[I 210624 00:03:00 models:110]\u001b[39m 799 1024 train loss: 0.0000016 valid loss: 0.3250421 P@1: 0.60000 P@3: 0.50000 P@5: 0.38600 N@3: 0.53061 N@5: 0.52755 early stop: 13\n","\u001b[33m[W 210624 00:03:10 models:137]\u001b[39m Clipping gradients with total norm 0.01381 and max norm 0.00026\n","\u001b[32m[I 210624 00:03:18 models:110]\u001b[39m 824 1024 train loss: 0.0000016 valid loss: 0.3284357 P@1: 0.60000 P@3: 0.50000 P@5: 0.38400 N@3: 0.53061 N@5: 0.52574 early stop: 14\n","\u001b[33m[W 210624 00:03:21 models:137]\u001b[39m Clipping gradients with total norm 0.00418 and max norm 0.00038\n","\u001b[32m[I 210624 00:03:36 models:110]\u001b[39m 849 1024 train loss: 0.0000013 valid loss: 0.3317408 P@1: 0.60000 P@3: 0.50000 P@5: 0.38400 N@3: 0.53061 N@5: 0.52589 early stop: 15\n","\u001b[33m[W 210624 00:03:50 models:137]\u001b[39m Clipping gradients with total norm 0.00099 and max norm 0.0002\n","\u001b[33m[W 210624 00:03:52 models:137]\u001b[39m Clipping gradients with total norm 0.00103 and max norm 0.00018\n","\u001b[32m[I 210624 00:03:54 models:110]\u001b[39m 874 1024 train loss: 0.0000013 valid loss: 0.3349921 P@1: 0.60000 P@3: 0.50000 P@5: 0.38400 N@3: 0.53122 N@5: 0.52671 early stop: 16\n","\u001b[33m[W 210624 00:03:56 models:137]\u001b[39m Clipping gradients with total norm 0.00079 and max norm 0.00015\n","\u001b[32m[I 210624 00:04:12 models:110]\u001b[39m 899 1024 train loss: 0.0000010 valid loss: 0.3381548 P@1: 0.60000 P@3: 0.50000 P@5: 0.38400 N@3: 0.53122 N@5: 0.52671 early stop: 17\n","\u001b[33m[W 210624 00:04:14 models:137]\u001b[39m Clipping gradients with total norm 0.00074 and max norm 9e-05\n","\u001b[32m[I 210624 00:04:30 models:110]\u001b[39m 924 1024 train loss: 0.0000010 valid loss: 0.3412432 P@1: 0.60000 P@3: 0.50000 P@5: 0.38400 N@3: 0.53122 N@5: 0.52686 early stop: 18\n","\u001b[33m[W 210624 00:04:35 models:137]\u001b[39m Clipping gradients with total norm 0.00068 and max norm 0.00011\n","\u001b[32m[I 210624 00:04:47 models:110]\u001b[39m 949 1024 train loss: 0.0000009 valid loss: 0.3442575 P@1: 0.61000 P@3: 0.50000 P@5: 0.38400 N@3: 0.53295 N@5: 0.52845 early stop: 19\n","\u001b[33m[W 210624 00:04:55 models:137]\u001b[39m Clipping gradients with total norm 0.00127 and max norm 0.00024\n","\u001b[33m[W 210624 00:04:59 models:137]\u001b[39m Clipping gradients with total norm 0.00668 and max norm 0.00013\n","\u001b[32m[I 210624 00:05:05 models:110]\u001b[39m 974 1024 train loss: 0.0000009 valid loss: 0.3472100 P@1: 0.61000 P@3: 0.50000 P@5: 0.38400 N@3: 0.53295 N@5: 0.52845 early stop: 20\n","\u001b[33m[W 210624 00:05:15 models:137]\u001b[39m Clipping gradients with total norm 0.00076 and max norm 0.00012\n","\u001b[33m[W 210624 00:05:22 models:137]\u001b[39m Clipping gradients with total norm 0.00299 and max norm 7e-05\n","\u001b[32m[I 210624 00:05:23 models:110]\u001b[39m 999 1024 train loss: 0.0000008 valid loss: 0.3501080 P@1: 0.61000 P@3: 0.50000 P@5: 0.38400 N@3: 0.53295 N@5: 0.52845 early stop: 21\n","\u001b[32m[I 210624 00:05:23 main:76]\u001b[39m Finish Training\n","\u001b[32m[I 210624 00:05:25 main:32]\u001b[39m Model Name: MATCH\n","\u001b[32m[I 210624 00:05:25 main:79]\u001b[39m Loading Test Set\n","\u001b[32m[I 210624 00:05:25 main:83]\u001b[39m Size of Test Set: 100\n","\u001b[32m[I 210624 00:05:25 main:85]\u001b[39m Predicting\n","\u001b[32m[I 210624 00:05:29 main:91]\u001b[39m Finish Predicting\n","Precision@1,3,5: 0.61 0.47 0.36\n","nDCG@1,3,5: 0.61 0.5058103299312516 0.5092618406049622\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"feVctRCvVwQV"},"source":["### Results without MAG labels, with MeSH labels"]},{"cell_type":"code","metadata":{"id":"oFedCxzXV4LK"},"source":["!python3 transform_data_PeTaL.py --dataset $DATASET"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fRjbvhvHWQEH","executionInfo":{"status":"ok","timestamp":1624494320147,"user_tz":420,"elapsed":2672,"user":{"displayName":"Eric Kong","photoUrl":"","userId":"07341482379174037459"}},"outputId":"1dad422b-860c-4b6d-8710-ce1a51af291f"},"source":["!python preprocess.py \\\n","--text-path {DATASET}/train_texts.txt \\\n","--label-path {DATASET}/train_labels.txt \\\n","--vocab-path {DATASET}/vocab.npy \\\n","--emb-path {DATASET}/emb_init.npy \\\n","--w2v-model {DATASET}/{DATASET}.joint.emb \\\n","\n","!python preprocess.py \\\n","--text-path {DATASET}/test_texts.txt \\\n","--label-path {DATASET}/test_labels.txt \\\n","--vocab-path {DATASET}/vocab.npy \\"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[32m[I 210624 00:25:18 preprocess:28]\u001b[39m Vocab Size: 26834\n","\u001b[32m[I 210624 00:25:18 preprocess:30]\u001b[39m Getting Dataset: PeTaL/train_texts.txt Max Length: 500\n","\u001b[32m[I 210624 00:25:18 preprocess:32]\u001b[39m Size of Samples: 900\n","\u001b[32m[I 210624 00:25:19 preprocess:28]\u001b[39m Vocab Size: 26834\n","\u001b[32m[I 210624 00:25:19 preprocess:30]\u001b[39m Getting Dataset: PeTaL/test_texts.txt Max Length: 500\n","\u001b[32m[I 210624 00:25:19 preprocess:32]\u001b[39m Size of Samples: 100\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NbXFztbYWSZp","executionInfo":{"status":"ok","timestamp":1624495048181,"user_tz":420,"elapsed":726413,"user":{"displayName":"Eric Kong","photoUrl":"","userId":"07341482379174037459"}},"outputId":"4513209d-5454-43ee-f0f9-0c44fb5a049c"},"source":["!PYTHONFAULTHANDLER=1 python main.py --data-cnf configure/datasets/{DATASET}.yaml --model-cnf configure/models/{MODEL}-{DATASET}.yaml --mode train --reg 1\n","!PYTHONFAULTHANDLER=1 python main.py --data-cnf configure/datasets/{DATASET}.yaml --model-cnf configure/models/{MODEL}-{DATASET}.yaml --mode eval\n","\n","!python evaluation.py \\\n","--results {DATASET}/results/{MODEL}-{DATASET}-labels.npy \\\n","--targets {DATASET}/test_labels.npy \\\n","--train-labels {DATASET}/train_labels.npy"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[32m[I 210624 00:25:22 main:32]\u001b[39m Model Name: MATCH\n","\u001b[32m[I 210624 00:25:22 main:35]\u001b[39m Loading Training and Validation Set\n","/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['absorb_and/or_filter_solids', 'chemically_break_down_inorganic_compounds', 'detox/purify', 'manage_environmental_disturbances_in_a_community', 'protect_from_fire', 'protect_from_gases', 'send_vibratory_signals'] will be ignored\n","  .format(sorted(unknown, key=str)))\n","/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['absorb_and/or_filter_solids', 'detox/purify', 'protect_from_gases'] will be ignored\n","  .format(sorted(unknown, key=str)))\n","\u001b[32m[I 210624 00:25:22 main:47]\u001b[39m Number of Labels: 124\n","\u001b[32m[I 210624 00:25:22 main:48]\u001b[39m Size of Training Set: 800\n","\u001b[32m[I 210624 00:25:22 main:49]\u001b[39m Size of Validation Set: 100\n","\u001b[32m[I 210624 00:25:22 main:66]\u001b[39m Number of Edges: 101\n","\u001b[32m[I 210624 00:25:22 main:68]\u001b[39m Training\n","\u001b[32m[I 210624 00:25:29 models:142]\u001b[39m SWA Initializing\n","\u001b[32m[I 210624 00:25:43 models:110]\u001b[39m 24 1024 train loss: 0.1273809 valid loss: 0.1429620 P@1: 0.25000 P@3: 0.27667 P@5: 0.24200 N@3: 0.27716 N@5: 0.29998 early stop: 0\n","\u001b[32m[I 210624 00:26:01 models:110]\u001b[39m 49 1024 train loss: 0.0264492 valid loss: 0.1411267 P@1: 0.45000 P@3: 0.38000 P@5: 0.31400 N@3: 0.40256 N@5: 0.41598 early stop: 0\n","\u001b[32m[I 210624 00:26:19 models:110]\u001b[39m 74 1024 train loss: 0.0111127 valid loss: 0.1483654 P@1: 0.53000 P@3: 0.40000 P@5: 0.33600 N@3: 0.43349 N@5: 0.45196 early stop: 0\n","\u001b[32m[I 210624 00:26:37 models:110]\u001b[39m 99 1024 train loss: 0.0041654 valid loss: 0.1563504 P@1: 0.55000 P@3: 0.43000 P@5: 0.34400 N@3: 0.46389 N@5: 0.47012 early stop: 0\n","\u001b[32m[I 210624 00:26:55 models:110]\u001b[39m 124 1024 train loss: 0.0054309 valid loss: 0.1628568 P@1: 0.57000 P@3: 0.45000 P@5: 0.35600 N@3: 0.48205 N@5: 0.48678 early stop: 0\n","\u001b[33m[W 210624 00:27:00 models:137]\u001b[39m Clipping gradients with total norm 0.11077 and max norm 0.02065\n","\u001b[32m[I 210624 00:27:12 models:110]\u001b[39m 149 1024 train loss: 0.0008083 valid loss: 0.1739119 P@1: 0.57000 P@3: 0.45000 P@5: 0.37800 N@3: 0.48143 N@5: 0.50538 early stop: 0\n","\u001b[32m[I 210624 00:27:30 models:110]\u001b[39m 174 1024 train loss: 0.0007127 valid loss: 0.1851107 P@1: 0.58000 P@3: 0.46667 P@5: 0.37600 N@3: 0.49551 N@5: 0.50734 early stop: 0\n","\u001b[32m[I 210624 00:27:48 models:110]\u001b[39m 199 1024 train loss: 0.0000953 valid loss: 0.1975456 P@1: 0.56000 P@3: 0.48000 P@5: 0.38600 N@3: 0.50143 N@5: 0.51297 early stop: 0\n","\u001b[32m[I 210624 00:28:06 models:110]\u001b[39m 224 1024 train loss: 0.0000158 valid loss: 0.2093399 P@1: 0.57000 P@3: 0.48000 P@5: 0.38400 N@3: 0.50316 N@5: 0.51233 early stop: 1\n","\u001b[32m[I 210624 00:28:23 models:110]\u001b[39m 249 1024 train loss: 0.0000116 valid loss: 0.2202685 P@1: 0.57000 P@3: 0.47667 P@5: 0.38600 N@3: 0.50143 N@5: 0.51354 early stop: 0\n","\u001b[33m[W 210624 00:28:29 models:137]\u001b[39m Clipping gradients with total norm 0.0075 and max norm 0.00142\n","\u001b[32m[I 210624 00:28:41 models:110]\u001b[39m 274 1024 train loss: 0.0000082 valid loss: 0.2305201 P@1: 0.57000 P@3: 0.48000 P@5: 0.39000 N@3: 0.50378 N@5: 0.51764 early stop: 0\n","\u001b[33m[W 210624 00:28:54 models:137]\u001b[39m Clipping gradients with total norm 0.00856 and max norm 0.00076\n","\u001b[33m[W 210624 00:28:54 models:137]\u001b[39m Clipping gradients with total norm 0.01035 and max norm 0.00153\n","\u001b[33m[W 210624 00:28:55 models:137]\u001b[39m Clipping gradients with total norm 0.00617 and max norm 0.00063\n","\u001b[32m[I 210624 00:28:59 models:110]\u001b[39m 299 1024 train loss: 0.0000067 valid loss: 0.2399982 P@1: 0.57000 P@3: 0.47333 P@5: 0.39000 N@3: 0.49909 N@5: 0.51674 early stop: 1\n","\u001b[33m[W 210624 00:29:03 models:137]\u001b[39m Clipping gradients with total norm 0.00398 and max norm 0.00064\n","\u001b[32m[I 210624 00:29:17 models:110]\u001b[39m 324 1024 train loss: 0.0000055 valid loss: 0.2489225 P@1: 0.57000 P@3: 0.47333 P@5: 0.38600 N@3: 0.49909 N@5: 0.51317 early stop: 2\n","\u001b[33m[W 210624 00:29:23 models:137]\u001b[39m Clipping gradients with total norm 0.01294 and max norm 0.00146\n","\u001b[33m[W 210624 00:29:34 models:137]\u001b[39m Clipping gradients with total norm 0.01237 and max norm 0.00108\n","\u001b[32m[I 210624 00:29:35 models:110]\u001b[39m 349 1024 train loss: 0.0000053 valid loss: 0.2573395 P@1: 0.57000 P@3: 0.47667 P@5: 0.38600 N@3: 0.50143 N@5: 0.51370 early stop: 3\n","\u001b[33m[W 210624 00:29:37 models:137]\u001b[39m Clipping gradients with total norm 0.00703 and max norm 0.00033\n","\u001b[33m[W 210624 00:29:39 models:137]\u001b[39m Clipping gradients with total norm 0.01181 and max norm 0.001\n","\u001b[33m[W 210624 00:29:42 models:137]\u001b[39m Clipping gradients with total norm 0.00523 and max norm 0.00102\n","\u001b[33m[W 210624 00:29:45 models:137]\u001b[39m Clipping gradients with total norm 0.00807 and max norm 0.00157\n","\u001b[33m[W 210624 00:29:50 models:137]\u001b[39m Clipping gradients with total norm 0.0281 and max norm 0.00093\n","\u001b[32m[I 210624 00:29:53 models:110]\u001b[39m 374 1024 train loss: 0.0000051 valid loss: 0.2652150 P@1: 0.57000 P@3: 0.47667 P@5: 0.38600 N@3: 0.50071 N@5: 0.51324 early stop: 4\n","\u001b[32m[I 210624 00:30:11 models:110]\u001b[39m 399 1024 train loss: 0.0000037 valid loss: 0.2726362 P@1: 0.57000 P@3: 0.47667 P@5: 0.38400 N@3: 0.50133 N@5: 0.51245 early stop: 5\n","\u001b[33m[W 210624 00:30:21 models:137]\u001b[39m Clipping gradients with total norm 0.00305 and max norm 0.00047\n","\u001b[33m[W 210624 00:30:26 models:137]\u001b[39m Clipping gradients with total norm 0.01366 and max norm 0.00041\n","\u001b[32m[I 210624 00:30:29 models:110]\u001b[39m 424 1024 train loss: 0.0000032 valid loss: 0.2796437 P@1: 0.57000 P@3: 0.47667 P@5: 0.38400 N@3: 0.50133 N@5: 0.51245 early stop: 6\n","\u001b[33m[W 210624 00:30:32 models:137]\u001b[39m Clipping gradients with total norm 0.00303 and max norm 0.00039\n","\u001b[33m[W 210624 00:30:36 models:137]\u001b[39m Clipping gradients with total norm 0.00405 and max norm 0.00075\n","\u001b[32m[I 210624 00:30:47 models:110]\u001b[39m 449 1024 train loss: 0.0000026 valid loss: 0.2861804 P@1: 0.58000 P@3: 0.47667 P@5: 0.38400 N@3: 0.50367 N@5: 0.51490 early stop: 7\n","\u001b[33m[W 210624 00:30:49 models:137]\u001b[39m Clipping gradients with total norm 0.00763 and max norm 0.00029\n","\u001b[33m[W 210624 00:30:55 models:137]\u001b[39m Clipping gradients with total norm 0.01513 and max norm 0.00024\n","\u001b[32m[I 210624 00:31:05 models:110]\u001b[39m 474 1024 train loss: 0.0000030 valid loss: 0.2923468 P@1: 0.57000 P@3: 0.47667 P@5: 0.38200 N@3: 0.50194 N@5: 0.51185 early stop: 8\n","\u001b[33m[W 210624 00:31:06 models:137]\u001b[39m Clipping gradients with total norm 0.00347 and max norm 0.00044\n","\u001b[32m[I 210624 00:31:22 models:110]\u001b[39m 499 1024 train loss: 0.0000021 valid loss: 0.2982874 P@1: 0.57000 P@3: 0.47667 P@5: 0.38600 N@3: 0.50194 N@5: 0.51498 early stop: 9\n","\u001b[33m[W 210624 00:31:26 models:137]\u001b[39m Clipping gradients with total norm 0.00453 and max norm 0.00046\n","\u001b[33m[W 210624 00:31:31 models:137]\u001b[39m Clipping gradients with total norm 0.00111 and max norm 0.00017\n","\u001b[33m[W 210624 00:31:35 models:137]\u001b[39m Clipping gradients with total norm 0.0026 and max norm 0.0003\n","\u001b[33m[W 210624 00:31:40 models:137]\u001b[39m Clipping gradients with total norm 0.00583 and max norm 0.00025\n","\u001b[32m[I 210624 00:31:40 models:110]\u001b[39m 524 1024 train loss: 0.0000022 valid loss: 0.3039745 P@1: 0.57000 P@3: 0.47667 P@5: 0.38600 N@3: 0.50133 N@5: 0.51446 early stop: 10\n","\u001b[33m[W 210624 00:31:48 models:137]\u001b[39m Clipping gradients with total norm 0.00147 and max norm 0.00019\n","\u001b[33m[W 210624 00:31:51 models:137]\u001b[39m Clipping gradients with total norm 0.00375 and max norm 0.00053\n","\u001b[32m[I 210624 00:31:58 models:110]\u001b[39m 549 1024 train loss: 0.0000017 valid loss: 0.3093106 P@1: 0.57000 P@3: 0.47667 P@5: 0.38400 N@3: 0.50133 N@5: 0.51271 early stop: 11\n","\u001b[33m[W 210624 00:32:01 models:137]\u001b[39m Clipping gradients with total norm 0.00426 and max norm 0.00074\n","\u001b[33m[W 210624 00:32:04 models:137]\u001b[39m Clipping gradients with total norm 0.00251 and max norm 0.00031\n","\u001b[33m[W 210624 00:32:09 models:137]\u001b[39m Clipping gradients with total norm 0.0042 and max norm 0.0003\n","\u001b[33m[W 210624 00:32:14 models:137]\u001b[39m Clipping gradients with total norm 0.00187 and max norm 0.00037\n","\u001b[32m[I 210624 00:32:16 models:110]\u001b[39m 574 1024 train loss: 0.0000017 valid loss: 0.3144954 P@1: 0.57000 P@3: 0.48000 P@5: 0.38600 N@3: 0.50367 N@5: 0.51425 early stop: 12\n","\u001b[33m[W 210624 00:32:18 models:137]\u001b[39m Clipping gradients with total norm 0.00314 and max norm 0.00023\n","\u001b[33m[W 210624 00:32:28 models:137]\u001b[39m Clipping gradients with total norm 0.00132 and max norm 0.00023\n","\u001b[33m[W 210624 00:32:30 models:137]\u001b[39m Clipping gradients with total norm 0.00104 and max norm 0.00019\n","\u001b[33m[W 210624 00:32:33 models:137]\u001b[39m Clipping gradients with total norm 0.00496 and max norm 0.0004\n","\u001b[32m[I 210624 00:32:34 models:110]\u001b[39m 599 1024 train loss: 0.0000015 valid loss: 0.3194538 P@1: 0.58000 P@3: 0.48000 P@5: 0.38600 N@3: 0.50541 N@5: 0.51569 early stop: 13\n","\u001b[33m[W 210624 00:32:37 models:137]\u001b[39m Clipping gradients with total norm 0.00566 and max norm 0.00028\n","\u001b[32m[I 210624 00:32:52 models:110]\u001b[39m 624 1024 train loss: 0.0000015 valid loss: 0.3241620 P@1: 0.57000 P@3: 0.48000 P@5: 0.38400 N@3: 0.50367 N@5: 0.51263 early stop: 14\n","\u001b[33m[W 210624 00:33:04 models:137]\u001b[39m Clipping gradients with total norm 0.00391 and max norm 0.00035\n","\u001b[32m[I 210624 00:33:10 models:110]\u001b[39m 649 1024 train loss: 0.0000014 valid loss: 0.3287477 P@1: 0.57000 P@3: 0.48000 P@5: 0.38400 N@3: 0.50367 N@5: 0.51263 early stop: 15\n","\u001b[33m[W 210624 00:33:15 models:137]\u001b[39m Clipping gradients with total norm 0.00101 and max norm 0.00017\n","\u001b[33m[W 210624 00:33:19 models:137]\u001b[39m Clipping gradients with total norm 0.00097 and max norm 0.00011\n","\u001b[32m[I 210624 00:33:28 models:110]\u001b[39m 674 1024 train loss: 0.0000010 valid loss: 0.3331743 P@1: 0.57000 P@3: 0.48000 P@5: 0.38400 N@3: 0.50367 N@5: 0.51263 early stop: 16\n","\u001b[33m[W 210624 00:33:44 models:137]\u001b[39m Clipping gradients with total norm 0.00122 and max norm 0.00018\n","\u001b[32m[I 210624 00:33:46 models:110]\u001b[39m 699 1024 train loss: 0.0000009 valid loss: 0.3374749 P@1: 0.57000 P@3: 0.47667 P@5: 0.38400 N@3: 0.50133 N@5: 0.51239 early stop: 17\n","\u001b[33m[W 210624 00:33:56 models:137]\u001b[39m Clipping gradients with total norm 0.00066 and max norm 9e-05\n","\u001b[33m[W 210624 00:34:03 models:137]\u001b[39m Clipping gradients with total norm 0.00056 and max norm 0.00011\n","\u001b[32m[I 210624 00:34:04 models:110]\u001b[39m 724 1024 train loss: 0.0000009 valid loss: 0.3416067 P@1: 0.57000 P@3: 0.47667 P@5: 0.38400 N@3: 0.50194 N@5: 0.51301 early stop: 18\n","\u001b[33m[W 210624 00:34:08 models:137]\u001b[39m Clipping gradients with total norm 0.00103 and max norm 0.00019\n","\u001b[33m[W 210624 00:34:10 models:137]\u001b[39m Clipping gradients with total norm 0.00065 and max norm 0.00011\n","\u001b[32m[I 210624 00:34:22 models:110]\u001b[39m 749 1024 train loss: 0.0000009 valid loss: 0.3455681 P@1: 0.57000 P@3: 0.48000 P@5: 0.38400 N@3: 0.50429 N@5: 0.51324 early stop: 19\n","\u001b[32m[I 210624 00:34:39 models:110]\u001b[39m 774 1024 train loss: 0.0000008 valid loss: 0.3493751 P@1: 0.57000 P@3: 0.48000 P@5: 0.38400 N@3: 0.50429 N@5: 0.51324 early stop: 20\n","\u001b[32m[I 210624 00:34:57 models:110]\u001b[39m 799 1024 train loss: 0.0000007 valid loss: 0.3531148 P@1: 0.57000 P@3: 0.48000 P@5: 0.38400 N@3: 0.50429 N@5: 0.51324 early stop: 21\n","\u001b[33m[W 210624 00:34:58 models:137]\u001b[39m Clipping gradients with total norm 0.00157 and max norm 0.00031\n","\u001b[33m[W 210624 00:35:04 models:137]\u001b[39m Clipping gradients with total norm 0.00169 and max norm 0.00014\n","\u001b[33m[W 210624 00:35:05 models:137]\u001b[39m Clipping gradients with total norm 0.00183 and max norm 0.00029\n","\u001b[33m[W 210624 00:35:13 models:137]\u001b[39m Clipping gradients with total norm 0.00192 and max norm 0.0002\n","\u001b[32m[I 210624 00:35:15 models:110]\u001b[39m 824 1024 train loss: 0.0000009 valid loss: 0.3567611 P@1: 0.57000 P@3: 0.48000 P@5: 0.38200 N@3: 0.50429 N@5: 0.51143 early stop: 22\n","\u001b[33m[W 210624 00:35:19 models:137]\u001b[39m Clipping gradients with total norm 0.00383 and max norm 0.00012\n","\u001b[33m[W 210624 00:35:24 models:137]\u001b[39m Clipping gradients with total norm 0.00078 and max norm 0.00015\n","\u001b[32m[I 210624 00:35:33 models:110]\u001b[39m 849 1024 train loss: 0.0000006 valid loss: 0.3603132 P@1: 0.57000 P@3: 0.48000 P@5: 0.38200 N@3: 0.50429 N@5: 0.51143 early stop: 23\n","\u001b[33m[W 210624 00:35:34 models:137]\u001b[39m Clipping gradients with total norm 0.00176 and max norm 5e-05\n","\u001b[33m[W 210624 00:35:40 models:137]\u001b[39m Clipping gradients with total norm 0.00164 and max norm 0.00013\n","\u001b[33m[W 210624 00:35:46 models:137]\u001b[39m Clipping gradients with total norm 0.00131 and max norm 0.00024\n","\u001b[33m[W 210624 00:35:50 models:137]\u001b[39m Clipping gradients with total norm 0.00086 and max norm 0.00016\n","\u001b[32m[I 210624 00:35:51 models:110]\u001b[39m 874 1024 train loss: 0.0000006 valid loss: 0.3637739 P@1: 0.57000 P@3: 0.48000 P@5: 0.38200 N@3: 0.50429 N@5: 0.51143 early stop: 24\n","\u001b[33m[W 210624 00:35:52 models:137]\u001b[39m Clipping gradients with total norm 0.00052 and max norm 7e-05\n","\u001b[33m[W 210624 00:35:59 models:137]\u001b[39m Clipping gradients with total norm 0.0008 and max norm 0.00012\n","\u001b[32m[I 210624 00:36:09 models:110]\u001b[39m 899 1024 train loss: 0.0000005 valid loss: 0.3671342 P@1: 0.57000 P@3: 0.48000 P@5: 0.38000 N@3: 0.50429 N@5: 0.50961 early stop: 25\n","\u001b[33m[W 210624 00:36:13 models:137]\u001b[39m Clipping gradients with total norm 0.00087 and max norm 6e-05\n","\u001b[33m[W 210624 00:36:19 models:137]\u001b[39m Clipping gradients with total norm 0.00145 and max norm 0.00016\n","\u001b[33m[W 210624 00:36:20 models:137]\u001b[39m Clipping gradients with total norm 0.00638 and max norm 8e-05\n","\u001b[33m[W 210624 00:36:25 models:137]\u001b[39m Clipping gradients with total norm 0.00065 and max norm 0.0001\n","\u001b[32m[I 210624 00:36:27 models:110]\u001b[39m 924 1024 train loss: 0.0000006 valid loss: 0.3704598 P@1: 0.57000 P@3: 0.48000 P@5: 0.38000 N@3: 0.50429 N@5: 0.50961 early stop: 26\n","\u001b[33m[W 210624 00:36:30 models:137]\u001b[39m Clipping gradients with total norm 0.00041 and max norm 6e-05\n","\u001b[33m[W 210624 00:36:31 models:137]\u001b[39m Clipping gradients with total norm 0.00127 and max norm 8e-05\n","\u001b[33m[W 210624 00:36:41 models:137]\u001b[39m Clipping gradients with total norm 0.00379 and max norm 0.00053\n","\u001b[33m[W 210624 00:36:44 models:137]\u001b[39m Clipping gradients with total norm 0.00217 and max norm 7e-05\n","\u001b[33m[W 210624 00:36:44 models:137]\u001b[39m Clipping gradients with total norm 0.00093 and max norm 0.00017\n","\u001b[32m[I 210624 00:36:45 models:110]\u001b[39m 949 1024 train loss: 0.0000006 valid loss: 0.3736735 P@1: 0.57000 P@3: 0.48000 P@5: 0.38000 N@3: 0.50429 N@5: 0.50976 early stop: 27\n","\u001b[33m[W 210624 00:36:49 models:137]\u001b[39m Clipping gradients with total norm 0.002 and max norm 5e-05\n","\u001b[33m[W 210624 00:36:50 models:137]\u001b[39m Clipping gradients with total norm 0.00061 and max norm 5e-05\n","\u001b[33m[W 210624 00:36:59 models:137]\u001b[39m Clipping gradients with total norm 0.0008 and max norm 0.00013\n","\u001b[33m[W 210624 00:37:01 models:137]\u001b[39m Clipping gradients with total norm 0.00287 and max norm 0.00029\n","\u001b[32m[I 210624 00:37:03 models:110]\u001b[39m 974 1024 train loss: 0.0000005 valid loss: 0.3768439 P@1: 0.57000 P@3: 0.48000 P@5: 0.37800 N@3: 0.50429 N@5: 0.50845 early stop: 28\n","\u001b[33m[W 210624 00:37:03 models:137]\u001b[39m Clipping gradients with total norm 0.00055 and max norm 9e-05\n","\u001b[33m[W 210624 00:37:05 models:137]\u001b[39m Clipping gradients with total norm 0.00037 and max norm 7e-05\n","\u001b[33m[W 210624 00:37:09 models:137]\u001b[39m Clipping gradients with total norm 0.00105 and max norm 9e-05\n","\u001b[32m[I 210624 00:37:21 models:110]\u001b[39m 999 1024 train loss: 0.0000005 valid loss: 0.3799064 P@1: 0.57000 P@3: 0.48000 P@5: 0.37800 N@3: 0.50490 N@5: 0.50889 early stop: 29\n","\u001b[32m[I 210624 00:37:21 main:76]\u001b[39m Finish Training\n","\u001b[32m[I 210624 00:37:22 main:32]\u001b[39m Model Name: MATCH\n","\u001b[32m[I 210624 00:37:22 main:79]\u001b[39m Loading Test Set\n","\u001b[32m[I 210624 00:37:22 main:83]\u001b[39m Size of Test Set: 100\n","\u001b[32m[I 210624 00:37:22 main:85]\u001b[39m Predicting\n","\u001b[32m[I 210624 00:37:26 main:91]\u001b[39m Finish Predicting\n","Precision@1,3,5: 0.63 0.5133333333333333 0.394\n","nDCG@1,3,5: 0.63 0.5428495108215929 0.5469096994042273\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-4gUb3gaZeUJ"},"source":["### Results without MAG labels, without MeSH labels\n","\n"]},{"cell_type":"code","metadata":{"id":"rut4NXhSZnow"},"source":["!python3 transform_data_PeTaL.py --dataset $DATASET"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UynO1c2Ga_Oj","executionInfo":{"status":"ok","timestamp":1624570376117,"user_tz":420,"elapsed":2672,"user":{"displayName":"Eric Kong","photoUrl":"","userId":"07341482379174037459"}},"outputId":"3c48df05-6b35-4f73-e699-2a2154f9364f"},"source":["!python preprocess.py \\\n","--text-path {DATASET}/train_texts.txt \\\n","--label-path {DATASET}/train_labels.txt \\\n","--vocab-path {DATASET}/vocab.npy \\\n","--emb-path {DATASET}/emb_init.npy \\\n","--w2v-model {DATASET}/{DATASET}.joint.emb \\\n","\n","!python preprocess.py \\\n","--text-path {DATASET}/test_texts.txt \\\n","--label-path {DATASET}/test_labels.txt \\\n","--vocab-path {DATASET}/vocab.npy \\"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Traceback (most recent call last):\n","  File \"preprocess.py\", line 8, in <module>\n","    from deepxml.data_utils import build_vocab, convert_to_binary\n","  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n","  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n","  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n","  File \"<frozen importlib._bootstrap_external>\", line 724, in exec_module\n","  File \"<frozen importlib._bootstrap_external>\", line 818, in get_code\n","  File \"<frozen importlib._bootstrap_external>\", line 917, in get_data\n","KeyboardInterrupt\n","Traceback (most recent call last):\n","  File \"preprocess.py\", line 8, in <module>\n","    from deepxml.data_utils import build_vocab, convert_to_binary\n","  File \"/content/drive/Shareddrives/MATCH Attempt/MATCH/deepxml/data_utils.py\", line 5, in <module>\n","    from sklearn.preprocessing import MultiLabelBinarizer, normalize\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/__init__.py\", line 6, in <module>\n","    from ._function_transformer import FunctionTransformer\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_function_transformer.py\", line 5, in <module>\n","    from ..utils.testing import assert_allclose_dense_sparse\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/testing.py\", line 21, in <module>\n","    import scipy.io\n","  File \"/usr/local/lib/python3.7/dist-packages/scipy/io/__init__.py\", line 97, in <module>\n","    from .matlab import loadmat, savemat, whosmat, byteordercodes\n","  File \"/usr/local/lib/python3.7/dist-packages/scipy/io/matlab/__init__.py\", line 13, in <module>\n","    from .mio import loadmat, savemat, whosmat\n","  File \"/usr/local/lib/python3.7/dist-packages/scipy/io/matlab/mio.py\", line 10, in <module>\n","    from .miobase import get_matfile_version, docfiller\n","  File \"/usr/local/lib/python3.7/dist-packages/scipy/io/matlab/miobase.py\", line 22, in <module>\n","    from scipy.misc import doccer\n","  File \"/usr/local/lib/python3.7/dist-packages/scipy/misc/__init__.py\", line 100, in <module>\n","    from .pilutil import *\n","  File \"/usr/local/lib/python3.7/dist-packages/scipy/misc/pilutil.py\", line 19, in <module>\n","    from PIL import Image, ImageFilter\n","  File \"/usr/local/lib/python3.7/dist-packages/PIL/Image.py\", line 123, in <module>\n","    import cffi\n","  File \"/usr/local/lib/python3.7/dist-packages/cffi/__init__.py\", line 4, in <module>\n","    from .api import FFI\n","  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n","  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n","  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n","  File \"<frozen importlib._bootstrap_external>\", line 724, in exec_module\n","  File \"<frozen importlib._bootstrap_external>\", line 857, in get_code\n","  File \"<frozen importlib._bootstrap_external>\", line 525, in _compile_bytecode\n","KeyboardInterrupt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6u51rKSrbCIY","executionInfo":{"status":"ok","timestamp":1624570389472,"user_tz":420,"elapsed":13358,"user":{"displayName":"Eric Kong","photoUrl":"","userId":"07341482379174037459"}},"outputId":"9e0b9e36-8ee1-423a-b088-3584a0420a58"},"source":["!PYTHONFAULTHANDLER=1 python main.py --data-cnf configure/datasets/{DATASET}.yaml --model-cnf configure/models/{MODEL}-{DATASET}.yaml --mode train --reg 1\n","!PYTHONFAULTHANDLER=1 python main.py --data-cnf configure/datasets/{DATASET}.yaml --model-cnf configure/models/{MODEL}-{DATASET}.yaml --mode eval\n","\n","!python evaluation.py \\\n","--results {DATASET}/results/{MODEL}-{DATASET}-labels.npy \\\n","--targets {DATASET}/test_labels.npy \\\n","--train-labels {DATASET}/train_labels.npy"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Aborted!\n","\u001b[32m[I 210624 21:33:03 main:32]\u001b[39m Model Name: MATCH\n","\u001b[32m[I 210624 21:33:03 main:79]\u001b[39m Loading Test Set\n","\u001b[32m[I 210624 21:33:03 main:83]\u001b[39m Size of Test Set: 100\n","\u001b[32m[I 210624 21:33:03 main:85]\u001b[39m Predicting\n","\u001b[32m[I 210624 21:33:06 main:91]\u001b[39m Finish Predicting\n","Precision@1,3,5: 0.66 0.53 0.42\n","nDCG@1,3,5: 0.66 0.5620467187130749 0.5699775424270964\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tsYoo4ymh-Dy"},"source":["## Ablation study: Turn off hypernymy regularization\n","\n","Investigating the effect of the hierarachy (PeTaL/taxonomy.txt)."]},{"cell_type":"code","metadata":{"id":"z28moIQCiENw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624548428432,"user_tz":420,"elapsed":965907,"user":{"displayName":"Eric Kong","photoUrl":"","userId":"07341482379174037459"}},"outputId":"bfb791e1-0563-440e-eef3-b272f43cf469"},"source":["# note: --reg 0 turns of hypernymy regularization\n","!PYTHONFAULTHANDLER=1 python main.py --data-cnf configure/datasets/PeTaL.yaml --model-cnf configure/models/MATCH-PeTaL.yaml --mode train --reg 0\n","!PYTHONFAULTHANDLER=1 python main.py --data-cnf configure/datasets/PeTaL.yaml --model-cnf configure/models/MATCH-PeTaL.yaml --mode eval\n","\n","!python evaluation.py --results PeTaL/results/MATCH-PeTaL-labels.npy --targets PeTaL/test_labels.npy --train-labels PeTaL/train_labels.npy"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[32m[I 210624 15:11:07 main:32]\u001b[39m Model Name: MATCH\n","\u001b[32m[I 210624 15:11:07 main:35]\u001b[39m Loading Training and Validation Set\n","/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['absorb_and/or_filter_solids', 'chemically_break_down_inorganic_compounds', 'detox/purify', 'manage_environmental_disturbances_in_a_community', 'protect_from_fire', 'protect_from_gases', 'send_vibratory_signals'] will be ignored\n","  .format(sorted(unknown, key=str)))\n","/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['absorb_and/or_filter_solids', 'detox/purify', 'protect_from_gases'] will be ignored\n","  .format(sorted(unknown, key=str)))\n","\u001b[32m[I 210624 15:11:08 main:47]\u001b[39m Number of Labels: 124\n","\u001b[32m[I 210624 15:11:08 main:48]\u001b[39m Size of Training Set: 800\n","\u001b[32m[I 210624 15:11:08 main:49]\u001b[39m Size of Validation Set: 100\n","\u001b[32m[I 210624 15:11:08 main:68]\u001b[39m Training\n","\u001b[32m[I 210624 15:11:14 models:142]\u001b[39m SWA Initializing\n","\u001b[32m[I 210624 15:11:35 models:110]\u001b[39m 24 1024 train loss: 0.1316223 valid loss: 0.1447000 P@1: 0.21000 P@3: 0.19333 P@5: 0.17200 N@3: 0.19867 N@5: 0.20960 early stop: 0\n","\u001b[32m[I 210624 15:11:59 models:110]\u001b[39m 49 1024 train loss: 0.0376488 valid loss: 0.1427039 P@1: 0.37000 P@3: 0.26000 P@5: 0.24200 N@3: 0.28756 N@5: 0.31248 early stop: 0\n","\u001b[32m[I 210624 15:12:22 models:110]\u001b[39m 74 1024 train loss: 0.0113640 valid loss: 0.1442004 P@1: 0.49000 P@3: 0.39000 P@5: 0.31600 N@3: 0.42163 N@5: 0.42813 early stop: 0\n","\u001b[32m[I 210624 15:12:46 models:110]\u001b[39m 99 1024 train loss: 0.0058840 valid loss: 0.1494539 P@1: 0.61000 P@3: 0.47333 P@5: 0.36200 N@3: 0.51002 N@5: 0.50068 early stop: 0\n","\u001b[32m[I 210624 15:13:10 models:110]\u001b[39m 124 1024 train loss: 0.0012710 valid loss: 0.1610811 P@1: 0.63000 P@3: 0.50667 P@5: 0.37800 N@3: 0.54266 N@5: 0.52604 early stop: 0\n","\u001b[32m[I 210624 15:13:34 models:110]\u001b[39m 149 1024 train loss: 0.0008330 valid loss: 0.1722448 P@1: 0.67000 P@3: 0.51667 P@5: 0.38800 N@3: 0.55663 N@5: 0.54090 early stop: 0\n","\u001b[32m[I 210624 15:13:57 models:110]\u001b[39m 174 1024 train loss: 0.0020319 valid loss: 0.1844395 P@1: 0.66000 P@3: 0.51333 P@5: 0.40000 N@3: 0.55306 N@5: 0.54949 early stop: 0\n","\u001b[32m[I 210624 15:14:21 models:110]\u001b[39m 199 1024 train loss: 0.0007097 valid loss: 0.1958750 P@1: 0.66000 P@3: 0.52000 P@5: 0.40200 N@3: 0.55775 N@5: 0.55229 early stop: 0\n","\u001b[32m[I 210624 15:14:45 models:110]\u001b[39m 224 1024 train loss: 0.0091214 valid loss: 0.1984477 P@1: 0.67000 P@3: 0.52333 P@5: 0.40600 N@3: 0.56183 N@5: 0.55719 early stop: 0\n","\u001b[32m[I 210624 15:15:09 models:110]\u001b[39m 249 1024 train loss: 0.0009238 valid loss: 0.2055015 P@1: 0.66000 P@3: 0.52667 P@5: 0.40400 N@3: 0.56244 N@5: 0.55483 early stop: 1\n","\u001b[32m[I 210624 15:15:33 models:110]\u001b[39m 274 1024 train loss: 0.0000434 valid loss: 0.2125400 P@1: 0.66000 P@3: 0.52667 P@5: 0.40200 N@3: 0.56244 N@5: 0.55374 early stop: 2\n","\u001b[32m[I 210624 15:15:57 models:110]\u001b[39m 299 1024 train loss: 0.0000251 valid loss: 0.2195170 P@1: 0.66000 P@3: 0.53000 P@5: 0.41200 N@3: 0.56541 N@5: 0.56033 early stop: 0\n","\u001b[32m[I 210624 15:16:20 models:110]\u001b[39m 324 1024 train loss: 0.0000178 valid loss: 0.2263328 P@1: 0.65000 P@3: 0.53333 P@5: 0.41000 N@3: 0.56602 N@5: 0.55932 early stop: 1\n","\u001b[33m[W 210624 15:16:23 models:137]\u001b[39m Clipping gradients with total norm 0.02378 and max norm 0.00173\n","\u001b[32m[I 210624 15:16:44 models:110]\u001b[39m 349 1024 train loss: 0.0000151 valid loss: 0.2328805 P@1: 0.65000 P@3: 0.53333 P@5: 0.40800 N@3: 0.56541 N@5: 0.55762 early stop: 2\n","\u001b[33m[W 210624 15:16:45 models:137]\u001b[39m Clipping gradients with total norm 0.00934 and max norm 0.00128\n","\u001b[33m[W 210624 15:17:04 models:137]\u001b[39m Clipping gradients with total norm 0.00943 and max norm 0.00124\n","\u001b[32m[I 210624 15:17:08 models:110]\u001b[39m 374 1024 train loss: 0.0000105 valid loss: 0.2391268 P@1: 0.66000 P@3: 0.53667 P@5: 0.40800 N@3: 0.57010 N@5: 0.56043 early stop: 0\n","\u001b[33m[W 210624 15:17:11 models:137]\u001b[39m Clipping gradients with total norm 0.01799 and max norm 0.00061\n","\u001b[33m[W 210624 15:17:28 models:137]\u001b[39m Clipping gradients with total norm 0.00906 and max norm 0.00107\n","\u001b[32m[I 210624 15:17:31 models:110]\u001b[39m 399 1024 train loss: 0.0000091 valid loss: 0.2451547 P@1: 0.67000 P@3: 0.53667 P@5: 0.41400 N@3: 0.57244 N@5: 0.56731 early stop: 0\n","\u001b[33m[W 210624 15:17:36 models:137]\u001b[39m Clipping gradients with total norm 0.00593 and max norm 0.00109\n","\u001b[33m[W 210624 15:17:38 models:137]\u001b[39m Clipping gradients with total norm 0.00301 and max norm 0.00046\n","\u001b[32m[I 210624 15:17:55 models:110]\u001b[39m 424 1024 train loss: 0.0000070 valid loss: 0.2508962 P@1: 0.67000 P@3: 0.54333 P@5: 0.41400 N@3: 0.57775 N@5: 0.56931 early stop: 0\n","\u001b[32m[I 210624 15:18:19 models:110]\u001b[39m 449 1024 train loss: 0.0000053 valid loss: 0.2563647 P@1: 0.67000 P@3: 0.54667 P@5: 0.41200 N@3: 0.57948 N@5: 0.56781 early stop: 1\n","\u001b[33m[W 210624 15:18:25 models:137]\u001b[39m Clipping gradients with total norm 0.00392 and max norm 0.00026\n","\u001b[32m[I 210624 15:18:42 models:110]\u001b[39m 474 1024 train loss: 0.0000049 valid loss: 0.2616051 P@1: 0.67000 P@3: 0.54667 P@5: 0.41200 N@3: 0.57948 N@5: 0.56781 early stop: 2\n","\u001b[32m[I 210624 15:19:06 models:110]\u001b[39m 499 1024 train loss: 0.0000041 valid loss: 0.2666784 P@1: 0.67000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58010 N@5: 0.57170 early stop: 0\n","\u001b[32m[I 210624 15:19:30 models:110]\u001b[39m 524 1024 train loss: 0.0000035 valid loss: 0.2715280 P@1: 0.67000 P@3: 0.54667 P@5: 0.41600 N@3: 0.58071 N@5: 0.57273 early stop: 0\n","\u001b[32m[I 210624 15:19:54 models:110]\u001b[39m 549 1024 train loss: 0.0000031 valid loss: 0.2761523 P@1: 0.67000 P@3: 0.55333 P@5: 0.41800 N@3: 0.58541 N@5: 0.57464 early stop: 0\n","\u001b[33m[W 210624 15:20:15 models:137]\u001b[39m Clipping gradients with total norm 0.00665 and max norm 0.00047\n","\u001b[32m[I 210624 15:20:17 models:110]\u001b[39m 574 1024 train loss: 0.0000028 valid loss: 0.2805956 P@1: 0.66000 P@3: 0.55333 P@5: 0.41800 N@3: 0.58429 N@5: 0.57362 early stop: 1\n","\u001b[32m[I 210624 15:20:41 models:110]\u001b[39m 599 1024 train loss: 0.0000024 valid loss: 0.2848959 P@1: 0.66000 P@3: 0.55333 P@5: 0.41800 N@3: 0.58429 N@5: 0.57351 early stop: 2\n","\u001b[33m[W 210624 15:20:54 models:137]\u001b[39m Clipping gradients with total norm 0.00217 and max norm 0.0002\n","\u001b[32m[I 210624 15:21:05 models:110]\u001b[39m 624 1024 train loss: 0.0000021 valid loss: 0.2890317 P@1: 0.66000 P@3: 0.55333 P@5: 0.42000 N@3: 0.58367 N@5: 0.57471 early stop: 0\n","\u001b[32m[I 210624 15:21:29 models:110]\u001b[39m 649 1024 train loss: 0.0000020 valid loss: 0.2930167 P@1: 0.66000 P@3: 0.55333 P@5: 0.42000 N@3: 0.58367 N@5: 0.57471 early stop: 1\n","\u001b[33m[W 210624 15:21:37 models:137]\u001b[39m Clipping gradients with total norm 0.0009 and max norm 0.00016\n","\u001b[33m[W 210624 15:21:42 models:137]\u001b[39m Clipping gradients with total norm 0.00165 and max norm 0.00025\n","\u001b[33m[W 210624 15:21:50 models:137]\u001b[39m Clipping gradients with total norm 0.00133 and max norm 0.00023\n","\u001b[33m[W 210624 15:21:53 models:137]\u001b[39m Clipping gradients with total norm 0.00136 and max norm 0.00025\n","\u001b[32m[I 210624 15:21:53 models:110]\u001b[39m 674 1024 train loss: 0.0000018 valid loss: 0.2968616 P@1: 0.65000 P@3: 0.55000 P@5: 0.42000 N@3: 0.57898 N@5: 0.57291 early stop: 2\n","\u001b[33m[W 210624 15:22:06 models:137]\u001b[39m Clipping gradients with total norm 0.0052 and max norm 0.00044\n","\u001b[33m[W 210624 15:22:13 models:137]\u001b[39m Clipping gradients with total norm 0.00504 and max norm 0.00012\n","\u001b[32m[I 210624 15:22:17 models:110]\u001b[39m 699 1024 train loss: 0.0000017 valid loss: 0.3005809 P@1: 0.65000 P@3: 0.55000 P@5: 0.42000 N@3: 0.57898 N@5: 0.57291 early stop: 3\n","\u001b[32m[I 210624 15:22:40 models:110]\u001b[39m 724 1024 train loss: 0.0000014 valid loss: 0.3041970 P@1: 0.65000 P@3: 0.55000 P@5: 0.41800 N@3: 0.57960 N@5: 0.57098 early stop: 4\n","\u001b[33m[W 210624 15:22:43 models:137]\u001b[39m Clipping gradients with total norm 0.00235 and max norm 7e-05\n","\u001b[33m[W 210624 15:22:46 models:137]\u001b[39m Clipping gradients with total norm 0.00094 and max norm 0.00014\n","\u001b[33m[W 210624 15:22:58 models:137]\u001b[39m Clipping gradients with total norm 0.00589 and max norm 0.00025\n","\u001b[32m[I 210624 15:23:04 models:110]\u001b[39m 749 1024 train loss: 0.0000014 valid loss: 0.3076957 P@1: 0.65000 P@3: 0.54667 P@5: 0.41800 N@3: 0.57725 N@5: 0.57075 early stop: 5\n","\u001b[33m[W 210624 15:23:20 models:137]\u001b[39m Clipping gradients with total norm 0.00194 and max norm 0.00027\n","\u001b[32m[I 210624 15:23:27 models:110]\u001b[39m 774 1024 train loss: 0.0000012 valid loss: 0.3110780 P@1: 0.65000 P@3: 0.54667 P@5: 0.41800 N@3: 0.57725 N@5: 0.57075 early stop: 6\n","\u001b[33m[W 210624 15:23:38 models:137]\u001b[39m Clipping gradients with total norm 0.00133 and max norm 0.00011\n","\u001b[33m[W 210624 15:23:43 models:137]\u001b[39m Clipping gradients with total norm 0.00084 and max norm 9e-05\n","\u001b[32m[I 210624 15:23:51 models:110]\u001b[39m 799 1024 train loss: 0.0000011 valid loss: 0.3143861 P@1: 0.66000 P@3: 0.54667 P@5: 0.41800 N@3: 0.57898 N@5: 0.57248 early stop: 7\n","\u001b[33m[W 210624 15:24:12 models:137]\u001b[39m Clipping gradients with total norm 0.00596 and max norm 7e-05\n","\u001b[32m[I 210624 15:24:15 models:110]\u001b[39m 824 1024 train loss: 0.0000010 valid loss: 0.3175977 P@1: 0.66000 P@3: 0.54333 P@5: 0.42000 N@3: 0.57725 N@5: 0.57417 early stop: 8\n","\u001b[33m[W 210624 15:24:24 models:137]\u001b[39m Clipping gradients with total norm 0.00767 and max norm 9e-05\n","\u001b[33m[W 210624 15:24:33 models:137]\u001b[39m Clipping gradients with total norm 0.01256 and max norm 0.00042\n","\u001b[33m[W 210624 15:24:38 models:137]\u001b[39m Clipping gradients with total norm 0.00293 and max norm 0.00025\n","\u001b[32m[I 210624 15:24:38 models:110]\u001b[39m 849 1024 train loss: 0.0000013 valid loss: 0.3207212 P@1: 0.66000 P@3: 0.54333 P@5: 0.41800 N@3: 0.57725 N@5: 0.57271 early stop: 9\n","\u001b[33m[W 210624 15:24:46 models:137]\u001b[39m Clipping gradients with total norm 0.0006 and max norm 0.00012\n","\u001b[33m[W 210624 15:24:58 models:137]\u001b[39m Clipping gradients with total norm 0.00112 and max norm 0.00021\n","\u001b[32m[I 210624 15:25:02 models:110]\u001b[39m 874 1024 train loss: 0.0000008 valid loss: 0.3237781 P@1: 0.67000 P@3: 0.54667 P@5: 0.42000 N@3: 0.58133 N@5: 0.57608 early stop: 0\n","\u001b[32m[I 210624 15:25:26 models:110]\u001b[39m 899 1024 train loss: 0.0000007 valid loss: 0.3267716 P@1: 0.67000 P@3: 0.55000 P@5: 0.42000 N@3: 0.58367 N@5: 0.57640 early stop: 0\n","\u001b[33m[W 210624 15:25:29 models:137]\u001b[39m Clipping gradients with total norm 0.00269 and max norm 0.00021\n","\u001b[32m[I 210624 15:25:49 models:110]\u001b[39m 924 1024 train loss: 0.0000008 valid loss: 0.3297160 P@1: 0.67000 P@3: 0.55333 P@5: 0.42200 N@3: 0.58602 N@5: 0.57849 early stop: 0\n","\u001b[33m[W 210624 15:26:02 models:137]\u001b[39m Clipping gradients with total norm 0.00169 and max norm 0.00016\n","\u001b[32m[I 210624 15:26:13 models:110]\u001b[39m 949 1024 train loss: 0.0000007 valid loss: 0.3326059 P@1: 0.67000 P@3: 0.55333 P@5: 0.42400 N@3: 0.58602 N@5: 0.57980 early stop: 0\n","\u001b[33m[W 210624 15:26:18 models:137]\u001b[39m Clipping gradients with total norm 0.00199 and max norm 0.00014\n","\u001b[33m[W 210624 15:26:20 models:137]\u001b[39m Clipping gradients with total norm 0.02167 and max norm 0.00033\n","\u001b[33m[W 210624 15:26:22 models:137]\u001b[39m Clipping gradients with total norm 0.00422 and max norm 0.00011\n","\u001b[33m[W 210624 15:26:24 models:137]\u001b[39m Clipping gradients with total norm 0.0054 and max norm 0.00021\n","\u001b[33m[W 210624 15:26:29 models:137]\u001b[39m Clipping gradients with total norm 0.00138 and max norm 0.00027\n","\u001b[33m[W 210624 15:26:36 models:137]\u001b[39m Clipping gradients with total norm 0.00096 and max norm 0.00014\n","\u001b[32m[I 210624 15:26:37 models:110]\u001b[39m 974 1024 train loss: 0.0000017 valid loss: 0.3354567 P@1: 0.67000 P@3: 0.55333 P@5: 0.42400 N@3: 0.58602 N@5: 0.57995 early stop: 0\n","\u001b[33m[W 210624 15:26:47 models:137]\u001b[39m Clipping gradients with total norm 0.00035 and max norm 6e-05\n","\u001b[33m[W 210624 15:26:49 models:137]\u001b[39m Clipping gradients with total norm 0.00077 and max norm 0.00015\n","\u001b[32m[I 210624 15:27:01 models:110]\u001b[39m 999 1024 train loss: 0.0000006 valid loss: 0.3382077 P@1: 0.67000 P@3: 0.55333 P@5: 0.42400 N@3: 0.58602 N@5: 0.57995 early stop: 1\n","\u001b[32m[I 210624 15:27:01 main:76]\u001b[39m Finish Training\n","\u001b[32m[I 210624 15:27:02 main:32]\u001b[39m Model Name: MATCH\n","\u001b[32m[I 210624 15:27:02 main:79]\u001b[39m Loading Test Set\n","\u001b[32m[I 210624 15:27:02 main:83]\u001b[39m Size of Test Set: 100\n","\u001b[32m[I 210624 15:27:02 main:85]\u001b[39m Predicting\n","\u001b[32m[I 210624 15:27:05 main:91]\u001b[39m Finish Predicting\n","Precision@1,3,5: 0.66 0.53 0.42\n","nDCG@1,3,5: 0.66 0.5620467187130749 0.5699775424270964\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GBhMpPdH3DuI"},"source":["## Study: Effect of Dataset Size on MATCH Performance\n","\n","| Train size |   P@1 |\n","|------------|-------|\n","|        200 | 0.324 |\n","|        300 | 0.424 |\n","|        400 | 0.441 |\n","|        500 | 0.547 |\n","|        600 | 0.534 |\n","|        700 | 0.555 |\n","|        800 | 0.627 |"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sEhF-2cM8wnk","executionInfo":{"status":"ok","timestamp":1624640491057,"user_tz":420,"elapsed":684,"user":{"displayName":"Eric Kong","photoUrl":"","userId":"07341482379174037459"}},"outputId":"eb151674-c029-4176-cfee-3e66cfaba3d6"},"source":["# Note: I fiddled with the hardcoded train-test split in PeTal/Split.py before running this.\n","# Probably better to add a CLI option to specify train-test split.\n","%cd PeTaL/\n","!python3 Split.py\n","%cd ..\n","!wc PeTaL/train.json"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/Shareddrives/MATCH Attempt/MATCH/PeTaL\n","131\n","/content/drive/Shareddrives/MATCH Attempt/MATCH\n","    800  275452 4644161 PeTaL/train.json\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-0ccg2sz_SCc","executionInfo":{"status":"ok","timestamp":1624640495420,"user_tz":420,"elapsed":2916,"user":{"displayName":"Eric Kong","photoUrl":"","userId":"07341482379174037459"}},"outputId":"de48dd4e-e9f1-4b33-8aad-36b6ec405af8"},"source":["# Slightly modified preprocess.sh\n","\n","!python3 transform_data_PeTaL.py --dataset $DATASET\n","\n","!python preprocess.py \\\n","--text-path {DATASET}/train_texts.txt \\\n","--label-path {DATASET}/train_labels.txt \\\n","--vocab-path {DATASET}/vocab.npy \\\n","--emb-path {DATASET}/emb_init.npy \\\n","--w2v-model {DATASET}/{DATASET}.joint.emb \\\n","\n","!python preprocess.py \\\n","--text-path {DATASET}/test_texts.txt \\\n","--label-path {DATASET}/test_labels.txt \\\n","--vocab-path {DATASET}/vocab.npy \\"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[32m[I 210625 17:01:33 preprocess:28]\u001b[39m Vocab Size: 26834\n","\u001b[32m[I 210625 17:01:33 preprocess:30]\u001b[39m Getting Dataset: PeTaL/train_texts.txt Max Length: 500\n","\u001b[32m[I 210625 17:01:33 preprocess:32]\u001b[39m Size of Samples: 900\n","\u001b[32m[I 210625 17:01:34 preprocess:28]\u001b[39m Vocab Size: 26834\n","\u001b[32m[I 210625 17:01:34 preprocess:30]\u001b[39m Getting Dataset: PeTaL/test_texts.txt Max Length: 500\n","\u001b[32m[I 210625 17:01:34 preprocess:32]\u001b[39m Size of Samples: 100\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dv1TKNj5_n3k","executionInfo":{"status":"ok","timestamp":1624641492470,"user_tz":420,"elapsed":994907,"user":{"displayName":"Eric Kong","photoUrl":"","userId":"07341482379174037459"}},"outputId":"01c829d3-62a6-49f0-a74b-5021c4f01f69"},"source":["# Slightly modified run_models.sh\n","\n","!PYTHONFAULTHANDLER=1 python main.py --data-cnf configure/datasets/{DATASET}.yaml --model-cnf configure/models/{MODEL}-{DATASET}.yaml --mode train --reg 1\n","!PYTHONFAULTHANDLER=1 python main.py --data-cnf configure/datasets/{DATASET}.yaml --model-cnf configure/models/{MODEL}-{DATASET}.yaml --mode eval\n","\n","!python evaluation.py \\\n","--results {DATASET}/results/{MODEL}-{DATASET}-labels.npy \\\n","--targets {DATASET}/test_labels.npy \\\n","--train-labels {DATASET}/train_labels.npy"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[32m[I 210625 17:01:38 main:32]\u001b[39m Model Name: MATCH\n","\u001b[32m[I 210625 17:01:38 main:35]\u001b[39m Loading Training and Validation Set\n","/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['absorb_and/or_filter_solids', 'chemically_break_down_inorganic_compounds', 'detox/purify', 'manage_environmental_disturbances_in_a_community', 'protect_from_fire', 'protect_from_gases', 'send_vibratory_signals'] will be ignored\n","  .format(sorted(unknown, key=str)))\n","/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['absorb_and/or_filter_solids', 'detox/purify', 'protect_from_gases'] will be ignored\n","  .format(sorted(unknown, key=str)))\n","\u001b[32m[I 210625 17:01:38 main:47]\u001b[39m Number of Labels: 124\n","\u001b[32m[I 210625 17:01:38 main:48]\u001b[39m Size of Training Set: 800\n","\u001b[32m[I 210625 17:01:38 main:49]\u001b[39m Size of Validation Set: 100\n","\u001b[32m[I 210625 17:01:38 main:66]\u001b[39m Number of Edges: 101\n","\u001b[32m[I 210625 17:01:38 main:68]\u001b[39m Training\n","\u001b[32m[I 210625 17:01:45 models:142]\u001b[39m SWA Initializing\n","\u001b[32m[I 210625 17:02:05 models:110]\u001b[39m 24 1024 train loss: 0.1285587 valid loss: 0.1464014 P@1: 0.43000 P@3: 0.27667 P@5: 0.23800 N@3: 0.31060 N@5: 0.32622 early stop: 0\n","\u001b[32m[I 210625 17:02:29 models:110]\u001b[39m 49 1024 train loss: 0.0270746 valid loss: 0.1439123 P@1: 0.47000 P@3: 0.35667 P@5: 0.29600 N@3: 0.38437 N@5: 0.39816 early stop: 0\n","\u001b[32m[I 210625 17:02:54 models:110]\u001b[39m 74 1024 train loss: 0.0104609 valid loss: 0.1469670 P@1: 0.50000 P@3: 0.38000 P@5: 0.32800 N@3: 0.41336 N@5: 0.43343 early stop: 0\n","\u001b[32m[I 210625 17:03:19 models:110]\u001b[39m 99 1024 train loss: 0.0040509 valid loss: 0.1580660 P@1: 0.53000 P@3: 0.40333 P@5: 0.32800 N@3: 0.43682 N@5: 0.44306 early stop: 0\n","\u001b[32m[I 210625 17:03:43 models:110]\u001b[39m 124 1024 train loss: 0.0046082 valid loss: 0.1709425 P@1: 0.53000 P@3: 0.40667 P@5: 0.34400 N@3: 0.43845 N@5: 0.45508 early stop: 0\n","\u001b[32m[I 210625 17:04:08 models:110]\u001b[39m 149 1024 train loss: 0.0022198 valid loss: 0.1842978 P@1: 0.51000 P@3: 0.42000 P@5: 0.34800 N@3: 0.44560 N@5: 0.45866 early stop: 0\n","\u001b[33m[W 210625 17:04:30 models:137]\u001b[39m Clipping gradients with total norm 0.30143 and max norm 0.03403\n","\u001b[32m[I 210625 17:04:33 models:110]\u001b[39m 174 1024 train loss: 0.0004330 valid loss: 0.1975498 P@1: 0.51000 P@3: 0.42667 P@5: 0.35800 N@3: 0.45029 N@5: 0.46643 early stop: 0\n","\u001b[32m[I 210625 17:04:57 models:110]\u001b[39m 199 1024 train loss: 0.0007769 valid loss: 0.2098621 P@1: 0.52000 P@3: 0.44000 P@5: 0.36200 N@3: 0.46325 N@5: 0.47581 early stop: 0\n","\u001b[32m[I 210625 17:05:22 models:110]\u001b[39m 224 1024 train loss: 0.0000366 valid loss: 0.2219777 P@1: 0.53000 P@3: 0.45333 P@5: 0.36600 N@3: 0.47552 N@5: 0.48305 early stop: 0\n","\u001b[33m[W 210625 17:05:46 models:137]\u001b[39m Clipping gradients with total norm 0.04468 and max norm 0.0008\n","\u001b[32m[I 210625 17:05:47 models:110]\u001b[39m 249 1024 train loss: 0.0000136 valid loss: 0.2332115 P@1: 0.55000 P@3: 0.46000 P@5: 0.36600 N@3: 0.48367 N@5: 0.48704 early stop: 0\n","\u001b[33m[W 210625 17:05:50 models:137]\u001b[39m Clipping gradients with total norm 0.00543 and max norm 0.00071\n","\u001b[33m[W 210625 17:05:54 models:137]\u001b[39m Clipping gradients with total norm 0.01207 and max norm 0.0015\n","\u001b[33m[W 210625 17:06:05 models:137]\u001b[39m Clipping gradients with total norm 0.01006 and max norm 0.00119\n","\u001b[33m[W 210625 17:06:07 models:137]\u001b[39m Clipping gradients with total norm 0.01397 and max norm 0.00143\n","\u001b[32m[I 210625 17:06:11 models:110]\u001b[39m 274 1024 train loss: 0.0000117 valid loss: 0.2436063 P@1: 0.55000 P@3: 0.46667 P@5: 0.37000 N@3: 0.48898 N@5: 0.49119 early stop: 0\n","\u001b[33m[W 210625 17:06:24 models:137]\u001b[39m Clipping gradients with total norm 0.0116 and max norm 0.00221\n","\u001b[33m[W 210625 17:06:32 models:137]\u001b[39m Clipping gradients with total norm 0.0037 and max norm 0.00071\n","\u001b[32m[I 210625 17:06:36 models:110]\u001b[39m 299 1024 train loss: 0.0000081 valid loss: 0.2533838 P@1: 0.56000 P@3: 0.47000 P@5: 0.37400 N@3: 0.49244 N@5: 0.49545 early stop: 0\n","\u001b[33m[W 210625 17:06:47 models:137]\u001b[39m Clipping gradients with total norm 0.00942 and max norm 0.00089\n","\u001b[32m[I 210625 17:07:00 models:110]\u001b[39m 324 1024 train loss: 0.0000064 valid loss: 0.2622784 P@1: 0.57000 P@3: 0.47333 P@5: 0.37600 N@3: 0.49714 N@5: 0.49917 early stop: 0\n","\u001b[33m[W 210625 17:07:14 models:137]\u001b[39m Clipping gradients with total norm 0.00307 and max norm 0.00058\n","\u001b[33m[W 210625 17:07:21 models:137]\u001b[39m Clipping gradients with total norm 0.00417 and max norm 0.0008\n","\u001b[32m[I 210625 17:07:25 models:110]\u001b[39m 349 1024 train loss: 0.0000055 valid loss: 0.2706714 P@1: 0.57000 P@3: 0.47333 P@5: 0.37800 N@3: 0.49714 N@5: 0.50152 early stop: 0\n","\u001b[33m[W 210625 17:07:30 models:137]\u001b[39m Clipping gradients with total norm 0.00684 and max norm 0.00052\n","\u001b[33m[W 210625 17:07:49 models:137]\u001b[39m Clipping gradients with total norm 0.00393 and max norm 0.00047\n","\u001b[32m[I 210625 17:07:50 models:110]\u001b[39m 374 1024 train loss: 0.0000045 valid loss: 0.2785430 P@1: 0.57000 P@3: 0.47667 P@5: 0.37800 N@3: 0.50010 N@5: 0.50282 early stop: 0\n","\u001b[33m[W 210625 17:08:00 models:137]\u001b[39m Clipping gradients with total norm 0.00343 and max norm 0.00041\n","\u001b[32m[I 210625 17:08:14 models:110]\u001b[39m 399 1024 train loss: 0.0000039 valid loss: 0.2859447 P@1: 0.57000 P@3: 0.47333 P@5: 0.37800 N@3: 0.49775 N@5: 0.50255 early stop: 1\n","\u001b[33m[W 210625 17:08:19 models:137]\u001b[39m Clipping gradients with total norm 0.00199 and max norm 0.00039\n","\u001b[33m[W 210625 17:08:36 models:137]\u001b[39m Clipping gradients with total norm 0.02951 and max norm 0.00033\n","\u001b[32m[I 210625 17:08:39 models:110]\u001b[39m 424 1024 train loss: 0.0000039 valid loss: 0.2928540 P@1: 0.57000 P@3: 0.47667 P@5: 0.38000 N@3: 0.50010 N@5: 0.50439 early stop: 0\n","\u001b[33m[W 210625 17:08:53 models:137]\u001b[39m Clipping gradients with total norm 0.01288 and max norm 0.00135\n","\u001b[32m[I 210625 17:09:03 models:110]\u001b[39m 449 1024 train loss: 0.0000031 valid loss: 0.2994399 P@1: 0.57000 P@3: 0.48000 P@5: 0.38400 N@3: 0.50244 N@5: 0.50790 early stop: 0\n","\u001b[33m[W 210625 17:09:05 models:137]\u001b[39m Clipping gradients with total norm 0.00781 and max norm 0.00031\n","\u001b[33m[W 210625 17:09:17 models:137]\u001b[39m Clipping gradients with total norm 0.00147 and max norm 0.00027\n","\u001b[33m[W 210625 17:09:19 models:137]\u001b[39m Clipping gradients with total norm 0.00767 and max norm 0.00093\n","\u001b[32m[I 210625 17:09:28 models:110]\u001b[39m 474 1024 train loss: 0.0000029 valid loss: 0.3056272 P@1: 0.57000 P@3: 0.48333 P@5: 0.38400 N@3: 0.50418 N@5: 0.50793 early stop: 0\n","\u001b[33m[W 210625 17:09:33 models:137]\u001b[39m Clipping gradients with total norm 0.00391 and max norm 0.00055\n","\u001b[33m[W 210625 17:09:39 models:137]\u001b[39m Clipping gradients with total norm 0.00757 and max norm 0.00066\n","\u001b[32m[I 210625 17:09:53 models:110]\u001b[39m 499 1024 train loss: 0.0000027 valid loss: 0.3115870 P@1: 0.57000 P@3: 0.48333 P@5: 0.38400 N@3: 0.50479 N@5: 0.50844 early stop: 0\n","\u001b[33m[W 210625 17:09:56 models:137]\u001b[39m Clipping gradients with total norm 0.00583 and max norm 0.00054\n","\u001b[32m[I 210625 17:10:17 models:110]\u001b[39m 524 1024 train loss: 0.0000023 valid loss: 0.3172634 P@1: 0.57000 P@3: 0.48333 P@5: 0.38400 N@3: 0.50479 N@5: 0.50844 early stop: 1\n","\u001b[33m[W 210625 17:10:19 models:137]\u001b[39m Clipping gradients with total norm 0.00333 and max norm 0.00032\n","\u001b[33m[W 210625 17:10:25 models:137]\u001b[39m Clipping gradients with total norm 0.00594 and max norm 0.00063\n","\u001b[33m[W 210625 17:10:33 models:137]\u001b[39m Clipping gradients with total norm 0.00277 and max norm 0.00051\n","\u001b[32m[I 210625 17:10:42 models:110]\u001b[39m 549 1024 train loss: 0.0000021 valid loss: 0.3227161 P@1: 0.57000 P@3: 0.48000 P@5: 0.38600 N@3: 0.50244 N@5: 0.50943 early stop: 0\n","\u001b[33m[W 210625 17:10:56 models:137]\u001b[39m Clipping gradients with total norm 0.00189 and max norm 0.00024\n","\u001b[33m[W 210625 17:11:04 models:137]\u001b[39m Clipping gradients with total norm 0.00162 and max norm 0.00016\n","\u001b[32m[I 210625 17:11:07 models:110]\u001b[39m 574 1024 train loss: 0.0000016 valid loss: 0.3278964 P@1: 0.57000 P@3: 0.48667 P@5: 0.38600 N@3: 0.50714 N@5: 0.51003 early stop: 0\n","\u001b[33m[W 210625 17:11:09 models:137]\u001b[39m Clipping gradients with total norm 0.0042 and max norm 0.00065\n","\u001b[33m[W 210625 17:11:11 models:137]\u001b[39m Clipping gradients with total norm 0.00995 and max norm 0.00103\n","\u001b[33m[W 210625 17:11:18 models:137]\u001b[39m Clipping gradients with total norm 0.00715 and max norm 0.00079\n","\u001b[33m[W 210625 17:11:20 models:137]\u001b[39m Clipping gradients with total norm 0.00165 and max norm 0.00027\n","\u001b[33m[W 210625 17:11:24 models:137]\u001b[39m Clipping gradients with total norm 0.00151 and max norm 0.00027\n","\u001b[32m[I 210625 17:11:32 models:110]\u001b[39m 599 1024 train loss: 0.0000022 valid loss: 0.3328350 P@1: 0.57000 P@3: 0.48667 P@5: 0.38600 N@3: 0.50714 N@5: 0.51003 early stop: 1\n","\u001b[33m[W 210625 17:11:35 models:137]\u001b[39m Clipping gradients with total norm 0.0007 and max norm 0.00012\n","\u001b[33m[W 210625 17:11:44 models:137]\u001b[39m Clipping gradients with total norm 0.00166 and max norm 0.00029\n","\u001b[33m[W 210625 17:11:53 models:137]\u001b[39m Clipping gradients with total norm 0.00236 and max norm 0.00018\n","\u001b[32m[I 210625 17:11:56 models:110]\u001b[39m 624 1024 train loss: 0.0000015 valid loss: 0.3375337 P@1: 0.57000 P@3: 0.49333 P@5: 0.38800 N@3: 0.51183 N@5: 0.51169 early stop: 0\n","\u001b[33m[W 210625 17:12:11 models:137]\u001b[39m Clipping gradients with total norm 0.002 and max norm 0.00021\n","\u001b[33m[W 210625 17:12:20 models:137]\u001b[39m Clipping gradients with total norm 0.00366 and max norm 0.00032\n","\u001b[32m[I 210625 17:12:21 models:110]\u001b[39m 649 1024 train loss: 0.0000013 valid loss: 0.3420465 P@1: 0.57000 P@3: 0.49333 P@5: 0.38800 N@3: 0.51183 N@5: 0.51190 early stop: 0\n","\u001b[33m[W 210625 17:12:24 models:137]\u001b[39m Clipping gradients with total norm 0.00152 and max norm 0.00027\n","\u001b[33m[W 210625 17:12:29 models:137]\u001b[39m Clipping gradients with total norm 0.00511 and max norm 0.00046\n","\u001b[33m[W 210625 17:12:31 models:137]\u001b[39m Clipping gradients with total norm 0.00379 and max norm 0.00018\n","\u001b[32m[I 210625 17:12:45 models:110]\u001b[39m 674 1024 train loss: 0.0000014 valid loss: 0.3464078 P@1: 0.57000 P@3: 0.49333 P@5: 0.39200 N@3: 0.51183 N@5: 0.51568 early stop: 0\n","\u001b[32m[I 210625 17:13:10 models:110]\u001b[39m 699 1024 train loss: 0.0000011 valid loss: 0.3505856 P@1: 0.57000 P@3: 0.49333 P@5: 0.39200 N@3: 0.51183 N@5: 0.51568 early stop: 1\n","\u001b[33m[W 210625 17:13:16 models:137]\u001b[39m Clipping gradients with total norm 0.00104 and max norm 0.00014\n","\u001b[33m[W 210625 17:13:35 models:137]\u001b[39m Clipping gradients with total norm 0.00159 and max norm 0.00018\n","\u001b[32m[I 210625 17:13:35 models:110]\u001b[39m 724 1024 train loss: 0.0000010 valid loss: 0.3546435 P@1: 0.57000 P@3: 0.49333 P@5: 0.39400 N@3: 0.51244 N@5: 0.51760 early stop: 0\n","\u001b[33m[W 210625 17:13:56 models:137]\u001b[39m Clipping gradients with total norm 0.00229 and max norm 0.00041\n","\u001b[32m[I 210625 17:14:00 models:110]\u001b[39m 749 1024 train loss: 0.0000010 valid loss: 0.3585332 P@1: 0.57000 P@3: 0.49333 P@5: 0.39400 N@3: 0.51244 N@5: 0.51760 early stop: 1\n","\u001b[33m[W 210625 17:14:19 models:137]\u001b[39m Clipping gradients with total norm 0.00137 and max norm 8e-05\n","\u001b[32m[I 210625 17:14:24 models:110]\u001b[39m 774 1024 train loss: 0.0000008 valid loss: 0.3623462 P@1: 0.57000 P@3: 0.49333 P@5: 0.39400 N@3: 0.51306 N@5: 0.51822 early stop: 0\n","\u001b[33m[W 210625 17:14:37 models:137]\u001b[39m Clipping gradients with total norm 0.00321 and max norm 0.00021\n","\u001b[33m[W 210625 17:14:40 models:137]\u001b[39m Clipping gradients with total norm 0.00244 and max norm 9e-05\n","\u001b[32m[I 210625 17:14:49 models:110]\u001b[39m 799 1024 train loss: 0.0000009 valid loss: 0.3659998 P@1: 0.57000 P@3: 0.49333 P@5: 0.39400 N@3: 0.51306 N@5: 0.51822 early stop: 1\n","\u001b[33m[W 210625 17:14:50 models:137]\u001b[39m Clipping gradients with total norm 0.00427 and max norm 0.00022\n","\u001b[33m[W 210625 17:14:53 models:137]\u001b[39m Clipping gradients with total norm 0.00128 and max norm 7e-05\n","\u001b[33m[W 210625 17:14:58 models:137]\u001b[39m Clipping gradients with total norm 0.00063 and max norm 0.00012\n","\u001b[33m[W 210625 17:15:04 models:137]\u001b[39m Clipping gradients with total norm 0.00098 and max norm 0.00016\n","\u001b[32m[I 210625 17:15:13 models:110]\u001b[39m 824 1024 train loss: 0.0000008 valid loss: 0.3695699 P@1: 0.56000 P@3: 0.49333 P@5: 0.39400 N@3: 0.51194 N@5: 0.51693 early stop: 2\n","\u001b[33m[W 210625 17:15:28 models:137]\u001b[39m Clipping gradients with total norm 0.01811 and max norm 6e-05\n","\u001b[33m[W 210625 17:15:32 models:137]\u001b[39m Clipping gradients with total norm 0.00502 and max norm 0.00012\n","\u001b[32m[I 210625 17:15:38 models:110]\u001b[39m 849 1024 train loss: 0.0000012 valid loss: 0.3730476 P@1: 0.55000 P@3: 0.49333 P@5: 0.39400 N@3: 0.51021 N@5: 0.51520 early stop: 3\n","\u001b[33m[W 210625 17:15:53 models:137]\u001b[39m Clipping gradients with total norm 0.00197 and max norm 0.00015\n","\u001b[32m[I 210625 17:16:03 models:110]\u001b[39m 874 1024 train loss: 0.0000007 valid loss: 0.3764400 P@1: 0.56000 P@3: 0.49333 P@5: 0.39400 N@3: 0.51194 N@5: 0.51645 early stop: 4\n","\u001b[33m[W 210625 17:16:09 models:137]\u001b[39m Clipping gradients with total norm 0.00594 and max norm 8e-05\n","\u001b[33m[W 210625 17:16:12 models:137]\u001b[39m Clipping gradients with total norm 0.01333 and max norm 0.00061\n","\u001b[33m[W 210625 17:16:15 models:137]\u001b[39m Clipping gradients with total norm 0.00313 and max norm 0.00016\n","\u001b[33m[W 210625 17:16:22 models:137]\u001b[39m Clipping gradients with total norm 0.00224 and max norm 0.00035\n","\u001b[33m[W 210625 17:16:24 models:137]\u001b[39m Clipping gradients with total norm 0.0015 and max norm 0.00011\n","\u001b[32m[I 210625 17:16:27 models:110]\u001b[39m 899 1024 train loss: 0.0000009 valid loss: 0.3797420 P@1: 0.55000 P@3: 0.49333 P@5: 0.39600 N@3: 0.51021 N@5: 0.51672 early stop: 5\n","\u001b[33m[W 210625 17:16:41 models:137]\u001b[39m Clipping gradients with total norm 0.00097 and max norm 9e-05\n","\u001b[33m[W 210625 17:16:45 models:137]\u001b[39m Clipping gradients with total norm 0.00043 and max norm 7e-05\n","\u001b[33m[W 210625 17:16:51 models:137]\u001b[39m Clipping gradients with total norm 0.00044 and max norm 6e-05\n","\u001b[32m[I 210625 17:16:52 models:110]\u001b[39m 924 1024 train loss: 0.0000005 valid loss: 0.3829843 P@1: 0.55000 P@3: 0.49000 P@5: 0.39400 N@3: 0.50786 N@5: 0.51517 early stop: 6\n","\u001b[33m[W 210625 17:16:58 models:137]\u001b[39m Clipping gradients with total norm 0.00038 and max norm 5e-05\n","\u001b[32m[I 210625 17:17:17 models:110]\u001b[39m 949 1024 train loss: 0.0000005 valid loss: 0.3861414 P@1: 0.55000 P@3: 0.49000 P@5: 0.39400 N@3: 0.50786 N@5: 0.51537 early stop: 7\n","\u001b[33m[W 210625 17:17:29 models:137]\u001b[39m Clipping gradients with total norm 0.03717 and max norm 0.0001\n","\u001b[32m[I 210625 17:17:41 models:110]\u001b[39m 974 1024 train loss: 0.0000020 valid loss: 0.3892423 P@1: 0.55000 P@3: 0.49000 P@5: 0.39400 N@3: 0.50848 N@5: 0.51599 early stop: 8\n","\u001b[33m[W 210625 17:17:50 models:137]\u001b[39m Clipping gradients with total norm 0.00031 and max norm 5e-05\n","\u001b[33m[W 210625 17:17:55 models:137]\u001b[39m Clipping gradients with total norm 0.00031 and max norm 6e-05\n","\u001b[32m[I 210625 17:18:06 models:110]\u001b[39m 999 1024 train loss: 0.0000004 valid loss: 0.3922627 P@1: 0.55000 P@3: 0.49000 P@5: 0.39400 N@3: 0.50848 N@5: 0.51599 early stop: 9\n","\u001b[32m[I 210625 17:18:06 main:76]\u001b[39m Finish Training\n","\u001b[32m[I 210625 17:18:07 main:32]\u001b[39m Model Name: MATCH\n","\u001b[32m[I 210625 17:18:07 main:79]\u001b[39m Loading Test Set\n","\u001b[32m[I 210625 17:18:07 main:83]\u001b[39m Size of Test Set: 100\n","\u001b[32m[I 210625 17:18:07 main:85]\u001b[39m Predicting\n","\u001b[32m[I 210625 17:18:10 main:91]\u001b[39m Finish Predicting\n","Precision@1,3,5: 0.52 0.44 0.346\n","nDCG@1,3,5: 0.52 0.4599936085661029 0.4694861074706761\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"A2qso0E6CLCl"},"source":["idea: do k-fold cross validation?"]},{"cell_type":"code","metadata":{"id":"a4b9bR44_iyF"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"54Ur6eGk_q3C"},"source":["# Results of MATCH Quick Start\n","\n","Running MATCH on MAG-CS dataset as described in the paper."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dNemHI_6auA0","executionInfo":{"elapsed":267658,"status":"ok","timestamp":1623944883111,"user":{"displayName":"Eric Kong","photoUrl":"","userId":"07341482379174037459"},"user_tz":420},"outputId":"077ce78c-9144-413b-d5a2-59fc0a56bceb"},"source":["!./preprocess.sh"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[32m[I 210617 15:44:06 preprocess:28]\u001b[39m Vocab Size: 500000\n","\u001b[32m[I 210617 15:44:06 preprocess:30]\u001b[39m Getting Dataset: MAG/train_texts.txt Max Length: 500\n","tcmalloc: large alloc 2539503616 bytes == 0x558fac048000 @  0x7f91aba831e7 0x7f91a934bea1 0x7f91a93b0928 0x7f91a93b4070 0x7f91a93b45e5 0x7f91a944d40d 0x558ed8136d54 0x558ed8136a50 0x558ed81ab105 0x558ed81a54ae 0x558ed81383ea 0x558ed81aa7f0 0x558ed81a57ad 0x558ed81383ea 0x558ed81a63b5 0x558ed81a57ad 0x558ed81383ea 0x558ed81a63b5 0x558ed81a54ae 0x558ed8077e2c 0x558ed81a7bb5 0x558ed81a54ae 0x558ed8138c9f 0x558ed8138ea1 0x558ed81a7bb5 0x558ed813830a 0x558ed81a660e 0x558ed81a54ae 0x558ed8138a81 0x558ed8138ea1 0x558ed81a7bb5\n","\u001b[32m[I 210617 15:46:44 preprocess:32]\u001b[39m Size of Samples: 634874\n","\u001b[32m[I 210617 15:47:41 preprocess:28]\u001b[39m Vocab Size: 500000\n","\u001b[32m[I 210617 15:47:41 preprocess:30]\u001b[39m Getting Dataset: MAG/test_texts.txt Max Length: 500\n","\u001b[32m[I 210617 15:47:56 preprocess:32]\u001b[39m Size of Samples: 70533\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"LahoKA3ofc2w","outputId":"74303e99-cf22-4a18-da53-5d17d01a3eb5"},"source":["!./run_models.sh"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[32m[I 210617 15:48:17 main:32]\u001b[39m Model Name: MATCH\n","\u001b[32m[I 210617 15:48:17 main:35]\u001b[39m Loading Training and Validation Set\n","tcmalloc: large alloc 2539503616 bytes == 0x55777a22e000 @  0x7f4bd84601e7 0x7f4bd5d28ea1 0x7f4bd5d92b75 0x7f4bd5d9370e 0x7f4bd5e2c71e 0x55775d768d54 0x55775d768a50 0x55775d7dd105 0x55775d7d74ae 0x55775d76a3ea 0x55775d7d932a 0x55775d7d74ae 0x55775d76a3ea 0x55775d7d932a 0x55775d7d74ae 0x55775d76a3ea 0x55775d7d83b5 0x55775d7d74ae 0x55775d6a9e2c 0x55775d7d9bb5 0x55775d7d74ae 0x55775d76ac9f 0x55775d76aea1 0x55775d7d9bb5 0x55775d76a30a 0x55775d7d860e 0x55775d7d74ae 0x55775d76aa81 0x55775d76aea1 0x55775d7d9bb5 0x55775d7d74ae\n","tcmalloc: large alloc 2257362944 bytes == 0x5578127f4000 @  0x7f4bd84601e7 0x7f4bd5d28ea1 0x7f4bd5d8d928 0x7f4bd5d8da43 0x7f4bd5ddd2d4 0x7f4bd5e1cb90 0x55775d768d54 0x55775d768a50 0x55775d7dd105 0x55775d7d77ad 0x55775d76a3ea 0x55775d7d83b5 0x55775d85aec8 0x55775d850d8e 0x55775d840b95 0x55775d777a34 0x55775d7a8cc4 0x55775d769462 0x55775d7dc715 0x55775d7d77ad 0x55775d76a3ea 0x55775d7d932a 0x55775d7d74ae 0x55775d6a9e2c 0x55775d7d9bb5 0x55775d7d74ae 0x55775d76ac9f 0x55775d76aea1 0x55775d7d9bb5 0x55775d76a30a 0x55775d7d860e\n","\u001b[32m[I 210617 15:48:47 main:47]\u001b[39m Number of Labels: 15308\n","\u001b[32m[I 210617 15:48:47 main:48]\u001b[39m Size of Training Set: 564340\n","\u001b[32m[I 210617 15:48:47 main:49]\u001b[39m Size of Validation Set: 70534\n","\u001b[32m[I 210617 15:48:54 main:66]\u001b[39m Number of Edges: 26491\n","\u001b[32m[I 210617 15:48:54 main:68]\u001b[39m Training\n","\u001b[32m[I 210617 15:52:33 models:110]\u001b[39m 0 25600 train loss: 0.0265692 valid loss: 0.0023821 P@1: 0.14950 P@3: 0.13431 P@5: 0.11884 N@3: 0.14536 N@5: 0.14271 early stop: 0\n","\u001b[32m[I 210617 15:55:57 models:110]\u001b[39m 0 51200 train loss: 0.0023748 valid loss: 0.0023726 P@1: 0.15261 P@3: 0.13432 P@5: 0.11884 N@3: 0.14591 N@5: 0.14319 early stop: 0\n","\u001b[32m[I 210617 15:59:21 models:110]\u001b[39m 0 76800 train loss: 0.0023770 valid loss: 0.0023646 P@1: 0.14959 P@3: 0.14534 P@5: 0.12222 N@3: 0.15358 N@5: 0.14813 early stop: 0\n","\u001b[32m[I 210617 16:02:44 models:110]\u001b[39m 0 102400 train loss: 0.0023563 valid loss: 0.0023499 P@1: 0.16239 P@3: 0.14967 P@5: 0.12749 N@3: 0.16323 N@5: 0.15912 early stop: 0\n","\u001b[32m[I 210617 16:06:08 models:110]\u001b[39m 0 128000 train loss: 0.0023532 valid loss: 0.0023100 P@1: 0.22043 P@3: 0.19512 P@5: 0.15106 N@3: 0.21118 N@5: 0.19175 early stop: 0\n","\u001b[32m[I 210617 16:09:30 models:110]\u001b[39m 0 153600 train loss: 0.0022600 valid loss: 0.0022188 P@1: 0.28236 P@3: 0.22166 P@5: 0.17591 N@3: 0.24681 N@5: 0.22702 early stop: 0\n","\u001b[32m[I 210617 16:12:52 models:110]\u001b[39m 0 179200 train loss: 0.0022004 valid loss: 0.0021808 P@1: 0.29547 P@3: 0.23071 P@5: 0.18188 N@3: 0.25665 N@5: 0.23578 early stop: 0\n","\u001b[32m[I 210617 16:16:13 models:110]\u001b[39m 0 204800 train loss: 0.0021617 valid loss: 0.0021420 P@1: 0.32641 P@3: 0.23688 P@5: 0.18348 N@3: 0.26946 N@5: 0.24493 early stop: 0\n","\u001b[32m[I 210617 16:19:36 models:110]\u001b[39m 0 230400 train loss: 0.0021457 valid loss: 0.0021426 P@1: 0.32132 P@3: 0.23919 P@5: 0.18677 N@3: 0.27180 N@5: 0.24823 early stop: 0\n","\u001b[32m[I 210617 16:22:58 models:110]\u001b[39m 0 256000 train loss: 0.0021209 valid loss: 0.0020963 P@1: 0.35172 P@3: 0.26092 P@5: 0.19503 N@3: 0.29614 N@5: 0.26304 early stop: 0\n","\u001b[32m[I 210617 16:26:20 models:110]\u001b[39m 0 281600 train loss: 0.0020804 valid loss: 0.0020582 P@1: 0.36778 P@3: 0.27336 P@5: 0.20879 N@3: 0.31183 N@5: 0.28089 early stop: 0\n","\u001b[32m[I 210617 16:29:40 models:110]\u001b[39m 0 307200 train loss: 0.0020335 valid loss: 0.0019928 P@1: 0.42092 P@3: 0.29898 P@5: 0.22828 N@3: 0.34537 N@5: 0.31121 early stop: 0\n","\u001b[32m[I 210617 16:33:02 models:110]\u001b[39m 0 332800 train loss: 0.0019721 valid loss: 0.0019144 P@1: 0.48201 P@3: 0.33649 P@5: 0.25268 N@3: 0.39146 N@5: 0.34977 early stop: 0\n","\u001b[32m[I 210617 16:36:24 models:110]\u001b[39m 0 358400 train loss: 0.0019078 valid loss: 0.0018660 P@1: 0.50603 P@3: 0.34696 P@5: 0.26208 N@3: 0.40541 N@5: 0.36294 early stop: 0\n","\u001b[32m[I 210617 16:39:47 models:110]\u001b[39m 0 384000 train loss: 0.0018624 valid loss: 0.0018374 P@1: 0.52631 P@3: 0.35881 P@5: 0.26892 N@3: 0.42091 N@5: 0.37551 early stop: 0\n","\u001b[32m[I 210617 16:43:08 models:110]\u001b[39m 0 409600 train loss: 0.0018494 valid loss: 0.0018107 P@1: 0.54036 P@3: 0.36706 P@5: 0.27759 N@3: 0.43100 N@5: 0.38672 early stop: 0\n","\u001b[32m[I 210617 16:46:30 models:110]\u001b[39m 0 435200 train loss: 0.0018314 valid loss: 0.0018014 P@1: 0.54781 P@3: 0.37196 P@5: 0.27996 N@3: 0.43717 N@5: 0.39110 early stop: 0\n","\u001b[32m[I 210617 16:49:52 models:110]\u001b[39m 0 460800 train loss: 0.0018039 valid loss: 0.0017720 P@1: 0.56476 P@3: 0.37900 P@5: 0.28616 N@3: 0.44721 N@5: 0.40062 early stop: 0\n","\u001b[32m[I 210617 16:53:14 models:110]\u001b[39m 0 486400 train loss: 0.0017857 valid loss: 0.0017528 P@1: 0.57863 P@3: 0.38857 P@5: 0.29338 N@3: 0.45818 N@5: 0.41038 early stop: 0\n","\u001b[32m[I 210617 16:56:35 models:110]\u001b[39m 0 512000 train loss: 0.0017501 valid loss: 0.0017372 P@1: 0.58402 P@3: 0.39438 P@5: 0.29893 N@3: 0.46393 N@5: 0.41597 early stop: 0\n","\u001b[32m[I 210617 16:59:57 models:110]\u001b[39m 0 537600 train loss: 0.0017435 valid loss: 0.0017118 P@1: 0.59217 P@3: 0.39886 P@5: 0.30266 N@3: 0.46961 N@5: 0.42156 early stop: 0\n","\u001b[32m[I 210617 17:03:19 models:110]\u001b[39m 0 563200 train loss: 0.0017158 valid loss: 0.0016959 P@1: 0.59981 P@3: 0.40828 P@5: 0.30906 N@3: 0.47908 N@5: 0.42963 early stop: 0\n","\u001b[32m[I 210617 17:06:42 models:110]\u001b[39m 1 24320 train loss: 0.0016824 valid loss: 0.0016590 P@1: 0.61375 P@3: 0.41818 P@5: 0.31990 N@3: 0.49059 N@5: 0.44245 early stop: 0\n","\u001b[32m[I 210617 17:10:04 models:110]\u001b[39m 1 49920 train loss: 0.0016564 valid loss: 0.0016372 P@1: 0.62744 P@3: 0.42740 P@5: 0.32534 N@3: 0.50170 N@5: 0.45171 early stop: 0\n","\u001b[32m[I 210617 17:13:27 models:110]\u001b[39m 1 75520 train loss: 0.0016280 valid loss: 0.0016063 P@1: 0.64141 P@3: 0.43951 P@5: 0.33472 N@3: 0.51480 N@5: 0.46378 early stop: 0\n","\u001b[32m[I 210617 17:16:49 models:110]\u001b[39m 1 101120 train loss: 0.0016009 valid loss: 0.0015791 P@1: 0.65227 P@3: 0.44835 P@5: 0.34204 N@3: 0.52492 N@5: 0.47337 early stop: 0\n","\u001b[32m[I 210617 17:20:12 models:110]\u001b[39m 1 126720 train loss: 0.0015781 valid loss: 0.0015600 P@1: 0.66033 P@3: 0.45637 P@5: 0.34943 N@3: 0.53316 N@5: 0.48174 early stop: 0\n","\u001b[32m[I 210617 17:23:35 models:110]\u001b[39m 1 152320 train loss: 0.0015549 valid loss: 0.0015378 P@1: 0.67156 P@3: 0.46345 P@5: 0.35537 N@3: 0.54216 N@5: 0.49071 early stop: 0\n","\u001b[32m[I 210617 17:26:58 models:110]\u001b[39m 1 177920 train loss: 0.0015375 valid loss: 0.0015130 P@1: 0.68136 P@3: 0.47286 P@5: 0.36404 N@3: 0.55178 N@5: 0.50038 early stop: 0\n","\u001b[32m[I 210617 17:30:21 models:110]\u001b[39m 1 203520 train loss: 0.0015060 valid loss: 0.0014948 P@1: 0.68700 P@3: 0.47965 P@5: 0.36992 N@3: 0.55900 N@5: 0.50762 early stop: 0\n","\u001b[32m[I 210617 17:33:46 models:110]\u001b[39m 1 229120 train loss: 0.0014988 valid loss: 0.0014775 P@1: 0.69199 P@3: 0.48550 P@5: 0.37434 N@3: 0.56471 N@5: 0.51295 early stop: 0\n","\u001b[32m[I 210617 17:37:10 models:110]\u001b[39m 1 254720 train loss: 0.0014922 valid loss: 0.0014642 P@1: 0.69822 P@3: 0.49187 P@5: 0.37987 N@3: 0.57169 N@5: 0.51998 early stop: 0\n","\u001b[32m[I 210617 17:40:34 models:110]\u001b[39m 1 280320 train loss: 0.0014746 valid loss: 0.0014556 P@1: 0.69997 P@3: 0.49482 P@5: 0.38262 N@3: 0.57524 N@5: 0.52398 early stop: 0\n","\u001b[32m[I 210617 17:43:59 models:110]\u001b[39m 1 305920 train loss: 0.0014568 valid loss: 0.0014346 P@1: 0.70852 P@3: 0.50125 P@5: 0.38899 N@3: 0.58215 N@5: 0.53133 early stop: 0\n","\u001b[32m[I 210617 17:47:24 models:110]\u001b[39m 1 331520 train loss: 0.0014452 valid loss: 0.0014233 P@1: 0.71346 P@3: 0.50791 P@5: 0.39326 N@3: 0.58895 N@5: 0.53687 early stop: 0\n","\u001b[32m[I 210617 17:50:48 models:110]\u001b[39m 1 357120 train loss: 0.0014329 valid loss: 0.0014109 P@1: 0.71448 P@3: 0.50955 P@5: 0.39553 N@3: 0.59106 N@5: 0.53975 early stop: 0\n","\u001b[32m[I 210617 17:54:13 models:110]\u001b[39m 1 382720 train loss: 0.0014245 valid loss: 0.0013965 P@1: 0.72138 P@3: 0.51770 P@5: 0.40278 N@3: 0.59938 N@5: 0.54824 early stop: 0\n","\u001b[32m[I 210617 17:57:37 models:110]\u001b[39m 1 408320 train loss: 0.0014104 valid loss: 0.0013856 P@1: 0.72667 P@3: 0.52247 P@5: 0.40582 N@3: 0.60475 N@5: 0.55278 early stop: 0\n","\u001b[32m[I 210617 18:01:01 models:110]\u001b[39m 1 433920 train loss: 0.0013963 valid loss: 0.0013689 P@1: 0.73074 P@3: 0.52864 P@5: 0.41113 N@3: 0.61083 N@5: 0.55897 early stop: 0\n","\u001b[32m[I 210617 18:04:26 models:110]\u001b[39m 1 459520 train loss: 0.0013808 valid loss: 0.0013642 P@1: 0.73428 P@3: 0.52987 P@5: 0.41247 N@3: 0.61289 N@5: 0.56131 early stop: 0\n","\u001b[32m[I 210617 18:07:51 models:110]\u001b[39m 1 485120 train loss: 0.0013666 valid loss: 0.0013488 P@1: 0.73950 P@3: 0.53607 P@5: 0.41801 N@3: 0.61949 N@5: 0.56798 early stop: 0\n","\u001b[32m[I 210617 18:11:15 models:110]\u001b[39m 1 510720 train loss: 0.0013648 valid loss: 0.0013369 P@1: 0.74662 P@3: 0.54214 P@5: 0.42243 N@3: 0.62642 N@5: 0.57422 early stop: 0\n","\u001b[32m[I 210617 18:14:40 models:110]\u001b[39m 1 536320 train loss: 0.0013452 valid loss: 0.0013256 P@1: 0.75074 P@3: 0.54653 P@5: 0.42656 N@3: 0.63072 N@5: 0.57880 early stop: 0\n","\u001b[32m[I 210617 18:18:05 models:110]\u001b[39m 1 561920 train loss: 0.0013250 valid loss: 0.0013153 P@1: 0.75053 P@3: 0.55075 P@5: 0.42991 N@3: 0.63460 N@5: 0.58268 early stop: 0\n","\u001b[32m[I 210617 18:21:31 models:110]\u001b[39m 2 23040 train loss: 0.0012830 valid loss: 0.0013054 P@1: 0.75583 P@3: 0.55582 P@5: 0.43489 N@3: 0.63969 N@5: 0.58816 early stop: 0\n","\u001b[32m[I 210617 18:24:55 models:110]\u001b[39m 2 48640 train loss: 0.0012741 valid loss: 0.0012989 P@1: 0.76063 P@3: 0.55852 P@5: 0.43605 N@3: 0.64352 N@5: 0.59083 early stop: 0\n","\u001b[32m[I 210617 18:28:19 models:110]\u001b[39m 2 74240 train loss: 0.0012666 valid loss: 0.0012838 P@1: 0.77105 P@3: 0.56806 P@5: 0.44364 N@3: 0.65364 N@5: 0.60039 early stop: 0\n","\u001b[32m[I 210617 18:31:41 models:110]\u001b[39m 2 99840 train loss: 0.0012571 valid loss: 0.0012665 P@1: 0.77526 P@3: 0.57240 P@5: 0.44893 N@3: 0.65871 N@5: 0.60688 early stop: 0\n","\u001b[32m[I 210617 18:35:04 models:110]\u001b[39m 2 125440 train loss: 0.0012584 valid loss: 0.0012588 P@1: 0.78029 P@3: 0.57746 P@5: 0.45228 N@3: 0.66394 N@5: 0.61118 early stop: 0\n","\u001b[32m[I 210617 18:38:27 models:110]\u001b[39m 2 151040 train loss: 0.0012406 valid loss: 0.0012501 P@1: 0.78033 P@3: 0.58119 P@5: 0.45586 N@3: 0.66727 N@5: 0.61500 early stop: 0\n","\u001b[32m[I 210617 18:41:50 models:110]\u001b[39m 2 176640 train loss: 0.0012300 valid loss: 0.0012360 P@1: 0.78829 P@3: 0.58915 P@5: 0.46187 N@3: 0.67578 N@5: 0.62260 early stop: 0\n","\u001b[32m[I 210617 18:45:13 models:110]\u001b[39m 2 202240 train loss: 0.0012235 valid loss: 0.0012298 P@1: 0.79409 P@3: 0.59476 P@5: 0.46588 N@3: 0.68183 N@5: 0.62803 early stop: 0\n","\u001b[32m[I 210617 18:48:35 models:110]\u001b[39m 2 227840 train loss: 0.0012101 valid loss: 0.0012141 P@1: 0.80238 P@3: 0.60216 P@5: 0.47143 N@3: 0.68977 N@5: 0.63518 early stop: 0\n","\u001b[32m[I 210617 18:51:57 models:110]\u001b[39m 2 253440 train loss: 0.0011967 valid loss: 0.0012073 P@1: 0.81131 P@3: 0.60747 P@5: 0.47533 N@3: 0.69682 N@5: 0.64188 early stop: 0\n","\u001b[32m[I 210617 18:55:20 models:110]\u001b[39m 2 279040 train loss: 0.0011927 valid loss: 0.0012004 P@1: 0.81344 P@3: 0.61079 P@5: 0.47872 N@3: 0.70005 N@5: 0.64531 early stop: 0\n","\u001b[32m[I 210617 18:58:42 models:110]\u001b[39m 2 304640 train loss: 0.0011787 valid loss: 0.0011803 P@1: 0.81935 P@3: 0.61729 P@5: 0.48515 N@3: 0.70733 N@5: 0.65332 early stop: 0\n","\u001b[32m[I 210617 19:02:04 models:110]\u001b[39m 2 330240 train loss: 0.0011766 valid loss: 0.0011776 P@1: 0.82491 P@3: 0.62133 P@5: 0.48763 N@3: 0.71186 N@5: 0.65699 early stop: 0\n","\u001b[32m[I 210617 19:05:26 models:110]\u001b[39m 2 355840 train loss: 0.0011581 valid loss: 0.0011581 P@1: 0.82719 P@3: 0.62689 P@5: 0.49248 N@3: 0.71690 N@5: 0.66201 early stop: 0\n","\u001b[32m[I 210617 19:08:49 models:110]\u001b[39m 2 381440 train loss: 0.0011503 valid loss: 0.0011487 P@1: 0.83531 P@3: 0.63216 P@5: 0.49667 N@3: 0.72335 N@5: 0.66820 early stop: 0\n","\u001b[32m[I 210617 19:12:12 models:110]\u001b[39m 2 407040 train loss: 0.0011422 valid loss: 0.0011384 P@1: 0.83840 P@3: 0.63867 P@5: 0.50205 N@3: 0.72952 N@5: 0.67415 early stop: 0\n","\u001b[32m[I 210617 19:15:34 models:110]\u001b[39m 2 432640 train loss: 0.0011397 valid loss: 0.0011285 P@1: 0.84339 P@3: 0.64390 P@5: 0.50727 N@3: 0.73458 N@5: 0.67964 early stop: 0\n","\u001b[32m[I 210617 19:18:57 models:110]\u001b[39m 2 458240 train loss: 0.0011229 valid loss: 0.0011206 P@1: 0.84792 P@3: 0.64789 P@5: 0.51001 N@3: 0.73919 N@5: 0.68368 early stop: 0\n","\u001b[32m[I 210617 19:22:20 models:110]\u001b[39m 2 483840 train loss: 0.0011186 valid loss: 0.0011147 P@1: 0.85226 P@3: 0.65158 P@5: 0.51345 N@3: 0.74335 N@5: 0.68790 early stop: 0\n","\u001b[32m[I 210617 19:25:44 models:110]\u001b[39m 2 509440 train loss: 0.0011007 valid loss: 0.0010974 P@1: 0.85496 P@3: 0.65613 P@5: 0.51775 N@3: 0.74759 N@5: 0.69241 early stop: 0\n","\u001b[32m[I 210617 19:29:07 models:110]\u001b[39m 2 535040 train loss: 0.0011005 valid loss: 0.0010905 P@1: 0.86056 P@3: 0.66233 P@5: 0.52277 N@3: 0.75404 N@5: 0.69851 early stop: 0\n","\u001b[32m[I 210617 19:32:30 models:110]\u001b[39m 2 560640 train loss: 0.0010961 valid loss: 0.0010916 P@1: 0.86225 P@3: 0.66584 P@5: 0.52429 N@3: 0.75749 N@5: 0.70104 early stop: 0\n","\u001b[32m[I 210617 19:35:55 models:110]\u001b[39m 3 21760 train loss: 0.0010145 valid loss: 0.0010886 P@1: 0.86313 P@3: 0.66647 P@5: 0.52666 N@3: 0.75797 N@5: 0.70273 early stop: 0\n","\u001b[32m[I 210617 19:39:20 models:110]\u001b[39m 3 47360 train loss: 0.0009862 valid loss: 0.0010812 P@1: 0.86208 P@3: 0.67008 P@5: 0.53058 N@3: 0.76091 N@5: 0.70631 early stop: 0\n","\u001b[32m[I 210617 19:42:42 models:110]\u001b[39m 3 72960 train loss: 0.0009927 valid loss: 0.0010753 P@1: 0.86313 P@3: 0.67141 P@5: 0.53181 N@3: 0.76174 N@5: 0.70687 early stop: 0\n","\u001b[32m[I 210617 19:46:07 models:110]\u001b[39m 3 98560 train loss: 0.0009866 valid loss: 0.0010806 P@1: 0.86510 P@3: 0.67196 P@5: 0.53157 N@3: 0.76246 N@5: 0.70719 early stop: 0\n","\u001b[32m[I 210617 19:49:32 models:110]\u001b[39m 3 124160 train loss: 0.0009774 valid loss: 0.0010677 P@1: 0.86860 P@3: 0.67909 P@5: 0.53724 N@3: 0.76967 N@5: 0.71397 early stop: 0\n","\u001b[32m[I 210617 19:52:54 models:110]\u001b[39m 3 149760 train loss: 0.0009804 valid loss: 0.0010555 P@1: 0.87090 P@3: 0.68016 P@5: 0.53896 N@3: 0.77075 N@5: 0.71548 early stop: 0\n","\u001b[32m[I 210617 19:56:15 models:110]\u001b[39m 3 175360 train loss: 0.0009753 valid loss: 0.0010526 P@1: 0.87322 P@3: 0.68519 P@5: 0.54404 N@3: 0.77538 N@5: 0.72071 early stop: 0\n","\u001b[32m[I 210617 19:59:39 models:110]\u001b[39m 3 200960 train loss: 0.0009741 valid loss: 0.0010398 P@1: 0.87507 P@3: 0.68979 P@5: 0.54780 N@3: 0.77950 N@5: 0.72456 early stop: 0\n","\u001b[32m[I 210617 20:03:02 models:110]\u001b[39m 3 226560 train loss: 0.0009578 valid loss: 0.0010343 P@1: 0.87827 P@3: 0.69188 P@5: 0.55013 N@3: 0.78214 N@5: 0.72750 early stop: 0\n","\u001b[32m[I 210617 20:06:27 models:110]\u001b[39m 3 252160 train loss: 0.0009562 valid loss: 0.0010280 P@1: 0.88091 P@3: 0.69474 P@5: 0.55172 N@3: 0.78500 N@5: 0.72967 early stop: 0\n","\u001b[32m[I 210617 20:09:50 models:110]\u001b[39m 3 277760 train loss: 0.0009481 valid loss: 0.0010191 P@1: 0.88289 P@3: 0.69971 P@5: 0.55683 N@3: 0.78985 N@5: 0.73521 early stop: 0\n","\u001b[32m[I 210617 20:13:13 models:110]\u001b[39m 3 303360 train loss: 0.0009507 valid loss: 0.0010119 P@1: 0.88111 P@3: 0.70076 P@5: 0.55830 N@3: 0.78989 N@5: 0.73565 early stop: 0\n","\u001b[32m[I 210617 20:16:39 models:110]\u001b[39m 3 328960 train loss: 0.0009392 valid loss: 0.0009986 P@1: 0.88567 P@3: 0.70567 P@5: 0.56280 N@3: 0.79496 N@5: 0.74072 early stop: 0\n","\u001b[32m[I 210617 20:20:05 models:110]\u001b[39m 3 354560 train loss: 0.0009372 valid loss: 0.0009939 P@1: 0.88489 P@3: 0.70732 P@5: 0.56644 N@3: 0.79614 N@5: 0.74360 early stop: 0\n","\u001b[32m[I 210617 20:23:28 models:110]\u001b[39m 3 380160 train loss: 0.0009356 valid loss: 0.0009856 P@1: 0.88811 P@3: 0.71212 P@5: 0.56981 N@3: 0.80094 N@5: 0.74789 early stop: 0\n","\u001b[32m[I 210617 20:26:50 models:110]\u001b[39m 3 405760 train loss: 0.0009234 valid loss: 0.0009790 P@1: 0.88977 P@3: 0.71291 P@5: 0.57123 N@3: 0.80212 N@5: 0.74946 early stop: 0\n","\u001b[32m[I 210617 20:30:12 models:110]\u001b[39m 3 431360 train loss: 0.0009157 valid loss: 0.0009711 P@1: 0.88959 P@3: 0.71445 P@5: 0.57281 N@3: 0.80301 N@5: 0.75078 early stop: 0\n","\u001b[32m[I 210617 20:33:34 models:110]\u001b[39m 3 456960 train loss: 0.0009132 valid loss: 0.0009629 P@1: 0.89317 P@3: 0.72194 P@5: 0.58005 N@3: 0.81004 N@5: 0.75830 early stop: 0\n","\u001b[32m[I 210617 20:36:54 models:110]\u001b[39m 3 482560 train loss: 0.0009110 valid loss: 0.0009569 P@1: 0.89285 P@3: 0.72096 P@5: 0.58038 N@3: 0.80921 N@5: 0.75818 early stop: 1\n","\u001b[32m[I 210617 20:40:15 models:110]\u001b[39m 3 508160 train loss: 0.0009077 valid loss: 0.0009474 P@1: 0.89523 P@3: 0.72593 P@5: 0.58452 N@3: 0.81415 N@5: 0.76286 early stop: 0\n","\u001b[32m[I 210617 20:43:37 models:110]\u001b[39m 3 533760 train loss: 0.0008964 valid loss: 0.0009401 P@1: 0.89666 P@3: 0.72788 P@5: 0.58682 N@3: 0.81564 N@5: 0.76504 early stop: 0\n","\u001b[32m[I 210617 20:46:58 models:110]\u001b[39m 3 559360 train loss: 0.0008934 valid loss: 0.0009288 P@1: 0.89811 P@3: 0.73080 P@5: 0.58948 N@3: 0.81844 N@5: 0.76792 early stop: 0\n","\u001b[32m[I 210617 20:47:34 models:142]\u001b[39m SWA Initializing\n","\u001b[32m[I 210617 20:50:20 models:110]\u001b[39m 4 20480 train loss: 0.0007784 valid loss: 0.0009100 P@1: 0.90155 P@3: 0.73876 P@5: 0.59822 N@3: 0.82564 N@5: 0.77642 early stop: 0\n","\u001b[32m[I 210617 20:53:40 models:110]\u001b[39m 4 46080 train loss: 0.0007484 valid loss: 0.0009058 P@1: 0.90256 P@3: 0.74187 P@5: 0.60096 N@3: 0.82841 N@5: 0.77909 early stop: 0\n","\u001b[32m[I 210617 20:56:59 models:110]\u001b[39m 4 71680 train loss: 0.0007517 valid loss: 0.0008998 P@1: 0.90393 P@3: 0.74375 P@5: 0.60412 N@3: 0.83023 N@5: 0.78199 early stop: 0\n","\u001b[32m[I 210617 21:00:18 models:110]\u001b[39m 4 97280 train loss: 0.0007541 valid loss: 0.0008971 P@1: 0.90423 P@3: 0.74486 P@5: 0.60574 N@3: 0.83118 N@5: 0.78347 early stop: 0\n","\u001b[32m[I 210617 21:03:37 models:110]\u001b[39m 4 122880 train loss: 0.0007429 valid loss: 0.0008944 P@1: 0.90531 P@3: 0.74640 P@5: 0.60714 N@3: 0.83262 N@5: 0.78493 early stop: 0\n","\u001b[32m[I 210617 21:06:54 models:110]\u001b[39m 4 148480 train loss: 0.0007556 valid loss: 0.0008904 P@1: 0.90579 P@3: 0.74751 P@5: 0.60857 N@3: 0.83363 N@5: 0.78626 early stop: 0\n","\u001b[32m[I 210617 21:10:14 models:110]\u001b[39m 4 174080 train loss: 0.0007498 valid loss: 0.0008869 P@1: 0.90687 P@3: 0.74875 P@5: 0.60960 N@3: 0.83488 N@5: 0.78746 early stop: 0\n","\u001b[32m[I 210617 21:13:34 models:110]\u001b[39m 4 199680 train loss: 0.0007422 valid loss: 0.0008839 P@1: 0.90726 P@3: 0.75003 P@5: 0.61076 N@3: 0.83607 N@5: 0.78874 early stop: 0\n","\u001b[32m[I 210617 21:16:55 models:110]\u001b[39m 4 225280 train loss: 0.0007445 valid loss: 0.0008812 P@1: 0.90779 P@3: 0.75085 P@5: 0.61173 N@3: 0.83680 N@5: 0.78959 early stop: 0\n","\u001b[32m[I 210617 21:20:16 models:110]\u001b[39m 4 250880 train loss: 0.0007428 valid loss: 0.0008780 P@1: 0.90803 P@3: 0.75143 P@5: 0.61276 N@3: 0.83737 N@5: 0.79056 early stop: 0\n","\u001b[32m[I 210617 21:23:38 models:110]\u001b[39m 4 276480 train loss: 0.0007412 valid loss: 0.0008755 P@1: 0.90901 P@3: 0.75237 P@5: 0.61398 N@3: 0.83828 N@5: 0.79176 early stop: 0\n","\u001b[32m[I 210617 21:27:00 models:110]\u001b[39m 4 302080 train loss: 0.0007415 valid loss: 0.0008720 P@1: 0.90962 P@3: 0.75381 P@5: 0.61512 N@3: 0.83962 N@5: 0.79297 early stop: 0\n","\u001b[32m[I 210617 21:30:20 models:110]\u001b[39m 4 327680 train loss: 0.0007431 valid loss: 0.0008690 P@1: 0.90997 P@3: 0.75457 P@5: 0.61608 N@3: 0.84037 N@5: 0.79395 early stop: 0\n","\u001b[32m[I 210617 21:33:40 models:110]\u001b[39m 4 353280 train loss: 0.0007445 valid loss: 0.0008660 P@1: 0.91045 P@3: 0.75539 P@5: 0.61724 N@3: 0.84112 N@5: 0.79500 early stop: 0\n","\u001b[32m[I 210617 21:37:00 models:110]\u001b[39m 4 378880 train loss: 0.0007463 valid loss: 0.0008628 P@1: 0.91075 P@3: 0.75634 P@5: 0.61835 N@3: 0.84192 N@5: 0.79600 early stop: 0\n","\u001b[32m[I 210617 21:40:19 models:110]\u001b[39m 4 404480 train loss: 0.0007477 valid loss: 0.0008595 P@1: 0.91104 P@3: 0.75694 P@5: 0.61926 N@3: 0.84256 N@5: 0.79693 early stop: 0\n","\u001b[32m[I 210617 21:43:40 models:110]\u001b[39m 4 430080 train loss: 0.0007382 valid loss: 0.0008568 P@1: 0.91122 P@3: 0.75763 P@5: 0.62009 N@3: 0.84315 N@5: 0.79769 early stop: 0\n","\u001b[32m[I 210617 21:47:01 models:110]\u001b[39m 4 455680 train loss: 0.0007361 valid loss: 0.0008543 P@1: 0.91150 P@3: 0.75807 P@5: 0.62095 N@3: 0.84357 N@5: 0.79843 early stop: 0\n","\u001b[32m[I 210617 21:50:20 models:110]\u001b[39m 4 481280 train loss: 0.0007377 valid loss: 0.0008513 P@1: 0.91184 P@3: 0.75879 P@5: 0.62185 N@3: 0.84425 N@5: 0.79934 early stop: 0\n","\u001b[32m[I 210617 21:53:39 models:110]\u001b[39m 4 506880 train loss: 0.0007387 valid loss: 0.0008486 P@1: 0.91237 P@3: 0.75965 P@5: 0.62288 N@3: 0.84514 N@5: 0.80039 early stop: 0\n","\u001b[32m[I 210617 21:56:57 models:110]\u001b[39m 4 532480 train loss: 0.0007365 valid loss: 0.0008457 P@1: 0.91257 P@3: 0.76034 P@5: 0.62346 N@3: 0.84574 N@5: 0.80097 early stop: 0\n","\u001b[32m[I 210617 22:00:16 models:110]\u001b[39m 4 558080 train loss: 0.0007327 valid loss: 0.0008433 P@1: 0.91295 P@3: 0.76108 P@5: 0.62435 N@3: 0.84638 N@5: 0.80178 early stop: 0\n","\u001b[32m[I 210617 22:03:40 models:110]\u001b[39m 5 19200 train loss: 0.0006103 valid loss: 0.0008432 P@1: 0.91326 P@3: 0.76140 P@5: 0.62480 N@3: 0.84670 N@5: 0.80222 early stop: 0\n","\u001b[32m[I 210617 22:07:00 models:110]\u001b[39m 5 44800 train loss: 0.0005825 valid loss: 0.0008429 P@1: 0.91356 P@3: 0.76176 P@5: 0.62510 N@3: 0.84700 N@5: 0.80254 early stop: 0\n","\u001b[32m[I 210617 22:10:19 models:110]\u001b[39m 5 70400 train loss: 0.0005801 valid loss: 0.0008427 P@1: 0.91380 P@3: 0.76238 P@5: 0.62546 N@3: 0.84750 N@5: 0.80288 early stop: 0\n","\u001b[32m[I 210617 22:13:40 models:110]\u001b[39m 5 96000 train loss: 0.0005884 valid loss: 0.0008424 P@1: 0.91376 P@3: 0.76257 P@5: 0.62580 N@3: 0.84765 N@5: 0.80316 early stop: 0\n","\u001b[32m[I 210617 22:17:00 models:110]\u001b[39m 5 121600 train loss: 0.0005847 valid loss: 0.0008422 P@1: 0.91404 P@3: 0.76280 P@5: 0.62604 N@3: 0.84790 N@5: 0.80340 early stop: 0\n","\u001b[32m[I 210617 22:20:23 models:110]\u001b[39m 5 147200 train loss: 0.0005903 valid loss: 0.0008419 P@1: 0.91417 P@3: 0.76310 P@5: 0.62651 N@3: 0.84815 N@5: 0.80383 early stop: 0\n","\u001b[32m[I 210617 22:23:43 models:110]\u001b[39m 5 172800 train loss: 0.0005914 valid loss: 0.0008418 P@1: 0.91428 P@3: 0.76326 P@5: 0.62685 N@3: 0.84831 N@5: 0.80414 early stop: 0\n","\u001b[32m[I 210617 22:27:06 models:110]\u001b[39m 5 198400 train loss: 0.0005954 valid loss: 0.0008413 P@1: 0.91411 P@3: 0.76355 P@5: 0.62715 N@3: 0.84847 N@5: 0.80434 early stop: 0\n","\u001b[32m[I 210617 22:30:29 models:110]\u001b[39m 5 224000 train loss: 0.0005967 valid loss: 0.0008410 P@1: 0.91428 P@3: 0.76393 P@5: 0.62747 N@3: 0.84880 N@5: 0.80464 early stop: 0\n","\u001b[32m[I 210617 22:33:51 models:110]\u001b[39m 5 249600 train loss: 0.0005987 valid loss: 0.0008403 P@1: 0.91451 P@3: 0.76426 P@5: 0.62780 N@3: 0.84908 N@5: 0.80495 early stop: 0\n","\u001b[32m[I 210617 22:37:12 models:110]\u001b[39m 5 275200 train loss: 0.0006024 valid loss: 0.0008398 P@1: 0.91462 P@3: 0.76437 P@5: 0.62815 N@3: 0.84917 N@5: 0.80524 early stop: 0\n","\u001b[32m[I 210617 22:40:35 models:110]\u001b[39m 5 300800 train loss: 0.0005998 valid loss: 0.0008393 P@1: 0.91491 P@3: 0.76452 P@5: 0.62846 N@3: 0.84934 N@5: 0.80554 early stop: 0\n","\u001b[32m[I 210617 22:43:56 models:110]\u001b[39m 5 326400 train loss: 0.0006079 valid loss: 0.0008387 P@1: 0.91488 P@3: 0.76474 P@5: 0.62883 N@3: 0.84953 N@5: 0.80585 early stop: 0\n","\u001b[32m[I 210617 22:47:18 models:110]\u001b[39m 5 352000 train loss: 0.0006045 valid loss: 0.0008384 P@1: 0.91502 P@3: 0.76505 P@5: 0.62900 N@3: 0.84980 N@5: 0.80602 early stop: 0\n","\u001b[32m[I 210617 22:50:39 models:110]\u001b[39m 5 377600 train loss: 0.0006060 valid loss: 0.0008379 P@1: 0.91503 P@3: 0.76535 P@5: 0.62945 N@3: 0.85002 N@5: 0.80639 early stop: 0\n","\u001b[32m[I 210617 22:54:02 models:110]\u001b[39m 5 403200 train loss: 0.0006094 valid loss: 0.0008371 P@1: 0.91520 P@3: 0.76560 P@5: 0.63002 N@3: 0.85024 N@5: 0.80683 early stop: 0\n","\u001b[32m[I 210617 22:57:24 models:110]\u001b[39m 5 428800 train loss: 0.0006118 valid loss: 0.0008362 P@1: 0.91537 P@3: 0.76583 P@5: 0.63026 N@3: 0.85045 N@5: 0.80707 early stop: 0\n","\u001b[32m[I 210617 23:00:46 models:110]\u001b[39m 5 454400 train loss: 0.0006081 valid loss: 0.0008354 P@1: 0.91559 P@3: 0.76616 P@5: 0.63061 N@3: 0.85079 N@5: 0.80743 early stop: 0\n","\u001b[32m[I 210617 23:04:10 models:110]\u001b[39m 5 480000 train loss: 0.0006091 valid loss: 0.0008345 P@1: 0.91559 P@3: 0.76656 P@5: 0.63087 N@3: 0.85109 N@5: 0.80769 early stop: 0\n","\u001b[32m[I 210617 23:07:31 models:110]\u001b[39m 5 505600 train loss: 0.0006183 valid loss: 0.0008337 P@1: 0.91563 P@3: 0.76692 P@5: 0.63123 N@3: 0.85140 N@5: 0.80801 early stop: 0\n","\u001b[32m[I 210617 23:10:52 models:110]\u001b[39m 5 531200 train loss: 0.0006118 valid loss: 0.0008329 P@1: 0.91564 P@3: 0.76711 P@5: 0.63157 N@3: 0.85157 N@5: 0.80830 early stop: 0\n","\u001b[32m[I 210617 23:14:14 models:110]\u001b[39m 5 556800 train loss: 0.0006150 valid loss: 0.0008319 P@1: 0.91596 P@3: 0.76737 P@5: 0.63186 N@3: 0.85181 N@5: 0.80861 early stop: 0\n","\u001b[32m[I 210617 23:17:37 models:110]\u001b[39m 6 17920 train loss: 0.0005040 valid loss: 0.0008320 P@1: 0.91611 P@3: 0.76760 P@5: 0.63208 N@3: 0.85201 N@5: 0.80882 early stop: 0\n","\u001b[32m[I 210617 23:20:58 models:110]\u001b[39m 6 43520 train loss: 0.0004629 valid loss: 0.0008323 P@1: 0.91620 P@3: 0.76774 P@5: 0.63223 N@3: 0.85211 N@5: 0.80895 early stop: 0\n","\u001b[32m[I 210617 23:24:19 models:110]\u001b[39m 6 69120 train loss: 0.0004694 valid loss: 0.0008326 P@1: 0.91621 P@3: 0.76786 P@5: 0.63253 N@3: 0.85222 N@5: 0.80919 early stop: 0\n","\u001b[32m[I 210617 23:27:40 models:110]\u001b[39m 6 94720 train loss: 0.0004745 valid loss: 0.0008330 P@1: 0.91617 P@3: 0.76804 P@5: 0.63270 N@3: 0.85232 N@5: 0.80927 early stop: 0\n","\u001b[32m[I 210617 23:31:01 models:110]\u001b[39m 6 120320 train loss: 0.0004792 valid loss: 0.0008335 P@1: 0.91610 P@3: 0.76823 P@5: 0.63292 N@3: 0.85243 N@5: 0.80942 early stop: 0\n","\u001b[32m[I 210617 23:34:20 models:110]\u001b[39m 6 145920 train loss: 0.0004814 valid loss: 0.0008339 P@1: 0.91604 P@3: 0.76834 P@5: 0.63310 N@3: 0.85250 N@5: 0.80955 early stop: 0\n","\u001b[32m[I 210617 23:37:39 models:110]\u001b[39m 6 171520 train loss: 0.0004856 valid loss: 0.0008342 P@1: 0.91620 P@3: 0.76840 P@5: 0.63337 N@3: 0.85255 N@5: 0.80977 early stop: 0\n","\u001b[32m[I 210617 23:40:59 models:110]\u001b[39m 6 197120 train loss: 0.0004848 valid loss: 0.0008347 P@1: 0.91625 P@3: 0.76860 P@5: 0.63345 N@3: 0.85274 N@5: 0.80984 early stop: 0\n","\u001b[32m[I 210617 23:44:17 models:110]\u001b[39m 6 222720 train loss: 0.0004948 valid loss: 0.0008350 P@1: 0.91608 P@3: 0.76875 P@5: 0.63368 N@3: 0.85282 N@5: 0.81000 early stop: 0\n","\u001b[32m[I 210617 23:47:38 models:110]\u001b[39m 6 248320 train loss: 0.0004913 valid loss: 0.0008352 P@1: 0.91622 P@3: 0.76884 P@5: 0.63384 N@3: 0.85295 N@5: 0.81015 early stop: 0\n","\u001b[32m[I 210617 23:51:01 models:110]\u001b[39m 6 273920 train loss: 0.0004954 valid loss: 0.0008354 P@1: 0.91624 P@3: 0.76904 P@5: 0.63390 N@3: 0.85312 N@5: 0.81025 early stop: 0\n","\u001b[32m[I 210617 23:54:22 models:110]\u001b[39m 6 299520 train loss: 0.0005021 valid loss: 0.0008356 P@1: 0.91638 P@3: 0.76928 P@5: 0.63407 N@3: 0.85332 N@5: 0.81040 early stop: 0\n","\u001b[32m[I 210617 23:57:42 models:110]\u001b[39m 6 325120 train loss: 0.0005009 valid loss: 0.0008358 P@1: 0.91632 P@3: 0.76936 P@5: 0.63423 N@3: 0.85339 N@5: 0.81057 early stop: 0\n","\u001b[32m[I 210618 00:01:02 models:110]\u001b[39m 6 350720 train loss: 0.0005023 valid loss: 0.0008359 P@1: 0.91630 P@3: 0.76941 P@5: 0.63434 N@3: 0.85340 N@5: 0.81063 early stop: 0\n","\u001b[32m[I 210618 00:04:23 models:110]\u001b[39m 6 376320 train loss: 0.0005116 valid loss: 0.0008360 P@1: 0.91634 P@3: 0.76944 P@5: 0.63443 N@3: 0.85343 N@5: 0.81073 early stop: 0\n","\u001b[32m[I 210618 00:07:44 models:110]\u001b[39m 6 401920 train loss: 0.0005106 valid loss: 0.0008362 P@1: 0.91642 P@3: 0.76941 P@5: 0.63457 N@3: 0.85345 N@5: 0.81086 early stop: 0\n","\u001b[32m[I 210618 00:11:06 models:110]\u001b[39m 6 427520 train loss: 0.0005120 valid loss: 0.0008363 P@1: 0.91652 P@3: 0.76956 P@5: 0.63475 N@3: 0.85357 N@5: 0.81101 early stop: 0\n","\u001b[32m[I 210618 00:14:27 models:110]\u001b[39m 6 453120 train loss: 0.0005148 valid loss: 0.0008364 P@1: 0.91661 P@3: 0.76972 P@5: 0.63505 N@3: 0.85371 N@5: 0.81127 early stop: 0\n","\u001b[32m[I 210618 00:17:48 models:110]\u001b[39m 6 478720 train loss: 0.0005191 valid loss: 0.0008363 P@1: 0.91689 P@3: 0.76978 P@5: 0.63517 N@3: 0.85377 N@5: 0.81139 early stop: 0\n","\u001b[32m[I 210618 00:21:09 models:110]\u001b[39m 6 504320 train loss: 0.0005146 valid loss: 0.0008361 P@1: 0.91693 P@3: 0.77005 P@5: 0.63550 N@3: 0.85400 N@5: 0.81168 early stop: 0\n","\u001b[32m[I 210618 00:24:29 models:110]\u001b[39m 6 529920 train loss: 0.0005200 valid loss: 0.0008360 P@1: 0.91706 P@3: 0.77013 P@5: 0.63576 N@3: 0.85410 N@5: 0.81190 early stop: 0\n","\u001b[32m[I 210618 00:27:50 models:110]\u001b[39m 6 555520 train loss: 0.0005172 valid loss: 0.0008359 P@1: 0.91708 P@3: 0.77031 P@5: 0.63597 N@3: 0.85424 N@5: 0.81204 early stop: 0\n","\u001b[32m[I 210618 00:31:14 models:110]\u001b[39m 7 16640 train loss: 0.0004291 valid loss: 0.0008365 P@1: 0.91712 P@3: 0.77043 P@5: 0.63604 N@3: 0.85434 N@5: 0.81209 early stop: 0\n","\u001b[32m[I 210618 00:34:37 models:110]\u001b[39m 7 42240 train loss: 0.0003800 valid loss: 0.0008370 P@1: 0.91712 P@3: 0.77049 P@5: 0.63606 N@3: 0.85438 N@5: 0.81210 early stop: 0\n","\u001b[32m[I 210618 00:38:01 models:110]\u001b[39m 7 67840 train loss: 0.0003812 valid loss: 0.0008378 P@1: 0.91706 P@3: 0.77059 P@5: 0.63622 N@3: 0.85445 N@5: 0.81221 early stop: 0\n","\u001b[32m[I 210618 00:41:22 models:110]\u001b[39m 7 93440 train loss: 0.0003912 valid loss: 0.0008385 P@1: 0.91709 P@3: 0.77080 P@5: 0.63633 N@3: 0.85459 N@5: 0.81232 early stop: 0\n","\u001b[32m[I 210618 00:44:41 models:110]\u001b[39m 7 119040 train loss: 0.0003994 valid loss: 0.0008392 P@1: 0.91705 P@3: 0.77078 P@5: 0.63651 N@3: 0.85457 N@5: 0.81245 early stop: 0\n","\u001b[32m[I 210618 00:47:58 models:110]\u001b[39m 7 144640 train loss: 0.0004050 valid loss: 0.0008398 P@1: 0.91720 P@3: 0.77095 P@5: 0.63658 N@3: 0.85476 N@5: 0.81258 early stop: 0\n","\u001b[32m[I 210618 00:51:17 models:110]\u001b[39m 7 170240 train loss: 0.0004054 valid loss: 0.0008404 P@1: 0.91712 P@3: 0.77103 P@5: 0.63675 N@3: 0.85481 N@5: 0.81268 early stop: 0\n","\u001b[32m[I 210618 00:54:36 models:110]\u001b[39m 7 195840 train loss: 0.0004108 valid loss: 0.0008411 P@1: 0.91686 P@3: 0.77112 P@5: 0.63683 N@3: 0.85483 N@5: 0.81270 early stop: 0\n","\u001b[32m[I 210618 00:57:54 models:110]\u001b[39m 7 221440 train loss: 0.0004130 valid loss: 0.0008416 P@1: 0.91689 P@3: 0.77118 P@5: 0.63683 N@3: 0.85490 N@5: 0.81268 early stop: 1\n","\u001b[32m[I 210618 01:01:12 models:110]\u001b[39m 7 247040 train loss: 0.0004155 valid loss: 0.0008422 P@1: 0.91689 P@3: 0.77117 P@5: 0.63696 N@3: 0.85490 N@5: 0.81278 early stop: 0\n","\u001b[32m[I 210618 01:04:31 models:110]\u001b[39m 7 272640 train loss: 0.0004189 valid loss: 0.0008428 P@1: 0.91674 P@3: 0.77132 P@5: 0.63709 N@3: 0.85495 N@5: 0.81284 early stop: 0\n","\u001b[32m[I 210618 01:07:49 models:110]\u001b[39m 7 298240 train loss: 0.0004203 valid loss: 0.0008435 P@1: 0.91669 P@3: 0.77130 P@5: 0.63710 N@3: 0.85494 N@5: 0.81285 early stop: 0\n","\u001b[32m[I 210618 01:11:08 models:110]\u001b[39m 7 323840 train loss: 0.0004232 valid loss: 0.0008441 P@1: 0.91674 P@3: 0.77139 P@5: 0.63721 N@3: 0.85502 N@5: 0.81296 early stop: 0\n","\u001b[32m[I 210618 01:14:27 models:110]\u001b[39m 7 349440 train loss: 0.0004298 valid loss: 0.0008447 P@1: 0.91679 P@3: 0.77139 P@5: 0.63733 N@3: 0.85501 N@5: 0.81302 early stop: 0\n","\u001b[32m[I 210618 01:17:46 models:110]\u001b[39m 7 375040 train loss: 0.0004293 valid loss: 0.0008452 P@1: 0.91674 P@3: 0.77155 P@5: 0.63744 N@3: 0.85512 N@5: 0.81311 early stop: 0\n","\u001b[32m[I 210618 01:21:06 models:110]\u001b[39m 7 400640 train loss: 0.0004313 valid loss: 0.0008456 P@1: 0.91682 P@3: 0.77155 P@5: 0.63760 N@3: 0.85517 N@5: 0.81328 early stop: 0\n","\u001b[32m[I 210618 01:24:25 models:110]\u001b[39m 7 426240 train loss: 0.0004309 valid loss: 0.0008460 P@1: 0.91689 P@3: 0.77160 P@5: 0.63767 N@3: 0.85522 N@5: 0.81335 early stop: 0\n","\u001b[32m[I 210618 01:27:44 models:110]\u001b[39m 7 451840 train loss: 0.0004411 valid loss: 0.0008464 P@1: 0.91685 P@3: 0.77165 P@5: 0.63777 N@3: 0.85526 N@5: 0.81342 early stop: 0\n","\u001b[32m[I 210618 01:31:01 models:110]\u001b[39m 7 477440 train loss: 0.0004382 valid loss: 0.0008469 P@1: 0.91669 P@3: 0.77169 P@5: 0.63784 N@3: 0.85524 N@5: 0.81346 early stop: 0\n","\u001b[32m[I 210618 01:34:19 models:110]\u001b[39m 7 503040 train loss: 0.0004409 valid loss: 0.0008471 P@1: 0.91668 P@3: 0.77170 P@5: 0.63801 N@3: 0.85527 N@5: 0.81360 early stop: 0\n","\u001b[32m[I 210618 01:37:37 models:110]\u001b[39m 7 528640 train loss: 0.0004435 valid loss: 0.0008474 P@1: 0.91675 P@3: 0.77188 P@5: 0.63813 N@3: 0.85540 N@5: 0.81370 early stop: 0\n","\u001b[32m[I 210618 01:40:56 models:110]\u001b[39m 7 554240 train loss: 0.0004443 valid loss: 0.0008477 P@1: 0.91669 P@3: 0.77193 P@5: 0.63815 N@3: 0.85542 N@5: 0.81371 early stop: 0\n","\u001b[32m[I 210618 01:44:18 models:110]\u001b[39m 8 15360 train loss: 0.0003721 valid loss: 0.0008486 P@1: 0.91662 P@3: 0.77195 P@5: 0.63823 N@3: 0.85543 N@5: 0.81375 early stop: 0\n","\u001b[32m[I 210618 01:47:36 models:110]\u001b[39m 8 40960 train loss: 0.0003174 valid loss: 0.0008494 P@1: 0.91651 P@3: 0.77195 P@5: 0.63834 N@3: 0.85541 N@5: 0.81380 early stop: 0\n","\u001b[32m[I 210618 01:50:55 models:110]\u001b[39m 8 66560 train loss: 0.0003286 valid loss: 0.0008503 P@1: 0.91651 P@3: 0.77205 P@5: 0.63840 N@3: 0.85548 N@5: 0.81383 early stop: 0\n","\u001b[32m[I 210618 01:54:14 models:110]\u001b[39m 8 92160 train loss: 0.0003289 valid loss: 0.0008513 P@1: 0.91654 P@3: 0.77207 P@5: 0.63855 N@3: 0.85549 N@5: 0.81392 early stop: 0\n","\u001b[32m[I 210618 01:57:34 models:110]\u001b[39m 8 117760 train loss: 0.0003364 valid loss: 0.0008522 P@1: 0.91659 P@3: 0.77214 P@5: 0.63858 N@3: 0.85557 N@5: 0.81397 early stop: 0\n","\u001b[32m[I 210618 02:00:53 models:110]\u001b[39m 8 143360 train loss: 0.0003436 valid loss: 0.0008531 P@1: 0.91655 P@3: 0.77223 P@5: 0.63872 N@3: 0.85564 N@5: 0.81408 early stop: 0\n","\u001b[32m[I 210618 02:04:11 models:110]\u001b[39m 8 168960 train loss: 0.0003458 valid loss: 0.0008538 P@1: 0.91654 P@3: 0.77226 P@5: 0.63883 N@3: 0.85566 N@5: 0.81417 early stop: 0\n","\u001b[32m[I 210618 02:07:31 models:110]\u001b[39m 8 194560 train loss: 0.0003432 valid loss: 0.0008546 P@1: 0.91662 P@3: 0.77218 P@5: 0.63888 N@3: 0.85561 N@5: 0.81422 early stop: 0\n","\u001b[32m[I 210618 02:10:47 models:110]\u001b[39m 8 220160 train loss: 0.0003516 valid loss: 0.0008553 P@1: 0.91662 P@3: 0.77222 P@5: 0.63885 N@3: 0.85561 N@5: 0.81418 early stop: 1\n","\u001b[32m[I 210618 02:14:03 models:110]\u001b[39m 8 245760 train loss: 0.0003523 valid loss: 0.0008561 P@1: 0.91655 P@3: 0.77222 P@5: 0.63879 N@3: 0.85559 N@5: 0.81411 early stop: 2\n","\u001b[32m[I 210618 02:17:21 models:110]\u001b[39m 8 271360 train loss: 0.0003596 valid loss: 0.0008569 P@1: 0.91647 P@3: 0.77218 P@5: 0.63880 N@3: 0.85554 N@5: 0.81409 early stop: 3\n","\u001b[32m[I 210618 02:20:37 models:110]\u001b[39m 8 296960 train loss: 0.0003582 valid loss: 0.0008577 P@1: 0.91645 P@3: 0.77228 P@5: 0.63886 N@3: 0.85562 N@5: 0.81415 early stop: 4\n","\u001b[32m[I 210618 02:23:53 models:110]\u001b[39m 8 322560 train loss: 0.0003640 valid loss: 0.0008585 P@1: 0.91655 P@3: 0.77242 P@5: 0.63887 N@3: 0.85575 N@5: 0.81419 early stop: 5\n","\u001b[32m[I 210618 02:27:12 models:110]\u001b[39m 8 348160 train loss: 0.0003660 valid loss: 0.0008591 P@1: 0.91652 P@3: 0.77253 P@5: 0.63892 N@3: 0.85583 N@5: 0.81424 early stop: 0\n","\u001b[32m[I 210618 02:30:29 models:110]\u001b[39m 8 373760 train loss: 0.0003670 valid loss: 0.0008597 P@1: 0.91652 P@3: 0.77249 P@5: 0.63902 N@3: 0.85581 N@5: 0.81431 early stop: 0\n","\u001b[32m[I 210618 02:33:48 models:110]\u001b[39m 8 399360 train loss: 0.0003705 valid loss: 0.0008604 P@1: 0.91645 P@3: 0.77263 P@5: 0.63908 N@3: 0.85591 N@5: 0.81434 early stop: 0\n","\u001b[32m[I 210618 02:37:07 models:110]\u001b[39m 8 424960 train loss: 0.0003744 valid loss: 0.0008611 P@1: 0.91638 P@3: 0.77256 P@5: 0.63918 N@3: 0.85584 N@5: 0.81441 early stop: 0\n","\u001b[32m[I 210618 02:40:25 models:110]\u001b[39m 8 450560 train loss: 0.0003781 valid loss: 0.0008617 P@1: 0.91639 P@3: 0.77260 P@5: 0.63919 N@3: 0.85589 N@5: 0.81444 early stop: 0\n","\u001b[32m[I 210618 02:43:43 models:110]\u001b[39m 8 476160 train loss: 0.0003797 valid loss: 0.0008623 P@1: 0.91625 P@3: 0.77261 P@5: 0.63918 N@3: 0.85585 N@5: 0.81440 early stop: 1\n","\u001b[32m[I 210618 02:46:59 models:110]\u001b[39m 8 501760 train loss: 0.0003808 valid loss: 0.0008629 P@1: 0.91625 P@3: 0.77261 P@5: 0.63923 N@3: 0.85586 N@5: 0.81443 early stop: 2\n","\u001b[32m[I 210618 02:50:15 models:110]\u001b[39m 8 527360 train loss: 0.0003855 valid loss: 0.0008634 P@1: 0.91624 P@3: 0.77262 P@5: 0.63927 N@3: 0.85589 N@5: 0.81449 early stop: 0\n","\u001b[32m[I 210618 02:53:33 models:110]\u001b[39m 8 552960 train loss: 0.0003866 valid loss: 0.0008640 P@1: 0.91621 P@3: 0.77259 P@5: 0.63931 N@3: 0.85587 N@5: 0.81451 early stop: 0\n","\u001b[32m[I 210618 02:56:53 models:110]\u001b[39m 9 14080 train loss: 0.0003286 valid loss: 0.0008648 P@1: 0.91627 P@3: 0.77266 P@5: 0.63947 N@3: 0.85591 N@5: 0.81464 early stop: 0\n","\u001b[32m[I 210618 03:00:09 models:110]\u001b[39m 9 39680 train loss: 0.0002730 valid loss: 0.0008658 P@1: 0.91627 P@3: 0.77263 P@5: 0.63953 N@3: 0.85590 N@5: 0.81470 early stop: 0\n","\u001b[32m[I 210618 03:03:26 models:110]\u001b[39m 9 65280 train loss: 0.0002801 valid loss: 0.0008670 P@1: 0.91625 P@3: 0.77262 P@5: 0.63951 N@3: 0.85589 N@5: 0.81468 early stop: 1\n","\u001b[32m[I 210618 03:06:41 models:110]\u001b[39m 9 90880 train loss: 0.0002865 valid loss: 0.0008680 P@1: 0.91631 P@3: 0.77259 P@5: 0.63958 N@3: 0.85588 N@5: 0.81471 early stop: 0\n","\u001b[32m[I 210618 03:09:58 models:110]\u001b[39m 9 116480 train loss: 0.0002899 valid loss: 0.0008689 P@1: 0.91630 P@3: 0.77263 P@5: 0.63965 N@3: 0.85590 N@5: 0.81475 early stop: 0\n","\u001b[32m[I 210618 03:13:15 models:110]\u001b[39m 9 142080 train loss: 0.0002951 valid loss: 0.0008700 P@1: 0.91618 P@3: 0.77266 P@5: 0.63960 N@3: 0.85590 N@5: 0.81471 early stop: 1\n","\u001b[32m[I 210618 03:16:31 models:110]\u001b[39m 9 167680 train loss: 0.0002942 valid loss: 0.0008710 P@1: 0.91622 P@3: 0.77266 P@5: 0.63970 N@3: 0.85590 N@5: 0.81478 early stop: 0\n","\u001b[32m[I 210618 03:19:51 models:110]\u001b[39m 9 193280 train loss: 0.0003016 valid loss: 0.0008720 P@1: 0.91615 P@3: 0.77268 P@5: 0.63975 N@3: 0.85590 N@5: 0.81482 early stop: 0\n","\u001b[32m[I 210618 03:23:08 models:110]\u001b[39m 9 218880 train loss: 0.0003021 valid loss: 0.0008731 P@1: 0.91608 P@3: 0.77273 P@5: 0.63977 N@3: 0.85593 N@5: 0.81484 early stop: 0\n","\u001b[32m[I 210618 03:26:26 models:110]\u001b[39m 9 244480 train loss: 0.0003085 valid loss: 0.0008740 P@1: 0.91604 P@3: 0.77283 P@5: 0.63985 N@3: 0.85598 N@5: 0.81488 early stop: 0\n","\u001b[32m[I 210618 03:29:45 models:110]\u001b[39m 9 270080 train loss: 0.0003107 valid loss: 0.0008750 P@1: 0.91615 P@3: 0.77284 P@5: 0.63989 N@3: 0.85599 N@5: 0.81491 early stop: 0\n","\u001b[32m[I 210618 03:33:06 models:110]\u001b[39m 9 295680 train loss: 0.0003124 valid loss: 0.0008758 P@1: 0.91625 P@3: 0.77287 P@5: 0.63996 N@3: 0.85604 N@5: 0.81500 early stop: 0\n","\u001b[32m[I 210618 03:36:24 models:110]\u001b[39m 9 321280 train loss: 0.0003144 valid loss: 0.0008768 P@1: 0.91622 P@3: 0.77291 P@5: 0.63998 N@3: 0.85605 N@5: 0.81499 early stop: 1\n","\u001b[32m[I 210618 03:39:40 models:110]\u001b[39m 9 346880 train loss: 0.0003190 valid loss: 0.0008777 P@1: 0.91620 P@3: 0.77302 P@5: 0.64010 N@3: 0.85611 N@5: 0.81509 early stop: 0\n","\u001b[32m[I 210618 03:42:59 models:110]\u001b[39m 9 372480 train loss: 0.0003228 valid loss: 0.0008786 P@1: 0.91632 P@3: 0.77313 P@5: 0.64019 N@3: 0.85624 N@5: 0.81520 early stop: 0\n","\u001b[32m[I 210618 03:46:17 models:110]\u001b[39m 9 398080 train loss: 0.0003219 valid loss: 0.0008794 P@1: 0.91624 P@3: 0.77319 P@5: 0.64020 N@3: 0.85628 N@5: 0.81523 early stop: 0\n","\u001b[32m[I 210618 03:49:38 models:110]\u001b[39m 9 423680 train loss: 0.0003261 valid loss: 0.0008803 P@1: 0.91620 P@3: 0.77325 P@5: 0.64027 N@3: 0.85631 N@5: 0.81525 early stop: 0\n","\u001b[32m[I 210618 03:52:59 models:110]\u001b[39m 9 449280 train loss: 0.0003291 valid loss: 0.0008811 P@1: 0.91611 P@3: 0.77329 P@5: 0.64038 N@3: 0.85634 N@5: 0.81532 early stop: 0\n","\u001b[32m[I 210618 03:56:20 models:110]\u001b[39m 9 474880 train loss: 0.0003297 valid loss: 0.0008819 P@1: 0.91613 P@3: 0.77319 P@5: 0.64035 N@3: 0.85626 N@5: 0.81530 early stop: 1\n","\u001b[32m[I 210618 03:59:39 models:110]\u001b[39m 9 500480 train loss: 0.0003328 valid loss: 0.0008826 P@1: 0.91617 P@3: 0.77321 P@5: 0.64040 N@3: 0.85627 N@5: 0.81532 early stop: 0\n","\u001b[32m[I 210618 04:02:57 models:110]\u001b[39m 9 526080 train loss: 0.0003348 valid loss: 0.0008833 P@1: 0.91611 P@3: 0.77325 P@5: 0.64041 N@3: 0.85627 N@5: 0.81531 early stop: 1\n","\u001b[32m[I 210618 04:06:15 models:110]\u001b[39m 9 551680 train loss: 0.0003400 valid loss: 0.0008840 P@1: 0.91621 P@3: 0.77330 P@5: 0.64047 N@3: 0.85634 N@5: 0.81537 early stop: 0\n","\u001b[32m[I 210618 04:09:35 models:110]\u001b[39m 10 12800 train loss: 0.0002882 valid loss: 0.0008850 P@1: 0.91625 P@3: 0.77340 P@5: 0.64051 N@3: 0.85643 N@5: 0.81543 early stop: 0\n","\u001b[32m[I 210618 04:12:54 models:110]\u001b[39m 10 38400 train loss: 0.0002400 valid loss: 0.0008861 P@1: 0.91613 P@3: 0.77336 P@5: 0.64056 N@3: 0.85634 N@5: 0.81540 early stop: 1\n","\u001b[32m[I 210618 04:16:11 models:110]\u001b[39m 10 64000 train loss: 0.0002460 valid loss: 0.0008872 P@1: 0.91604 P@3: 0.77340 P@5: 0.64053 N@3: 0.85636 N@5: 0.81538 early stop: 2\n","\u001b[32m[I 210618 04:19:28 models:110]\u001b[39m 10 89600 train loss: 0.0002503 valid loss: 0.0008884 P@1: 0.91603 P@3: 0.77334 P@5: 0.64052 N@3: 0.85632 N@5: 0.81538 early stop: 3\n","\u001b[32m[I 210618 04:22:45 models:110]\u001b[39m 10 115200 train loss: 0.0002527 valid loss: 0.0008895 P@1: 0.91603 P@3: 0.77335 P@5: 0.64054 N@3: 0.85634 N@5: 0.81541 early stop: 4\n","\u001b[32m[I 210618 04:26:01 models:110]\u001b[39m 10 140800 train loss: 0.0002551 valid loss: 0.0008906 P@1: 0.91596 P@3: 0.77335 P@5: 0.64054 N@3: 0.85629 N@5: 0.81536 early stop: 5\n","\u001b[32m[I 210618 04:29:15 models:110]\u001b[39m 10 166400 train loss: 0.0002585 valid loss: 0.0008917 P@1: 0.91594 P@3: 0.77335 P@5: 0.64054 N@3: 0.85628 N@5: 0.81536 early stop: 6\n","\u001b[32m[I 210618 04:32:31 models:110]\u001b[39m 10 192000 train loss: 0.0002629 valid loss: 0.0008928 P@1: 0.91588 P@3: 0.77337 P@5: 0.64054 N@3: 0.85629 N@5: 0.81535 early stop: 7\n","\u001b[32m[I 210618 04:35:47 models:110]\u001b[39m 10 217600 train loss: 0.0002662 valid loss: 0.0008939 P@1: 0.91588 P@3: 0.77333 P@5: 0.64053 N@3: 0.85625 N@5: 0.81533 early stop: 8\n","\u001b[32m[I 210618 04:39:02 models:110]\u001b[39m 10 243200 train loss: 0.0002673 valid loss: 0.0008950 P@1: 0.91586 P@3: 0.77329 P@5: 0.64056 N@3: 0.85620 N@5: 0.81534 early stop: 9\n","\u001b[32m[I 210618 04:42:18 models:110]\u001b[39m 10 268800 train loss: 0.0002713 valid loss: 0.0008960 P@1: 0.91574 P@3: 0.77322 P@5: 0.64063 N@3: 0.85613 N@5: 0.81536 early stop: 10\n","\u001b[32m[I 210618 04:45:34 models:110]\u001b[39m 10 294400 train loss: 0.0002742 valid loss: 0.0008970 P@1: 0.91570 P@3: 0.77325 P@5: 0.64064 N@3: 0.85615 N@5: 0.81537 early stop: 11\n","\u001b[32m[I 210618 04:48:49 models:110]\u001b[39m 10 320000 train loss: 0.0002811 valid loss: 0.0008982 P@1: 0.91571 P@3: 0.77322 P@5: 0.64055 N@3: 0.85611 N@5: 0.81529 early stop: 12\n","\u001b[32m[I 210618 04:52:04 models:110]\u001b[39m 10 345600 train loss: 0.0002788 valid loss: 0.0008992 P@1: 0.91570 P@3: 0.77319 P@5: 0.64062 N@3: 0.85609 N@5: 0.81533 early stop: 13\n","\u001b[32m[I 210618 04:55:20 models:110]\u001b[39m 10 371200 train loss: 0.0002844 valid loss: 0.0009002 P@1: 0.91567 P@3: 0.77323 P@5: 0.64068 N@3: 0.85609 N@5: 0.81535 early stop: 14\n","\u001b[32m[I 210618 04:58:38 models:110]\u001b[39m 10 396800 train loss: 0.0002839 valid loss: 0.0009012 P@1: 0.91577 P@3: 0.77319 P@5: 0.64069 N@3: 0.85609 N@5: 0.81537 early stop: 15\n","\u001b[32m[I 210618 05:01:54 models:110]\u001b[39m 10 422400 train loss: 0.0002879 valid loss: 0.0009021 P@1: 0.91574 P@3: 0.77321 P@5: 0.64075 N@3: 0.85610 N@5: 0.81542 early stop: 16\n","\u001b[32m[I 210618 05:05:11 models:110]\u001b[39m 10 448000 train loss: 0.0002850 valid loss: 0.0009030 P@1: 0.91577 P@3: 0.77325 P@5: 0.64087 N@3: 0.85613 N@5: 0.81551 early stop: 0\n","\u001b[32m[I 210618 05:08:28 models:110]\u001b[39m 10 473600 train loss: 0.0002942 valid loss: 0.0009039 P@1: 0.91587 P@3: 0.77330 P@5: 0.64085 N@3: 0.85616 N@5: 0.81548 early stop: 1\n","\u001b[32m[I 210618 05:11:44 models:110]\u001b[39m 10 499200 train loss: 0.0002922 valid loss: 0.0009049 P@1: 0.91573 P@3: 0.77323 P@5: 0.64082 N@3: 0.85609 N@5: 0.81543 early stop: 2\n","\u001b[32m[I 210618 05:14:59 models:110]\u001b[39m 10 524800 train loss: 0.0002951 valid loss: 0.0009058 P@1: 0.91559 P@3: 0.77325 P@5: 0.64083 N@3: 0.85607 N@5: 0.81543 early stop: 3\n","\u001b[32m[I 210618 05:18:14 models:110]\u001b[39m 10 550400 train loss: 0.0002992 valid loss: 0.0009066 P@1: 0.91562 P@3: 0.77323 P@5: 0.64087 N@3: 0.85604 N@5: 0.81543 early stop: 4\n","\u001b[32m[I 210618 05:21:30 models:110]\u001b[39m 11 11520 train loss: 0.0002634 valid loss: 0.0009077 P@1: 0.91560 P@3: 0.77317 P@5: 0.64093 N@3: 0.85601 N@5: 0.81549 early stop: 5\n","\u001b[32m[I 210618 05:24:47 models:110]\u001b[39m 11 37120 train loss: 0.0002148 valid loss: 0.0009089 P@1: 0.91567 P@3: 0.77319 P@5: 0.64094 N@3: 0.85602 N@5: 0.81549 early stop: 6\n","\u001b[32m[I 210618 05:28:07 models:110]\u001b[39m 11 62720 train loss: 0.0002138 valid loss: 0.0009101 P@1: 0.91584 P@3: 0.77312 P@5: 0.64094 N@3: 0.85600 N@5: 0.81552 early stop: 0\n","\u001b[32m[I 210618 05:31:25 models:110]\u001b[39m 11 88320 train loss: 0.0002180 valid loss: 0.0009112 P@1: 0.91576 P@3: 0.77315 P@5: 0.64101 N@3: 0.85602 N@5: 0.81555 early stop: 0\n","\u001b[32m[I 210618 05:34:44 models:110]\u001b[39m 11 113920 train loss: 0.0002237 valid loss: 0.0009124 P@1: 0.91573 P@3: 0.77319 P@5: 0.64108 N@3: 0.85603 N@5: 0.81560 early stop: 0\n","\u001b[32m[I 210618 05:38:01 models:110]\u001b[39m 11 139520 train loss: 0.0002240 valid loss: 0.0009136 P@1: 0.91570 P@3: 0.77319 P@5: 0.64109 N@3: 0.85601 N@5: 0.81558 early stop: 1\n","\u001b[32m[I 210618 05:41:14 models:110]\u001b[39m 11 165120 train loss: 0.0002317 valid loss: 0.0009147 P@1: 0.91570 P@3: 0.77319 P@5: 0.64105 N@3: 0.85599 N@5: 0.81554 early stop: 2\n","\u001b[32m[I 210618 05:44:28 models:110]\u001b[39m 11 190720 train loss: 0.0002337 valid loss: 0.0009159 P@1: 0.91560 P@3: 0.77316 P@5: 0.64112 N@3: 0.85595 N@5: 0.81556 early stop: 3\n","\u001b[32m[I 210618 05:47:42 models:110]\u001b[39m 11 216320 train loss: 0.0002372 valid loss: 0.0009170 P@1: 0.91574 P@3: 0.77309 P@5: 0.64109 N@3: 0.85592 N@5: 0.81555 early stop: 4\n","\u001b[32m[I 210618 05:50:56 models:110]\u001b[39m 11 241920 train loss: 0.0002379 valid loss: 0.0009182 P@1: 0.91560 P@3: 0.77306 P@5: 0.64112 N@3: 0.85585 N@5: 0.81553 early stop: 5\n","\u001b[32m[I 210618 05:54:11 models:110]\u001b[39m 11 267520 train loss: 0.0002439 valid loss: 0.0009192 P@1: 0.91567 P@3: 0.77303 P@5: 0.64112 N@3: 0.85584 N@5: 0.81553 early stop: 6\n","\u001b[32m[I 210618 05:57:25 models:110]\u001b[39m 11 293120 train loss: 0.0002449 valid loss: 0.0009204 P@1: 0.91553 P@3: 0.77302 P@5: 0.64114 N@3: 0.85581 N@5: 0.81552 early stop: 7\n","\u001b[32m[I 210618 06:00:40 models:110]\u001b[39m 11 318720 train loss: 0.0002490 valid loss: 0.0009214 P@1: 0.91550 P@3: 0.77297 P@5: 0.64108 N@3: 0.85576 N@5: 0.81547 early stop: 8\n","\u001b[32m[I 210618 06:03:54 models:110]\u001b[39m 11 344320 train loss: 0.0002502 valid loss: 0.0009224 P@1: 0.91545 P@3: 0.77288 P@5: 0.64114 N@3: 0.85569 N@5: 0.81549 early stop: 9\n","\u001b[32m[I 210618 06:07:08 models:110]\u001b[39m 11 369920 train loss: 0.0002489 valid loss: 0.0009235 P@1: 0.91547 P@3: 0.77287 P@5: 0.64116 N@3: 0.85570 N@5: 0.81551 early stop: 10\n","\u001b[32m[I 210618 06:10:22 models:110]\u001b[39m 11 395520 train loss: 0.0002535 valid loss: 0.0009245 P@1: 0.91547 P@3: 0.77289 P@5: 0.64116 N@3: 0.85569 N@5: 0.81550 early stop: 11\n","\u001b[32m[I 210618 06:13:36 models:110]\u001b[39m 11 421120 train loss: 0.0002548 valid loss: 0.0009254 P@1: 0.91545 P@3: 0.77285 P@5: 0.64116 N@3: 0.85564 N@5: 0.81550 early stop: 12\n","\u001b[32m[I 210618 06:16:50 models:110]\u001b[39m 11 446720 train loss: 0.0002584 valid loss: 0.0009264 P@1: 0.91537 P@3: 0.77282 P@5: 0.64117 N@3: 0.85561 N@5: 0.81549 early stop: 13\n","\u001b[32m[I 210618 06:20:05 models:110]\u001b[39m 11 472320 train loss: 0.0002642 valid loss: 0.0009273 P@1: 0.91546 P@3: 0.77285 P@5: 0.64120 N@3: 0.85565 N@5: 0.81552 early stop: 14\n","\u001b[32m[I 210618 06:23:19 models:110]\u001b[39m 11 497920 train loss: 0.0002593 valid loss: 0.0009282 P@1: 0.91543 P@3: 0.77285 P@5: 0.64119 N@3: 0.85564 N@5: 0.81550 early stop: 15\n","\u001b[32m[I 210618 06:26:37 models:110]\u001b[39m 11 523520 train loss: 0.0002641 valid loss: 0.0009291 P@1: 0.91542 P@3: 0.77284 P@5: 0.64123 N@3: 0.85563 N@5: 0.81554 early stop: 16\n","\u001b[32m[I 210618 06:29:53 models:110]\u001b[39m 11 549120 train loss: 0.0002661 valid loss: 0.0009301 P@1: 0.91539 P@3: 0.77283 P@5: 0.64128 N@3: 0.85562 N@5: 0.81557 early stop: 17\n","\u001b[32m[I 210618 06:33:11 models:110]\u001b[39m 12 10240 train loss: 0.0002382 valid loss: 0.0009312 P@1: 0.91535 P@3: 0.77287 P@5: 0.64129 N@3: 0.85563 N@5: 0.81557 early stop: 18\n","\u001b[32m[I 210618 06:36:28 models:110]\u001b[39m 12 35840 train loss: 0.0001891 valid loss: 0.0009323 P@1: 0.91539 P@3: 0.77285 P@5: 0.64133 N@3: 0.85564 N@5: 0.81559 early stop: 19\n","\u001b[32m[I 210618 06:39:42 models:110]\u001b[39m 12 61440 train loss: 0.0001937 valid loss: 0.0009334 P@1: 0.91540 P@3: 0.77285 P@5: 0.64131 N@3: 0.85563 N@5: 0.81558 early stop: 20\n","\u001b[32m[I 210618 06:43:00 models:110]\u001b[39m 12 87040 train loss: 0.0001939 valid loss: 0.0009347 P@1: 0.91554 P@3: 0.77286 P@5: 0.64133 N@3: 0.85567 N@5: 0.81562 early stop: 0\n","\u001b[32m[I 210618 06:46:18 models:110]\u001b[39m 12 112640 train loss: 0.0002008 valid loss: 0.0009359 P@1: 0.91545 P@3: 0.77287 P@5: 0.64132 N@3: 0.85566 N@5: 0.81559 early stop: 1\n","\u001b[32m[I 210618 06:49:35 models:110]\u001b[39m 12 138240 train loss: 0.0002039 valid loss: 0.0009370 P@1: 0.91547 P@3: 0.77281 P@5: 0.64130 N@3: 0.85562 N@5: 0.81557 early stop: 2\n","\u001b[32m[I 210618 06:52:53 models:110]\u001b[39m 12 163840 train loss: 0.0002051 valid loss: 0.0009382 P@1: 0.91563 P@3: 0.77280 P@5: 0.64130 N@3: 0.85564 N@5: 0.81559 early stop: 3\n","\u001b[32m[I 210618 06:56:10 models:110]\u001b[39m 12 189440 train loss: 0.0002087 valid loss: 0.0009394 P@1: 0.91557 P@3: 0.77278 P@5: 0.64129 N@3: 0.85560 N@5: 0.81554 early stop: 4\n","\u001b[32m[I 210618 06:59:30 models:110]\u001b[39m 12 215040 train loss: 0.0002097 valid loss: 0.0009405 P@1: 0.91557 P@3: 0.77278 P@5: 0.64129 N@3: 0.85559 N@5: 0.81554 early stop: 5\n","\u001b[32m[I 210618 07:02:47 models:110]\u001b[39m 12 240640 train loss: 0.0002147 valid loss: 0.0009417 P@1: 0.91563 P@3: 0.77276 P@5: 0.64129 N@3: 0.85559 N@5: 0.81555 early stop: 6\n","\u001b[32m[I 210618 07:06:04 models:110]\u001b[39m 12 266240 train loss: 0.0002148 valid loss: 0.0009429 P@1: 0.91557 P@3: 0.77280 P@5: 0.64131 N@3: 0.85562 N@5: 0.81556 early stop: 7\n","\u001b[32m[I 210618 07:09:20 models:110]\u001b[39m 12 291840 train loss: 0.0002177 valid loss: 0.0009441 P@1: 0.91552 P@3: 0.77279 P@5: 0.64123 N@3: 0.85559 N@5: 0.81549 early stop: 8\n","\u001b[32m[I 210618 07:12:36 models:110]\u001b[39m 12 317440 train loss: 0.0002202 valid loss: 0.0009452 P@1: 0.91550 P@3: 0.77278 P@5: 0.64128 N@3: 0.85557 N@5: 0.81550 early stop: 9\n","\u001b[32m[I 210618 07:15:53 models:110]\u001b[39m 12 343040 train loss: 0.0002239 valid loss: 0.0009463 P@1: 0.91559 P@3: 0.77277 P@5: 0.64128 N@3: 0.85558 N@5: 0.81553 early stop: 10\n","\u001b[32m[I 210618 07:19:08 models:110]\u001b[39m 12 368640 train loss: 0.0002263 valid loss: 0.0009474 P@1: 0.91566 P@3: 0.77281 P@5: 0.64126 N@3: 0.85561 N@5: 0.81552 early stop: 11\n","\u001b[32m[I 210618 07:22:24 models:110]\u001b[39m 12 394240 train loss: 0.0002290 valid loss: 0.0009484 P@1: 0.91554 P@3: 0.77290 P@5: 0.64127 N@3: 0.85566 N@5: 0.81553 early stop: 12\n","\u001b[32m[I 210618 07:25:39 models:110]\u001b[39m 12 419840 train loss: 0.0002313 valid loss: 0.0009495 P@1: 0.91559 P@3: 0.77291 P@5: 0.64128 N@3: 0.85569 N@5: 0.81555 early stop: 13\n","\u001b[32m[I 210618 07:28:55 models:110]\u001b[39m 12 445440 train loss: 0.0002345 valid loss: 0.0009506 P@1: 0.91552 P@3: 0.77291 P@5: 0.64125 N@3: 0.85568 N@5: 0.81551 early stop: 14\n","\u001b[32m[I 210618 07:32:11 models:110]\u001b[39m 12 471040 train loss: 0.0002312 valid loss: 0.0009517 P@1: 0.91547 P@3: 0.77295 P@5: 0.64124 N@3: 0.85570 N@5: 0.81551 early stop: 15\n","\u001b[32m[I 210618 07:35:26 models:110]\u001b[39m 12 496640 train loss: 0.0002367 valid loss: 0.0009527 P@1: 0.91553 P@3: 0.77297 P@5: 0.64129 N@3: 0.85572 N@5: 0.81554 early stop: 16\n","\u001b[32m[I 210618 07:38:43 models:110]\u001b[39m 12 522240 train loss: 0.0002391 valid loss: 0.0009538 P@1: 0.91549 P@3: 0.77292 P@5: 0.64132 N@3: 0.85566 N@5: 0.81554 early stop: 17\n","\u001b[32m[I 210618 07:41:59 models:110]\u001b[39m 12 547840 train loss: 0.0002406 valid loss: 0.0009547 P@1: 0.91549 P@3: 0.77289 P@5: 0.64134 N@3: 0.85563 N@5: 0.81554 early stop: 18\n","\u001b[32m[I 210618 07:45:20 models:110]\u001b[39m 13 8960 train loss: 0.0002209 valid loss: 0.0009557 P@1: 0.91562 P@3: 0.77295 P@5: 0.64139 N@3: 0.85569 N@5: 0.81559 early stop: 19\n","\u001b[32m[I 210618 07:48:38 models:110]\u001b[39m 13 34560 train loss: 0.0001721 valid loss: 0.0009569 P@1: 0.91563 P@3: 0.77284 P@5: 0.64138 N@3: 0.85562 N@5: 0.81559 early stop: 20\n","\u001b[32m[I 210618 07:51:57 models:110]\u001b[39m 13 60160 train loss: 0.0001749 valid loss: 0.0009582 P@1: 0.91564 P@3: 0.77275 P@5: 0.64141 N@3: 0.85556 N@5: 0.81559 early stop: 21\n","\u001b[32m[I 210618 07:55:19 models:110]\u001b[39m 13 85760 train loss: 0.0001785 valid loss: 0.0009593 P@1: 0.91570 P@3: 0.77270 P@5: 0.64139 N@3: 0.85554 N@5: 0.81560 early stop: 22\n","\u001b[32m[I 210618 07:58:38 models:110]\u001b[39m 13 111360 train loss: 0.0001786 valid loss: 0.0009605 P@1: 0.91574 P@3: 0.77276 P@5: 0.64144 N@3: 0.85560 N@5: 0.81566 early stop: 0\n","\u001b[32m[I 210618 08:01:56 models:110]\u001b[39m 13 136960 train loss: 0.0001828 valid loss: 0.0009617 P@1: 0.91570 P@3: 0.77279 P@5: 0.64143 N@3: 0.85559 N@5: 0.81563 early stop: 1\n","\u001b[32m[I 210618 08:05:15 models:110]\u001b[39m 13 162560 train loss: 0.0001861 valid loss: 0.0009629 P@1: 0.91574 P@3: 0.77274 P@5: 0.64145 N@3: 0.85557 N@5: 0.81563 early stop: 2\n","\u001b[32m[I 210618 08:08:34 models:110]\u001b[39m 13 188160 train loss: 0.0001886 valid loss: 0.0009640 P@1: 0.91574 P@3: 0.77277 P@5: 0.64149 N@3: 0.85559 N@5: 0.81566 early stop: 0\n","\u001b[32m[I 210618 08:11:53 models:110]\u001b[39m 13 213760 train loss: 0.0001933 valid loss: 0.0009651 P@1: 0.91567 P@3: 0.77278 P@5: 0.64151 N@3: 0.85557 N@5: 0.81566 early stop: 1\n","\u001b[32m[I 210618 08:15:11 models:110]\u001b[39m 13 239360 train loss: 0.0001952 valid loss: 0.0009662 P@1: 0.91554 P@3: 0.77279 P@5: 0.64149 N@3: 0.85555 N@5: 0.81562 early stop: 2\n","\u001b[32m[I 210618 08:18:28 models:110]\u001b[39m 13 264960 train loss: 0.0001957 valid loss: 0.0009673 P@1: 0.91549 P@3: 0.77278 P@5: 0.64148 N@3: 0.85554 N@5: 0.81561 early stop: 3\n","\u001b[32m[I 210618 08:21:45 models:110]\u001b[39m 13 290560 train loss: 0.0001962 valid loss: 0.0009685 P@1: 0.91554 P@3: 0.77265 P@5: 0.64152 N@3: 0.85546 N@5: 0.81563 early stop: 4\n","\u001b[32m[I 210618 08:25:02 models:110]\u001b[39m 13 316160 train loss: 0.0002003 valid loss: 0.0009696 P@1: 0.91549 P@3: 0.77263 P@5: 0.64155 N@3: 0.85543 N@5: 0.81564 early stop: 5\n","\u001b[32m[I 210618 08:28:20 models:110]\u001b[39m 13 341760 train loss: 0.0002014 valid loss: 0.0009708 P@1: 0.91540 P@3: 0.77261 P@5: 0.64152 N@3: 0.85539 N@5: 0.81559 early stop: 6\n","\u001b[32m[I 210618 08:31:36 models:110]\u001b[39m 13 367360 train loss: 0.0002045 valid loss: 0.0009719 P@1: 0.91545 P@3: 0.77261 P@5: 0.64152 N@3: 0.85541 N@5: 0.81561 early stop: 7\n","\u001b[32m[I 210618 08:34:55 models:110]\u001b[39m 13 392960 train loss: 0.0002085 valid loss: 0.0009730 P@1: 0.91547 P@3: 0.77264 P@5: 0.64151 N@3: 0.85544 N@5: 0.81560 early stop: 8\n","\u001b[32m[I 210618 08:38:13 models:110]\u001b[39m 13 418560 train loss: 0.0002088 valid loss: 0.0009742 P@1: 0.91549 P@3: 0.77263 P@5: 0.64156 N@3: 0.85542 N@5: 0.81562 early stop: 9\n","\u001b[32m[I 210618 08:41:29 models:110]\u001b[39m 13 444160 train loss: 0.0002106 valid loss: 0.0009752 P@1: 0.91560 P@3: 0.77262 P@5: 0.64151 N@3: 0.85544 N@5: 0.81560 early stop: 10\n","\u001b[32m[I 210618 08:44:46 models:110]\u001b[39m 13 469760 train loss: 0.0002152 valid loss: 0.0009762 P@1: 0.91545 P@3: 0.77264 P@5: 0.64157 N@3: 0.85542 N@5: 0.81560 early stop: 11\n","\u001b[32m[I 210618 08:48:03 models:110]\u001b[39m 13 495360 train loss: 0.0002146 valid loss: 0.0009773 P@1: 0.91550 P@3: 0.77263 P@5: 0.64151 N@3: 0.85543 N@5: 0.81557 early stop: 12\n","\u001b[32m[I 210618 08:51:19 models:110]\u001b[39m 13 520960 train loss: 0.0002186 valid loss: 0.0009782 P@1: 0.91547 P@3: 0.77263 P@5: 0.64152 N@3: 0.85543 N@5: 0.81558 early stop: 13\n","\u001b[32m[I 210618 08:54:36 models:110]\u001b[39m 13 546560 train loss: 0.0002188 valid loss: 0.0009792 P@1: 0.91547 P@3: 0.77260 P@5: 0.64153 N@3: 0.85541 N@5: 0.81559 early stop: 14\n","\u001b[32m[I 210618 08:57:53 models:110]\u001b[39m 14 7680 train loss: 0.0002018 valid loss: 0.0009803 P@1: 0.91543 P@3: 0.77264 P@5: 0.64156 N@3: 0.85543 N@5: 0.81560 early stop: 15\n","\u001b[32m[I 210618 09:01:10 models:110]\u001b[39m 14 33280 train loss: 0.0001563 valid loss: 0.0009815 P@1: 0.91545 P@3: 0.77264 P@5: 0.64159 N@3: 0.85543 N@5: 0.81562 early stop: 16\n","\u001b[32m[I 210618 09:04:25 models:110]\u001b[39m 14 58880 train loss: 0.0001598 valid loss: 0.0009827 P@1: 0.91549 P@3: 0.77274 P@5: 0.64163 N@3: 0.85552 N@5: 0.81566 early stop: 17\n","\u001b[32m[I 210618 09:07:40 models:110]\u001b[39m 14 84480 train loss: 0.0001618 valid loss: 0.0009839 P@1: 0.91547 P@3: 0.77270 P@5: 0.64155 N@3: 0.85548 N@5: 0.81560 early stop: 18\n","\u001b[32m[I 210618 09:10:58 models:110]\u001b[39m 14 110080 train loss: 0.0001652 valid loss: 0.0009852 P@1: 0.91546 P@3: 0.77272 P@5: 0.64156 N@3: 0.85551 N@5: 0.81561 early stop: 19\n","\u001b[32m[I 210618 09:14:14 models:110]\u001b[39m 14 135680 train loss: 0.0001675 valid loss: 0.0009864 P@1: 0.91540 P@3: 0.77278 P@5: 0.64159 N@3: 0.85554 N@5: 0.81562 early stop: 20\n","\u001b[32m[I 210618 09:17:30 models:110]\u001b[39m 14 161280 train loss: 0.0001730 valid loss: 0.0009876 P@1: 0.91535 P@3: 0.77278 P@5: 0.64153 N@3: 0.85552 N@5: 0.81558 early stop: 21\n","\u001b[32m[I 210618 09:20:45 models:110]\u001b[39m 14 186880 train loss: 0.0001729 valid loss: 0.0009888 P@1: 0.91542 P@3: 0.77275 P@5: 0.64156 N@3: 0.85551 N@5: 0.81560 early stop: 22\n","\u001b[32m[I 210618 09:24:01 models:110]\u001b[39m 14 212480 train loss: 0.0001759 valid loss: 0.0009900 P@1: 0.91543 P@3: 0.77275 P@5: 0.64152 N@3: 0.85549 N@5: 0.81556 early stop: 23\n","\u001b[32m[I 210618 09:27:18 models:110]\u001b[39m 14 238080 train loss: 0.0001783 valid loss: 0.0009912 P@1: 0.91547 P@3: 0.77277 P@5: 0.64155 N@3: 0.85552 N@5: 0.81558 early stop: 24\n","\u001b[32m[I 210618 09:30:34 models:110]\u001b[39m 14 263680 train loss: 0.0001798 valid loss: 0.0009923 P@1: 0.91545 P@3: 0.77273 P@5: 0.64153 N@3: 0.85550 N@5: 0.81557 early stop: 25\n","\u001b[32m[I 210618 09:33:52 models:110]\u001b[39m 14 289280 train loss: 0.0001833 valid loss: 0.0009934 P@1: 0.91543 P@3: 0.77274 P@5: 0.64155 N@3: 0.85550 N@5: 0.81558 early stop: 26\n","\u001b[32m[I 210618 09:37:10 models:110]\u001b[39m 14 314880 train loss: 0.0001864 valid loss: 0.0009946 P@1: 0.91543 P@3: 0.77271 P@5: 0.64150 N@3: 0.85547 N@5: 0.81554 early stop: 27\n","\u001b[32m[I 210618 09:40:28 models:110]\u001b[39m 14 340480 train loss: 0.0001847 valid loss: 0.0009957 P@1: 0.91539 P@3: 0.77269 P@5: 0.64153 N@3: 0.85543 N@5: 0.81554 early stop: 28\n","\u001b[32m[I 210618 09:43:44 models:110]\u001b[39m 14 366080 train loss: 0.0001861 valid loss: 0.0009968 P@1: 0.91542 P@3: 0.77264 P@5: 0.64157 N@3: 0.85541 N@5: 0.81557 early stop: 29\n","\u001b[32m[I 210618 09:47:00 models:110]\u001b[39m 14 391680 train loss: 0.0001915 valid loss: 0.0009979 P@1: 0.91537 P@3: 0.77261 P@5: 0.64155 N@3: 0.85537 N@5: 0.81553 early stop: 30\n","\u001b[32m[I 210618 09:50:17 models:110]\u001b[39m 14 417280 train loss: 0.0001923 valid loss: 0.0009991 P@1: 0.91527 P@3: 0.77263 P@5: 0.64152 N@3: 0.85536 N@5: 0.81548 early stop: 31\n","\u001b[32m[I 210618 09:53:35 models:110]\u001b[39m 14 442880 train loss: 0.0001932 valid loss: 0.0010001 P@1: 0.91530 P@3: 0.77255 P@5: 0.64153 N@3: 0.85531 N@5: 0.81549 early stop: 32\n","\u001b[32m[I 210618 09:56:51 models:110]\u001b[39m 14 468480 train loss: 0.0001938 valid loss: 0.0010012 P@1: 0.91527 P@3: 0.77250 P@5: 0.64154 N@3: 0.85525 N@5: 0.81548 early stop: 33\n","\u001b[32m[I 210618 10:00:07 models:110]\u001b[39m 14 494080 train loss: 0.0001954 valid loss: 0.0010022 P@1: 0.91527 P@3: 0.77252 P@5: 0.64154 N@3: 0.85526 N@5: 0.81548 early stop: 34\n","\u001b[32m[I 210618 10:03:23 models:110]\u001b[39m 14 519680 train loss: 0.0001962 valid loss: 0.0010032 P@1: 0.91530 P@3: 0.77251 P@5: 0.64153 N@3: 0.85526 N@5: 0.81548 early stop: 35\n","\u001b[32m[I 210618 10:06:39 models:110]\u001b[39m 14 545280 train loss: 0.0002007 valid loss: 0.0010042 P@1: 0.91522 P@3: 0.77254 P@5: 0.64155 N@3: 0.85528 N@5: 0.81548 early stop: 36\n","\u001b[32m[I 210618 10:08:46 main:76]\u001b[39m Finish Training\n","\u001b[32m[I 210618 10:08:57 main:32]\u001b[39m Model Name: MATCH\n","\u001b[32m[I 210618 10:08:57 main:79]\u001b[39m Loading Test Set\n","\u001b[32m[I 210618 10:09:01 main:83]\u001b[39m Size of Test Set: 70533\n","\u001b[32m[I 210618 10:09:01 main:85]\u001b[39m Predicting\n","\u001b[32m[I 210618 10:09:30 main:91]\u001b[39m Finish Predicting\n","Precision@1,3,5: 0.9148909021309174 0.7706416381929971 0.6392610551089561\n","nDCG@1,3,5: 0.9148909021309174 0.8551977454602886 0.8154306415429102\n"],"name":"stdout"}]}]}